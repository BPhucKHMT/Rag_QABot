{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e90eda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.4 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.4 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
      "s3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2025.10.0 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.4 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.4 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.4 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
      "s3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2025.10.0 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.4 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "torchaudio 2.7.1 requires torch==2.7.1, but you have torch 2.9.0 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.4 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.4 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
      "s3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2025.10.0 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.4 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "torchaudio 2.7.1 requires torch==2.7.1, but you have torch 2.9.0 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.4 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.4 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
      "s3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2025.10.0 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.4 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "tokenizers 0.21.4 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 1.0.1 which is incompatible.\n",
      "torchaudio 2.7.1 requires torch==2.7.1, but you have torch 2.9.0 which is incompatible.\n",
      "transformers 4.52.4 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.1 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.4 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.4 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
      "s3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2025.10.0 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.4 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "tokenizers 0.21.4 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 1.0.1 which is incompatible.\n",
      "torchaudio 2.7.1 requires torch==2.7.1, but you have torch 2.9.0 which is incompatible.\n",
      "transformers 4.52.4 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.1 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires tenacity<9,>=8.1.0, but you have tenacity 9.1.2 which is incompatible.\n",
      "tokenizers 0.21.4 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 1.0.1 which is incompatible.\n",
      "torchaudio 2.7.1 requires torch==2.7.1, but you have torch 2.9.0 which is incompatible.\n",
      "transformers 4.52.4 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.1 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires tenacity<9,>=8.1.0, but you have tenacity 9.1.2 which is incompatible.\n",
      "tokenizers 0.21.4 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 1.0.1 which is incompatible.\n",
      "torchaudio 2.7.1 requires torch==2.7.1, but you have torch 2.9.0 which is incompatible.\n",
      "transformers 4.52.4 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.1 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
      "streamlit 1.37.1 requires tenacity<9,>=8.1.0, but you have tenacity 9.1.2 which is incompatible.\n",
      "tokenizers 0.21.4 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 1.0.1 which is incompatible.\n",
      "transformers 4.52.4 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.1 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
      "streamlit 1.37.1 requires tenacity<9,>=8.1.0, but you have tenacity 9.1.2 which is incompatible.\n",
      "tokenizers 0.21.4 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 1.0.1 which is incompatible.\n",
      "transformers 4.52.4 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.1 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.4 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.4 which is incompatible.\n",
      "langchain 0.3.25 requires langchain-core<1.0.0,>=0.3.58, but you have langchain-core 1.0.3 which is incompatible.\n",
      "langchain 0.3.25 requires langsmith<0.4,>=0.1.17, but you have langsmith 0.4.40 which is incompatible.\n",
      "langchainhub 0.1.21 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "mdit-py-plugins 0.3.0 requires markdown-it-py<3.0.0,>=1.0.0, but you have markdown-it-py 4.0.0 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
      "s3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2025.10.0 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.4 which is incompatible.\n",
      "streamlit 1.37.1 requires cachetools<6,>=4.0, but you have cachetools 6.2.1 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires protobuf<6,>=3.20, but you have protobuf 6.33.0 which is incompatible.\n",
      "streamlit 1.37.1 requires rich<14,>=10.14.0, but you have rich 14.2.0 which is incompatible.\n",
      "streamlit 1.37.1 requires tenacity<9,>=8.1.0, but you have tenacity 9.1.2 which is incompatible.\n",
      "torchaudio 2.7.1 requires torch==2.7.1, but you have torch 2.9.0 which is incompatible.\n",
      "transformers 4.52.4 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.1 which is incompatible.\n",
      "transformers 4.52.4 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.22.1 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.4 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.4 which is incompatible.\n",
      "langchain 0.3.25 requires langchain-core<1.0.0,>=0.3.58, but you have langchain-core 1.0.3 which is incompatible.\n",
      "langchain 0.3.25 requires langsmith<0.4,>=0.1.17, but you have langsmith 0.4.40 which is incompatible.\n",
      "langchainhub 0.1.21 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "mdit-py-plugins 0.3.0 requires markdown-it-py<3.0.0,>=1.0.0, but you have markdown-it-py 4.0.0 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
      "s3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2025.10.0 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.4 which is incompatible.\n",
      "streamlit 1.37.1 requires cachetools<6,>=4.0, but you have cachetools 6.2.1 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires protobuf<6,>=3.20, but you have protobuf 6.33.0 which is incompatible.\n",
      "streamlit 1.37.1 requires rich<14,>=10.14.0, but you have rich 14.2.0 which is incompatible.\n",
      "streamlit 1.37.1 requires tenacity<9,>=8.1.0, but you have tenacity 9.1.2 which is incompatible.\n",
      "torchaudio 2.7.1 requires torch==2.7.1, but you have torch 2.9.0 which is incompatible.\n",
      "transformers 4.52.4 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.1 which is incompatible.\n",
      "transformers 4.52.4 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.22.1 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.4 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.4 which is incompatible.\n",
      "kubernetes 34.1.0 requires urllib3<2.4.0,>=1.24.2, but you have urllib3 2.5.0 which is incompatible.\n",
      "langchainhub 0.1.21 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
      "s3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2025.10.0 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.4 which is incompatible.\n",
      "streamlit 1.37.1 requires cachetools<6,>=4.0, but you have cachetools 6.2.1 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires protobuf<6,>=3.20, but you have protobuf 6.33.0 which is incompatible.\n",
      "streamlit 1.37.1 requires rich<14,>=10.14.0, but you have rich 14.2.0 which is incompatible.\n",
      "streamlit 1.37.1 requires tenacity<9,>=8.1.0, but you have tenacity 9.1.2 which is incompatible.\n",
      "torchaudio 2.7.1 requires torch==2.7.1, but you have torch 2.9.0 which is incompatible.\n",
      "transformers 4.52.4 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.1 which is incompatible.\n",
      "transformers 4.52.4 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.22.1 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.4 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.4 which is incompatible.\n",
      "kubernetes 34.1.0 requires urllib3<2.4.0,>=1.24.2, but you have urllib3 2.5.0 which is incompatible.\n",
      "langchainhub 0.1.21 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
      "s3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2025.10.0 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.4 which is incompatible.\n",
      "streamlit 1.37.1 requires cachetools<6,>=4.0, but you have cachetools 6.2.1 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires protobuf<6,>=3.20, but you have protobuf 6.33.0 which is incompatible.\n",
      "streamlit 1.37.1 requires rich<14,>=10.14.0, but you have rich 14.2.0 which is incompatible.\n",
      "streamlit 1.37.1 requires tenacity<9,>=8.1.0, but you have tenacity 9.1.2 which is incompatible.\n",
      "torchaudio 2.7.1 requires torch==2.7.1, but you have torch 2.9.0 which is incompatible.\n",
      "transformers 4.52.4 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.1 which is incompatible.\n",
      "transformers 4.52.4 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.22.1 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.4 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.4 which is incompatible.\n",
      "kubernetes 34.1.0 requires urllib3<2.4.0,>=1.24.2, but you have urllib3 2.5.0 which is incompatible.\n",
      "langchainhub 0.1.21 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
      "s3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2025.10.0 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.4 which is incompatible.\n",
      "streamlit 1.37.1 requires cachetools<6,>=4.0, but you have cachetools 6.2.1 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires protobuf<6,>=3.20, but you have protobuf 6.33.0 which is incompatible.\n",
      "streamlit 1.37.1 requires rich<14,>=10.14.0, but you have rich 14.2.0 which is incompatible.\n",
      "streamlit 1.37.1 requires tenacity<9,>=8.1.0, but you have tenacity 9.1.2 which is incompatible.\n",
      "torchaudio 2.7.1 requires torch==2.7.1, but you have torch 2.9.0 which is incompatible.\n",
      "transformers 4.52.4 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.1 which is incompatible.\n",
      "transformers 4.52.4 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.22.1 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.4 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.4 which is incompatible.\n",
      "kubernetes 34.1.0 requires urllib3<2.4.0,>=1.24.2, but you have urllib3 2.5.0 which is incompatible.\n",
      "langchainhub 0.1.21 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
      "s3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2025.10.0 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.4 which is incompatible.\n",
      "streamlit 1.37.1 requires cachetools<6,>=4.0, but you have cachetools 6.2.1 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires protobuf<6,>=3.20, but you have protobuf 6.33.0 which is incompatible.\n",
      "streamlit 1.37.1 requires rich<14,>=10.14.0, but you have rich 14.2.0 which is incompatible.\n",
      "streamlit 1.37.1 requires tenacity<9,>=8.1.0, but you have tenacity 9.1.2 which is incompatible.\n",
      "torchaudio 2.7.1 requires torch==2.7.1, but you have torch 2.9.0 which is incompatible.\n",
      "transformers 4.52.4 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.1 which is incompatible.\n",
      "transformers 4.52.4 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.22.1 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.4 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.4 which is incompatible.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.16.3 which is incompatible.\n",
      "kubernetes 34.1.0 requires urllib3<2.4.0,>=1.24.2, but you have urllib3 2.5.0 which is incompatible.\n",
      "langchainhub 0.1.21 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "langchain-community 0.3.24 requires langsmith<0.4,>=0.1.125, but you have langsmith 0.4.40 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
      "s3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2025.10.0 which is incompatible.\n",
      "streamlit 1.37.1 requires cachetools<6,>=4.0, but you have cachetools 6.2.1 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires pillow<11,>=7.1.0, but you have pillow 12.0.0 which is incompatible.\n",
      "streamlit 1.37.1 requires protobuf<6,>=3.20, but you have protobuf 6.33.0 which is incompatible.\n",
      "streamlit 1.37.1 requires rich<14,>=10.14.0, but you have rich 14.2.0 which is incompatible.\n",
      "streamlit 1.37.1 requires tenacity<9,>=8.1.0, but you have tenacity 9.1.2 which is incompatible.\n",
      "torchaudio 2.7.1 requires torch==2.7.1, but you have torch 2.9.0 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.4 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.4 which is incompatible.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.16.3 which is incompatible.\n",
      "kubernetes 34.1.0 requires urllib3<2.4.0,>=1.24.2, but you have urllib3 2.5.0 which is incompatible.\n",
      "langchainhub 0.1.21 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "langchain-community 0.3.24 requires langsmith<0.4,>=0.1.125, but you have langsmith 0.4.40 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
      "s3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2025.10.0 which is incompatible.\n",
      "streamlit 1.37.1 requires cachetools<6,>=4.0, but you have cachetools 6.2.1 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires pillow<11,>=7.1.0, but you have pillow 12.0.0 which is incompatible.\n",
      "streamlit 1.37.1 requires protobuf<6,>=3.20, but you have protobuf 6.33.0 which is incompatible.\n",
      "streamlit 1.37.1 requires rich<14,>=10.14.0, but you have rich 14.2.0 which is incompatible.\n",
      "streamlit 1.37.1 requires tenacity<9,>=8.1.0, but you have tenacity 9.1.2 which is incompatible.\n",
      "torchaudio 2.7.1 requires torch==2.7.1, but you have torch 2.9.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --force transformers==4.52.4\n",
    "!pip install -U bitsandbytes\n",
    "!pip install -q --force accelerate==1.7.0\n",
    "!pip install -q --force langchain==0.3.25\n",
    "!pip install -q --force langchainhub==0.1.21\n",
    "!pip install -q --force langchain-chroma==0.2.4\n",
    "!pip install -q --force langchain_experimental==0.3.4\n",
    "!pip install -q --force langchain-community==0.3.24\n",
    "!pip install -q --force langchain_huggingface==0.2.0\n",
    "!pip install -q --force python-dotenv==1.1.0\n",
    "!pip install -q --force pypdf\n",
    "!pip install langchain_openai\n",
    "!pip install langchain-google-genai\n",
    "!pip install rank_bm25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f10335a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env in project root\n",
    "load_dotenv()\n",
    "\n",
    "googleAPIKey = os.getenv('googleAPIKey')\n",
    "gptkey =  os.getenv('myAPIKey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f489afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoModelForSequenceClassification,AutoTokenizer, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain import hub\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f13b12",
   "metadata": {},
   "source": [
    "## Load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fa42b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dir = \"C:\\\\uit_HK5\\\\CS431\\\\final_project\\\\data\\\\metadata.json\"\n",
    "transcript_dir = \"C:\\\\uit_HK5\\\\CS431\\\\final_project\\\\data\\\\transcripts_final\"\n",
    "output_dir = \"C:\\\\uit_HK5\\\\CS431\\\\final_project\\\\data\\\\semantic_chunks\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3d51e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "class Loader:\n",
    "    @staticmethod\n",
    "    def parse_transcript(file_path: str) -> tuple[str, list[dict], str]:\n",
    "        \"\"\"Đọc file transcript, tách từng dòng thành block có start-end-text\"\"\"\n",
    "        full_text = \"\"\n",
    "        position_map = []  # lưu vị trí start của mỗi đoạn text trong full_text\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line or \"[âm nhạc]\" in line.lower():\n",
    "                    continue\n",
    "                filename = os.path.basename(file_path).replace(\".txt\", \"\") # lấy file name\n",
    "                match = re.match(r\"(\\d+:\\d+:\\d+)\\s*-\\s*(\\d+:\\d+:\\d+),\\s*(.+)\", line)\n",
    "                if match:\n",
    "                    start, end, text = match.groups()\n",
    "                    pos = len(full_text)\n",
    "                    full_text += text + \" \"\n",
    "                    position_map.append({\n",
    "                        \"start\": start,\n",
    "                        \"end\": end,\n",
    "                        \"text\": text,\n",
    "                        \"pos_start\": pos, # vị trí bắt đầu của đoạn text trong full_text\n",
    "                        \"pos_end\": len(full_text) # vị trí kết thúc của đoạn text trong full_text\n",
    "                    })\n",
    "\n",
    "        return full_text.strip(), position_map, filename\n",
    "    \n",
    "    def map_metadata(self, metadata_path: str, filename: str) -> tuple[Union[str, None], Union[str, None]]:\n",
    "        \"\"\"Đọc file metadata và trả về dict mapping id -> metadata\"\"\"\n",
    "\n",
    "        with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            metadata_list = json.load(f)\n",
    "        metadata = metadata_list[\"videos\"]\n",
    "\n",
    "        video_title, video_url = next(((item[\"title\"], item[\"url\"]) for item in metadata if item[\"video_id\"] == filename), (None, None))\n",
    "\n",
    "        return video_title, video_url\n",
    "    \n",
    "    def load_dir(self, transcript_dir: str, metadata_path: str) -> list[dict]:\n",
    "        \"\"\"Đọc tất cả file transcript trong thư mục và trả về danh sách dict chứa full_text, position_map, filename, title, url\"\"\"\n",
    "        import glob\n",
    "\n",
    "        file_paths = glob.glob(os.path.join(transcript_dir, \"*.txt\"))\n",
    "        data = []\n",
    "\n",
    "        for file_path in file_paths:\n",
    "            full_text, position_map, filename = self.parse_transcript(file_path)\n",
    "            title, url = self.map_metadata(metadata_path, filename)\n",
    "\n",
    "            data.append({\n",
    "                \"full_text\": full_text,\n",
    "                \"position_map\": position_map,\n",
    "                \"filename\": filename,\n",
    "                \"title\": title,\n",
    "                \"url\": url\n",
    "            })\n",
    "\n",
    "        return data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3959c9a9",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "7573b359",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec452405",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyparsing import line\n",
    "\n",
    "\n",
    "class TranscriptChunker:\n",
    "    def __init__(self, open_api_key: str):\n",
    "        self.embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-large\",\n",
    "            openai_api_key=open_api_key\n",
    "        )\n",
    "        self.splitter = SemanticChunker(\n",
    "            embeddings=self.embeddings,\n",
    "            breakpoint_threshold_type=\"percentile\",\n",
    "            breakpoint_threshold_amount=85,\n",
    "            min_chunk_size=300,\n",
    "\n",
    "            add_start_index=True,\n",
    "            buffer_size=1\n",
    "        )\n",
    "        self.loader = Loader()\n",
    "\n",
    "    def chunk_dir(self, transcript_dir: str, metadata_path: str, output_dir: str) -> list:\n",
    "        data = self.loader.load_dir(transcript_dir, metadata_path)\n",
    "        all_chunks = []\n",
    "\n",
    "        for item in data:\n",
    "            full_text = item[\"full_text\"]\n",
    "            position_map = item[\"position_map\"]\n",
    "            filename = item[\"filename\"]\n",
    "            title = item[\"title\"]\n",
    "            url = item[\"url\"]\n",
    "\n",
    "            # dùng lại logic mapping timestamp\n",
    "            chunks = self.splitter.create_documents(\n",
    "                texts=[full_text],\n",
    "                metadatas=[{\n",
    "                    \"video_url\": url,\n",
    "                    \"filename\": filename,\n",
    "                    \"title\": title\n",
    "                }]\n",
    "            )\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                start_index = chunk.metadata.pop(\"start_index\")\n",
    "                end_index = start_index + len(chunk.page_content)  # tự tính end_index\n",
    "\n",
    "                \n",
    "                # tìm timestamp đầu tiên bao phủ đoạn text này\n",
    "                matched_ts = [\n",
    "                    pos for pos in position_map\n",
    "                    if not (pos[\"pos_end\"] < start_index or pos[\"pos_start\"] > end_index)\n",
    "                ]\n",
    "\n",
    "                if matched_ts:\n",
    "                    chunk.metadata[\"start_timestamp\"] = matched_ts[0][\"start\"]\n",
    "                    chunk.metadata[\"end_timestamp\"] = matched_ts[-1][\"end\"]\n",
    "                else:\n",
    "                    chunk.metadata[\"start_timestamp\"] = None\n",
    "                    chunk.metadata[\"end_timestamp\"] = None\n",
    "\n",
    "                chunk.metadata[\"chunk_id\"] = i\n",
    "            all_chunks.extend(chunks)\n",
    "        # lưu tất cả chunks vào file json\n",
    "        output_path = os.path.join(output_dir, \"semantic_chunks.json\")\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump([{\n",
    "                \"page_content\": chunk.page_content,\n",
    "                \"metadata\": chunk.metadata\n",
    "            } for chunk in all_chunks], f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Saved {len(all_chunks)} chunks to {output_path}\")\n",
    "        return all_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "990fd650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 461 chunks to C:\\uit_HK5\\CS431\\final_project\\data\\semantic_chunks\\semantic_chunks.json\n"
     ]
    }
   ],
   "source": [
    "splitter = TranscriptChunker(\n",
    "    open_api_key= gptkey\n",
    ")\n",
    "data = splitter.chunk_dir(transcript_dir, metadata_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60d6dde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Như vậy thì trong phần số 2 này thì chúng ta đã cùng tìm hiểu về những chủ đề sau. Đầu tiên là chúng ta tìm hiểu về maximum likelihood cho cái log của PX. Chúng ta mong muốn có được một mô hình để tạo ra một cái ảnh x giống thật, giống với lại cái Pdata. Thế thì để đạt được cái việc này thì cái likelihood của log P này phải là lớn nhất. Và khi đưa cái log của PX này lên cực đại thì nó sẽ đưa đến một cái giải pháp, đó là chúng ta sẽ đẩy cái chặn dưới của log P. Thì đó chính là cái ELBO là evidence lower bound.\n",
      "0:00:14 0:01:06\n",
      "https://youtube.com/watch?v=--6FInuIyys\n",
      "[CS315 - Chương 3] Deep Generative Models (2) - Tổng kết\n",
      "-----\n",
      "Đẩy cái ELBO này lên, maximum ELBO này lên. Và khi chúng ta maximum ELBO này lên thì chúng ta sẽ có hai cái mô hình, đó là VAE và mô hình diffusion. Và đối với cái mô hình diffusion thì chúng ta sẽ có cái bước gọi là khuếch tán thuận. Và trong cái khuếch tán thuận này thì chúng ta sẽ thêm nhiễu vào cái ảnh của mình. Và ở đây là chúng ta không có tham số để học, không có tham số huấn luyện. Cái điều này nó giúp cho chúng ta đơn giản hóa cái việc huấn luyện của cái mô hình diffusion mà chỉ dành cái dư địa để huấn luyện cho cái phần denoising, phần khử nhiễu. Chúng ta chỉ học khử nhiễu và học bằng cả ba cách. Cách đầu tiên đó là chúng ta sẽ tối ưu để sao cho cái x mũ theta xấp xỉ với lại x0. Cách thứ hai đó là chúng ta tối ưu cái epsilon theta sao cho xấp xỉ với lại cái epsilon. Và cách số ba đó là chúng ta sẽ tối ưu để cho cái x mũ theta xấp xỉ với lại cái gradient của log p theta. Thì đây giống như là cái hướng để khử nhiễu của mình.\n",
      "0:00:58 0:02:29\n",
      "https://youtube.com/watch?v=--6FInuIyys\n",
      "[CS315 - Chương 3] Deep Generative Models (2) - Tổng kết\n",
      "-----\n",
      "Thì đây là ba cái cách. Và sau đó thì chúng ta đã tìm hiểu về cách để điều hướng với hai kỹ thuật đó là classifier guidance. Với mỗi một cái condition mới, thì một cái condition chúng ta sẽ ra một cái classifier. Như vậy thì nó sẽ không có linh động trong cái việc là update hoặc là thay cái condition. Chúng ta sẽ có kỹ thuật khác cải tiến đó chính là classifier free guidance. Tức là chúng ta sẽ bỏ luôn cái classifier này mà chúng ta chỉ đi fine-tune lại cái mô hình diffusion. Chỉ fine-tune lại diffusion, không có dùng thêm cái classifier nào để cho nó có thể là train được từ đầu, fine-tune từ đầu đến cuối. Sau đó chúng ta đã nói qua những cái vấn đề về độ phân giải khi chúng ta làm việc với latent, xin lỗi khi làm việc với diffusion. Và cái kỹ thuật mà cascade diffusion thì nó rất là cồng kềnh. Vì nó phải sử dụng đến hai ba cái mô hình nối tiếp nhau và độc lập nhau. Nó không có end to end, tức là kết nối từ đầu đến cuối. Do đó thì chúng ta có cái mô hình latent diffusion và mở ra một cái hướng nó gọi là end to end diffusion. Thì latent diffusion có thể nói là một trong những cái mô hình mà cho cái impact rất là lớn trong cộng đồng nghiên cứu. Là vì nó đã giúp cho chúng ta tính toán nhanh, rồi cái mô hình của mình đạt được cái độ phân giải rất là cao, chất lượng rất là tốt.\n",
      "0:02:26 0:04:16\n",
      "https://youtube.com/watch?v=--6FInuIyys\n",
      "[CS315 - Chương 3] Deep Generative Models (2) - Tổng kết\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "#test print first 3 chunks\n",
    "for chunk in data[:3]:\n",
    "    print(chunk.page_content)\n",
    "    print(chunk.metadata[\"start_timestamp\"], chunk.metadata[\"end_timestamp\"])\n",
    "    print(chunk.metadata[\"video_url\"])\n",
    "    print(chunk.metadata[\"title\"])\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5e8e6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chúng ta sẽ cùng đến với các mô hình tạo sinh học sau Deep Generated Model phần 2, mô hình Diffusion. Các mô hình tạo sinh hình ảnh đều có gốc gác sử dụng mô hình phát tán, mô hình Diffusion Model. Đây có thể nói là một trong những mô hình có tính ứng dụng rất cao do tạo ra những ảnh có độ phân giải cao, đồng thời có thể cho chúng ta can thiệp và điều hướng nội dung của tấm ảnh. Vậy thì ý tưởng của Diffusion là gì và cách thức huấn luyện ra sao, chúng ta sẽ cùng tìm hiểu trong bài ngày hôm nay. Các vấn đề chính khi chúng ta tìm hiểu một mô hình Diffusion Model, mô hình phát tán, đó là chúng ta sẽ tìm hiểu về mô hình tạo sinh tổng quát. Mô hình tạo sinh tổng quát này sẽ dựa trên lý thuyết về xác suất thống kê.\n"
     ]
    }
   ],
   "source": [
    "result = retriever.invoke(\"diffusion là gì\")\n",
    "\n",
    "print(result[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6704daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# 🔹 Lấy toàn bộ embedding và metadata từ Chroma\n",
    "# (nếu bạn đã load vector_db = Chroma.from_documents(...) như trên)\n",
    "data = vector_db.get()  \n",
    "\n",
    "vectors = vector_db.get(include=[\"embeddings\"])\n",
    "vectors = np.array(vectors[\"embeddings\"])\n",
    "documents = data[\"documents\"]\n",
    "metadatas = data[\"metadatas\"]\n",
    "\n",
    "# 🔹 Nếu bạn có nhiều loại document, có thể trích ra từ metadata\n",
    "doc_types = [m.get(\"title\", \"unknown\") for m in metadatas]\n",
    "colors = [\"blue\" if t == \"unknown\" else \"red\" for t in doc_types]\n",
    "\n",
    "# 🔹 Giảm số chiều xuống 2D để trực quan hóa\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=1)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# 🔹 Vẽ biểu đồ scatter 2D\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=6, color=colors, opacity=0.8),\n",
    "    text=[\n",
    "        f\"<b>Loại:</b> {t}<br><b>Văn bản:</b> {d[:200]}...\" \n",
    "        for t, d in zip(doc_types, documents)\n",
    "    ],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='📊 Phân bố embedding trong Chroma Vector Store (2D)',\n",
    "    xaxis_title='TSNE Dimension 1',\n",
    "    yaxis_title='TSNE Dimension 2',\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=20, b=10, l=10, t=40),\n",
    ")\n",
    "\n",
    "# 🔹 Hiển thị trực tiếp trên browser\n",
    "fig.show(renderer=\"browser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e70cc",
   "metadata": {},
   "source": [
    "## Reranking by BM25 + create hybrid search with semantic search + bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d62483",
   "metadata": {},
   "source": [
    "## Load db + vector retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5adf85b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1' coro=<main_evaluation() done, defined at C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_6816\\3633396511.py:93> exception=OpenAIError('The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable')>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python310\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_6816\\3633396511.py\", line 243, in main_evaluation\n",
      "    result = evaluate(\n",
      "  File \"d:\\DAI_HOC\\CS431\\DoAn\\Rag_QABot\\venv310\\lib\\site-packages\\ragas\\_analytics.py\", line 277, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"d:\\DAI_HOC\\CS431\\DoAn\\Rag_QABot\\venv310\\lib\\site-packages\\ragas\\evaluation.py\", line 461, in evaluate\n",
      "    return run(_async_wrapper())\n",
      "  File \"d:\\DAI_HOC\\CS431\\DoAn\\Rag_QABot\\venv310\\lib\\site-packages\\ragas\\async_utils.py\", line 156, in run\n",
      "    return asyncio.run(coro)\n",
      "  File \"d:\\DAI_HOC\\CS431\\DoAn\\Rag_QABot\\venv310\\lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "  File \"d:\\DAI_HOC\\CS431\\DoAn\\Rag_QABot\\venv310\\lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "  File \"C:\\Program Files\\Python310\\lib\\asyncio\\futures.py\", line 201, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Program Files\\Python310\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\DAI_HOC\\CS431\\DoAn\\Rag_QABot\\venv310\\lib\\site-packages\\ragas\\evaluation.py\", line 434, in _async_wrapper\n",
      "    return await aevaluate(\n",
      "  File \"d:\\DAI_HOC\\CS431\\DoAn\\Rag_QABot\\venv310\\lib\\site-packages\\ragas\\evaluation.py\", line 170, in aevaluate\n",
      "    client = OpenAI()\n",
      "  File \"d:\\DAI_HOC\\CS431\\DoAn\\Rag_QABot\\venv310\\lib\\site-packages\\openai\\_client.py\", line 137, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"            # đa ngôn ngữ, gọn nhẹ, khuyên dùng\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=model_name,model_kwargs={\"device\": \"cuda\"})\n",
    "\n",
    "## lưu vào db\n",
    "vector_db = Chroma( embedding_function= embedding, persist_directory=\"../database\")\n",
    "vector_retriever = vector_db.as_retriever( search_type=\"mmr\", search_kwargs={\"k\": 40, \"fetch_k\": 80, \"lambda_mult\": 0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4c7c8a",
   "metadata": {},
   "source": [
    "## Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e227cd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "rerank_model_name = \"BAAI/bge-reranker-base\"\n",
    "tok = AutoTokenizer.from_pretrained(rerank_model_name)\n",
    "reranker = AutoModelForSequenceClassification.from_pretrained(rerank_model_name)\n",
    "reranker.to(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "reranker.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03d3db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def batch_crossencoder_scores(q: str, texts: List[str], batch_size: int = 16, max_len: int = 512) -> List[float]:\n",
    "    scores = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        inputs = tok([q]*len(batch), batch, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device=\"cuda\" if torch.cuda.is_available() else \"cpu\") for k, v in inputs.items()} # chuyển lên gpu\n",
    "        logits = reranker(**inputs).logits.squeeze(-1)\n",
    "        scores.extend(logits.tolist())\n",
    "    return scores\n",
    "\n",
    "\n",
    "def crossencoder_rerank(docs, query: str, top_k: int = 10):\n",
    "    texts = [d.page_content for d in docs]\n",
    "    scores = batch_crossencoder_scores(query, texts, batch_size=16, max_len=512)\n",
    "    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    BAD_HINTS = (\"Cảm ơn các bạn đã xem\", \"đăng ký kênh\", \"subscribe\", \"like và share\")\n",
    "    final_docs = []\n",
    "    for d, s in ranked:\n",
    "        if all(h.lower() not in d.page_content.lower() for h in BAD_HINTS):\n",
    "            final_docs.append(d)\n",
    "        if len(final_docs) >= top_k:\n",
    "            break\n",
    "    return final_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5d9c44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]  | [CS315 - Chương 3] Deep Generative Models (2) - Part 1 | https://youtube.com/watch?v=SCNZncN1Hvk\n",
      "    0:00:14 → 0:01:11\n",
      "   Chúng ta sẽ cùng đến với các mô hình tạo sinh học sau Deep Generated Model phần 2, mô hình Diffusion. Các mô hình tạo sinh hình ảnh đều có gốc gác sử dụng mô hình phát tán, mô hình Diffusion Model. Đây có thể nói là một trong những mô hình có tính ứng dụng rất cao do tạo ra những ảnh có độ phân giải...\n",
      "--------------------------------------------------------------------------------\n",
      "[1]  | [CS315 - Chương 0] Giới thiệu môn học (Phần 1) | https://youtube.com/watch?v=RU8d6QAuX0k\n",
      "    0:04:07 → 0:05:48\n",
      "   Đó chính là mô hình dựa trên xác suất và cụ thể của một mô hình dựa trên xác suất đó chính là mô hình khuếch tán là Diffusion Model. Sau đó sang tuần thứ 11 thì chúng ta sẽ cùng tìm hiểu về những mô hình học sâu nhưng mà có sự tham gia của ngôn ngữ và thị giác hay còn gọi là Vision Language Model. Đ...\n",
      "--------------------------------------------------------------------------------\n",
      "[2]  | [CS315 - Chương 3] Deep Generative Models (2) - Part 4 | https://youtube.com/watch?v=NJVpvCzceRk\n",
      "    0:00:14 → 0:01:08\n",
      "   Rồi trong những phần trước thì chúng ta đã cùng tìm hiểu về sự khác nhau giữa VAE, Variational Autoencoder và Diffusion Cả hai mô hình VAE và Diffusion đều dựa trên lý thuyết đó là chúng ta sẽ cực đại hóa log của PX này, tức là làm sao cho kỳ ảnh X của mình giống thật nhất. Thay vì chúng ta cực đại ...\n",
      "--------------------------------------------------------------------------------\n",
      "[3]  | [CS315 - Chương 3] Deep Generative Models (2) - Part 8 | https://youtube.com/watch?v=nZc5vWo2Rrg\n",
      "    0:00:14 → 0:01:10\n",
      "   Tiếp theo chúng ta sẽ bàn về tốc độ của mô hình Diffusion Đối với mô hình Diffusion thì vấn đề lớn nhất là nó phải sampling rất nhiều bước trung gian để có thể encode và decode Như vậy thì làm sao để có thể sinh ra ảnh với tốc độ nhanh hơn Nguyên nhân là trong quá trình thêm nhiễu vào, nhiễu sau sẽ ...\n",
      "--------------------------------------------------------------------------------\n",
      "[4]  | [CS315 - Chương 3] Deep Generative Models (2) - Part 4 | https://youtube.com/watch?v=NJVpvCzceRk\n",
      "    0:13:08 → 0:14:21\n",
      "   Như vậy thì làm sao chúng ta có thể khử nhiễu được? Vì dựa trên cái công thức của cái ELBO mà chúng ta đã có ở những slide trước. Qua một số cái phép biến đổi, thì ở đây là do số lượng phép biến đổi quá nhiều. Nên chúng ta chỉ ghi cái kết quả cuối cùng ở đây thôi. Thì nó sẽ có ba cái thành phần số h...\n",
      "--------------------------------------------------------------------------------\n",
      "[5]  | [CS315 - Chương 3] Deep Generative Models (2) - Tổng kết | https://youtube.com/watch?v=--6FInuIyys\n",
      "    0:02:26 → 0:04:16\n",
      "   Thì đây là ba cái cách. Và sau đó thì chúng ta đã tìm hiểu về cách để điều hướng với hai kỹ thuật đó là classifier guidance. Với mỗi một cái condition mới, thì một cái condition chúng ta sẽ ra một cái classifier. Như vậy thì nó sẽ không có linh động trong cái việc là update hoặc là thay cái conditio...\n",
      "--------------------------------------------------------------------------------\n",
      "[6]  | [CS315 - Chương 3] Deep Generative Models (2) - Part 4 | https://youtube.com/watch?v=NJVpvCzceRk\n",
      "    0:16:35 → 0:17:31\n",
      "   Đây chính là cái sự đặc thù riêng của cái diffusion model. Như vậy thì làm sao để có thể huấn luyện được cái hàm P này. Sao cho nó khớp với lại cái q. Thì ở trong cái hình này, nó minh họa một cái trực quan. Đó là cái quá trình, cái màu tím ở đây, tương ứng là cái màu hồng ở đây. Là cái phân bố của ...\n",
      "--------------------------------------------------------------------------------\n",
      "[7]  | [CS315 - Chương 3] Deep Generative Models (2) - Part 8 | https://youtube.com/watch?v=nZc5vWo2Rrg\n",
      "    0:01:08 → 0:12:20\n",
      "   Và ngược lại khi chúng ta denoise cũng như thế. Như vậy thì thời gian chạy của diffusion sẽ là bằng t nhân cho thời gian chạy của GAN và VAE. Vậy thì chúng ta sẽ nhắc lại công thức tạo sinh của mô hình của mình. Trong mô hình tạo sinh của mình thì công thức sử dụng theo cách thức số 2 của chúng ta đ...\n",
      "--------------------------------------------------------------------------------\n",
      "[8]  | [CS315 - Chương 3] Deep Generative Models (2) - Part 4 | https://youtube.com/watch?v=NJVpvCzceRk\n",
      "    0:01:28 → 0:02:11\n",
      "   X0 sẽ là ảnh đầu vào được sampling theo phân bố của data X. Vậy thì công thức ở trên sẽ được đưa về và bài toán mô hình Diffusion model sẽ đưa về việc cực đại hóa kỳ vọng của x1 cho đến XT cho trước X0. Thì đây chính là latent của mình. Và xT sẽ là gồm x0 cho đến XT, nó mở, trộn lại với nhau. Còn cô...\n",
      "--------------------------------------------------------------------------------\n",
      "[9]  | [CS315 - Chương 3] Deep Generative Models (2) - Part 7 | https://youtube.com/watch?v=79WZow7G8fE\n",
      "    0:02:32 → 0:03:21\n",
      "   Và nó sẽ dễ gây ra cái hiện tượng đó là lan truyền lỗi. Ví dụ điều gì xảy ra nếu như cái ảnh 64 x 64 này nó có những cái artifact, tức là những cái dấu hiệu để mà không có được đẹp và có những cái lỗi trong hình ảnh. Vì vậy thì nó sẽ lan truyền cái lỗi đó đến những cái ảnh sau mà không có cái cơ chế...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## test reranker\n",
    "query = \"diffusion là gì\"\n",
    "docs = vector_retriever.get_relevant_documents(query)\n",
    "reranked_docs = crossencoder_rerank(docs, query, top_k=10)\n",
    "\n",
    "for i, d in enumerate(reranked_docs):\n",
    "    m = d.metadata\n",
    "    print(f\"[{i}]  | {m.get('title', '')} | {m.get('video_url', '')}\")\n",
    "    print(f\"    {m.get('start_timestamp', '?')} → {m.get('end_timestamp', '?')}\")\n",
    "    text = d.page_content.replace(\"\\n\", \" \").strip()\n",
    "    print(\"   \" + (text[:300] + (\"...\" if len(text) > 300 else \"\")))\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4592cfd",
   "metadata": {},
   "source": [
    "## bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51d7ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.schema import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "# ===== 1. Lấy doc từ Chroma =====\n",
    "raw = vector_db.get(include=[\"documents\", \"metadatas\"])\n",
    "docs = []\n",
    "for content, metadata in zip(raw[\"documents\"], raw[\"metadatas\"]):\n",
    "    docs.append(Document(\n",
    "        page_content=content,\n",
    "        metadata={\n",
    "            \"filename\": metadata.get(\"filename\", \"\"),\n",
    "            \"video_url\": metadata.get(\"video_url\", \"\"),\n",
    "            \"start_timestamp\": metadata.get(\"start_timestamp\", \"\")\n",
    "        }\n",
    "    ))\n",
    "\n",
    "# ===== 2. BM25 retriever an toàn =====\n",
    "if docs:\n",
    "    corpus_tokens = [doc.page_content.split() for doc in docs]\n",
    "    bm25_model = BM25Okapi(corpus_tokens)\n",
    "\n",
    "    def bm25_retriever(query, top_k=5):\n",
    "        tokenized_query = query.split()\n",
    "        scores = bm25_model.get_scores(tokenized_query)\n",
    "        top_k = min(top_k, len(docs))  # bảo vệ out-of-range\n",
    "        top_indices = scores.argsort()[-top_k:][::-1]\n",
    "        return [docs[i] for i in top_indices]\n",
    "\n",
    "    class BM25Retriever:\n",
    "        def __init__(self, top_k=30):\n",
    "            self.top_k = top_k\n",
    "\n",
    "        def get_relevant_documents(self, query: str):\n",
    "            return bm25_retriever(query, top_k=self.top_k)\n",
    "\n",
    "    keyword_retriever = BM25Retriever(top_k=30)\n",
    "    bm25_runnable = RunnableLambda(lambda x: keyword_retriever.get_relevant_documents(x))\n",
    "else:\n",
    "    # fallback nếu không có docs\n",
    "    bm25_runnable = RunnableLambda(lambda x: [])\n",
    "\n",
    "\n",
    "\n",
    "# ===== 4. Hybrid retriever =====\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_runnable, vector_retriever],\n",
    "    weights=[0.5, 0.5]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925223f5",
   "metadata": {},
   "source": [
    "## generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82af4a97",
   "metadata": {},
   "source": [
    "### Chọn mô hình (qwen 0.6b hoặc api gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7569d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.float16,   # dùng float16 cho GPU\n",
    "    #device_map=\"auto\"      # tự động đặt model lên GPU\n",
    ")\n",
    "model.to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True,trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24760950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    device=0  # chạy trên CPU\n",
    "\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=model_pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4208c76",
   "metadata": {},
   "source": [
    "## gemini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf300fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",   # hoặc gemini-1.5-pro, gemini-2.0-flash\n",
    "    temperature=0.0,\n",
    "    google_api_key=googleAPIKey  # 👈 thêm dòng này\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ea8d389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableMap,RunnableLambda\n",
    "\n",
    "\n",
    "class VideoAnswer(BaseModel):\n",
    "    text: str = Field(description=\"Câu trả lời tóm tắt trong 3 câu\")\n",
    "    filename: str = Field(description=\"Tên file transcript gốc\")\n",
    "    video_url: str = Field(description=\"URL của video gốc\")\n",
    "    start_timestamp: str = Field(description=\"Thời điểm bắt đầu (format: HH:MM:SS)\")\n",
    "    end_timestamp: str = Field(description=\"Thời điểm kết thúc (format: HH:MM:SS)\")\n",
    "    confidence: str = Field(description=\"Độ tin cậy: zero/low/medium/high\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=VideoAnswer)\n",
    "\n",
    "# ===== Prompt =====\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Dựa vào transcript sau, trả lời câu hỏi của người dùng bằng tiếng Việt.Phần tóm tắt nội dung thì nên tóm tắt trong 3 câu, \n",
    "dựa vào các đoạn transcript được cung cấp và chỉ ra đoạn video chứa thông tin đó (video url, thời điểm bắt đầu và kết thúc).\n",
    "Đồng thời làm mượt lại nội dung tóm tắt đó\n",
    "Khi trích dẫn thông tin, **luôn sử dụng đúng [Video URL] và [Start] từ doc chứa nội dung đó**.\n",
    "Nếu không biết câu trả lời thì cứ trả lời là tôi không biết và độ tin cậy là zero\n",
    "Nếu câu hỏi không liên quan đến nội dung video thì trả lời tôi chỉ được huấn luyện trả lời các câu hỏi liên quan đến nội dung video và độ tin cậy là zero\n",
    "Không bịa ra thông tin không có căn cứ, không trả lời sai format\n",
    "Nếu bạn cực kỳ chắc chắn về câu trả lời, hãy đặt độ tin cậy là high. Nếu bạn khá chắc chắn, hãy đặt độ tin cậy là medium. Nếu bạn không chắc chắn về câu trả lời, hãy đặt độ tin cậy là low.\n",
    "Định dạng đầu ra phải tuân theo JSON schema sau:\n",
    "{format_instructions}\n",
    "Transcript:\n",
    "{context}\n",
    "\n",
    "Câu hỏi: {question}\n",
    "\\nAnswer:                                          \n",
    "\"\"\")\n",
    "\n",
    "def format_doc(docs):\n",
    "    formatted = []\n",
    "    for doc in docs:\n",
    "        url = doc.metadata.get(\"video_url\", \"\")\n",
    "        filename = doc.metadata.get(\"filename\", \"\")\n",
    "        start = doc.metadata.get(\"start_timestamp\", \"\")\n",
    "        end = doc.metadata.get(\"end_timestamp\", \"\")\n",
    "        content = doc.page_content\n",
    "        formatted.append(f\"\"\"[Video URL]: {url}\n",
    "[Filename]: {filename}\n",
    "[Start]: {start}\n",
    "[End]: {end}\n",
    "[Content]: {content}\"\"\")\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "def extract_json_from_output(output: str) -> str:\n",
    "    return output.split('Answer')[1].strip()\n",
    "    \n",
    "    # Hàm rerank lấy docs và query\n",
    "def rerank_with_query(docs_and_query) -> List[Document]:\n",
    "    docs, query = docs_and_query\n",
    "    reranked = crossencoder_rerank(docs, query, top_k=10)\n",
    "    return reranked\n",
    "\n",
    "# ===== Tạo RAG chain =====\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": RunnableLambda(lambda query: (\n",
    "            vector_retriever.get_relevant_documents(query),\n",
    "            query\n",
    "        ))\n",
    "        | RunnableLambda(rerank_with_query)\n",
    "        | RunnableLambda(lambda docs: format_doc(docs))\n",
    "    }\n",
    "    | prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "    | llm\n",
    "    #| #extract_json_from_output # lấy phần đằng sau answer ( là định dạng json đã chuẩn bị)\n",
    "    #RunnableLambda(lambda x: extract_json_from_output(x))\n",
    "    #|parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af26ca61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"text\": \"Việc tự huấn luyện mô hình CLIP từ đầu là không khả thi vì nó đòi hỏi một lượng dữ liệu cực kỳ lớn. Ngoài ra, quá trình này còn yêu cầu tài nguyên tính toán khổng lồ, khả năng xử lý song song và các GPU rất đắt tiền. Do đó, giải pháp khả thi nhất là sử dụng các mô hình CLIP đã được tiền huấn luyện để giải quyết các tác vụ hiện có.\",\n",
      "  \"filename\": \"yPzXzbEhUW0\",\n",
      "  \"video_url\": \"https://youtube.com/watch?v=yPzXzbEhUW0\",\n",
      "  \"start_timestamp\": \"00:00:14\",\n",
      "  \"end_timestamp\": \"00:01:03\",\n",
      "  \"confidence\": \"high\"\n",
      "}\n",
      "```\n",
      "Thời gian: 38.36908531188965\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "result = rag_chain.invoke(\"Tại sao việc tự huấn luyện mô hình CLIP từ đầu là khó khả thi?\")\n",
    "print(result.content)\n",
    "end = time.time()\n",
    "print(\"Thời gian:\", end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd8b0c",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e8b3273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chuẩn bị chạy đánh giá...\n",
      "Bắt đầu quá trình đánh giá RAG...\n",
      "Đang lấy chunk từ Vector DB để tạo bộ test...\n",
      "Đang dùng LLM để tạo 5 mẫu test...\n",
      "Đã tạo xong 5 mẫu test.\n",
      "Đang chạy pipeline trên bộ test set...\n",
      "Đang xử lý mẫu 1/5: Định dạng cần chuẩn bị trước cho phần văn bản là g...\n",
      "Đang xử lý mẫu 2/5: Một ví dụ về bài toán phức tạp mà mạng Perceptron ...\n",
      "Đang xử lý mẫu 3/5: Mục tiêu của việc phát triển mô hình là gì?...\n",
      "Đang xử lý mẫu 4/5: Tại sao việc tự huấn luyện mô hình CLIP lại không ...\n",
      "Đang xử lý mẫu 5/5: Mô hình tạo sinh giúp ích gì trong việc huấn luyện...\n",
      "Đã thu thập xong dữ liệu. Chuẩn bị cho Ragas...\n",
      "Đã phát hiện ground_truth_context, sẽ đo context_recall.\n",
      "Đang chạy Ragas evaluate... (Sử dụng Gemini làm Judge, việc này có thể mất vài phút)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 51.496811783s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 51.496741869s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 51.496752115s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 51.496671122s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 51.49684834s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 51.496673368s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 51.496368037s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 49.202838703s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 49.156954674s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 49.132680147s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 49.112031383s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 49.083631652s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "].\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   5%|▌         | 1/20 [00:11<03:42, 11.73s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 40.363242363s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 40\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 40.298957561s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 40\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 40.299245525s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 40\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 40.291730951s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 40\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 40.288960545s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 40\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 40.267363124s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 40\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 40.241085706s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 40\n",
      "}\n",
      "].\n",
      "Evaluating:  25%|██▌       | 5/20 [00:30<01:19,  5.33s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 21.757657617s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 21\n",
      "}\n",
      "].\n",
      "Evaluating:  30%|███       | 6/20 [00:36<01:18,  5.62s/it]Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 15.349542967s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 15.295304813s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 15.290156002s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 15.274207362s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 15.260215631s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 15.135015424s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "].\n",
      "Evaluating:  35%|███▌      | 7/20 [00:44<01:22,  6.34s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 7.135367741s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 7\n",
      "}\n",
      "].\n",
      "Evaluating:  50%|█████     | 10/20 [00:49<00:37,  3.77s/it]Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 2.299320478s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 2\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 2.23468038s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 2\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 2.216258345s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 2\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 2.193732634s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 2\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 59.990770073s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 59\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 59.905463965s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 59\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 58.262602969s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 58\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 57.927056183s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 57.868565225s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 54.593105889s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 54\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 54.491272749s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 54\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 51.121522421s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 51.071466927s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 51.07085856s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 51.056624861s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 49.953949259s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 49.633699746s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 49.530712347s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 46.313158592s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 46\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 46.179472792s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 46\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 33.651442426s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 33\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 33.36726652s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 33\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 33.239645919s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 33\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 30.015442319s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 30\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 29.878776353s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 29\n",
      "}\n",
      "].\n",
      "Evaluating:  60%|██████    | 12/20 [01:40<01:29, 11.18s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  65%|██████▌   | 13/20 [01:45<01:09,  9.98s/it]Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 56.732051613s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 56\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 53.966546065s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 53\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 52.533487422s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 52\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 52.504249696s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 52\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 52.497786518s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 52\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 51.061986998s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 51.048039749s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 50.502316957s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 50\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 49.074647266s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 49.069832593s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 49.050974767s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 48.769554798s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 48\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 46.77559295s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 46\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 46.209536339s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 46\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 44.780932888s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 44\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 44.778167159s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 44\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 44.771218715s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 44\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 44.494455828s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 44\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 37.902690556s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 37\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 36.503960033s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 36\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 36.483752872s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 36\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 36.471437857s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 36\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 36.172211163s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 36\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 31.984774368s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 31\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 29.688617535s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 29\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 57.176623615s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 57.162658238s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "].\n",
      "Evaluating:  75%|███████▌  | 15/20 [02:56<01:35, 19.06s/it]Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 55.698921929s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 55\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 55.688094059s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 55\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 55.678552753s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 55\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 53.693089836s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 53\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 53.673515953s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 53\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 53.381191954s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 53\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 53.375164324s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 53\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 53.363435254s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 53\n",
      "}\n",
      "].\n",
      "Exception raised in Job[14]: TimeoutError()\n",
      "Exception raised in Job[6]: TimeoutError()\n",
      "Exception raised in Job[10]: TimeoutError()\n",
      "Exception raised in Job[2]: TimeoutError()\n",
      "Evaluating:  80%|████████  | 16/20 [03:00<01:03, 15.91s/it]Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 49.056767403s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 40.754633617s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 40\n",
      "}\n",
      "].\n",
      "Exception raised in Job[18]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 20/20 [03:23<00:00, 10.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- KẾT QUẢ ĐÁNH GIÁ RAGAS ---\n",
      "{'faithfulness': 0.8333, 'answer_relevancy': 0.9508, 'context_precision': nan, 'context_recall': 1.0000}\n",
      "\n",
      "--- Bảng kết quả chi tiết ---\n",
      "| user_input                                                                    | retrieved_contexts                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | response                                                                                                                                                                                                                                                                                                                                                                                                                           | reference                                                                                                                                                                                  |   faithfulness |   answer_relevancy |   context_precision |   context_recall |\n",
      "|:------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------:|-------------------:|--------------------:|-----------------:|\n",
      "| Định dạng cần chuẩn bị trước cho phần văn bản là gì?                          | ['Chúng ta sẽ sử dụng một cái hình khác. Ví dụ như ở đây là chúng ta... thử tải cái này về. Bằng cái link khác. Rồi, ở đây chúng ta sẽ tải về. Rồi, ở đây chúng ta sẽ tải về. Rồi, ở đây chúng ta sẽ tải về. Bằng cái link khác. À, cái link này chính là cái link ban đầu hồi nãy. Rồi, chúng ta sẽ name là Elephant. Rồi, ở đây là... lấy tấm hình này đi. Rồi, chúng ta sẽ lấy cái đường dẫn trên. Và đưa cái link ảnh vào. Rồi, chúng ta đã tải được. Và chúng ta sẽ sửa lại là Elephant. Rồi, như vậy chúng ta đã có 3 cái công cụ này. Bây giờ, đối với cái phần văn bản, thì chúng ta sẽ có cái định dạng... chúng ta chuẩn bị trước cái định dạng đó là... tên của cái file của mình. Và cái câu mô tả... tương ứng với cái nội dung đó.', 'Hoặc là về lĩnh vực, ví dụ như tóm tắt văn bản. Thì cái many to many dạng 1 có rất nhiều những cái ứng dụng hiện đại. Many to many dạng 2 tức là nó khác biệt so với dạng 1 đó là đối với dạng 1 chúng ta phải xem hết toàn bộ input xong đó chúng ta mới đi trả lời. Còn many to many dạng 2 thì chúng ta chỉ... chúng ta nhận được đến đâu thì chúng ta sẽ ra cái output trên đó.', 'Tức là chúng ta sẽ đi lấy mẫu. Đầu tiên đó là chúng ta lấy mẫu. Ví dụ chúng ta muốn tạo ra những cái ảnh mặt người giống với lại cái ảnh thật như vậy. Trong cái ví dụ ở slide đầu tiên thì chúng ta sẽ đi lấy mẫu. Và chúng ta sẽ lấy mẫu những cái người trong thế giới thực của mình. Đó là những cái ảnh thật.', 'Và đây là một trong những cái công cụ rất là hiệu quả. Nhưng mà thông thường trong lập trình thì thường chúng ta sẽ dùng đến 4D tensor. Thường chúng ta sẽ dùng đến một cấp độ là 4D tensor. Trong đó cái chiều đầu tiên có thể là cái chiều về số lượng cái mẫu dữ liệu của mình. Ví dụ như là batch size, tức là cái kích thước dữ liệu của mình. Sau đó là chúng ta sẽ đến cái chiều độ sâu, depth.', 'Hoặc là chúng ta lấy cái salient, tức là những cái biên cạnh mà nổi bật nhất. Hoặc là chúng ta làm các cái thao tác là transformation. Tức là cái thao tác biến đổi về mặt hình học, như flip, quay, tỷ lệ, tịnh tiến, v.v. Từ đó, từ một ảnh mức xám, thì từ một ảnh chúng ta sẽ tạo ra N ảnh và cái N ảnh này thì N ảnh phiên bản khác nhau. Còn đối với lĩnh vực về xử lý ngôn ngữ tự nhiên, thì chúng ta cũng có một số kỹ thuật. Trong đó đơn giản và dễ hiểu nhất đó chính là kỹ thuật Back Translation. Tức là với cái văn bản gốc, thì chúng ta chuyển nó dịch nó sang cái ngôn ngữ khác. Thí dụ như là tiếng Anh, sau đó chúng ta dịch ngược trở lại, sang trở lại tiếng Anh. Thì từ tiếng Anh sang tiếng Pháp, xong rồi từ tiếng Pháp dịch ngược lại tiếng Anh, thì chúng ta đã có một cái phiên bản mới.', 'Đầu tiên cái vế bên trong cùng đó là hàm max. Tức là chúng ta sẽ đi huấn luyện D trước. Tức là Discriminator trước. Discriminator nó phải có khả năng phân biệt đối tượng 1 cách chắc chắn. Thì khi đó nó mới có thể thách thức cái G. Sao cho nó khó được. Do đó thì ở đây chúng ta sẽ sử dụng, ở đây nhìn chung. Nhìn chung đó là chúng ta công thức tương tự như công thức của Binary Cross entropy. Là so sánh giữa cái giá trị dự đoán và giá trị thực tế.', 'Do đó cái công thức này nó là hoàn toàn tương đương. Và chúng ta sẽ chạy thuật toán Backpropagation và cập nhật lại cái tham số cho D. Sau đó chúng ta sẽ sang bước số 2 là khởi tạo optimizer cho G, generator. Bước đầu tiên đó là chúng ta sẽ tạo ra cái dữ liệu từ noise. Thì chúng ta sẽ bắt chước cái code ở trên. Chúng ta sẽ bắt chước cái code ở trên. Rồi, thì ở đây chúng ta sẽ có là xgen là bằng G của noise. Rồi, chấm detach.', 'Chúng ta sẽ nhân với lại cái... Chúng ta sẽ lấy để... Xin lỗi ở đây chúng ta không phải là nhân vô hướng và chúng ta sẽ dùng cái hàm cross entropy Chúng ta sẽ dùng cái hàm cross entropy để mà tính như vậy thì f.cross entropy Rồi chúng ta sẽ truyền vào cái logic và cái one-hot vector tức là cái label của mình Thì ở trong trường hợp này cái label của mình chính là cái nhãn mà chúng ta đã setup ở phía trên Đây, cái label này Thì được gán từ một, tức là ảnh thứ nhất, nhãn là một, ảnh thứ hai, nhãn là hai, và cái cặp ảnh thứ n, nhãn là n Rồi, chúng ta sẽ show... Chúng ta sẽ đi tính cái loss này bằng cách lấy logic nhân với lại cái label Rồi, bây giờ chúng ta sẽ tính cái hàm loss này là logic và label Rồi, sau đó chúng ta sẽ đi print nó ra Rồi, chúng ta sẽ tính tương tự như vậy cho cái loss của t, tức là theo text Với text tức là gì? Chúng ta sẽ đi cố định cái text, ví dụ text là t3 và chúng ta sẽ cho y chạy từ trên xuống Thế thì bản chất cái cách tính của t3 với trên theo hàng y3 và t3 thì chúng ta chỉ cần lật cái ma trận lại là xong Rồi, sau đó ở đây chúng ta sẽ sửa lại cái code Đó là chấm, chúng ta thêm một cái thành phần chuyển vị vào đây ha Thì chúng ta sẽ lấy logic này, chuyển vị, cross entropy Và loss tổng hợp thì sẽ là bằng trung bình cộng của hai loss này Đó là bằng loss của y cộng cho loss của t Rồi, thì bây giờ chúng ta sẽ lần lượt chạy cái code Đây là theo cột ha, đây là trực quan hóa theo cột Chúng ta lấy ra, thì nếu đúng cái cột này, nó sẽ phải hướng về cái vector này Và nó sẽ bật sáng lên tại cái hàng thứ 3 Rồi, bây giờ chúng ta sẽ tính thử Thì cái loss của mình đó là loss theo trực y, nó ra là 2,410 Và theo cái trực t, rất là văn bản, thì là 2,2406 Và trung bình cộng là 248', 'Tại vì sao? Nhãn của nó thật ra cũng chính là cái x. Nhãn của nó cũng chính là x. Chúng ta lấy x trừ cho x mũ rồi bình phương. Thì đây là mean squared error. Và với cái biểu diễn gọn gàng hơn, tức là thay vì chúng ta sẽ phải có nhiều lớp biến đổi như thế này, rồi nhiều lớp biến đổi như thế này. Thì từ nay về sau chúng ta chỉ cần dùng một cái ký hiệu hình thang. Hai ký hiệu hình thang đó là từ x chúng ta đưa về z và chúng ta dùng cái hình thang là phía bên trái là lớn, và phía bên phải đó là cái cạnh nhỏ, tức là ám chỉ từ một không gian nhiều chiều về không gian ít chiều. Sau đó decoder, đây là encoder. Còn decoder là chúng ta sẽ làm một cái hình thang ngược lại, là từ không gian ít chiều hơn về không gian nhiều chiều hơn. Nó sẽ tạo ra cái thằng x mũ.', 'Ở trên là và, ở dưới là hoặc. Và một số kỹ thuật cụ thể, ví dụ như là Dropout, Regularization, tăng cường. Sửa lỗi Regularization là chính quy hóa và tăng cường dữ liệu. Thì đối với kỹ thuật Regularization thì cái hàm lỗi của mình là bên cạnh cái sai số giữa giá trị dự đoán và cái giá trị ground truth thì chúng ta có thể bổ sung thêm thành phần chính quy hóa. Trong cái công thức hàm lỗi ở đây chúng ta thấy loss của y, ngã và y, đây chính là cái sai số giữa giá trị dự đoán và ground truth. Và thành phần chính quy hóa mà chúng ta đang muốn đề cập chính là ở đây.']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Đối với phần văn bản, định dạng cần chuẩn bị trước bao gồm tên của file. Kèm theo đó là một câu mô tả tương ứng với nội dung của file. Việc này giúp đảm bảo văn bản được tổ chức rõ ràng và dễ hiểu.                                                                                                                                                                                                                              | Đối với cái phần văn bản, thì chúng ta sẽ có cái định dạng... chúng ta chuẩn bị trước cái định dạng đó là... tên của cái file của mình. Và cái câu mô tả... tương ứng với cái nội dung đó. |       0.5      |           0.955833 |                 nan |                1 |\n",
      "| Một ví dụ về bài toán phức tạp mà mạng Perceptron không thể giải quyết là gì? | ['Cụ thể ở đây, các tham số theta 0, theta 1, theta 2 và theta m, thì đây chính là các tham số của phương trình đường thẳng. Và chính xác hơn là của mặt phẳng tại vì chúng ta sẽ xét trong không gian nhiều chiều. Và cái mặt phẳng này nó sẽ chia không gian ra làm hai phần, một cách tuyến tính. Và ví dụ như chúng ta có tập hợp các cái điểm như thế này, thì nếu như các điểm mà có thể phân chia được bởi một cái siêu phẳng hay là một cái mặt phẳng, thì khi đó chúng ta có thể sử dụng cái mạng Logistic Regression hoặc là Perceptron. Tại vì với cái đường thẳng này thì nó sẽ tách ra làm hai, một cách dễ dàng đối với dữ liệu tuyến tính. Vấn đề xảy ra đó là mỗi một cái Perceptron thì nó chỉ có thể giải quyết được một cái bài toán tuyến tính. Trong khi đó sẽ có rất nhiều những cái bài toán phức tạp hơn và chúng ta không thể sử dụng một cái mạng Perceptron để có thể giải quyết được. Chúng ta lấy một cái tình huống ví dụ đó là hai cái tập điểm nằm trong và nằm ngoài vòng tròn.', 'Đó là hình tròn và dấu cộng, các điểm tròn và cộng như thế này. Thì chúng ta thấy là với một đường thẳng thì không thể nào tách ra làm hai phần được, mà chúng ta cần phải có một cái tổ hợp các cái đường thẳng. Cụ thể đó là chúng ta sẽ cần có cái tổ hợp này để mà chia ra làm hai. Thế thì để mà có thể phối hợp và tổng hợp được các cái thông tin của một loạt các cái đặc trưng như thế này, thì nó đòi hỏi chúng ta không phải chỉ tăng theo cái chiều dọc như thế này. Tại vì với mỗi một cái neuron này thì chúng ta sẽ có một cái bộ phân loại yếu, đó là một cái mạng phẳng. Nhưng các cái mạng phẳng này muốn mà có thể giải quyết được bài toán vi tuyến thì nó đòi hỏi phải có một cái thao tác để tổng hợp. Và thao tác tổng hợp đó thì nó sẽ được đặt ở cái layer tiếp theo. Còn nếu không có cái layer này thì các cái neuron nó sẽ là độc lập nhau. Neuron số 1, neuron số 2 và neuron số 3 là độc lập. Nó sẽ không thể phối hợp với nhau. Nhưng nhờ có cái neuron số 4 ở cái layer tiếp theo, nó tổng hợp lại để phối hợp và tạo ra một cái đặc trưng mạnh hơn. Đó là ý nghĩa của việc mà chúng ta tăng thêm một cái layer, nó sẽ giúp chúng ta tạo ra một cái đặc trưng mới. Vì vậy, với một neuron thì đặc trưng của chúng ta quá đơn giản nên không giải quyết được các bài toán phức tạp hoặc là bài toán vi tuyến. Vậy thì đến với cái mạng neural network, nhờ có các cái hidden layer, tức là các cái lớp ẩn. Các cái lớp ẩn này thì nó sẽ tổng hợp, ví dụ như một cái node ở đây, chúng ta thấy là kết nối với các cái node ở phía trước. Tức là nó đang tổng hợp đặc trưng. Các cái đặc trưng ở lớp trước, đó là những đặc trưng đơn giản. Nhưng mà qua cái quá trình tổng hợp ở đây thì nó sẽ tạo ra các cái đặc trưng mới phức tạp hơn và nó phi tuyến hơn. Rõ ràng các cái đặc trưng phi tuyến tính, nó sẽ giúp cho chúng ta giải quyết được những cái bài toán phức tạp. Các cái đặc trưng phức tạp sẽ giúp cho chúng ta giải quyết được các cái bài toán phi tuyến tính. Và MLP, tức là cái mạng neural network của mình, một cái tên gọi khác của mạng neural network, nó sẽ tổng hợp đặc trưng phức tạp hơn từ các cái đặc trưng đơn giản qua các cái lớp ẩn. Với lớp ẩn này, tổng hợp đặc trưng đơn giản để tạo ra thành các đặc trưng mới.', 'Hay nó cách khác đó là đặc trưng đơn giản. Nó không thể giúp chúng ta giải quyết được các bài toán phức tạp. Do đó chúng ta cần có một cái nhu cầu, đó là tăng cái độ sâu theo cái trục đứng như thế này. Thì để làm chuyện đó chúng ta sẽ dùng cái kiến trúc đó là DeepStack chúng ta sẽ trồng nhiều lớp lên với nhau. Ở đây là layer 1, layer 2 và layer 3, nó được trồng lên nhau. Và với mỗi cái layer thì chúng ta sẽ tổng hợp được cái thông tin đặc trưng ở một cái cấp độ. Ví dụ như ở đây sẽ là low level feature, đây là mid level feature, đây là low và đây sẽ là high level feature. Thì khi đến cái đặc trưng ở trên tầng số 3 thì nó cũng giống như cái mạng CNN của một xử lý ảnh. Đó là đặc trưng của mình đã có cái tính chất gọi là đặc trưng phức tạp hơn, phi tuyến tính hơn. Thì hy vọng là nó giúp chúng ta giải quyết được cái bài toán khó. Thì như vậy khi chúng ta trồng các layer lên thì chúng ta sẽ có cái trạng thái ẩn từ lớp thứ y. Thì SI sẽ là đầu vào cho cái layer thứ y cộng 1, như vậy cái trạng thái ẩn của layer thứ y chính là cái SI. Nó sẽ là cái input để đi tính cái SI cộng 1. Thì layer số 1 nó sẽ truyền lên layer số 2, rồi lên layer số 3. Thì đó là cái ý nghĩa. Và chúng ta sẽ có cái công thức ở layer số 1 là S1 là ở đây. Và ở đây, ví dụ như là S1 tại vị trí T, thì nó sẽ được tính toán giống như một cái ANN bình thường. Nó cũng sẽ nhận vào cái thông tin của quá khứ tại cái tầng thứ 1. Nó nhận thông tin từ đây sang. Rồi kết hợp với thông tin tại cái thời điểm XT hiện tại để tổng hợp ra cái S1T. Sau đó chúng ta sẽ đi tính S2T. Nó sẽ tổng hợp thông tin từ 2 phía. Thứ nhất là từ quá khứ. Nhưng mà quá khứ tại cái tầng thứ 2, tức là ST-1-2. Sau đó nó sẽ tổng hợp, nó sẽ nhận cái thông tin từ ST1, S1T từ dưới lên. Tức là layer số 2 sẽ được tính toán từ layer thứ 1. Đây chính là chúng ta đang tạo ra một đặc trưng mới. Một đặc trưng tổng hợp mới từ cái lớp phía trước đó. Tương tự như vậy, từ S2T, chúng ta sẽ đi tính S3T. Và đồng thời nó sẽ còn có kết hợp thông tin của S3T-1 ở phía trước là thông tin của quá khứ, ở tầng thứ 3. Như vậy thì đây chính là công thức và kiến trúc của DeepStack ANN. Trong một số tài liệu người ta luôn khuyến nghị, đó là chúng ta nên sử dụng DeepStack nhưng mà không nên dùng quá nhiều. Ví dụ như là chúng ta dùng từ 2 cho đến khoảng 4 lớp là vừa đủ.', 'Vậy thì bây giờ chúng ta sẽ có những cái giải pháp gì? Đó là chúng ta sẽ tái tham số lớp lấy mẫu. Thì cái lớp lấy mẫu của chúng ta chỉ là cái lớp này. Đây là cái lớp lấy mẫu. Hay là sampling layer. Rồi, nếu như chúng ta giữ nguyên cái công thức đó là z là xấp xỉ, z là sampling theo cái phân bố mi và sigma bình phương. Rồi, thì rõ ràng chúng ta sẽ không có thể tính toán được.', 'thì cách mà chúng ta sẽ thực nghiệm đó chính là chúng ta sẽ làm cái lưới từ trừ 5 cho đến 10 và từ trừ 10 cho đến 5 rồi, thì chúng ta sẽ thử ha chúng ta sẽ vẽ cái lưới này rồi, trừ 5 là ở đây rồi chúng ta sẽ lấy cái lưới chụp cái màn hình này thì trừ 5 sẽ là ở đây rồi 10 sẽ là ở đây trừ 5 cho đến 10 trừ 5 cho đến 10 thì 10 sẽ là ở đây rồi và trục tung thì là sẽ từ trừ 10 cho đến 5 trừ 10 cho đến 5 là ở đây trừ 10 cho đến 5 là ở đây rồi, như vậy cái lưới mà dự định chúng ta sẽ vẽ là ở khu vực này trục hoành là từ trừ 5 cho đến 10 và trục tung là từ trừ 10 cho đến 5 như vậy thì chúng ta sẽ có một cái khung vuông rồi, sau đó chiến thuật của chúng ta là gì? chúng ta sẽ chia lưới nó ra ví dụ như trong trường hợp này chúng ta sẽ chia ra làm 12 khoảng cách đều nhau rồi, sau đó chúng ta cũng sẽ chia lưới theo chiều dọc như vậy rồi, thì với mỗi một cái điểm trên cái mắt lưới này lấy ví dụ như cái điểm ở đây chúng ta sẽ dùng một điểm đen ví dụ như một cái điểm trên cái mắt lưới này thì nó sẽ là một cái vector z mà chúng ta lấy ra, chấm lên trên cái lưới và chúng ta sẽ qua cái hàm decoder để xem nó ra một cái tấm ảnh gì nó như thế nào sau đó chúng ta lấy cái ảnh này chúng ta sẽ vẽ lên trên thành một cái ma trận với cái ảnh này chúng ta sẽ vẽ ra một ảnh với một cái điểm z ở đây chúng ta sẽ decode ra và vẽ ảnh lên với cái điểm này, điểm này chúng ta decode ra và vẽ ảnh lên thì chúng ta sẽ ra một cái ma trận các điểm ảnh và chúng ta sẽ quan sát xem nó có cái tính chất gì đặc biệt phải không Rồi, thì đó chính là cái ý tưởng của cái hàm Plot Reconstructed chúng ta sẽ chia ra thành các latent space với n ở đây là bằng 12, chúng ta sẽ chia ra làm 12 khoảng rồi lấy cái x và cái y này chúng ta sẽ có được cái latent z là cái điểm màu đen mà chúng ta đã vẽ rồi qua cái hàm decoder chúng ta sẽ ra được x hat tức là cái ảnh được tạo sinh ra và ảnh này sẽ được reset về cái kích thước là 28 x 28 và sau đó vẽ lên cái ma trận ảnh rồi, thì chúng ta sẽ chạy cái đoạn code này đó thì với cái ví dụ này chúng ta có thể thấy đó là ở những cái khu vực phía trên đó sẽ ra một cái hình thù gì đấy và chúng ta không cảm nhận được đó là một cái con số khu vực ở giữa ở đây cũng vậy nó sẽ có lẫn lộn rất nhiều những cái hình ảnh ở trong đó có thể có chứa nhiều con số dẫn đến là mình nhìn mình không cảm nhận được đó là số gì khu vực này cũng vậy thế thì chiếu lên trên cái hình ảnh của cái không gian của mình thì những ảnh ở góc phía trên bên đây là ở khu vực trừ 5 cho đến 5 trừ 5 cho đến 5 tức là nó nằm ở cái khu vực màu trắng và cái khu vực màu trắng này khi chúng ta decode ra thì nó sẽ tạo ra một cái ảnh không có ý nghĩa không giống cái con số nào hết đó thì đây chính là cái điểm yếu của mô hình auto encoder', 'Tại vì nó phải nhớ hết các cái dữ liệu. Và nó có thể mắc kẹt ở những cái điểm cực tiểu cục bộ. Thì cái nguyên nhân đó là vì nó thiếu những cái bước nhảy vọt để thoát ra. Thì chút nữa chúng ta sẽ cùng phân tích là cái thiếu bước nhảy vọt này nó thể hiện như thế nào. Trong stochastic gradient descent thì nó có những cái ưu điểm vượt trội so với lại cái thuật toán Batch gradient descent. Tuy nhiên là cái điểm yếu của nó vẫn là nó phải tính toán nhiều lần.', 'Bean, tương ứng là cái hàm denoise Và ở đây chúng ta denoise khi chúng ta không biết trước cái đáp án, chúng ta chỉ có ảnh, trước đó là XT thôi Và chúng ta sẽ phải đi xác định XT trừ một Trong khi đó cái anh chàng này thì anh có cái đáp án là x0 nên anh có thể xác định được cái phân bố nhiễu của này một cách chính xác Thì anh này sẽ tìm cách để bắt chước cái anh này Và cái quá trình huấn luyện thì chúng ta sẽ tính toán trên cái Q của XT trừ một Và dựa trên cái công thức xác suất có điều kiện và Bayes Và cái kết quả thu được, cuối cùng thu được đó là một cái phân bố Gauss Q XT cho trước XT trừ một của mình, đó là một cái phân bố Gauss như thế này Đó là phân bố màu xanh lá như thế này Và đây là Routroot Và cái phân bố này thì nó sẽ được tham số hóa bởi cái công thức đó là Mu của Q, XT, X0 và Sigma của Q T Thế thì hai cái Mu và Sigma này đó là Mean và Variance được tạo ra từ cái X0, XT và T Trong đó cái thành phần variance ở đây là chỉ phụ thuộc vào T thôi Nó không phụ thuộc vào các cái X0 và XT Lý do đó là vì các cái bán kính này là giống nhau, không thay đổi Nó là những cái hình tròn giống nhau cùng một cái bán kính Bán kính của nó sẽ thay đổi theo T Nhưng mà nó sẽ là chỉ phụ thuộc vào biến T thôi, không phụ thuộc vào các cái biến X của mình Và công thức này thì nó có một cái ý nghĩa khác, đó là chúng ta sẽ tìm cách để tối thiểu hóa cái Mean của hai phân phối Lý do đó là vì hai cái phân phối này có độ lệch giống nhau Hai cái phân bố này nó có độ lệch giống nhau Tại vì nó chỉ phụ thuộc vào T Nó chỉ là một cái biến phụ thuộc vào T hoặc chính xác hơn là phụ thuộc vào các cái alpha T Do đó thì hai cái này là khớp rồi Cái độ rộng của cái phân bố này là khớp rồi Giờ chỉ là làm sao để cái tâm của hai cái hình cầu, tâm của hai cái phân bố này là về khớp lại với nhau thôi Do đó thì cái việc này nó tương đương với việc chúng ta tối thiểu hóa Mean của hai phân phối Một xanh là công thức này và một nâu là công thức này Trong đó, cái Mean của cái phân phối P theta thì đó là một cái hàm tham số hóa bởi Phi Tức là nói cách khác, chúng ta sẽ xây dựng một cái hàm để ước lượng cái mu này Ước lượng cái mu này từ XT trước đó và cái giá trị T Vậy thì cái mô hình này thì chúng ta sẽ có hai cách, xin lỗi, có ba cách tính và diễn giải khác nhau Cái công thức trước đó nó sẽ được đưa về cái công thức này, tức là nó sẽ tương đương với việc chúng ta đi minimize hai cái phân bố Đây chính là hai cái Mean của hai phân phối Mean của cái phân phối Và chúng ta mong muốn hai cái phân phối này khớp với nhau, giống như trong cái nhận xét trước Vậy thì cái công thức của Mu-Q là nó sẽ có công thức như thế này Chúng ta hoàn toàn có thể tính được, chứng minh được cái công thức này nhưng mà nó sẽ hơi mất thời gian Chúng ta chỉ ghi cái kết quả cuối cùng thôi ha Thì cái Mu-Q nó sẽ có cái công thức như trên Và chúng ta sẽ có ba cách để chúng ta thực hiện cái việc mà tối ưu cái công thức này Cách thứ nhất đó là chúng ta sẽ đưa về cái Mu-Q Mu của theta x t là bằng cái công thức này Và khi đó chúng ta lấy cái Mu-Q trừ cho Mu theta, hai cái công thức này trừ cho nhau Thì chúng ta thấy là cái thành phần này loại bỏ Và cái thành phần này ở trên thì chúng ta sẽ xem như là hằng số Do đó thì chúng ta chỉ việc tối ưu sao cho cái x theta mũ x t t xấp xỉ với x0 Thế thì cái công thức này nói một cách khác Đó là chúng ta đang làm sao mà cái mô hình của mình có khả năng khôi phục được ảnh gốc từ mỗi step Lưu ý là trong cái công thức này chúng ta được tính tổng trên nhiều step chứ không phải chỉ tại một thời điểm T sẽ chạy từ 2 cho đến t lớn Và ở đây chính là cái ảnh mà mình khôi phục được Ảnh gốc, ảnh ban đầu khôi phục được Tức là chúng ta luôn mong muốn khôi phục lại cái ảnh ban đầu Và cái x mũ theta này nó phải xấp xỉ với x0 Đây là route root nè Đây là route root để mà chúng ta phải huấn luyện để mà bắt chước cái x0 này Thì đây là cái cách số 1 và cái cách số 1 cũng tương đương với lại cái cách số 2 Tức là nó chỉ là cái cách cách để mà biểu diễn khác nhau thôi Thì cái công thức mu của Q nó cũng có thể biểu diễn dưới dạng là một phần alpha t x t Nhân cho cái công thức này Với cái đại lượng epsilon này là tuân theo cái phân bố Gaussian 0 1 Và xt thì trong những slide trước chúng ta đã có cái công thức tính xt từ x0 và epsilon rồi Do đó chúng ta sẽ tính từ cái công thức này chúng ta sẽ suy ra được công thức của epsilon Thì epsilon là bằng cái công thức này Thì từ đó chúng ta sẽ ra được mu của theta xtt là bằng cái công thức này Và khi chúng ta lấy 2 cái hiệu số này chúng ta trừ cho nhau thì nó khử Và cái thành phần này là hằng số do đó thì nó sẽ tương đương với cái việc epsilon theta xt t trừ cho epsilon Hay nói cách khác đó là cái mô hình này là chúng ta dự đoán cái nhiễu của từng step Chúng ta đi dự đoán nhiễu của từng step Với step chạy từ 2 cho đến t thì làm sao cho cái nhiễu này là nhỏ nhất Với step chạy từ 2 cho đến t thì làm sao cho cái nhiễu này là nhỏ nhất Và công thức cách thức làm số 3 đó là chúng ta đi dự đoán cái vector gradient của logpt Vector gradient của logpt này hình ảnh nói hiểu một cách nôm na đó chính là cái hướng Vector hướng gradient là thể hiện hướng mà Thì cái hướng để mà hướng đến cái phân bố của cái ảnh xt Hướng đến cái phân bố của cái xt của mình Rồi thì cái s theta t xt này nó sẽ tìm cách là cực tiểu hóa hay nói cách khác là dự đoán được cái hướng này Thì tương tự như vậy chúng ta có 2 cái công thức này và khi chúng ta trừ cho nhau Thì nó sẽ 2 cái thành phần này là hằng số thì nó sẽ đưa về cái công thức này Thì đây là cái cách làm số 3 và cái công thức này nó gọi là score function Rồi, vậy thì bản chất của 3 cái cách này đó là giống nhau và chúng ta thực hiện theo cái cách số 1 Hay là chúng ta làm theo cái cách số 2 thì cũng giống nhau Cách số 1 đó là chúng ta tìm cách để tính cái L2 Tức là cái sai số giữa cái hàm dự đoán cái ảnh so với lại cái x0 ban đầu Còn đối với cái công thức của, xin lỗi trong cái công thức này thì nó để nhầm Nó không phải là mu theta xt mà nó sẽ là x mũ theta xt 1 Rồi, thì chúng ta mong muốn là cái x mũ theta này là khớp với lại cái x0 ban đầu Còn trong cái công thức của cái cách số 2 đó là qua cái hàm, qua cái mô hình có cái theta ở đây Vậy thì các bạn có thể nhận ra cái nhiễu này, thì chúng ta sẽ dự đoán được cái nhiễu Và cái nhiễu này thì chúng ta tính cái sai số với lại cái nhiễu ban đầu này của mình Tức là yêu cầu cái mô hình dự đoán được cái nhiễu mà chúng ta đã thêm vào trước đó Và cách làm số 3 đó là chúng ta đi dự đoán cái hướng để cho cái mô hình của mình dịch chuyển vào Vậy để khái quát hóa các cái cách làm của khác nhau thì chúng ta sẽ dùng cái không gian latent như sau Một cái không gian này chính là cái phân bố Pdata của mình Và x0 đây chính là cái ảnh mà chúng ta đã lấy mẫu được Sau đó thì chúng ta nhân với lại căn của alpha t x0 thì đây chính là cái mean của cái xt Nhưng mà chưa có cái xt tại vì chúng ta sẽ phải xác định dựa trên cái variance nữa Và cái variance giả sử như cái epsilon của mình là sampling là ở đây theo phân bố Gauss Thế thì chúng ta sẽ nhân với lại căn của 1 trừ alpha t thì nó sẽ ra cái vector màu đỏ này Và sau đó chúng ta lấy cái vector màu đỏ này đem qua đây Thì chúng ta sẽ ra được cái xt của mình Đem ra, tính ra được cái xt của mình Như vậy thì cái xt nó sẽ là bằng căn của alpha t x0 cộng cho căn của 1 trừ alpha t epsilon thì nó là nằm ở đây Và bây giờ nhiệm vụ của chúng ta ở đây là cái quá trình encode Bây giờ chúng ta decode để làm sao cho từ cái xt này có thể đi được trở về cái xt x0 Vậy thì trong cái quá trình huấn luyện thì chúng ta sử dụng cái cách số 1 Cách số 1 của mình đó là gì? Là chúng ta sẽ đi ước lượng cái xtt hay cái khác đó là đang đi dự đoán ảnh gốc ban đầu x0 Và chúng ta thấy là cái điểm này nó mong muốn là làm sao cho 2 cái này là khoảng cách nhỏ nhất Với cái cách làm số 1 thì cái mu theta xtt nó sẽ có cái công thức này Trong đó cái thành phần x0 xtheta mũ này tức là cái giá trị mà chúng ta dự đoán Và khi chúng ta có được cái xt theta 0 này rồi thì chúng ta sẽ dịch chuyển Chúng ta sẽ dịch chuyển cái xt đi theo cái hướng này thì đây sẽ là cái xt trừ 1 Đi về cái phân bố của xt trừ 1 Đối với cái cách làm số 2 thì chúng ta sẽ đi dự đoán nhiễu dựa trên cái công thức này Thì chúng ta sẽ có cái nhiễu dự đoán và chúng ta kỳ vọng là cái nhiễu dự đoán của mình khớp với cái nhiễu thực tế Thì khi mà chúng ta xác định được cái nhiễu đó thì chúng ta cũng sẽ xác định được cái phân bố của mình là P của xt trừ 1 Và cái cách làm số 3 đó là chúng ta sẽ xác định dựa trên cái gradient Thế thì cái gradient của mình nó sẽ được ước lượng bởi cái công thức của xt theta Và chúng ta mong muốn là cái này nó sẽ xấp xỉ Cái xt theta này nó sẽ xấp xỉ với lại cái nabla của cái log của mình, p theta Mong muốn nó xấp xỉ Thế thì khi chúng ta tính ra được cái xt theta rồi thì xt của mình sẽ được dịch chuyển về hướng này Dịch chuyển đi về cái hướng này dựa trên cái công thức này Vậy thì tóm lại cho dù chúng ta làm theo cách nào đi chăng nữa thì nó cũng hoàn toàn tương đương nhau Đó là chúng ta đang dịch chuyển từ cái xt về cái xt trừ 1, sau đó từ xt trừ 1 về cái xt trừ 2 Cứ như vậy, kéo cho đến khi nào mà tiến về cái x0, sao cho nó khớp với lại cái x0 nhất Thì đó chính là cái cách thức mà cái mô hình của mình huấn luyện De-noise và mục tiêu của mình đó là làm sao tìm các cái tham số theta để cho 3 cái mục tiêu trên, đó là thỏa mãn Một đó là cái gradient để hướng đến cái phân bố là khớp nhất hoặc là dự đoán được cái nhiễu là chính xác nhất hoặc là chúng ta khôi phục lại được cái ảnh gốc giống với lại cái x0 nhất, thì đó là 3 cái cách khác nhau Thì trên đây chúng ta đã cùng tìm hiểu qua 3 cái cách thức tương đương để mà có thể huấn luyện được cái mô hình diffusion Hy vọng là các bạn có thể hình dung được cái cách thức mà mô hình nó vận hành qua cái bước gọi là encoding Và encoding thì chúng ta sẽ không có tham số nhưng mà decode, khi chúng ta decode ngược lại thì chúng ta sẽ đi tìm cái theta này Sao cho cái việc xấp xỉ, đó có 3 cái cách để xấp xỉ, cái cách đầu tiên đó là xấp xỉ nhân noise Cách thứ 2 đó là xấp xỉ theo cái x0 Cách đầu tiên là xấp xỉ theo nhân noise Cách thứ 2 đó là xấp xỉ theo x0 Cách thứ 3 đó là xấp xỉ theo cái log của cái p Theta, tức là cái hướng đi của mình, nó xấp xỉ Trong những phần tiếp theo thì chúng ta sẽ cùng tìm hiểu về những cái chủ đề mở rộng của cái mô hình diffusion liên quan đến cái việc là điều hướng liên quan đến việc tăng độ phân giải, tốc độ huấn luyện', 'Cho đến thời điểm này, mạng RNN, cho dù với các biến thể như Deep Stack, hay Bi-directional thì nó vẫn còn bị một cái điểm yếu rất là lớn đó chính là vấn đề về điểm nghẽn thông tin, hay còn gọi là Bottleneck Thì đây là cái hiện tượng gì? Chúng ta sẽ cùng xem một vài cái ví dụ để minh họa Trên đây, đó là một cái kiến trúc ANN được sử dụng để encode một cái câu văn bản nguồn Và decode ANN thì dùng để dịch sang một cái văn bản đích Ví dụ như đây là chúng ta dịch từ tiếng Anh sang tiếng Pháp Thế thì cái điểm nghẽn thông tin nó sẽ thể hiện ở cái bottleneck này Tại vị trí cuối cùng mà chúng ta encode câu văn bản nguồn thì mọi thông tin của văn bản nguồn đều được chứa trong duy nhất một vector này. Và đây chính là cái điểm nghẽn của mình, khiến cho mô hình của mình không đủ thông tin toàn cục và thông tin cần thiết tại một thời điểm để chúng ta dịch ra ví dụ tại cái vector này, chúng ta muốn dịch ra cái từ rơ thì có thể cái thông tin của cái từ i là cái từ quan trọng nhất để giúp chúng ta dịch cái từ rơ này nè thì cái hàm lượng thông tin của nó đã bị mai một rất là nhiều rồi vì từ i khi đến được cái điểm nghẽn này nè thì nó đã bị biến đổi một lần, hai lần, ba lần, bốn lần và năm lần và đây là một cái ví dụ rất là bé trong những cái bài toán phức tạp hơn ví dụ như là bài toán tóm tắt văn bản thì để kể từ lúc chúng ta đọc hết cái văn bản cho đến lúc chúng ta bắt đầu tóm tắt nó có thể lên đến hàng trăm hoặc thậm chí là hàng ngàn chữ hàng ngàn cái token thì dẫn đến là cái từ ở đầu tiên nó đến cái chỗ điểm nghẽn này nè nó đã bị quên từ ai đã bị quên nhiều mình nói nó quên hết thì cũng không đúng, tại vì có thể chúng ta sử dụng một số biến thể như LSTM để giúp chúng ta lưu được những thông tin quan trọng nhưng mà đâu đó nó vẫn sẽ bị quên quên đi, một phần, quên nhiều khi từ ai này bị quên nhiều, đến lúc chúng ta cần dịch ra từ r, chúng ta bắt đầu từ start là bắt đầu quá trình decode thì khả năng chúng ta ra từ r này là thấp trong khi đó nếu chúng ta bắt đầu dịch, mà sau khi chúng ta vừa đọc xong từ ai thì xác suất chúng ta dịch ra được từ r, từ r trong tiếng Pháp là từ ai của mình, thì nó sẽ cao hơn đó chính là mô tả cho hiện tượng là điểm nghẽn thông tin khi chúng ta dự đoán đến từ xua thì rõ ràng là thông tin của từ xua ban đầu ở đây khi chúng ta lan truyền đến từ xua này cũng hoàn toàn tương tự, nó đã bị mất thông tin rất nhiều rồi Do đó, điểm nghẽn thông tin này cần phải giải quyết. Cơ chế để giải quyết là cơ chế Attention.', 'Do đó nó sẽ chia cái không gian của mình ra. Chia cái không gian của mình ra. Ví dụ vậy thành 4 phần và với mỗi một cái điểm thì nó sẽ thuộc vào một phần chứ nó không có thuộc vào hai cái phần. Gây ra cái sự nhập nhằng giống như trong cái mô hình logistic regression.', 'Và vấn đề của momentum đó là gì? Đối với những khu vực có độ dốc bất thường thì ví dụ như chúng ta thấy trong sơ đồ này Cái phác thảo của HempLoss, chúng ta thấy là cái độ dốc của mình tăng lên, nó dốc xuống rất là cao, sau đó lập tức nó đi ngang, rồi sau đó nó lại lập tức đi lên Tức là nó thay đổi trạng thái, đi xuống, đi ngang và sau đó là đi lên một cách rất là ngắn như vậy, thì đó chính là cái độ dốc bất thường và nó sẽ khiến cho nó có rất nhiều những cái dao động mà bật qua bật lại, bật qua bật lại giữa hai cái thành dốc này. Cụ thể hơn, đó là chúng ta xét tại một cái điểm ở đây. Thì nếu như cái thành phần mà theo cái theta 1 của mình, mà đạo hàm của mình nó lớn, tức là cái giá trị độ lớn này, nó lớn hơn so với lại cái thành phần theo theta 2, khi chúng ta tổng hợp lại là cái vector tổng hợp thì nó sẽ bị thiên lệch về phía có thành phần radian lớn tức là cụ thể đây là theta 1 khi đó là nó sẽ không có cân bằng mà nó sẽ bật qua bên tay phải sau đó tại vị trí này chúng ta lại tiếp tục tính cái radian và chúng ta lại thấy cái hiện tượng mất cân bằng này lập lại thì khi chúng ta cập nhật thì nó cũng sẽ bị thiên lệch về cái phía mà có cái thành phần lớn hơn, đó chính là thành phần theta 1 tức là một khi cái đạo hàm của mình theo thành phần, một cái thành phần nào đó mà lớn thì nó sẽ gây ra cái hiện tượng là bật qua bật lại trong khi đó cái đường tối ưu, ideal path thì lẽ ra nó phải là đường đường màu xanh ở đây, nó sẽ phải đi theo cái trục này Nó phải đi theo cái trục này, hay là đi theo cái trục của theo cái hướng của theta, theta2 Thì ở đây nó lại cứ bật qua trái, bật qua phải Thì bây giờ chúng ta sẽ cải tiến cái momentum bằng cách nào Thì muốn cải tiến thì chúng ta sẽ phải xem cái nguyên nhân của nó là gì Nguyên nhân đó là vì khi chúng ta có hai cái thành phần radian theo theta1 và theta2 Nếu thành phần nào đó lớn thì lẽ ra chúng ta sẽ phải giảm learning rate của nó xuống Nguyên nhân đó là do chúng ta dùng chung learning rate alpha dùng cho g, g là đạo hàm của J theo theta 1 và đạo hàm của J theo theta 2 một cách tổng quát thì nó có thể là có theta 3 theta n thì alpha a đang dùng chung cho radian theo theta 1 và radian theo theta 2 Bây giờ chúng ta mong muốn mỗi thành phần này sẽ có 1 cái alpha riêng Và nó sẽ giúp chúng ta cân bằng lại, đó chính là ý tưởng của cải tiến adaptive learning rate Giảm dao động dựa trên độ lớn của radian cập nhật gần đây, chúng ta giảm sự dao động đó Ý tưởng chính đó là, giả sử chúng ta có vector g là bằng hai thành phần, là g1 và g2 Nếu như chúng ta lấy alpha nhân với g, thì alpha mà dùng chung Thì không, bây giờ chúng ta sẽ dùng riêng, mỗi cái thành phần này sẽ có một cái alpha riêng trong đó, ở đây là ý tưởng đầu tiên là tách riêng ha, tách learning rate cho từng tham số và cái ý tưởng tiếp theo, đó là cái radian mà lớn thì learning rate nó sẽ nhỏ tức là nếu cái thành phần theo G1 mà lớn, G2 mà nhỏ, ví dụ vậy thì chúng ta sẽ có cái hệ số alpha 1 cân bằng ngược trở lại nó cân bằng ngược trở lại và thành phần G2 mà nhỏ thì chúng ta sẽ có cái Alpha 2 và khi đó thì cái vector tổng hợp của mình nó sẽ đều hơn thay vì là nó bị thiên lệch như thế này Đường màu đỏ là đường mà nó bị thiên lệch. Nó bị lệch về phía theta, phía theta 1. Rồi, bằng ngược lại thì thành phần theta 2 nhờ có cái alpha 2 lớn, nó sẽ kéo ra để cho nó cân bằng cả hai hướng. Và chi tiết thuật toán Root Mean Square Propagation, đó là chúng ta sẽ khởi tạo với cái alpha là bằng 0.01 và beta thì đây là cái hệ số decay rate tức là cái hệ số mà để cập nhật cho cái momentum thì chúng ta nhìn một cái công thức của cái thuật toán root mean square là V tức là cái vận tốc cập nhật tham số của mình là cái thành phần ở phía trên mà chúng ta khoanh vùng ở đây Cách làm cũ của alpha là theta trừ alpha nhân cho nabla J Alpha là đây và đạo hàm radian là G Cách làm cũ là đạo hàm quá lớn, thành phần radian lớn sẽ lớn hơn những thành phần còn lại R là momentum để phục vụ chuẩn hóa learning rate Chuẩn hóa Learning Rate, tức là thành phần nào của radian này mà lớn thì cái alpha của nó sẽ nhỏ và thành phần nào mà của radian này mà nhỏ thì cái alpha của mình nó sẽ lớn Đó, thì cái cách làm của chúng ta là như vậy R này nó là một cái vector, chúng ta thấy là R được in đậm, nó là một cái vector thì khi alpha chia cho căn của một vector thì nó sẽ biến thành một vector thì chút nữa chúng ta sẽ ghi rõ hơn cái công thức của nó Đây là hệ số tỷ lệ tùy chỉnh, tức là Adaptive Scaling Factor Với mỗi thành phần của G, chúng ta sẽ có một cái alpha riêng Tại sao nó là như vậy? Tại sao alpha chỉ có một cái giá trị scalar là một giá trị? Tại sao khi chia cho căn R nó lại biến thành hệ số tùy chỉnh? Chút nữa chúng ta sẽ chứng minh chi tiết hơn Mỗi tham số sẽ có một cái hệ số tỷ lệ khác nhau và ở đây chúng ta sẽ thấy là nó có một cái thành phần hơi lạ đó là epsilon là bằng một cái con số rất là bé thì ở đây đó là để chống cho cái việc là có một cái thành phần r nào đó mà bằng 0 có một cái thành phần trong r nào đó mà bằng 0 thì khi đó chúng ta sẽ bị cái lỗi là division by zero chia cho xuống 0 thì chúng ta sẽ cộng thêm epsilon để chống cái hiện tượng đó Và phép toán mà r chia cho căn của epsilon cộng r được thực hiện trên từng phần tử Bây giờ chúng ta sẽ cùng xem xét Giả sử như g là bằng 2 thành phần là g1 và g2 Rồi, khi đó g-r là phép tích Hadamard trên từng phần tử d-r-g là g1 bình phương, g2 bình phương sau đó nó sẽ được cộng với thành phần chuẩn hóa ở phía quá khứ đây là beta của quá khứ, tức là 90% thành phần r chuẩn hóa R là thành phần để chúng ta chuẩn hóa learning rate theo kiểu đạo hàm thành phần nào mà càng nhỏ thì learning rate sẽ càng nhỏ Trong công thức thành phần chuẩn hóa này chúng ta sẽ chia cho cái căn, thế thì tại sao nó lại có cái căn Nếu chúng ta lại bỏ đi thành phần epsilon đây để cho nó dễ hình dung epsilon này chỉ mang tính chất đó là chống cái việc chia cho 0 thôi còn nó sẽ không có nhiều ý nghĩa lắm trong việc chuẩn hóa thì khi chúng ta lấy alpha mà chia cho căn của epsilon cộng cho r thì nó sẽ xấp xỉ tại vì có cái epsilon này nên mình mới để cái dấu xấp xỉ đó là alpha chia cho căn của vector g1 bình g2 bình cộng cho thành phần quá khứ nhưng tại thời điểm ban đầu chúng ta thấy là r bằng 0 nên xem như chúng ta tạm bỏ qua thành phần này để chúng ta dễ hình dung khái niệm của việc chuẩn hóa này là gì thì là g1 bình phương và g2 bình phương Thì ở đây là chúng ta thực hiện phép căn là phép căn trên ElementWise do đó nó sẽ là căn của các thành phần bên trong như vậy thì nó sẽ là trị tuyệt đối của G1 và trị tuyệt đối của G2 Rồi, và vì đây là cái phép chia trên phép áp dụng trên từng phần tử do đó thì cái này nó sẽ là bằng một cái vector trong đó alpha chia cho trị tuyệt đối của G1 và α chia cho trị tuyệt đối của G2 như vậy thì chúng ta nhìn lại nếu như G1 của chúng ta mà lớn nếu G1 mà lớn thì khi đó α chia cho G1 đó chính là tương ứng cái α1 của mình đề cập trong slide trước thì α1 là bằng α chia cho trị tuyệt đối G1 G1 lớn thì trị tuyệt đối của G1 sẽ lớn và 1 phần trị tuyệt đối của G1 sẽ nhỏ do đó thằng này sẽ là nhỏ Ngược lại, nếu như G2 mà nhỏ thì khi đó là 1 phần trị tuyệt đối của G2 sẽ lớn do đó thành phần này sẽ lớn như vậy là nó đáp ứng được yêu cầu mà chúng ta đã thiết kế ban đầu thành phần gradient nào mà lớn thì learning rate sẽ nhỏ và ngược lại Thì khi đó, đây chính là thành phần chấp chơi cho chúng ta chuẩn hóa Thì khi đó cái công thức V này của mình sẽ biến thành là V là bằng alpha 1, tức là alpha chia cho trị tuyệt đối của G1 nhân với lại G1 Thành phần thứ 2 sẽ là alpha chia cho trị tuyệt đối của G2 Ngoài việc chia chuẩn hóa này sẽ khiến cho 2 thành phần gradient cân bằng hơn Đây là ý tưởng để giúp chúng ta thoát ra khỏi vấn đề thiên lệch thì chúng ta xét trong hình ở bên tay phải ở đây chúng ta thấy là tại vị trí A thì nó bị thiên lệch về hướng theta 1 nhưng mà nhờ có cái thành phần alpha 1 và alpha 2 được chuẩn hóa ở đây đây là alpha 1, đây là alpha 2 thì khi đó Khi đó, vector tổng hợp của mình thay vì nó bị lệch về phía này thì nó sẽ đi đều hơn. Và khi đó thì chúng ta sẽ vẽ lại.'] | Mạng Perceptron chỉ có khả năng giải quyết các bài toán tuyến tính. Một ví dụ về bài toán phức tạp mà Perceptron không thể giải quyết là phân loại hai tập điểm nằm trong và nằm ngoài một vòng tròn, vì điều này đòi hỏi một sự phân chia phi tuyến tính. Để giải quyết các bài toán phi tuyến tính, cần có sự kết hợp của nhiều đường thẳng hoặc sử dụng các kiến trúc mạng neural phức tạp hơn với các lớp ẩn.                  | Chúng ta lấy một cái tình huống ví dụ đó là hai cái tập điểm nằm trong và nằm ngoài vòng tròn.                                                                                             |       1        |           0.965013 |                 nan |                1 |\n",
      "| Mục tiêu của việc phát triển mô hình là gì?                                   | ['Các môn trước đây, nếu chúng ta tìm cách để liên kết giữa hình ảnh và văn bản, thì Visual Programming đi theo một phương cách hoàn toàn khác. Vì Visual Programming là gì? Chúng ta sẽ cùng tìm hiểu trong phần tiếp theo. Đầu tiên là phát triển bài toán. Mục tiêu của chúng ta là hướng đến để phát triển một mô hình có khả năng thực hiện được nhiều task khác nhau, hay gọi là General Purpose AI. Cách tiếp cận thông thường của chúng ta là 1.', 'Chúng ta sẽ cùng đến với một mô hình tạo sinh đầu tiên, đó là mô hình Autoencoder hay còn gọi là bộ tự mã hóa. Nền tảng của mô hình này là dựa trên hướng tiếp cận là học, không giám sát. Mục tiêu đó là làm sao chúng ta sẽ biểu diễn được đặc trưng của dữ liệu gốc ban đầu. Ví dụ như đầu vào của chúng ta là ảnh của một con số viết tay. Chúng ta sẽ tìm cách biểu diễn ảnh này thành một vector đặc trưng, trong đó nó có chiều thấp hơn. Giả sử như ảnh của chữ số viết tay có kích thước 28 x 28, đây là kích thước chuẩn trong tập dữ liệu MNIST.', 'Trên đây là bài giảng về quá trình tiến hóa của các mô hình học sâu.', 'Ở đây là công thức của sinh ảnh không có điều kiện. Đối với sinh ảnh mà có điều kiện, chúng ta sẽ thêm vô một biến nữa là biến y. Đây chính là conditional signal. Đây là conditional signal. Sau này khi tổng quát lên, y này không nhất thiết phải là văn bản. nó có thể là một cái mask, nó có thể là một cái điểm v.v. thì công thức của chúng ta thay vì là S theta của Xt, t thì chúng ta sẽ thêm cái thành phần là y vào đây và khi đó thì cái St này sẽ xấp xỉ với lại cái gradient của log p Xt cho trước y Đây là conditional score Công thức trước là unconditional score Bây giờ chúng ta sẽ chuyển sang conditional score chúng ta sẽ đưa vô một xác suất có điều kiện là i Chứ cho trước i, thì xác suất để tìm ra xt khi cho trước i là bao nhiêu? Chúng ta sẽ triển khai Và dựa trên định lý Bayes thì công thức này xuất phát từ triển khai như sau đó là cái log của Px t cho trước y thì nó sẽ là bằng log của Pxt cho trước y thì nó sẽ là bằng Pxt nhân với lại Pi cho trước xt tất cả chia cho Pi Với công thức này, chúng ta sẽ triển khai ra và có được là bằng đạo hàm của log của PxT Nhân thì chúng ta sẽ đưa về dấu cộng, đó là cộng cho log của Py cho trước xT Chia thì chúng ta sẽ chuyển thành là dấu trừ cho log của Pi Với công thức này, chúng ta thấy là vì chúng ta đang muốn tính đạo hàm theo XT chúng ta đang tính đạo hàm theo XT Đây là đạo hàm theo XT Trong con mắt của XT, thì y của mình là hằng số Do đó chúng ta sẽ loại bỏ đi thành phần này đi Tại vì đạo hàm của một cái hằng số đối với xt thì nó sẽ là bằng 0 Do đó thì công thức này sẽ đưa về công thức ở trên Đó là log của pxt cộng cho log của pi cho trước xt Và với cái công thức này thì chúng ta sẽ thấy là cái xt của mình Khi chúng ta khôi phục, chúng ta decode Bình thường nó sẽ đi theo con đường này là Unconditioned là màu xanh lá Màu xanh lá tương ứng cho Unconditioned Bình thường nó sẽ đi theo đường màu xanh dương Và qua màu xanh lá thì chúng ta sẽ điều hướng đi qua mũi tên màu xanh và cộng 2 cái đó lại thì nó sẽ ra cái mũi tên màu cam. Thế thì bình thường là chúng ta sẽ đi theo cái con đường này. Nhờ có cái vector gradient của log y cho trước xt, nó bẻ lái để biến thành cái vector màu cam này. Chúng ta lưu ý ở đây nó sẽ có thêm một cái hệ số nữa, nó gọi là classifier guidance. Nếu như trong công thức chúng ta biến đổi ở phía trước là chúng ta không có cái gamma ở đây thì hàm ý đó là một cái Unconditional Score nó sẽ kết hợp với một cái Adversarial Gradient tức là cái vector điều hướng theo tỷ lệ đó là 1,1 nhưng mà chúng ta muốn nó nhanh điều hướng thì chúng ta sẽ tăng cái hệ số tỷ lệ đó lên hoặc là chúng ta muốn chậm lại thì chúng ta sẽ giảm cái hệ số tỷ lệ đó xuống Như vậy trong công thức này, gamma sẽ là hệ số để giảm tốc độ điều hướng của mình Vector màu cam sẽ là tổng hợp của vector màu đỏ trong điều kiện là Unconditional Kết hợp với adversarial thì nó sẽ đưa ra, bẻ cái hướng, thay vì chúng ta đi theo hướng này để mà tìm được đến đây thì bây giờ nó bẻ hướng lại, nó sẽ đi theo cái hướng này để đến cái ảnh mà có cái điều kiện giống với lại cái Y của mình thì đó chính là cái Classifier Guidance thế thì cái mô hình này sẽ được thực hiện như thế nào Trước tiên chúng ta sẽ nói về vai trò của Classifier Guidance, tức là gamma hồi nãy của mình Nếu chúng ta chọn gamma là bằng một, tức là dùng công thức gốc ban đầu Thì kết quả của mình sẽ không được điều hướng đủ tốt và đủ nhanh Dẫn đến là nó sẽ tạo ra những hình thù không có thật Tại sao không có thật? Tại vì nó vừa pha trộn của một cái ảnh, của một đối tượng có một đối tượng thật mà lẽ ra với vector Z tạo ra tạo ra cái x0 khi có sự tham gia của gamma vào thì gamma này nó bẻ lái nhưng nó bẻ chưa đủ nhanh dẫn đến đó là kết quả của mình nó tạo ra đối tượng lai lai ở giữa đây là cái x0 còn đây là cái x0 mới thì lẽ ra là chúng ta hướng đến chỗ này nhưng mà cái gamma của chúng ta chưa đủ nên thay vì là nó bẻ lái bình thường là đến đây đúng không thì nó sẽ bẻ lái đến giữa chừng và ở cái khúc giữa chừng này thì nó tạo ra những tấm ảnh như thế này trong khi đó nếu chúng ta cho classifier guidance tức là cái gamma lớn hơn ví dụ như gamma trong trường hợp này bằng 10 thì nó sẽ bẻ lái mạnh hơn để mà nó đến được đến cái xnew giống với lại cái nội dung mà chúng ta mong muốn đó là Pembroke Welsh Corgi Đây là một cái giống chó rất là hiếm Vậy thì quá trình sinh ở trên là quá trình sinh có điều kiện và nó có một cái Classifier Guidance Vậy thì chúng ta sẽ huấn luyện cái mô hình này như thế nào Thì cái cách thức huấn luyện đó là chúng ta sẽ có thêm một cái module chúng ta sẽ có thêm một cái mạng nữa, nó gọi là một cái Classifier hay còn gọi là Off-the-shelf Classifier Và cứ với mỗi cái i mà chúng ta đưa vào thì chúng ta sẽ đi huấn luyện cho một cái classifier như vậy là một cái i sẽ có một cái classifier riêng Và như vậy thì nó sẽ khiến cho cái mô hình của mình nó không có tính linh động Nó không có tính linh động vì khi chúng ta muốn tạo ra một cái đối tượng mới Bình thường chúng ta tạo ra 2 con mèo đeo kính, bây giờ chúng ta muốn tạo ra 1 con chó Welsh Corgi đeo kính chẳng hạn thì lúc đó chúng ta sẽ phải train 1 cái classifier mới cho cái y đó thì nó sẽ khiến cho cái mô hình của mình nó chạy không có tính thực tiễn cao do đó thì chúng ta sẽ chuyển sang 1 cái mô hình, nó gọi là cái mô hình mà sinh có điều kiện nhưng mà với Classifier Free Guidance, tức là không có classifier Vậy thì công thức của mình sẽ được sửa lại đó là bằng 1 trừ gamma nhân cho log của PXT thì đây là cái Unconditional Score kết hợp với Conditional Score và ở đây chúng ta sẽ huấn luyện trên chính mô hình Diffusion của mình luôn Đây chính là U-Net trong diffusion. U-Net trong diffusion này chúng ta sẽ huấn luyện bằng cách đưa 2 tình huống. Tình huống thứ nhất là chúng ta sẽ đưa một vector rỗng vào. Mục tiêu của mình tương đương như một mô hình sinh ảnh nhưng mà không có điều hướng. và chúng ta sẽ đưa y vào, thì y này sẽ là mẫu dữ liệu huấn luyện của chúng ta và y này sẽ cho trước một số mẫu condition mà chúng ta muốn huấn luyện để từ đó nó sẽ estimate ra cái x, xt, t, y Thứ nhất là chúng ta sẽ không có thêm, không có classifier mà chúng ta sẽ huấn luyện trên chính cái mô hình của diffusion của mình luôn trên chính cái decoder của mình luôn và khi chúng ta huấn luyện trên cái decoder này thì chúng ta sẽ có hai tình huống một đó là chúng ta sẽ truyền vô một cái condition là rỗng đây là một cái condition rỗng Mục tiêu của nó là tạo ra tấm ảnh không có cần điều hướng và đưa vào 1 condition trong data set của mình để chuẩn bị trước, đó chính là y. Mục tiêu của mình là điều hướng đến cái này là không điều hướng, còn cái này là có điều hướng. sau khi chúng ta huấn luyện xong, chúng ta cứ sử dụng decoder này để đưa y vào và nó sẽ tạo sinh ra mô hình của mình thì cái sơ đồ này sẽ tương tự như nó sẽ lấy từ mô hình mà chúng ta đã học trong những slide trước bình thường là chúng ta chỉ đưa vào xt và t, bây giờ chúng ta sẽ đưa vào thêm y nữa để làm được việc này thì chúng ta có thể sử dụng các mô hình của Transformer với attention sử dụng key-value của attention để điều hướng thì cái i này có thể là query', 'Chúng ta sẽ cùng đến với một chủ đề rất là hot trong thời gian gần đây, đó là mô hình ngôn ngữ thị giác. Xuất phát điểm ý tưởng của mô hình ngôn ngữ thị giác đó là các bài toán của chúng ta hiện nay đòi hỏi có sự tham gia của nhiều thể thức dữ liệu hay còn gọi là multimodality model. Mô hình ngôn ngữ thị giác có thể gồm các modalities là văn bản, hình ảnh, âm thanh và video. Thế thì tại sao các mô hình của mình có sự tham gia của các loại thể thức dữ liệu khác nhau, đó là do nhu cầu ứng dụng của mình. Ví dụ như chúng ta hiện nay chúng ta thấy các ứng dụng của AI hiện giờ là cho phép chúng ta có thể điều khiển hoặc là giao tiếp bằng giọng nói. Nhưng mà ứng với cái giọng nói đó thì chúng ta có thể tạo ra các hình ảnh hoặc là chúng ta có thể trò chuyện với hình ảnh.', 'Nhưng cái đó chưa phải là một cái tiêu chí duy nhất mà chúng ta sẽ còn rất nhiều những cái tiêu chí khác. Mình có thể kể một vài cái tiêu chí ví dụ như là nó có thể hoạt động tốt khi chúng ta làm việc hoạt động tốt. Với dữ liệu mà mất cân bằng. Tức là cái y này của mình nó sẽ có nhiều class ví dụ vậy và có những class thì xuất hiện rất nhiều nhưng có những cái class rất ít. Thì cái hàm lỗi này nó phải làm sao để cho hướng cái mô hình đến cái việc là kể cả những mẫu dữ liệu mà ít thì vẫn có thể được cho cái vai trò ngang bằng với lại những cái mẫu như nhiều. Rồi ngoài ra thì chúng ta sẽ có những cái tiêu chí nữa ví dụ như hàm lỗi như thế nào để cho cái việc huấn luyện nhanh, hội tụ, huấn luyện nhanh.', 'Và bây giờ chúng ta sẽ quay trở lại đến với cái chủ đề của chúng ta. Đó là mô hình tạo sinh hay còn gọi là generative model. Thì mục tiêu của cái mô hình tạo sinh đó là chúng ta sẽ lấy mẫu dữ liệu huấn luyện. Lấy mẫu dữ liệu huấn luyện đầu vào của một số cái phân bố, của một số cái phân phối. Và từ đó chúng ta có thể tái tạo lại các cái phân phối đó.', 'Thì đó chính là cái ý tưởng của thuật toán Visual Programming. Và kỹ thuật chính mà nó sử dụng ở đây chính là Prompt Engineering và In-context Learning, để cung cấp ngữ cảnh, cộng với cái ví dụ minh họa. Và cái mô hình sử dụng ở đây là GPT-3. Một cách tổng quát thì chúng ta có thể sử dụng các cái mô hình mà dạng decoder, các cái mô hình dạng decoder, bất kỳ, chứ không phải là GPT-3. Rồi, thì cái In-context Example, In-context Example nó sẽ là một cái ví dụ như đây, là chúng ta sẽ truyền vào một cái cặp Instruction Program. Thì đây giống như là những cái mẫu ví dụ để cho máy tính nó sẽ tự biết được là các cái thao tác cần phải thực hiện.', 'Chúng ta có thể dùng trong lĩnh vực về phân loại một cái đối tượng nào đó, bài toán hồi quy, dự đoán một cái giá trị có tính thứ tự nào đó. Rồi phát hiện đối tượng, phân đoạn ngữ nghĩa đối tượng v.v. thì học có giám sát nó đã đạt được những thành tựu hiện nay và có thể ứng dụng được rất là rộng rãi. Thế thì còn một cái mảng nữa đó chính là học không có giám sát. Thì học không có giám sát đó là dữ liệu đầu vào chúng ta sẽ không có y mà chúng ta chỉ có duy nhất một biến x đầu vào. Chúng ta không có nhãn, thì đây chính là sự khác biệt lớn nhất giữa học không giám sát và học có giám sát. Mục tiêu của học không giám sát đó là chúng ta sẽ học từ cái cấu trúc ẩn của dữ liệu, hay là chúng ta sẽ đi học cái phân bố của dữ liệu. Phân bố của dữ liệu. Rồi, thì mình có thể lấy một cái ví dụ như sau để chúng ta hiểu thế nào là chúng ta đi học một cái cấu trúc ẩn và học một cái phân bố dữ liệu. Bằng cách đó là chúng ta sẽ trả lời cho cái câu hỏi sau. Một bạn học sinh có điểm trung bình là ví dụ như là 8,8 điểm. Thì theo các bạn đó là bạn này sẽ có cái học lực là giỏi, khá xuất sắc hay là dưới trung bình. Thì đa số đó là chúng ta sẽ không biết được cái điểm này là cao hay thấp khi chúng ta không đặt nó ở trong cái phân bố của những cái bạn còn lại trong lớp. Thì nếu như cái điểm của lớp mình mà có cái phân bố như sau, điểm 8,8 thì nằm ở cái khu vực này. Ví dụ như điểm 8,8 là nằm ở khu vực này. Thì bạn này là một bạn có học lực giỏi tại vì nhìn trong cái phân bố này bạn nằm ở cái top mà những người có điểm cao. Nhưng ngược lại nếu trong một cái phân bố khác, điểm 8,8 của bạn nó lại nằm ở đây. 8,8 thì khi đó là với cái phân bố này thì điểm của bạn là có học lực dưới trung bình. Như vậy để kết luận được tính chất của dữ liệu x thì chúng ta phải học được cái phân bố. Vì vậy, trong cái phân bố chúng ta sẽ xác định được cái phân bố của dữ liệu và từ đó chúng ta hình thành được cái cấu trúc ẩn của dữ liệu. Ví dụ chúng ta chia cái không gian của mình ra làm 3 phần. Ví dụ vậy thì đây là những cái bạn mà có học lực kém. Đây là những bạn có học lực trung bình và đây là những bạn có học lực giỏi. Ví dụ vậy thì đây là chúng ta đang cấu trúc hóa cái không gian ẩn của mình. Và những cái bài toán mà kinh điển liên quan đến cái học không giám sát đó chính là bài toán phân cụm, bài toán giảm chiều dữ liệu. Thì đó là chúng ta đã cùng ôn lại một vài cái khái niệm về học có giám sát và học không có giám sát.', 'Nó đã tận dụng được rất nhiều thành tựu của các kiến trúc trước đó. Ví dụ như là Skip Connection, Ví dụ như là LayerNorm, Ví dụ như là cái optimizer là AdamW, Ví dụ như là StackLayer. Ví dụ như là Ví dụ như là TransGPT, Ví dụ như là Transformer. Và việc huấn luyện các mô hình mà Transformer hiện nay thì đã có rất nhiều những doanh nghiệp công ty, hoặc là tổ chức nghiên cứu lớn mà thường là tổ chức nghiên cứu ở bên trong doanh nghiệp lớn để tạo ra các mô hình cho cộng đồng có thể sử dụng, ví dụ như là DeepSix, Ví dụ như là Llama, v.v. Các mô hình này đã góp phần cho việc nghiên cứu các mô hình học sâu hiện đại ngày càng trở nên thuận tiện hơn.']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Mục tiêu chính của việc phát triển mô hình là hướng đến tạo ra một Trí tuệ nhân tạo đa năng (General Purpose AI), có khả năng thực hiện nhiều tác vụ khác nhau. Điều này khác biệt so với các phương pháp truyền thống thường tìm cách liên kết giữa hình ảnh và văn bản. Visual Programming được giới thiệu như một phương pháp tiếp cận hoàn toàn mới để đạt được mục tiêu phát triển AI đa năng này.                            | Mục tiêu của chúng ta là hướng đến để phát triển một mô hình có khả năng thực hiện được nhiều task khác nhau, hay gọi là General Purpose AI.                                               |       0.666667 |           0.884336 |                 nan |                1 |\n",
      "| Tại sao việc tự huấn luyện mô hình CLIP lại không khả thi?                    | ['Chúng ta sẽ cùng tìm hiểu về cách sử dụng một cái mô hình clip đã được tiền huấn luyện trên một bộ dữ liệu vô cùng lớn của OpenAI. Vì vậy, tự huấn luyện được môn clip là khá là không khả thi, vì nó cần lượng data cực kỳ lớn, tài nguyên tính toán, xử lý song song, GPU cũng cực kỳ mắc tiền. Vì vậy, khả duy nhất của chúng ta lúc này là chúng ta sẽ sử dụng một cái môn pre-drain để đi giải quyết những cái tác vụ đã có. Thế thì, đầu tiên chúng ta sẽ cài đặt một số cái thư viện cũng như là đưa cái môn clip, cái thư viện của clip về.', 'Thì đó là những cái yếu tố tác động đến cái mạng CNN đầu đời, mà khiến cho nó không có thể phát triển được. Thì cũng vì cái giai đoạn mà đầu đời, tức là trước những năm 2000, CPU là một cái tài nguyên không quá phổ biến và người ta cũng chưa có sử dụng nó, cũng như là chưa có nhiều thư viện để hỗ trợ lập trình với GPU. Nên giai đoạn này thì GPU chỉ dùng để chơi game chứ không có dùng để nghiên cứu. Thì đó chính là cái bối cảnh lịch sử trước khi mà AlexNet ra đời.', 'Và nó không có thực hiện cái phép nhân nhiều lần. Nhân đạo hàm nhiều lần.', 'Và nó sẽ dễ gây ra cái hiện tượng đó là lan truyền lỗi. Ví dụ điều gì xảy ra nếu như cái ảnh 64 x 64 này nó có những cái artifact, tức là những cái dấu hiệu để mà không có được đẹp và có những cái lỗi trong hình ảnh. Vì vậy thì nó sẽ lan truyền cái lỗi đó đến những cái ảnh sau mà không có cái cơ chế, nó không có cơ chế để sửa lỗi. Thế thì cái mô hình này cascade diffusion này thì nó sẽ không có phù hợp để mà có thể ứng dụng được. Lý do đó là vì thứ nhất là nó sẽ lan truyền lỗi.', 'Thay vì chúng ta làm một bước lớn thì nó sẽ khó để huấn luyện và bài toán của chúng ta phức tạp hơn. Thay vì chúng ta có một biến Z thì bây giờ nó sẽ là T biến là X1 cho đến XT, và do đó thì thành phần Z này sẽ biến thành từ X1 cho đến XT. Còn đối với biến X này của mình thì nó sẽ là X0, tức là cái giá trị ký hiệu ở đây.', 'Bình thường là chúng ta chỉ có tập các cái ảnh. Nhưng mà chúng ta sẽ không có cái ảnh đích, cái ảnh mà chúng ta muốn tạo thành. Còn kiến trúc của Pix2Pix tức là nó biến một cái picture thành một cái picture. Vâng, thì nó đòi hỏi chúng ta sẽ phải có một cặp ảnh để huấn luyện. Vì vậy, chúng ta sẽ có một cái cặp ảnh đó là ảnh thật và một cái ảnh mà có cái phân đoạn, semantic map. Thì đây là một cái cặp ảnh và nó tham gia vào quá trình huấn luyện cũng tương tự như GAN. Nó có generator và discriminator.', 'đó là những cái từ ít gặp Thì thậm chí là những cái từ mà có tính trừu tượng cao ví dụ như là the view, là khoanh vùng nguyên cái này là the view Rồi beautiful caribbean, caribe sea, thì ở đây chúng ta thấy là nó biết đây là một cái biển ở caribbean để mà nó khoanh vùng Thì đây chính là những cái khái niệm rất là trừu tượng và ít gặp trong thực tế mà clip nó vẫn có thể phát hiện bằng định vị được Vậy thì cách thức hoạt động và các cái module chính của clip là thực hiện như thế nào Thì đầu tiên đó là chúng ta sẽ tạo ra một cái code prompt tổng quát là tên của 80 vật thể trong tập dữ liệu coco được phân cách bởi dấu chấm Tức là mỗi một cái vật thể thì sẽ cách nhau với các vật thể khác, ví dụ chữ person, cách chữ bicycle là dấu chấm như thế này Và kèm theo là một cái câu mô tả, một cái cặp câu mô tả giữa hình ảnh và chúng ta sẽ đưa các cái từ này, cái code prompt Và cái câu mô tả này qua textencoder thì chúng ta sẽ tạo ra được là P0, P1, P2 v.v. Và tương tự như vậy thì cái ảnh chúng ta cũng sẽ khoanh vùng các cái đối tượng này và qua cái visualencoder để tạo ra cái O0, O1, O2 Lưu ý đó là trong cái mô hình clip thì chúng ta sẽ kết hợp hai cái loại dữ liệu là hình ảnh và văn bản ở bước trung gian G-clip thì là ở bước trung gian, trong khi đó với clip thì chúng ta chỉ kết hợp nó ở cái bước cuối cùng thôi Còn ở đây chúng ta sẽ thực hiện ở bước trung gian Thì tại sao lại có những cái việc như vậy? Đó là khi chúng ta tổng hợp ra cái đặc trưng thì ở phía trên là chỉ thuần nội dung cái đặc trưng về văn bản Còn ở bên dưới là các cái đặc trưng mà chúng ta thuần về hình ảnh và qua cái Fusion này thì chúng ta sẽ kết hợp được với nhau Đó là lấy cái đặc trưng của hình ảnh để bỏ vào văn bản và lấy nội dung của văn bản đưa vào hình ảnh để cho nó có thể tương tác để giúp cho hình ảnh nó có thể thực sự được thấy ở trong văn bản này Và cái sự khác biệt lớn nhất đó chính là cái module thứ 2, đó là chúng ta sẽ đo cái độ tương đồng giữa mỗi vùng và từ mô tả thay thế cho cái module phân lớp truyền thống Và ở trên đây chúng ta sẽ có m từ khóa là P1, P2, m token là P1, P2 và Pm Thì chúng ta có một cái lưu ý ở đây là có những cái từ mà dài và có hai, gọi là giống như tiếng Việt của mình là từ F là hair dryer thì nó sẽ tách ra làm hai, làm hai token hoặc có những cái từ hoặc cụm từ nó sẽ đi một cái combo với nhau, ví dụ như là blue dot, nó là một cái cụm từ thì nó sẽ tách ra làm hai token Thì khi đó chúng ta thấy là tính cái độ tương đồng nó sẽ là hai cái phần tử trên cái ma trận mà có cái giá trị lớn, ví dụ như đây là chữ tính từ và đây là danh từ thì chẳng hạn Hoặc trong cái ví dụ trên thì chúng ta thấy là hair dryer thì đây chính là hair và đây là dryer, hair và dryer này thì nó sẽ đi chung với nhau một cái combo khi chúng ta so sánh cái độ tương đồng với cái feature của cái hình ảnh là của cái máy sấy tóc Thì cái ý tưởng của nó cũng tương tự như clip nhưng mà clip thì sẽ tách nó ra thành các cái khu vực, các cái mảnh hình ảnh nhỏ rồi sau đó nó sẽ đi tính độ tương đồng với những cái từ ở bên trong cái chuỗi prompt của mình Cái token được tạo ra bởi cái prompt của mình Sau khi chúng ta đã tính được cái ma trận này rồi thì chúng ta sẽ đi tính cái alignment loss giữa cái ground truth, đây là cái ma trận ground truth Và ở trên đó là cái ma trận mà chúng ta được tính từ cái độ tương đồng giữa cái vùng ảnh, cục bộ với lại các cái token ở bên trong cái prompt của mình Và alignment loss này thì mục đích là để xác định xem cái đối tượng đó là gì, bên cạnh đó thì chúng ta sẽ có cái region feature để tính cái localization loss Mục tiêu đó là để cho chúng ta xác định xem cái vị trí, cái sai số khi dự đoán vị trí Thế thì quá trình huấn luyện của clip thì nó sẽ bao gồm đầu tiên đó là cái phrase routing Với mỗi một cái tấm ảnh ở đây, O1, O2, ON chúng ta sẽ truyền vào, truyền vào đây và chúng ta sẽ ra được O O chính là cái feature hoặc là cái embedding của cái vùng ảnh Và sau đó thì chúng ta sẽ lấy cái prompt, prompt của mình thì nó sẽ được hình thành từ cái câu là detect person bicycle car third brush v.v. Rồi, và cái ở đây thêm một cái ý nữa đó là cái feature mà biểu diễn cho cái đặc trưng hình ảnh thì đó là một cái đặc trưng có kích thước đó là D chiều Còn cái ảnh của mình thì nó sẽ có N cái vùng, N vùng Rồi tương tự như vậy thì prompt thì đặc trưng của mỗi từ hoặc là token thì chúng ta sẽ biểu diễn bởi các cái vector và đại diện nó là ma trận p Ma trận p này thì cũng tương tự đó là nó sẽ có chiều của cái đặc trưng văn bản sẽ là D chiều, giống như bên đây để mà sau này chúng ta có thể nhân tích vô hướng được Và số token của chúng ta thì sẽ có m token, và m này thì thường lớn hơn N rất là nhiều Rồi sau khi chúng ta lấy cái O và P chúng ta nhân với nhau thì chúng ta sẽ ra được cái S routing Thì cái S routing này nó sẽ có kích thước đó là N nhân m, trong đó N là số vật thể hoặc số vùng, m là số token Và cuối cùng là chúng ta sẽ dựa trên cái S routing này để chúng ta đi tính cái loss của mình, trong đó loss của mình thì nó sẽ có cái T phải là được mở rộng ra từ T bao gồm là N đối tượng Rồi, thế thì clip đó là cho phép chúng ta có thể tạo ra một cái câu prompt thủ công Ở trên ví dụ trên thì chúng ta có thể thấy những cái đối tượng hiếm và lạ thì ta có thể mô tả đối tượng đó bằng một cái nguồn tự nhiên Bình thường thì chúng ta mô tả đó là Car, Bus, Car, Dog, đây là những cái đối tượng tương đối là phổ biến Thì chúng ta sẽ không cần phải mô tả nhiều hơn, nhưng đối với cái từ lạ, ví dụ như là cá đuối, là stingray, thì nếu chúng ta đưa vào cái mô hình clip để mà phát hiện đối tượng thì chúng ta thấy là cái khả năng phát hiện của nó khá là thấp, nó chỉ phát hiện được một con và với cái Confidence là khoảng 0.21 Nhưng khi chúng ta mô tả nó dài dòng hơn, nhiều thông tin hơn, ví dụ như là stingray with a flat and round, thì ở đây chúng ta sẽ, mô hình của mình nó sẽ hiểu và nó sẽ phát hiện ra cái đối tượng nhiều hơn và dày đặc hơn.', 'Ở đây là công thức của sinh ảnh không có điều kiện. Đối với sinh ảnh mà có điều kiện, chúng ta sẽ thêm vô một biến nữa là biến y. Đây chính là conditional signal. Đây là conditional signal. Sau này khi tổng quát lên, y này không nhất thiết phải là văn bản. nó có thể là một cái mask, nó có thể là một cái điểm v.v. thì công thức của chúng ta thay vì là S theta của Xt, t thì chúng ta sẽ thêm cái thành phần là y vào đây và khi đó thì cái St này sẽ xấp xỉ với lại cái gradient của log p Xt cho trước y Đây là conditional score Công thức trước là unconditional score Bây giờ chúng ta sẽ chuyển sang conditional score chúng ta sẽ đưa vô một xác suất có điều kiện là i Chứ cho trước i, thì xác suất để tìm ra xt khi cho trước i là bao nhiêu? Chúng ta sẽ triển khai Và dựa trên định lý Bayes thì công thức này xuất phát từ triển khai như sau đó là cái log của Px t cho trước y thì nó sẽ là bằng log của Pxt cho trước y thì nó sẽ là bằng Pxt nhân với lại Pi cho trước xt tất cả chia cho Pi Với công thức này, chúng ta sẽ triển khai ra và có được là bằng đạo hàm của log của PxT Nhân thì chúng ta sẽ đưa về dấu cộng, đó là cộng cho log của Py cho trước xT Chia thì chúng ta sẽ chuyển thành là dấu trừ cho log của Pi Với công thức này, chúng ta thấy là vì chúng ta đang muốn tính đạo hàm theo XT chúng ta đang tính đạo hàm theo XT Đây là đạo hàm theo XT Trong con mắt của XT, thì y của mình là hằng số Do đó chúng ta sẽ loại bỏ đi thành phần này đi Tại vì đạo hàm của một cái hằng số đối với xt thì nó sẽ là bằng 0 Do đó thì công thức này sẽ đưa về công thức ở trên Đó là log của pxt cộng cho log của pi cho trước xt Và với cái công thức này thì chúng ta sẽ thấy là cái xt của mình Khi chúng ta khôi phục, chúng ta decode Bình thường nó sẽ đi theo con đường này là Unconditioned là màu xanh lá Màu xanh lá tương ứng cho Unconditioned Bình thường nó sẽ đi theo đường màu xanh dương Và qua màu xanh lá thì chúng ta sẽ điều hướng đi qua mũi tên màu xanh và cộng 2 cái đó lại thì nó sẽ ra cái mũi tên màu cam. Thế thì bình thường là chúng ta sẽ đi theo cái con đường này. Nhờ có cái vector gradient của log y cho trước xt, nó bẻ lái để biến thành cái vector màu cam này. Chúng ta lưu ý ở đây nó sẽ có thêm một cái hệ số nữa, nó gọi là classifier guidance. Nếu như trong công thức chúng ta biến đổi ở phía trước là chúng ta không có cái gamma ở đây thì hàm ý đó là một cái Unconditional Score nó sẽ kết hợp với một cái Adversarial Gradient tức là cái vector điều hướng theo tỷ lệ đó là 1,1 nhưng mà chúng ta muốn nó nhanh điều hướng thì chúng ta sẽ tăng cái hệ số tỷ lệ đó lên hoặc là chúng ta muốn chậm lại thì chúng ta sẽ giảm cái hệ số tỷ lệ đó xuống Như vậy trong công thức này, gamma sẽ là hệ số để giảm tốc độ điều hướng của mình Vector màu cam sẽ là tổng hợp của vector màu đỏ trong điều kiện là Unconditional Kết hợp với adversarial thì nó sẽ đưa ra, bẻ cái hướng, thay vì chúng ta đi theo hướng này để mà tìm được đến đây thì bây giờ nó bẻ hướng lại, nó sẽ đi theo cái hướng này để đến cái ảnh mà có cái điều kiện giống với lại cái Y của mình thì đó chính là cái Classifier Guidance thế thì cái mô hình này sẽ được thực hiện như thế nào Trước tiên chúng ta sẽ nói về vai trò của Classifier Guidance, tức là gamma hồi nãy của mình Nếu chúng ta chọn gamma là bằng một, tức là dùng công thức gốc ban đầu Thì kết quả của mình sẽ không được điều hướng đủ tốt và đủ nhanh Dẫn đến là nó sẽ tạo ra những hình thù không có thật Tại sao không có thật? Tại vì nó vừa pha trộn của một cái ảnh, của một đối tượng có một đối tượng thật mà lẽ ra với vector Z tạo ra tạo ra cái x0 khi có sự tham gia của gamma vào thì gamma này nó bẻ lái nhưng nó bẻ chưa đủ nhanh dẫn đến đó là kết quả của mình nó tạo ra đối tượng lai lai ở giữa đây là cái x0 còn đây là cái x0 mới thì lẽ ra là chúng ta hướng đến chỗ này nhưng mà cái gamma của chúng ta chưa đủ nên thay vì là nó bẻ lái bình thường là đến đây đúng không thì nó sẽ bẻ lái đến giữa chừng và ở cái khúc giữa chừng này thì nó tạo ra những tấm ảnh như thế này trong khi đó nếu chúng ta cho classifier guidance tức là cái gamma lớn hơn ví dụ như gamma trong trường hợp này bằng 10 thì nó sẽ bẻ lái mạnh hơn để mà nó đến được đến cái xnew giống với lại cái nội dung mà chúng ta mong muốn đó là Pembroke Welsh Corgi Đây là một cái giống chó rất là hiếm Vậy thì quá trình sinh ở trên là quá trình sinh có điều kiện và nó có một cái Classifier Guidance Vậy thì chúng ta sẽ huấn luyện cái mô hình này như thế nào Thì cái cách thức huấn luyện đó là chúng ta sẽ có thêm một cái module chúng ta sẽ có thêm một cái mạng nữa, nó gọi là một cái Classifier hay còn gọi là Off-the-shelf Classifier Và cứ với mỗi cái i mà chúng ta đưa vào thì chúng ta sẽ đi huấn luyện cho một cái classifier như vậy là một cái i sẽ có một cái classifier riêng Và như vậy thì nó sẽ khiến cho cái mô hình của mình nó không có tính linh động Nó không có tính linh động vì khi chúng ta muốn tạo ra một cái đối tượng mới Bình thường chúng ta tạo ra 2 con mèo đeo kính, bây giờ chúng ta muốn tạo ra 1 con chó Welsh Corgi đeo kính chẳng hạn thì lúc đó chúng ta sẽ phải train 1 cái classifier mới cho cái y đó thì nó sẽ khiến cho cái mô hình của mình nó chạy không có tính thực tiễn cao do đó thì chúng ta sẽ chuyển sang 1 cái mô hình, nó gọi là cái mô hình mà sinh có điều kiện nhưng mà với Classifier Free Guidance, tức là không có classifier Vậy thì công thức của mình sẽ được sửa lại đó là bằng 1 trừ gamma nhân cho log của PXT thì đây là cái Unconditional Score kết hợp với Conditional Score và ở đây chúng ta sẽ huấn luyện trên chính mô hình Diffusion của mình luôn Đây chính là U-Net trong diffusion. U-Net trong diffusion này chúng ta sẽ huấn luyện bằng cách đưa 2 tình huống. Tình huống thứ nhất là chúng ta sẽ đưa một vector rỗng vào. Mục tiêu của mình tương đương như một mô hình sinh ảnh nhưng mà không có điều hướng. và chúng ta sẽ đưa y vào, thì y này sẽ là mẫu dữ liệu huấn luyện của chúng ta và y này sẽ cho trước một số mẫu condition mà chúng ta muốn huấn luyện để từ đó nó sẽ estimate ra cái x, xt, t, y Thứ nhất là chúng ta sẽ không có thêm, không có classifier mà chúng ta sẽ huấn luyện trên chính cái mô hình của diffusion của mình luôn trên chính cái decoder của mình luôn và khi chúng ta huấn luyện trên cái decoder này thì chúng ta sẽ có hai tình huống một đó là chúng ta sẽ truyền vô một cái condition là rỗng đây là một cái condition rỗng Mục tiêu của nó là tạo ra tấm ảnh không có cần điều hướng và đưa vào 1 condition trong data set của mình để chuẩn bị trước, đó chính là y. Mục tiêu của mình là điều hướng đến cái này là không điều hướng, còn cái này là có điều hướng. sau khi chúng ta huấn luyện xong, chúng ta cứ sử dụng decoder này để đưa y vào và nó sẽ tạo sinh ra mô hình của mình thì cái sơ đồ này sẽ tương tự như nó sẽ lấy từ mô hình mà chúng ta đã học trong những slide trước bình thường là chúng ta chỉ đưa vào xt và t, bây giờ chúng ta sẽ đưa vào thêm y nữa để làm được việc này thì chúng ta có thể sử dụng các mô hình của Transformer với attention sử dụng key-value của attention để điều hướng thì cái i này có thể là query', 'Chúng ta có thể học không giám sát trên tập dữ liệu rất lớn hay gọi là pre-training. Và cái pre-training này sẽ giúp chúng ta tạo ra những cái pre-trained model để có thể tổng quát hóa được các task, thì nó có thể tạo ra được các embeddings và dùng các embeddings đó để giải quyết các tác vụ của các bài toán khác nhau. Cách tiếp cận thứ 2 đó là học giám sát. Thì chúng ta sẽ fine-tune mô hình cho từng task cụ thể. Thế thì sẽ có bước pre-training trước và sau đó sẽ là fine-tuning sau.', 'Thì ở đây chúng ta sẽ dẫn nhập, giải thích cái ý này. Các cái mô hình của mình khi được huấn luyện thì lấy dữ liệu ở trong thực tế mà dữ liệu chúng ta sampling trong thực tế thì 95% Chúng ta lấy một cái ví dụ đó là dữ liệu mà lái xe trên đường. 95% dữ liệu mà chúng ta lái xe, tức là nó nằm ở trong cái khu vực này. Là trong cái khu vực này.']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Việc tự huấn luyện mô hình CLIP không khả thi vì nó đòi hỏi một lượng dữ liệu cực kỳ lớn. Đồng thời, quá trình này cần tài nguyên tính toán và khả năng xử lý song song đáng kể. Ngoài ra, chi phí cho các thiết bị GPU cũng rất cao, khiến việc tự huấn luyện trở nên không thực tế.                                                                                                                                              | Tự huấn luyện được môn clip là khá là không khả thi, vì nó cần lượng data cực kỳ lớn, tài nguyên tính toán, xử lý song song, GPU cũng cực kỳ mắc tiền.                                     |       1        |           0.997339 |                 nan |                1 |\n",
      "| Mô hình tạo sinh giúp ích gì trong việc huấn luyện mô hình?                   | ['Thu thập dữ liệu rất là nhiều. Thế thì nhờ có các cái mô hình tạo sinh tạo ra các cái ảnh tự động. Chúng ta chỉ cần đi lấy mẫu một cái điểm trong cái phân phối này. Xong rồi chúng ta tái tạo lại thì chúng ta đã có một cái ảnh mới. Thì cái mô hình tạo sinh nó đã giúp cho chúng ta tự động tăng cường dữ liệu lên. Giúp cho cái việc huấn luyện mô hình gọi là đỡ bị hiện tượng overfitting hơn. Thì đó chính là cái động cơ của mô hình tạo sinh. Và giả sử như chúng ta có một cái mẫu dữ liệu. Chúng ta lấy mẫu dữ liệu để huấn luyện. Chúng ta lấy được cái mẫu này. Sau đó chúng ta xác định được cái phân phối của nó. Đây là ước lượng một cái hàm mật độ.', 'Và bây giờ chúng ta sẽ quay trở lại đến với cái chủ đề của chúng ta. Đó là mô hình tạo sinh hay còn gọi là generative model. Thì mục tiêu của cái mô hình tạo sinh đó là chúng ta sẽ lấy mẫu dữ liệu huấn luyện. Lấy mẫu dữ liệu huấn luyện đầu vào của một số cái phân bố, của một số cái phân phối. Và từ đó chúng ta có thể tái tạo lại các cái phân phối đó.', 'Trong cái quá trình mà huấn luyện để tránh được những cái tình huống không thể đoán được. Tại vì khi chúng ta muốn xây dựng cái dữ liệu cho cái hệ thống xe tự lái, chúng ta phải lường trước những cái tình huống mà xe sẽ gặp những cái tình huống mà hiếm xuất hiện. Ví dụ như có một người băng qua trước mặt, hoặc là đường thì gập ghềnh, đường bị quanh co, hoặc là thời tiết xấu. Thế thì cái mà chúng ta mong muốn có cái dữ liệu để cho mô hình của mình, để cải thiện cái mô hình của mình, đó là những cái tình huống là phân bố ở bên ngoại vi, tức là outlier. Đây là cái outlier mà chúng ta mong muốn có dữ liệu để cho mô hình nó học. Ví dụ đó là có cái giải phân cách, hoặc là có cái tình huống là máy bay lớn đi trên đầu, hoặc là thời tiết xấu, thời tiết cực đoan, ví dụ như là mưa to, bão tuyết, hoặc là có tình huống người đi bộ băng qua đột ngột trước mặt mình, hoặc là đùa giỡn, v.v. Thì đó chính là những cái tình huống ngoại vi mà chúng ta mong muốn có cái dữ liệu này để cho mô hình của mình nó học được. Thì đó chính là cái lý do tại sao chúng ta cần có mô hình tạo sinh, để khi chúng ta sampling với những cái tình huống ngoại lệ này, thì chúng ta sẽ có được những cái dữ liệu này, nó sẽ giúp cho cái bộ dữ liệu huấn luyện của chúng ta nó cân bằng hơn. Nó giúp cân bằng hơn và dẫn đến là mô hình của mình nó sẽ không bị bias vào 95% những cái dữ liệu mà đẹp, dữ liệu bình thường ở đây. Và một trong những cái hướng tiếp cận để mà tạo ra các cái mô hình tạo sinh, đó là chúng ta sẽ sử dụng mô hình là Latent Variable, tức là mô hình dựa trên biến tiềm ẩn và có hai cái mô hình chúng ta sẽ cùng tìm hiểu trong cái buổi ngày hôm nay, đó là mô hình về Autoencoder, đó là Autoencoder phiên bản gốc và Variational Autoencoder hay viết tắt là VAE và Generative Adversarial Network, tức là mô hình tạo sinh đối kháng GAN. Thì đây là hai cái mô hình dựa trên cái biến tiềm ẩn.', 'Vấn đề đó là làm thế nào để khi gặp những cái đối tượng mới hoặc là hiếm trong dữ liệu. Rồi chiến lược đó là chúng ta sẽ tận dụng được các cái mô hình tạo sinh để phát hiện ra các điểm dữ liệu lạ trong cái phân bố. Rồi sau đó sử dụng các cái dữ liệu Outlier này để mà cho cái quá trình huấn luyện mô hình, nó sẽ cải thiện được cái mô hình tốt hơn.', 'Ở đây là công thức của sinh ảnh không có điều kiện. Đối với sinh ảnh mà có điều kiện, chúng ta sẽ thêm vô một biến nữa là biến y. Đây chính là conditional signal. Đây là conditional signal. Sau này khi tổng quát lên, y này không nhất thiết phải là văn bản. nó có thể là một cái mask, nó có thể là một cái điểm v.v. thì công thức của chúng ta thay vì là S theta của Xt, t thì chúng ta sẽ thêm cái thành phần là y vào đây và khi đó thì cái St này sẽ xấp xỉ với lại cái gradient của log p Xt cho trước y Đây là conditional score Công thức trước là unconditional score Bây giờ chúng ta sẽ chuyển sang conditional score chúng ta sẽ đưa vô một xác suất có điều kiện là i Chứ cho trước i, thì xác suất để tìm ra xt khi cho trước i là bao nhiêu? Chúng ta sẽ triển khai Và dựa trên định lý Bayes thì công thức này xuất phát từ triển khai như sau đó là cái log của Px t cho trước y thì nó sẽ là bằng log của Pxt cho trước y thì nó sẽ là bằng Pxt nhân với lại Pi cho trước xt tất cả chia cho Pi Với công thức này, chúng ta sẽ triển khai ra và có được là bằng đạo hàm của log của PxT Nhân thì chúng ta sẽ đưa về dấu cộng, đó là cộng cho log của Py cho trước xT Chia thì chúng ta sẽ chuyển thành là dấu trừ cho log của Pi Với công thức này, chúng ta thấy là vì chúng ta đang muốn tính đạo hàm theo XT chúng ta đang tính đạo hàm theo XT Đây là đạo hàm theo XT Trong con mắt của XT, thì y của mình là hằng số Do đó chúng ta sẽ loại bỏ đi thành phần này đi Tại vì đạo hàm của một cái hằng số đối với xt thì nó sẽ là bằng 0 Do đó thì công thức này sẽ đưa về công thức ở trên Đó là log của pxt cộng cho log của pi cho trước xt Và với cái công thức này thì chúng ta sẽ thấy là cái xt của mình Khi chúng ta khôi phục, chúng ta decode Bình thường nó sẽ đi theo con đường này là Unconditioned là màu xanh lá Màu xanh lá tương ứng cho Unconditioned Bình thường nó sẽ đi theo đường màu xanh dương Và qua màu xanh lá thì chúng ta sẽ điều hướng đi qua mũi tên màu xanh và cộng 2 cái đó lại thì nó sẽ ra cái mũi tên màu cam. Thế thì bình thường là chúng ta sẽ đi theo cái con đường này. Nhờ có cái vector gradient của log y cho trước xt, nó bẻ lái để biến thành cái vector màu cam này. Chúng ta lưu ý ở đây nó sẽ có thêm một cái hệ số nữa, nó gọi là classifier guidance. Nếu như trong công thức chúng ta biến đổi ở phía trước là chúng ta không có cái gamma ở đây thì hàm ý đó là một cái Unconditional Score nó sẽ kết hợp với một cái Adversarial Gradient tức là cái vector điều hướng theo tỷ lệ đó là 1,1 nhưng mà chúng ta muốn nó nhanh điều hướng thì chúng ta sẽ tăng cái hệ số tỷ lệ đó lên hoặc là chúng ta muốn chậm lại thì chúng ta sẽ giảm cái hệ số tỷ lệ đó xuống Như vậy trong công thức này, gamma sẽ là hệ số để giảm tốc độ điều hướng của mình Vector màu cam sẽ là tổng hợp của vector màu đỏ trong điều kiện là Unconditional Kết hợp với adversarial thì nó sẽ đưa ra, bẻ cái hướng, thay vì chúng ta đi theo hướng này để mà tìm được đến đây thì bây giờ nó bẻ hướng lại, nó sẽ đi theo cái hướng này để đến cái ảnh mà có cái điều kiện giống với lại cái Y của mình thì đó chính là cái Classifier Guidance thế thì cái mô hình này sẽ được thực hiện như thế nào Trước tiên chúng ta sẽ nói về vai trò của Classifier Guidance, tức là gamma hồi nãy của mình Nếu chúng ta chọn gamma là bằng một, tức là dùng công thức gốc ban đầu Thì kết quả của mình sẽ không được điều hướng đủ tốt và đủ nhanh Dẫn đến là nó sẽ tạo ra những hình thù không có thật Tại sao không có thật? Tại vì nó vừa pha trộn của một cái ảnh, của một đối tượng có một đối tượng thật mà lẽ ra với vector Z tạo ra tạo ra cái x0 khi có sự tham gia của gamma vào thì gamma này nó bẻ lái nhưng nó bẻ chưa đủ nhanh dẫn đến đó là kết quả của mình nó tạo ra đối tượng lai lai ở giữa đây là cái x0 còn đây là cái x0 mới thì lẽ ra là chúng ta hướng đến chỗ này nhưng mà cái gamma của chúng ta chưa đủ nên thay vì là nó bẻ lái bình thường là đến đây đúng không thì nó sẽ bẻ lái đến giữa chừng và ở cái khúc giữa chừng này thì nó tạo ra những tấm ảnh như thế này trong khi đó nếu chúng ta cho classifier guidance tức là cái gamma lớn hơn ví dụ như gamma trong trường hợp này bằng 10 thì nó sẽ bẻ lái mạnh hơn để mà nó đến được đến cái xnew giống với lại cái nội dung mà chúng ta mong muốn đó là Pembroke Welsh Corgi Đây là một cái giống chó rất là hiếm Vậy thì quá trình sinh ở trên là quá trình sinh có điều kiện và nó có một cái Classifier Guidance Vậy thì chúng ta sẽ huấn luyện cái mô hình này như thế nào Thì cái cách thức huấn luyện đó là chúng ta sẽ có thêm một cái module chúng ta sẽ có thêm một cái mạng nữa, nó gọi là một cái Classifier hay còn gọi là Off-the-shelf Classifier Và cứ với mỗi cái i mà chúng ta đưa vào thì chúng ta sẽ đi huấn luyện cho một cái classifier như vậy là một cái i sẽ có một cái classifier riêng Và như vậy thì nó sẽ khiến cho cái mô hình của mình nó không có tính linh động Nó không có tính linh động vì khi chúng ta muốn tạo ra một cái đối tượng mới Bình thường chúng ta tạo ra 2 con mèo đeo kính, bây giờ chúng ta muốn tạo ra 1 con chó Welsh Corgi đeo kính chẳng hạn thì lúc đó chúng ta sẽ phải train 1 cái classifier mới cho cái y đó thì nó sẽ khiến cho cái mô hình của mình nó chạy không có tính thực tiễn cao do đó thì chúng ta sẽ chuyển sang 1 cái mô hình, nó gọi là cái mô hình mà sinh có điều kiện nhưng mà với Classifier Free Guidance, tức là không có classifier Vậy thì công thức của mình sẽ được sửa lại đó là bằng 1 trừ gamma nhân cho log của PXT thì đây là cái Unconditional Score kết hợp với Conditional Score và ở đây chúng ta sẽ huấn luyện trên chính mô hình Diffusion của mình luôn Đây chính là U-Net trong diffusion. U-Net trong diffusion này chúng ta sẽ huấn luyện bằng cách đưa 2 tình huống. Tình huống thứ nhất là chúng ta sẽ đưa một vector rỗng vào. Mục tiêu của mình tương đương như một mô hình sinh ảnh nhưng mà không có điều hướng. và chúng ta sẽ đưa y vào, thì y này sẽ là mẫu dữ liệu huấn luyện của chúng ta và y này sẽ cho trước một số mẫu condition mà chúng ta muốn huấn luyện để từ đó nó sẽ estimate ra cái x, xt, t, y Thứ nhất là chúng ta sẽ không có thêm, không có classifier mà chúng ta sẽ huấn luyện trên chính cái mô hình của diffusion của mình luôn trên chính cái decoder của mình luôn và khi chúng ta huấn luyện trên cái decoder này thì chúng ta sẽ có hai tình huống một đó là chúng ta sẽ truyền vô một cái condition là rỗng đây là một cái condition rỗng Mục tiêu của nó là tạo ra tấm ảnh không có cần điều hướng và đưa vào 1 condition trong data set của mình để chuẩn bị trước, đó chính là y. Mục tiêu của mình là điều hướng đến cái này là không điều hướng, còn cái này là có điều hướng. sau khi chúng ta huấn luyện xong, chúng ta cứ sử dụng decoder này để đưa y vào và nó sẽ tạo sinh ra mô hình của mình thì cái sơ đồ này sẽ tương tự như nó sẽ lấy từ mô hình mà chúng ta đã học trong những slide trước bình thường là chúng ta chỉ đưa vào xt và t, bây giờ chúng ta sẽ đưa vào thêm y nữa để làm được việc này thì chúng ta có thể sử dụng các mô hình của Transformer với attention sử dụng key-value của attention để điều hướng thì cái i này có thể là query', 'Vân vân. Và sau đó thì chúng ta sẽ đi học để tái tạo lại phân phối đó. Từ đó có thể lấy mẫu một cái mẫu mới. Rồi sau đó chúng ta tạo ra một cái dữ liệu. Và cái dữ liệu này vì chúng ta đã học ra được cái phân phối của các cái không gian mà có chứa cái ảnh gương mặt. Nên khi chúng ta lấy mẫu trong cái phân phối này thì khi chúng ta khôi phục, chúng ta tái tạo lại thì nó cũng sẽ ra ảnh một cái gương mặt. Và đó chính là cái ý nghĩa của cái mô hình tạo sinh. Tức là chúng ta sẽ đi lấy mẫu dữ liệu, huấn luyện nó vào để huấn luyện cho một cái phân phối. Và sau đó để học cái cách để mà chúng ta học để mà tái tạo lại. Đây là tái tạo. Chúng ta sẽ tái tạo lại cái tấm ảnh mới. Thì cái quá trình tái tạo này chính là cái quá trình tạo sinh ảnh. Thế thì cái mô hình tạo sinh ở giai đoạn đầu nó được sử dụng để phục vụ cho cái công việc đó là tăng cường dữ liệu. Tại vì các cái mô hình học máy chúng ta biết rằng là các cái mô hình học máy thì nó sẽ bị cái hiện tượng gọi là hiện tượng overfitting.', 'Và huấn luyện nó cũng sẽ ổn định hơn. Thì đó chính là cái điều khiến cho latent diffusion model là một trong những cái mô hình tạo sinh hình ảnh mà được rất nhiều cái trích dẫn và được rất nhiều các cái bài báo cũng như là các cái nghiên cứu gần đây họ sử dụng để phát triển.', 'Thì đây là mẫu tạo sinh. Thì đối với cái mẫu đầu vào thì chúng ta sẽ ký hiệu đó là dữ liệu huấn luyện. Chúng ta sẽ sampling trong cái Pdata. Trong cái Pdata, tức là cái dữ liệu thế giới thực. Mẫu tạo sinh thì chúng ta sẽ đi sampling trong cái Pmodel. Tức là giả sử như chúng ta đã có cái model này rồi. Đây là cái model. Rồi thì chúng ta sẽ sampling trong cái dữ liệu trong cái không gian của cái mô hình mà mình ước lượng được. Đây là cái mô hình ước lượng được. Rồi từ đó chúng ta tái tạo lại. Và cái việc mà học một cái mô hình tạo sinh là chúng ta tìm cách nào đó để cho cái mô hình PmodelX nó sẽ có cái sự tương tự về mặt phân phối giống như là Pdata. Thì đây chính là cái mục tiêu của các cái mô hình tạo sinh. Pmodel phải xấp xỉ, phải tương tự với lại cái P của data. Vậy thì tại sao chúng ta cần phải có cái mô hình tạo sinh thì nó có khả năng là khám phá ra các đặc trưng cơ bản của dữ liệu. Ví dụ các đặc trưng về mặt màu da, các đặc trưng về tư thế đồng nhất.', 'Thì chúng ta thấy cái khu vực này là xuất hiện dày đặc. Nhưng mà nó hơi ít hơn so với cái khu vực bên tay phải. Khu vực này sẽ xuất hiện dày hơn. Còn ở cái khu vực ở giữa ở đây hoặc là ở ngoài rìa ở đây thì chúng ta thấy đó là khi lấy mẫu nó thưa hơn. Do đó khi chúng ta ước lượng ra cái hàm mật độ thì nó sẽ có cái dạng như thế này. Đó là hai đỉnh. Và khi chúng ta tạo sinh mẫu thì từ chúng ta sẽ tạo ra được những cái ảnh mới mà có cái tính chất giống như là cái tính chất của cái ảnh mà chúng ta đã lấy mẫu trước đó. Thì đây là, trong cái ví dụ này thì đây chính là cái mẫu đầu vào để huấn luyện. Đó là cái ảnh trong thế giới thực.', 'Như vậy thì chúng ta đã cùng tìm hiểu về những phương pháp để tạo sinh hình ảnh tuy nhiên những phương pháp trên bao gồm VAE, Diffusion thậm chí là GAN đó là chúng ta sẽ khởi tạo ngẫu nhiên một vector Z và từ đó nó sẽ tạo ra một tấm hình của mình qua hàm decoder nhưng mà quá trình decoder này thì chúng ta không kiểm soát được đầu ra của mình thì trong thực tế chúng ta sẽ có những nhu cầu đó là làm sao để kiểm soát được ảnh đầu ra tạo ra những tấm ảnh mà chúng ta muốn với nội dung mà chúng ta muốn Vậy thì, cái phần điều hướng ảnh tạo sinh là một cái phần để giúp chúng ta trả lời được câu hỏi đó là Làm sao để sinh ảnh được từ văn bản? Văn bản chỉ là một trong những cách để giúp chúng ta điều hướng cái ảnh của mình Trên hình đó là quá trình sinh ảnh mà không có điều kiện Tức là với một vector Z ngẫu nhiên qua hàm decoder với tham số theta mà chúng ta đã huấn luyện thì nó sẽ giúp chúng ta tạo ra các hình ảnh, ví dụ như là chó, mèo, gấu và bò nằm trong phân bố P theta X như thế này và quá trình sinh ảnh mà có điều kiện thì chúng ta sẽ đưa vào conditional signal, tức là một tín hiệu có điều kiện là I Ví dụ như trong ngữ cảnh này sẽ là a cat wearing sunglasses thì cái y này sẽ được đưa vào quá trình decoder tại mỗi step và khi đưa vào thì chúng ta sẽ điều hướng hoặc là bẻ lái thì đây là một cái từ mang tính hình tượng thôi đó là bẻ lái cái hướng Bình thường nếu như một cái vector z bất kỳ thì nó sẽ đưa đến cái khu vực ảnh mà ví dụ như có cái đối tượng không liên quan đến cái nội dung mình đang muốn Nhưng mà nhờ có conditional signal tham gia qua quá trình decode thì nó sẽ bẻ hướng để chúng ta đến vào khu vực ảnh mà có những con mèo đeo kính cool như thế này. Ở đây chúng ta sẽ có một mô hình để giúp chúng ta sinh ảnh có điều kiện.']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Mô hình tạo sinh giúp tự động tăng cường dữ liệu, từ đó giảm thiểu hiện tượng overfitting trong quá trình huấn luyện mô hình. Đặc biệt, chúng còn hỗ trợ phát hiện và tạo ra dữ liệu cho các tình huống hiếm gặp hoặc ngoại lệ, như các điều kiện đường xá bất thường hay thời tiết khắc nghiệt. Việc này giúp cân bằng bộ dữ liệu huấn luyện, ngăn chặn mô hình bị thiên vị bởi dữ liệu phổ biến và cải thiện hiệu suất tổng thể. | Mô hình tạo sinh giúp chúng ta tự động tăng cường dữ liệu lên, giúp cho việc huấn luyện mô hình đỡ bị hiện tượng overfitting hơn.                                                          |       1        |           0.951516 |                 nan |                1 |\n",
      "Hoàn tất đánh giá.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from typing import List, Dict\n",
    "from datasets import Dataset\n",
    "\n",
    "# --- Ragas & Datasets ---\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from datasets import Dataset\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# --- HÀM MỚI: Tạo bộ test tổng hợp từ Vector DB ---\n",
    "async def generate_synthetic_test_set_from_db(llm: ChatGoogleGenerativeAI, db: Chroma, num_samples: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Lấy các chunk từ Chroma DB, chọn ngẫu nhiên, và dùng LLM tạo câu hỏi + câu trả lời mẫu.\n",
    "    \"\"\"\n",
    "    print(f\"Đang lấy chunk từ Vector DB để tạo bộ test...\")\n",
    "    try:\n",
    "        # Lấy TẤT CẢ các chunk từ DB\n",
    "        raw_data = db.get(include=[\"documents\", \"metadatas\"])\n",
    "        all_chunks_data = [\n",
    "            {\"page_content\": doc, \"metadata\": meta}\n",
    "            for doc, meta in zip(raw_data[\"documents\"], raw_data[\"metadatas\"])\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"LỖI: Không thể lấy dữ liệu từ Chroma DB: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Lọc các chunk quá ngắn hoặc có nội dung không phù hợp\n",
    "    valid_chunks = [\n",
    "        chunk for chunk in all_chunks_data\n",
    "        if len(chunk[\"page_content\"]) > 200 and \"subscribe\" not in chunk[\"page_content\"].lower()\n",
    "    ]\n",
    "\n",
    "    if len(valid_chunks) < num_samples:\n",
    "        print(f\"Cảnh báo: Chỉ tìm thấy {len(valid_chunks)} chunk hợp lệ. Sẽ dùng tất cả.\")\n",
    "        num_samples = len(valid_chunks)\n",
    "        if num_samples == 0:\n",
    "            print(\"LỖI: Không có chunk hợp lệ để tạo bộ test.\")\n",
    "            return []\n",
    "\n",
    "    sampled_chunks = random.sample(valid_chunks, num_samples)\n",
    "\n",
    "    generation_prompt_template = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        Bạn là một chuyên gia tạo dữ liệu. Dựa vào NGỮ CẢNH (context) sau đây, hãy tạo ra 1 cặp (câu hỏi, câu trả lời) mà NGỮ CẢNH này có thể trả lời trực tiếp.\n",
    "        - Câu trả lời (answer) phải được rút ra TRỰC TIẾP từ NGỮ CẢNH.\n",
    "        - Câu hỏi (question) phải là câu hỏi mà một người xem video sẽ hỏi.\n",
    "        - Trả lời bằng tiếng Việt.\n",
    "        - Chỉ trả về 1 JSON object với 2 key: \"question\" và \"ground_truth\".\n",
    "\n",
    "        NGỮ CẢNH:\n",
    "        {context}\n",
    "\n",
    "        JSON:\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    generation_chain = generation_prompt_template | llm\n",
    "\n",
    "    test_set = []\n",
    "    print(f\"Đang dùng LLM để tạo {num_samples} mẫu test...\")\n",
    "    for chunk_data in sampled_chunks:\n",
    "        context = chunk_data[\"page_content\"]\n",
    "        try:\n",
    "            response = await generation_chain.ainvoke({\"context\": context})\n",
    "            # LLM có thể trả về ```json ... ```, cần làm sạch\n",
    "            json_str = response.content.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "            data = json.loads(json_str)\n",
    "\n",
    "            test_set.append({\n",
    "                \"question\": data[\"question\"],\n",
    "                \"ground_truth\": data[\"ground_truth\"], # Đây là câu trả lời mẫu\n",
    "                # Ragas cần 'ground_truth_context' là một list\n",
    "                \"ground_truth_context\": [context]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi tạo câu hỏi cho chunk: {e}\")\n",
    "            print(f\"Chunk lỗi: {context[:100]}...\")\n",
    "\n",
    "    print(f\"Đã tạo xong {len(test_set)} mẫu test.\")\n",
    "    return test_set\n",
    "\n",
    "# --- HÀM M MAIN ĐỂ CHẠY ĐÁNH GIÁ ---\n",
    "async def main_evaluation():\n",
    "    print(\"Bắt đầu quá trình đánh giá RAG...\")\n",
    "\n",
    "    # --- 1. Lấy các biến đã khởi tạo từ cell trước ---\n",
    "    try:\n",
    "        # SỬA ĐỔI: Thêm 'embedding' vào danh sách kiểm tra\n",
    "        if 'llm' not in globals() or 'vector_retriever' not in globals() or 'bm25_runnable' not in globals() or 'embedding' not in globals():\n",
    "            print(\"LỖI: 'llm', 'vector_retriever', 'bm25_runnable', hoặc 'embedding' chưa được định nghĩa.\")\n",
    "            print(\"Hãy chạy các cell ở trên trước khi chạy cell này.\")\n",
    "            return\n",
    "        \n",
    "        # Tạo bản sao (hoặc tham chiếu) để code dễ đọc hơn\n",
    "        current_llm = llm\n",
    "        current_vector_retriever = vector_retriever\n",
    "        current_bm25_runnable = bm25_runnable\n",
    "        current_vector_db = vector_db\n",
    "        current_embedding = embedding # SỬA ĐỔI: Lấy embedding đã được định nghĩa\n",
    "\n",
    "    except NameError as e:\n",
    "        print(f\"LỖI: Thiếu biến. Hãy chạy các cell ở trên trước. Lỗi: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Định nghĩa 2 chain RỜI RẠC để đánh giá ---\n",
    "\n",
    "    # Hàm rerank (đã có ở cell trên)\n",
    "    def rerank_with_query(docs_and_query) -> List[Document]:\n",
    "        docs, query = docs_and_query\n",
    "        reranked = crossencoder_rerank(docs, query, top_k=10) # Dùng hàm 'crossencoder_rerank' đã định nghĩa\n",
    "        return reranked\n",
    "\n",
    "    # Chain 2a: RETRIEVER CHAIN (Hybrid + Rerank)\n",
    "    hybrid_retriever = EnsembleRetriever(\n",
    "        retrievers=[current_bm25_runnable, current_vector_retriever],\n",
    "        weights=[0.5, 0.5]\n",
    "    )\n",
    "\n",
    "    retriever_chain = (\n",
    "        RunnableLambda(lambda query: (hybrid_retriever.get_relevant_documents(query), query))\n",
    "        | RunnableLambda(rerank_with_query)\n",
    "    )\n",
    "\n",
    "    # Chain 2b: GENERATION CHAIN (Prompt + LLM + Parser)\n",
    "    # Dùng 'prompt' và 'parser' đã định nghĩa ở cell trên\n",
    "    parser = JsonOutputParser(pydantic_object=VideoAnswer) # Đảm bảo VideoAnswer đã được định nghĩa\n",
    "    \n",
    "    generation_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Dựa vào transcript sau, trả lời câu hỏi của người dùng bằng tiếng Việt.Phần tóm tắt nội dung thì nên tóm tắt trong 3 câu, \n",
    "    dựa vào các đoạn transcript được cung cấp và chỉ ra đoạn video chứa thông tin đó (video url, thời điểm bắt đầu và kết thúc).\n",
    "    Đồng thời làm mượt lại nội dung tóm tắt đó\n",
    "    Khi trích dẫn thông tin, **luôn sử dụng đúng [Video URL] và [Start] từ doc chứa nội dung đó**.\n",
    "    Nếu không biết câu trả lời thì cứ trả lời là tôi không biết và độ tin cậy là zero\n",
    "    Nếu câu hỏi không liên quan đến nội dung video thì trả lời tôi chỉ được huấn luyện trả lời các câu hỏi liên quan đến nội dung video và độ tin cậy là zero\n",
    "    Không bịa ra thông tin không có căn cứ, không trả lời sai format\n",
    "    Nếu bạn cực kỳ chắc chắn về câu trả lời, hãy đặt độ tin cậy là high. Nếu bạn khá chắc chắn, hãy đặt độ tin cậy là medium. Nếu bạn không chắc chắn về câu trả lời, hãy đặt độ tin cậy là low.\n",
    "    Định dạng đầu ra phải tuân theo JSON schema sau:\n",
    "    {format_instructions}\n",
    "    Transcript:\n",
    "    {context}\n",
    "\n",
    "    Câu hỏi: {question}\n",
    "    \\nAnswer:\n",
    "    \"\"\") # Dùng prompt đã định nghĩa ở cell trên\n",
    "\n",
    "    generation_chain = (\n",
    "        generation_prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "        | current_llm\n",
    "    )\n",
    "\n",
    "    # --- 3. Tạo bộ Test Set ---\n",
    "    # Dùng LLM của RAG (Gemini) để tạo test set từ Chroma\n",
    "    test_set = await generate_synthetic_test_set_from_db(current_llm, current_vector_db, num_samples=5) # Tăng num_samples (vd: 20) để kết quả tin cậy hơn\n",
    "\n",
    "    if not test_set:\n",
    "        print(\"Không thể tạo test set. Dừng đánh giá.\")\n",
    "        return\n",
    "\n",
    "    # --- 4. Chạy pipeline và thu thập dữ liệu ---\n",
    "    print(\"Đang chạy pipeline trên bộ test set...\")\n",
    "    evaluation_data = []\n",
    "\n",
    "    for i, item in enumerate(test_set):\n",
    "        print(f\"Đang xử lý mẫu {i+1}/{len(test_set)}: {item['question'][:50]}...\")\n",
    "        question = item['question']\n",
    "\n",
    "        # 4a. Lấy contexts\n",
    "        retrieved_docs = await retriever_chain.ainvoke(question)\n",
    "        contexts_list = [d.page_content for d in retrieved_docs]\n",
    "\n",
    "        # 4b. Lấy answer\n",
    "        formatted_context_str = format_doc(retrieved_docs) # Dùng hàm format_doc đã định nghĩa\n",
    "        response_msg = await generation_chain.ainvoke({\n",
    "            \"question\": question,\n",
    "            \"context\": formatted_context_str\n",
    "        })\n",
    "\n",
    "        try:\n",
    "            # Trích xuất câu trả lời text từ JSON\n",
    "            # LLM của bạn trả về AIMessage(content=\"```json\\n{...}\\n```\")\n",
    "            json_str = response_msg.content.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "            answer_json = json.loads(json_str)\n",
    "            answer_text = answer_json.get(\"text\", \"\")\n",
    "        except Exception:\n",
    "            answer_text = response_msg.content # Fallback nếu JSON lỗi\n",
    "\n",
    "        evaluation_data.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer_text,\n",
    "            \"contexts\": contexts_list,\n",
    "            \"ground_truth\": item[\"ground_truth\"] # Câu trả lời mẫu\n",
    "        })\n",
    "\n",
    "    print(\"Đã thu thập xong dữ liệu. Chuẩn bị cho Ragas...\")\n",
    "\n",
    "    # --- 5. Chạy Ragas Evaluate ---\n",
    "    if not evaluation_data:\n",
    "        print(\"LỖI: Không có dữ liệu để đánh giá.\")\n",
    "        return\n",
    "\n",
    "    # Chuyển đổi list dictionary thành Hugging Face Dataset\n",
    "    dataset = Dataset.from_list(evaluation_data)\n",
    "\n",
    "\n",
    "    \n",
    "    metrics = [\n",
    "        faithfulness,     # Câu trả lời có bám sát context không? (Không bịa)\n",
    "        answer_relevancy, # Câu trả lời có liên quan đến câu hỏi không?\n",
    "        context_precision,# Context truy xuất có liên quan không?\n",
    "    ]\n",
    "    \n",
    "    # Thêm context_recall nếu bộ test có ground_truth_context\n",
    "    if \"ground_truth_context\" in test_set[0]:\n",
    "        print(\"Đã phát hiện ground_truth_context, sẽ đo context_recall.\")\n",
    "        # Thêm ground_truth_context vào evaluation_data cho Ragas\n",
    "        for i in range(len(evaluation_data)):\n",
    "            evaluation_data[i][\"ground_truth_context\"] = test_set[i][\"ground_truth_context\"]\n",
    "        \n",
    "        dataset = Dataset.from_list(evaluation_data) # Tạo lại dataset với key mới\n",
    "        metrics.append(context_recall)\n",
    "\n",
    "\n",
    "    print(\"Đang chạy Ragas evaluate... (Sử dụng Gemini làm Judge, việc này có thể mất vài phút)\")\n",
    "    \n",
    "    # SỬA ĐỔI: Cung cấp 'llm' và 'embeddings' của bạn cho Ragas\n",
    "    result = evaluate(\n",
    "        dataset=dataset,\n",
    "        metrics=metrics,\n",
    "        llm=current_llm,        # Yêu cầu Ragas dùng Gemini (đã có API key)\n",
    "        embeddings=current_embedding # Yêu cầu Ragas dùng BAAI/bge-m3\n",
    "    )\n",
    "\n",
    "    print(\"--- KẾT QUẢ ĐÁNH GIÁ RAGAS ---\")\n",
    "    print(result)\n",
    "\n",
    "    # Chuyển sang dataframe để xem cho đẹp\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        df = result.to_pandas()\n",
    "        print(\"\\n--- Bảng kết quả chi tiết ---\")\n",
    "        print(df.to_markdown(index=False))\n",
    "    except ImportError:\n",
    "        print(\"\\nCài 'pandas' và 'tabulate' để xem bảng kết quả đẹp hơn.\")\n",
    "\n",
    "# --- Bắt đầu chạy đánh giá ---\n",
    "print(\"Chuẩn bị chạy đánh giá...\")\n",
    "# Bọc main() trong một hàm run để bắt lỗi\n",
    "def run_evaluation_notebook():\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main_evaluation())\n",
    "    except RuntimeError as e:\n",
    "        if \"cannot be called from a running event loop\" in str(e):\n",
    "            print(\"\\nLỖI: Vẫn gặp lỗi asyncio.\")\n",
    "            print(\"Hãy thử khởi động lại kernel notebook và chạy lại cell này.\")\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "run_evaluation_notebook()\n",
    "print(\"Hoàn tất đánh giá.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv310)",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
