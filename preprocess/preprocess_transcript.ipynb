{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3078cbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\envs\\Phuc1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import glob\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eee78998",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_dir = \"C:\\\\uit_HK5\\\\CS431\\\\final_project\\\\data\\\\test\"\n",
    "output_dir = \"C:\\\\uit_HK5\\\\CS431\\\\final_project\\\\data\\\\transcripts_final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfc01782",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4db54e5",
   "metadata": {},
   "source": [
    "## Load c√°c file txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d751305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_transcripts = glob.glob(os.path.join(transcript_dir, \"*.txt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a9a31c",
   "metadata": {},
   "source": [
    "##Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa0cd0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "googleAPIKey = os.getenv('googleAPIKey')\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",   # ho·∫∑c gemini-1.5-pro, gemini-2.0-flash\n",
    "    temperature=0.0,\n",
    "    google_api_key=googleAPIKey  # üëà th√™m d√≤ng n√†y\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09ea9251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ x·ª≠ l√Ω: qCEs0PIGwek.txt\n",
      "ƒê√£ x·ª≠ l√Ω: Qj6HkeNtIuI.txt\n",
      "ƒê√£ x·ª≠ l√Ω: qV3LpPXdXCE.txt\n",
      "ƒê√£ x·ª≠ l√Ω: QZrUqMbEY8Q.txt\n",
      "ƒê√£ x·ª≠ l√Ω: RU8d6QAuX0k.txt\n",
      "ƒê√£ x·ª≠ l√Ω: RVj2LTBd7IU.txt\n",
      "ƒê√£ x·ª≠ l√Ω: SCNZncN1Hvk.txt\n",
      "ƒê√£ x·ª≠ l√Ω: t5t5G61ZtAg.txt\n",
      "ƒê√£ x·ª≠ l√Ω: TgONnvdebwU.txt\n",
      "ƒê√£ x·ª≠ l√Ω: uFYNI0yZYPo.txt\n",
      "ƒê√£ x·ª≠ l√Ω: VK2w0CO8t1M.txt\n",
      "ƒê√£ x·ª≠ l√Ω: Vm18bFk-RsI.txt\n",
      "ƒê√£ x·ª≠ l√Ω: VrmJPvNSoqM.txt\n",
      "ƒê√£ x·ª≠ l√Ω: wAAPpfRTckg.txt\n",
      "ƒê√£ x·ª≠ l√Ω: wus3iZ0TTFk.txt\n",
      "ƒê√£ x·ª≠ l√Ω: xgJniBOVdL8.txt\n",
      "ƒê√£ x·ª≠ l√Ω: XNqplvgdCKU.txt\n",
      "ƒê√£ x·ª≠ l√Ω: xSEyCajJWVo.txt\n",
      "ƒê√£ x·ª≠ l√Ω: yPzXzbEhUW0.txt\n"
     ]
    }
   ],
   "source": [
    "# H√†m s·ª≠a ch√≠nh t·∫£\n",
    "def correct_spelling(text):\n",
    "    prompt = f\"\"\"\n",
    "    B·∫°n nh·∫≠n ƒë∆∞·ª£c transcript g·ªìm nhi·ªÅu d√≤ng, m·ªói d√≤ng c√≥ timestamp d·∫°ng \"HH:MM:SS - HH:MM:SS, n·ªôi dung\".\n",
    "    H√£y l√†m ƒë√∫ng theo hai quy t·∫Øc, kh√¥ng l√†m th√™m g√¨ kh√°c:\n",
    "\n",
    "    1. Chu·∫©n h√≥a timestamp:\n",
    "       - B·ªè s·ªë 0 d∆∞ ·ªü ƒë·∫ßu gi·ªù, v√≠ d·ª• \"00:00:05\" -> \"0:00:05\".\n",
    "       - Thay nh·ªØng ƒëo·∫°n c√≥ ƒë·ªãnh d·∫°ng t·ª´ 00:00:00 ho·∫∑c  0:00:00 th√†nh 0:00:14.\n",
    "       - Kh√¥ng thay ƒë·ªïi ph√∫t, gi√¢y ho·∫∑c d·∫•u ph·∫©y.\n",
    "       - Gi·ªØ nguy√™n m·ªçi timestamp, kh√¥ng th√™m hay x√≥a d√≤ng.\n",
    "\n",
    "    2. S·ª≠a ch√≠nh t·∫£:\n",
    "       - Ch·ªâ s·ª≠a l·ªói ch√≠nh t·∫£ trong ph·∫ßn vƒÉn b·∫£n sau d·∫•u ph·∫©y.\n",
    "       - Gi·ªØ nguy√™n d·∫•u c√¢u, kho·∫£ng tr·∫Øng, ch·ªØ hoa/th∆∞·ªùng, c·∫•u tr√∫c c√¢u.\n",
    "       - Kh√¥ng th√™m, x√≥a, ho·∫∑c di chuy·ªÉn b·∫•t k·ª≥ k√Ω t·ª± n√†o ngo√†i s·ª≠a ch√≠nh t·∫£.\n",
    "\n",
    "    Transcript:\n",
    "    {text}\n",
    "\n",
    "    Ch·ªâ tr·∫£ v·ªÅ k·∫øt qu·∫£ transcript ƒë√£ ch·ªânh s·ª≠a, kh√¥ng gi·∫£i th√≠ch.\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "# Duy·ªát t·ª´ng file v√† ghi k·∫øt qu·∫£\n",
    "for file_path in list_transcripts:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    corrected_content = correct_spelling(content)\n",
    "\n",
    "    output_path = os.path.join(output_dir, os.path.basename(file_path))\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(corrected_content)\n",
    "\n",
    "    print(f\"ƒê√£ x·ª≠ l√Ω: {os.path.basename(file_path)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5c21f0",
   "metadata": {},
   "source": [
    "## test bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4adca60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\envs\\Phuc1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "corrector = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"bmd1905/vietnamese-correction-v2\"  # L∆∞u √Ω: d√πng d·∫•u '-' chu·∫©n ASCII\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1daa4ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "test = \"C:\\\\uit_HK5\\\\CS431\\\\final_project\\\\data\\\\text_with_timestamps\\\\_HLKpylxwMw.txt\"\n",
    "\n",
    "with open(test, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "corrected_lines = []\n",
    "for line in lines:\n",
    "    if ',' in line:\n",
    "        timestamp, text = line.split(',', 1)\n",
    "        text = text.strip()\n",
    "        corrected_text = corrector(text, max_length=512)[0]['generated_text']\n",
    "        corrected_lines.append(f\"{timestamp}, {corrected_text}\\n\")\n",
    "    else:\n",
    "        corrected_lines.append(line)  # gi·ªØ nguy√™n n·∫øu kh√¥ng ƒë√∫ng forma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7c183c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00:00:00 - 00:00:22, Ti·∫øp theo, ch√∫ng ta s·∫Ω c√πng ti·∫øn h√†nh √¥n t·∫≠p m·ªôt trong nh·ªØng ki·∫øn th·ª©c to√°n n·ªÅn t·∫£ng r·∫•t quan tr·ªçng trong m√¥n n√†y, ƒë√≥ ch√≠nh l√† ƒë·∫°i s·ªë tuy·∫øn t√≠nh.\\n', '00:00:22 - 00:00:36, T·∫°i sao ch√∫ng ta c·∫ßn ph·∫£i t√¨m hi·ªÉu v·ªÅ ƒë·∫°i s·ªë tuy·∫øn t√≠nh? V√¨ n√≥ gi√∫p ch√∫ng ta c√≥ ƒë∆∞·ª£c c√¥ng c·ª• ƒë·ªÉ ch√∫ng ta c√≥ th·ªÉ bi·ªÉu di·ªÖn ƒë∆∞·ª£c d·ªØ li·ªáu.\\n', '00:00:41 - 00:00:48, C√°i th·ª© hai l√† m·ªôt c√¥ng c·ª• ƒë·ªÉ gi√∫p ch√∫ng ta c√≥ th·ªÉ bi·∫øn ƒë·ªïi d·ªØ li·ªáu t·ª´ c√°i d·∫°ng n√†y sang d·∫°ng kh√°c.\\n', '00:00:52 - 00:01:08, V√† hay n√≥i trong ng√¥n ng·ªØ c·ªßa kh√¥ng gian ƒë·∫°i s·ªë tuy·∫øn t√≠nh th√¨ ch√∫ng ta bi·∫øn ƒë·ªïi t·ª´ kh√¥ng gian n√†y sang m·ªôt kh√¥ng gian kh√°c.\\n', '00:01:08 - 00:01:16, C√≤n trong ch·ªß ƒë·ªÅ v·ªÅ m√¥ h√¨nh m√°y h·ªçc th√¨ n√≥ s·∫Ω bi·∫øn ƒë·ªïi t·ª´ ƒë·∫∑c tr∆∞ng t·ª´ d·∫°ng n√†y sang d·∫°ng kh√°c.\\n', '00:01:16 - 00:01:25, T·ª´ ƒë·∫∑c tr∆∞ng ƒë∆°n gi·∫£n l√™n ƒë·∫∑c tr∆∞ng v·ª´a, ƒë·∫∑c tr∆∞ng v·ª´a l√™n ƒë·∫∑c tr∆∞ng c·∫•p cao. Nh·ªù c√°i c√¥ng c·ª• l√† ƒë·∫°i s·ªë tuy·∫øn t√≠nh.\\n', '00:01:25 - 00:01:34, V·∫≠y th√¨ ƒë·∫ßu ti√™n ch√∫ng ta s·∫Ω t√¨m hi·ªÉu v·ªÅ c√°c kh√°i ni·ªám c∆° b·∫£n trong ƒë·∫°i s·ªë tuy·∫øn t√≠nh, ƒë√≥ ch√≠nh l√† kh√°i ni·ªám v·ªÅ tensor.\\n', '00:01:34 - 00:01:40, Khi n√≥i tensor th√¨ ch√∫ng ta c√≥ th·ªÉ n√¢ng t·ª´ s·ªë chi·ªÅu t·ª´ kh√¥ng chi·ªÅu cho ƒë·∫øn nhi·ªÅu chi·ªÅu.\\n', '00:01:40 - 00:01:47, Th√¨ tensor kh√¥ng chi·ªÅu t∆∞∆°ng ƒë∆∞∆°ng v·ªõi m·ªôt c√°i gi√° tr·ªã scalar ƒë·ªÉ bi·ªÉu di·ªÖn cho nh·ªØng c√°i ƒë·∫°i l∆∞·ª£ng v√¥ h∆∞·ªõng.\\n', '00:01:47 - 00:01:55, Ngo√†i ra th√¨ ch√∫ng ta s·∫Ω c√≥ c√°i tensor m·ªôt chi·ªÅu ƒë·ªÉ bi·ªÉu di·ªÖn cho d·∫°ng l√† vector.\\n', '00:01:55 - 00:02:00, Vector s·∫Ω l√† t·∫≠p h·ª£p c·ªßa c√°c gi√° tr·ªã v√¥ h∆∞·ªõng l√† scalar.\\n', '00:02:01 - 00:02:14, Th√¨ ƒë√¢y l√† m·ªôt c√°i tensor. N·∫øu m√† d√πng ƒë·ªÉ bi·ªÉu di·ªÖn d·ªØ li·ªáu th√¨ c√≥ th·ªÉ d√πng ƒë·ªÉ bi·ªÉu di·ªÖn c√°c thu·ªôc t√≠nh ho·∫∑c c√°c ƒë·∫∑c tr∆∞ng c·ªßa m·ªôt ƒë·ªëi t∆∞·ª£ng n√†o ƒë√≥.\\n', '00:02:16 - 00:02:25, V√† khi ch√∫ng ta n√¢ng n√≥ l√™n t·ª´ m·ªôt chi·ªÅu l√™n hai chi·ªÅu th√¨ n√≥ t∆∞∆°ng ƒë∆∞∆°ng v·ªõi vi·ªác ch√∫ng ta l·∫•y nhi·ªÅu c√°i tensor 1D gh√©p l·∫°i v·ªõi nhau.\\n', '00:02:25 - 00:02:29, V√≠ d·ª• nh∆∞ trong h√¨nh n√†y th√¨ ch√∫ng ta th·∫•y l√† n√≥ s·∫Ω gh√©p c√°c c√°i c·ªôt n√†y l·∫°i.\\n', '00:02:30 - 00:02:35, V√† m·ªói m·ªôt c√°i ƒë·∫∑c tr∆∞ng n√†y th√¨ n√≥ s·∫Ω l√† m·ªôt ƒë·ªëi t∆∞·ª£ng.\\n', '00:02:35 - 00:02:41, Nh∆∞ v·∫≠y th√¨ ho·∫∑c l√† m·ªôt ƒë·∫∑c tr∆∞ng, m·ªôt lo·∫°i ƒë·∫∑c tr∆∞ng.\\n', '00:02:42 - 00:02:50, Nh∆∞ v·∫≠y th√¨ to√†n b·ªô c√°i ma tr·∫≠n n√†y hay c√≤n g·ªçi l√† 2D tensor n√†y s·∫Ω l√† m·ªôt t·∫≠p h·ª£p c√°c c√°i ƒë·ªëi t∆∞·ª£ng ho·∫∑c l√† m·ªôt t·∫≠p h·ª£p c√°c c√°i ƒë·∫∑c tr∆∞ng.\\n', '00:02:51 - 00:02:57, V√† khi ch√∫ng ta n√¢ng l√™n l√† ba chi·ªÅu th√¨ ch√∫ng ta s·∫Ω c√≥ c√°c c√°i l·∫Øc c√°t kh√°c nhau.\\n', '00:02:57 - 00:03:01, V√≠ d·ª• nh∆∞ m·ªói m·ªôt c√°i l·∫Øc c√°t n√†y s·∫Ω l√† m·ªôt t·∫≠p h·ª£p c√°c c√°i ƒë·∫∑c tr∆∞ng.\\n', '00:03:03 - 00:03:08, Gh√©p c√°c c√°i l·∫Øc c√°t ƒë√≥ l·∫°i v·ªõi nhau th√¨ ch√∫ng ta s·∫Ω c√≥ m·ªôt tensor ba chi·ªÅu.\\n', '00:03:09 - 00:03:13, V√† nhi·ªÅu c√°i tensor ba chi·ªÅu ch·ªìng l√™n th√¨ ch√∫ng ta s·∫Ω c√≥ 4D.\\n', '00:03:14 - 00:03:24, R·ªìi c·ª© nh∆∞ v·∫≠y th√¨ ch√∫ng ta l·∫°i n·ªëi, th√¨ ch√∫ng ta th·∫•y c√°i quy lu·∫≠t n√†y n√≥ s·∫Ω c·ª© l·∫∑p ƒëi l·∫∑p l·∫°i ƒë·ªÉ ch√∫ng ta c√≥ th·ªÉ tr·ª±c quan h√≥a ƒë∆∞·ª£c c√°i d·ªØ li·ªáu c·ªßa m√¨nh.\\n', '00:03:25 - 00:03:27, Th√¨ ch√∫ng ta s·ª≠ d·ª•ng c√°i tensor.\\n', '00:03:28 - 00:03:30, V√† ƒë√¢y l√† m·ªôt trong nh·ªØng c√°i c√¥ng c·ª• r·∫•t l√† hi·ªáu qu·∫£.\\n', '00:03:31 - 00:03:35, Nh∆∞ng m√† th√¥ng th∆∞·ªùng trong l·∫≠p tr√¨nh th√¨ th∆∞·ªùng ch√∫ng ta s·∫Ω d√πng ƒë·∫øn 4D tensor.\\n', '00:03:36 - 00:03:39, Th∆∞·ªùng ch√∫ng ta s·∫Ω d√πng ƒë·∫øn m·ªôt c·∫•p ƒë·ªô l√† 4D tensor.\\n', '00:03:39 - 00:03:46, Trong ƒë√≥ c√°i chi·ªÅu ƒë·∫ßu ti√™n c√≥ th·ªÉ l√† c√°i chi·ªÅu v·ªÅ s·ªë l∆∞·ª£ng c√°i m·∫´u d·ªØ li·ªáu c·ªßa m√¨nh.\\n', '00:03:46 - 00:03:50, V√≠ d·ª• nh∆∞ l√† batch size, t·ª©c l√† c√°i k√≠ch th∆∞·ªõc d·ªØ li·ªáu c·ªßa m√¨nh.\\n', '00:03:51 - 00:03:53, Sau ƒë√≥ l√† ch√∫ng ta s·∫Ω ƒë·∫øn c√°i chi·ªÅu ƒë·ªô s√¢u, depth.\\n', '00:03:54 - 00:03:56, Th√¨ ·ªü ƒë√¢y ch√≠nh l√† c√°i s·ªë ƒë·∫∑c tr∆∞ng c·ªßa m√¨nh.\\n', '00:03:58 - 00:04:01, S·ªë ƒë·∫∑c tr∆∞ng c·ªßa m·ªôt c√°i layer.\\n', '00:04:02 - 00:04:04, Sau ƒë√≥ s·∫Ω ƒë·∫øn l√† width v√† height.\\n', '00:04:05 - 00:04:07, T∆∞∆°ng ·ª©ng l√† hai c√°i chi·ªÅu kh√¥ng gian c·ªßa m√¨nh.\\n', '00:04:07 - 00:04:11, Chi·ªÅu ngang v√† chi·ªÅu cao trong chi·ªÅu tr·ª•c kh√¥ng gian c·ªßa m√¨nh.\\n', '00:04:12 - 00:04:22, Th√¨ ƒë√¢y l√† m·ªôt c√°i d·∫°ng bi·ªÉu di·ªÖn ho·∫∑c ·ª©ng d·ª•ng c·ªßa 4D tensor khi ch√∫ng ta √°p v√†o b√™n trong c√°i d·ªØ li·ªáu khi hu·∫•n luy·ªán v·ªõi c√°c m√¥n h·ªçc s√¢u.\\n', '00:04:24 - 00:04:27, Ngo√†i ra th√¨ ch√∫ng ta s·∫Ω c√≥ m·ªôt s·ªë c√°i ph√©p to√°n.\\n', '00:04:27 - 00:04:30, C√°i ph√©p to√°n ƒë·∫ßu ti√™n v√† ƒë∆∞·ª£c s·ª≠ d·ª•ng r·∫•t l√† nhi·ªÅu ƒë√≥ ch√≠nh l√† ph√©p chuy·ªÉn v·ªã.\\n', '00:04:31 - 00:04:36, M·ª•c ti√™u c·ªßa c√°i ph√©p chuy·ªÉn v·ªã n√†y ƒë√≥ l√† ƒë·ªÉ bi·∫øn c√°c c√°i vector d·∫°ng c·ªôt c·ªßa ma tr·∫≠n th√†nh d√≤ng.\\n', '00:04:37 - 00:04:40, Bi·∫øn t·ª´ c·ªôt th√†nh d·∫°ng d√≤ng ho·∫∑c ng∆∞·ª£c l·∫°i.\\n', '00:04:40 - 00:04:44, V√≠ d·ª• nh∆∞ ma tr·∫≠n A ch√∫ng ta ƒëang ·ªü d·∫°ng c·ªôt ch√∫ng ta s·∫Ω bi·∫øn n√≥ th√†nh d√≤ng.\\n', '00:04:45 - 00:04:53, Th√¨ m·ª•c ti√™u c·ªßa c√°i vi·ªác l√† chuy·ªÉn v·ªÅ c√°i d·∫°ng chuy·ªÉn v·ªã ƒë·ªÉ ph·ª•c v·ª• cho c√°i vi·ªác l√† t∆∞∆°ng t√°c nh√¢n gi·ªØa c√°c c√°i vector v·ªõi nhau.\\n', '00:04:53 - 00:04:55, Ho·∫∑c l√† nh√¢n gi·ªØa c√°c c√°i ma tr·∫≠n v·ªõi nhau.\\n', '00:04:55 - 00:04:57, Sau cho n√≥ ƒë·ªìng b·ªô.\\n', '00:04:58 - 00:05:00, Ti·∫øp theo ƒë√≥ l√† c√°i ph√©p c·ªông vector.\\n', '00:05:00 - 00:05:02, Th√¨ vi·ªác c·ªông vector n√≥ c≈©ng kh√° l√† ƒë∆°n gi·∫£n.\\n', '00:05:02 - 00:05:04, Ch√∫ng ta s·∫Ω c·ªông theo t·ª´ng ph·∫ßn t·ª≠.\\n', '00:05:04 - 00:05:11, V√≠ d·ª• nh∆∞ ·ªü ƒë√¢y ch√∫ng ta c√≥ x2 v√† y2 th√¨ c·ªông cho c√°i ph·∫ßn t·ª≠ x2 v√† y2 ch√∫ng ta s·∫Ω ra ƒë∆∞·ª£c c√°i vector m·ªõi nh∆∞ th·∫ø n√†y.\\n', '00:05:12 - 00:05:25, V√† chi·∫øu trong c√°i kh√¥ng gian th√¨ c√°i vector x c·ªßa m√¨nh v√† vector y th√¨ khi ch√∫ng ta c·ªông l·∫°i n√≥ s·∫Ω bi·∫øn th√†nh m·ªôt c√°i vector kh√°c.\\n', '00:05:27 - 00:05:29, V√≠ d·ª• nh∆∞ ƒë√¢y l√† x c·ªông y.\\n', '00:05:29 - 00:05:32, Vector x c·ªông y n√†y hi·ªÉu m·ªôt c√°ch nom na.\\n', '00:05:32 - 00:05:37, √ù nghƒ©a c·ªßa n√≥ l√† vector x l√† ƒë·∫∑c tr∆∞ng ban ƒë·∫ßu.\\n', '00:05:37 - 00:05:43, N√≥ s·∫Ω b·ªã ƒëi·ªÅu h∆∞·ªõng v√† chuy·ªÉn h∆∞·ªõng sang m·ªôt c√°i h∆∞·ªõng kh√°c.\\n', '00:05:43 - 00:05:48, Vector y ·ªü ƒë√¢y ch√∫ng ta s·∫Ω t∆∞∆°ng ƒë∆∞∆°ng v·ªõi vi·ªác ch√∫ng ta ƒë∆∞a vector y l√™n.\\n', '00:05:48 - 00:05:53, V√† khi ƒë√≥ x c·ªông y th√¨ n√≥ ƒë√£ b·ªã ƒëi·ªÅu h∆∞·ªõng v·ªÅ t∆∞·ªõng t·∫°i v·ªã tr√≠ m·ªõi l√† ·ªü ƒë√¢y.\\n', '00:05:53 - 00:06:00, Y l√† gi·ªëng nh∆∞ l√† m·ªôt vector ƒë·ªÉ ch√∫ng ta thay ƒë·ªïi h∆∞·ªõng.\\n', '00:06:04 - 00:06:08, C√°i n√†y trong v·∫≠t l√Ω ch√∫ng ta th·∫•y d√πng r·∫•t l√† nhi·ªÅu.\\n', '00:06:08 - 00:06:10, Trong m√°y h·ªçc th√¨ c≈©ng v·∫≠y.\\n', '00:06:10 - 00:06:14, C√°c ph√©p c·ªông hai ma tr·∫≠n v√† hai vector.\\n', '00:06:14 - 00:06:22, Ph√©p c·ªông ma tr·∫≠n th√¨ n√¢ng s·ªë chi·ªÅu l√™n thay v√¨ ch√∫ng ta c·ªông tr√™n hai tensor m·ªôt chi·ªÅu, t·ª©c l√† vector.\\n', '00:06:22 - 00:06:27, Th√¨ ch√∫ng ta s·∫Ω c·ªông hai tensor hai thi·ªÅu.\\n', '00:06:27 - 00:06:31, V√† ch√∫ng ta c≈©ng s·∫Ω c·ªông theo element-wise, t·ª©c l√† c·ªông theo t·ª´ng ph·∫ßn t·ª≠.\\n', '00:06:31 - 00:06:35, Th√¨ ch√∫ng ta s·∫Ω l·∫•y ph·∫ßn t·ª≠ n√†y, c·ªông v·ªõi ph·∫ßn t·ª≠ n√†y ƒë·ªÉ c·ªông v·ªõi ph·∫ßn t·ª≠ n√†y.\\n', '00:06:35 - 00:06:39, Th√¨ ch√∫ng ta s·∫Ω l·∫•y ph·∫ßn t·ª≠ n√†y, c·ªông v·ªõi ph·∫ßn t·ª≠ n√†y ƒë·ªÉ ra ƒë∆∞·ª£c ph·∫ßn t·ª≠ ·ªü ƒë√¢y.\\n', '00:06:39 - 00:06:43, L√† th·ª±c hi·ªán theo ƒë√∫ng th·ª© t·ª± v√† v·ªã tr√≠.\\n', '00:06:43 - 00:06:48, T∆∞∆°ng t·ª± nh∆∞ v·∫≠y th√¨ ch√∫ng ta s·∫Ω c√≥ ph√©p t√≠ch Hadamard cho t·ª´ng ph·∫ßn t·ª≠.\\n', '00:06:48 - 00:06:52, T·ª©c l√† l·∫•y t·∫°i m·ªôt v·ªã tr√≠, nh√¢n v·ªõi m·ªôt v·ªã tr√≠ t∆∞∆°ng ·ª©ng.\\n', '00:06:52 - 00:06:56, Sau ƒë√≥ s·∫Ω ra ƒë∆∞·ª£c gi√° tr·ªã t·∫°i v·ªã tr√≠ ƒë√≥.\\n', '00:06:56 - 00:06:58, Nh√¢n Element-wise.\\n', '00:06:58 - 00:07:04, V√† khi ƒë√≥ th√¨ k√≠ch th∆∞·ªõc c·ªßa ma tr·∫≠n Output c·ªßa m√¨nh s·∫Ω l√† gi·ªëng nh∆∞ l√† m·ªôt ph·∫ßn t·ª≠.\\n', '00:07:04 - 00:07:08, Gi·ªëng v·ªõi l·∫°i c√°i k√≠ch th∆∞·ªõc c·ªßa hai ma tr·∫≠n Output v√†o.\\n', '00:07:08 - 00:07:15, Th√¨ c√°i ph√©p n√†y hi·ªÉu m·ªôt c√°ch n√¥ na l√† ch√∫ng ta ƒëang ƒëi nh√¢n t·ª´ng ph·∫ßn t·ª≠ v√† ch√∫ng ta s·∫Ω nh√¢n tr·ªçng s·ªë.\\n', '00:07:15 - 00:07:21, V√≠ d·ª• nh∆∞ ma tr·∫≠n A l√† m·ªôt ƒë·∫∑c tr∆∞ng g·ªëc. ƒê√¢y l√† ƒë·∫∑c tr∆∞ng g·ªëc.\\n', '00:07:21 - 00:07:26, C√≤n ma tr·∫≠n B t∆∞∆°ng ·ª©ng l√† m·ªôt tr·ªçng s·ªë.\\n', '00:07:26 - 00:07:32, Th·ªÉ hi·ªán c√°i vai tr√≤ c·ªßa t·ª´ng ƒë·∫∑c tr∆∞ng ·ªü tr√™n ma tr·∫≠n A.\\n', '00:07:32 - 00:07:37, Th·∫ø th√¨ khi ch√∫ng ta l·∫•y c√°i ƒë·∫∑c tr∆∞ng A n√†y, ch√∫ng ta nh√¢n v·ªõi l·∫°i c√°i tr·ªçng s·ªë t·∫°i v·ªã tr√≠ n√†y.\\n', '00:07:37 - 00:07:43, ƒê·ªÉ t·∫°o ra m·ªôt c√°i ƒë·∫∑c tr∆∞ng m·ªõi c√≥ c√°i s·ª± gia gi·∫£m v·ªÅ c√°i vi·ªác m√† t√≠nh to√°n.\\n', '00:07:43 - 00:07:48, V√≠ d·ª• nh∆∞ ƒë·∫∑c tr∆∞ng n√†y. M√¨nh nh√¢n v·ªõi m·ªôt c√°i h·ªá s·ªë l√† b·∫±ng 0 ƒëi.\\n', '00:07:51 - 00:07:53, M√¨nh nh√¢n v·ªõi m·ªôt c√°i tr·ªçng s·ªë l√† b·∫±ng 0.\\n', '00:07:53 - 00:07:55, Th√¨ c√°i k·∫øt qu·∫£ ra ·ªü ƒë√¢y s·∫Ω l√† b·∫±ng 0.\\n', '00:07:55 - 00:08:00, Th√¨ h√†m √Ω ƒë√≥ l√† c√°i ƒë·∫∑c tr∆∞ng ·ªü ƒë√¢y s·∫Ω kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng.\\n', '00:08:00 - 00:08:06, Nh∆∞ng n·∫øu b√≠ d·ª• ·ªü ƒë√¢y, ch√∫ng ta c√≥ m·ªôt c√°i ƒë·∫∑c tr∆∞ng v√† tr·ªçng s·ªë ·ªü ƒë√¢y l√† b·∫±ng 1.\\n', '00:08:06 - 00:08:09, Th√¨ h√†m √Ω ƒë√≥ l√† ch√∫ng ta s·∫Ω l·∫•y tr·ªçng b·ªô c√°i ƒë·∫∑c tr∆∞ng n√†y.\\n', '00:08:09 - 00:08:11, V√†o ƒë√¢y, ch√©p qua ƒë√¢y.\\n', '00:08:11 - 00:08:14, C√≤n n·∫øu nh∆∞ c√°i tr·ªçng s·ªë c·ªßa ch√∫ng ta l√† 0.5.\\n', '00:08:14 - 00:08:22, Th√¨ t·ª©c l√† ch√∫ng ta ch·ªâ l·∫•y 50% c√°i l∆∞·ª£ng th√¥ng tin t·∫°i c√°i v·ªã tr√≠ n√†y ƒë·ªÉ ch√©p qua ƒë√¢y.\\n', '00:08:22 - 00:08:29, Th√¨ √Ω nghƒ©a v·ªÅ m·∫∑t t∆∞∆°ng t√°c trong m√°y t√≠nh trong m√¥ h√¨nh m√°y h·ªçc.\\n', '00:08:29 - 00:08:32, ƒê√≥ l√† c√≥ th·ªÉ l√† m·ªôt c√°i ph√©p nh√¢n tr·ªçng s·ªë t·ª´ng ph·∫ßn t·ª≠.\\n', '00:08:34 - 00:08:38, R·ªìi khi ch√∫ng ta n√≥i ƒë·∫øn c√°i ph√©p nh√¢n th√¨ ch√∫ng ta ph·∫£i ch√∫ √Ω.\\n', '00:08:38 - 00:08:41, ƒê√≥ l√† c√°i ph√©p v·ª´a r·ªìi l√† t√≠ch ph·∫ßn t·ª≠ t·ª´ng ph·∫ßn t·ª≠.\\n', '00:08:41 - 00:08:45, C√≤n b√¢y gi·ªù ch√∫ng ta s·∫Ω c√≥ c√°i kh√°i ni·ªám nh√¢n n·ªØa.\\n', '00:08:45 - 00:08:48, Nh∆∞ng m√† nh√¢n theo ki·ªÉu l√† ph√©p chi·∫øu.\\n', '00:08:48 - 00:08:50, Th√¨ ƒë√¢y ch√≠nh l√† m·ªôt c√°i ph√©p chi·∫øu.\\n', '00:08:50 - 00:08:52, B·∫£n ch·∫•t c·ªßa ƒë√≥ l√† m·ªôt ph√©p chi·∫øu.\\n', '00:08:54 - 00:08:58, V√≠ d·ª•, ch√∫ng ta nh√¢n v√¥ h∆∞·ªõng 2 c√°i vector.\\n', '00:08:58 - 00:09:01, C√°i n√†y c√≤n c√≥ m·ªôt c√°i t·ª´ kh√°c g·ªçi l√† dot v√† dot.\\n', '00:09:03 - 00:09:05, Th√¨ b·∫£n ch·∫•t c·ªßa c√°i ph√©p nh√¢n A,.\\n', '00:09:05 - 00:09:07, M√°y vector A v·ªõi vector B,.\\n', '00:09:07 - 00:09:09, Trong c√°i kh√¥ng gian ·∫£m ƒë·∫°m, chi·ªÅu,.\\n', '00:09:09 - 00:09:10, Vector n√†y g·ªìm c√≥ n chi·ªÅu.\\n', '00:09:10 - 00:09:15, V·ªÅ m·∫∑t t√≠nh to√°n th√¨ ch√∫ng ta s·∫Ω l·∫•y t·ª´ng ph·∫ßn t·ª≠ nh√¢n.\\n', '00:09:15 - 00:09:18, Nh∆∞ng m√† sau ƒë√≥ ch√∫ng ta s·∫Ω ƒëi c·ªông l·∫°i h·∫øt l·∫°i v·ªõi nhau.\\n', '00:09:18 - 00:09:20, ƒê·ªÉ ra m·ªôt c√°i t·ªïng.\\n', '00:09:20 - 00:09:22, V√† ƒë√¢y s·∫Ω l√† m·ªôt c√°i gi√° tr·ªã l√† scalar.\\n', '00:09:24 - 00:09:27, V√† √Ω nghƒ©a v·ªÅ m·∫∑t ƒë·∫°i s·ªë c·ªßa n√≥ ƒë√≥ l√† g√¨?\\n', '00:09:27 - 00:09:32, Vector A v√† vector B ·ªü ƒë√¢y th√¨ ch√∫ng ta s·∫Ω chi·∫øu A xu·ªëng B.\\n', '00:09:32 - 00:09:34, Chi·∫øu A xu·ªëng B.\\n', '00:09:34 - 00:09:36, V√† ƒë∆∞·ª£c m·ªôt c√°i ƒëi·ªÉm n√†y.\\n', '00:09:36 - 00:09:43, Th√¨ c√°i gi√° tr·ªã t√≠ch v√¥ h∆∞·ªõng c·ªßa A v√† B ch√≠nh l√† c√°i ƒë·ªô d√†i c·ªßa ƒëo·∫°n th·∫≥ng.\\n', '00:09:43 - 00:09:45, V√≠ d·ª• ƒë√¢y l√† OH, ƒë√¢y l√† H.\\n', '00:09:45 - 00:09:48, C·ªßa O.H nh√¢n v·ªõi l·∫°i OB.\\n', '00:09:48 - 00:09:50, ƒê√¢y l√† OA.\\n', '00:09:50 - 00:09:54, Th√¨ n√≥ s·∫Ω l√† b·∫±ng OH nh√¢n v·ªõi l·∫°i OB.\\n', '00:09:54 - 00:09:57, V√† n√≥ s·∫Ω l√† m·ªôt c√°i gi√° tr·ªã scalar.\\n', '00:09:59 - 00:10:00, Th√¨ ·ªü m·ªôt g√≥c ƒë·ªô kh√°c,.\\n', '00:10:00 - 00:10:06, C√°i ph√©p t√≠ch v√¥ h∆∞·ªõng n√†y c√≥ th·ªÉ hi·ªÉu l√† c√°i s·ª± t∆∞∆°ng ƒë·ªìng gi·ªØa hai vector A v√† B.\\n', '00:10:06 - 00:10:10, N·∫øu nh∆∞ hai vector A v√† B l·ªõn th√¨ c√°i ƒë·ªô t∆∞∆°ng ƒë·ªìng c·ªßa n√≥ s·∫Ω cao.\\n', '00:10:10 - 00:10:13, C√≤n ·ªü m·ªôt c√°i g√≥c ƒë·ªô ƒë·∫∑c tr∆∞ng,.\\n', '00:10:13 - 00:10:20, Ch√∫ng ta ƒëang chuy·ªÉn ƒë·ªïi t·ª´ m·ªôt c√°i ƒëi·ªÉm A chi·∫øu xu·ªëng d∆∞·ªõi m·ªôt c√°i ƒëi·ªÉm B ƒë·ªÉ ƒë∆∞a v·ªÅ c√°i kh√¥ng gian c·ªßa B.\\n', '00:10:24 - 00:10:29, V√† ch√∫ng ta c√≥ m·ªôt ch√∫ √Ω ƒë√≥ l√† A chi·∫øu l√™n B c≈©ng l√† b·∫±ng B chi·∫øu xu·ªëng A.\\n', '00:10:29 - 00:10:32, Do ƒë√≥ A chuy·ªÉn v·ªã nh√¢n B s·∫Ω l√† b·∫±ng B nh√¢n A.\\n', '00:10:32 - 00:10:34, Sau ƒë√≥ ch√∫ng ta n√¢ng c·∫•p l√™n.\\n', '00:10:34 - 00:10:37, ƒê√≥ l√† ph√©p nh√¢n gi·ªØa ma tr·∫≠n v·ªõi l·∫°i vector.\\n', '00:10:37 - 00:10:40, Thay v√¨ ch√∫ng ta th·ª±c hi·ªán gi·ªØa vector v·ªõi vector,.\\n', '00:10:40 - 00:10:42, B√¢y gi·ªù ch√∫ng ta s·∫Ω l√†m h√†ng ho·∫°t.\\n', '00:10:42 - 00:10:49, Ch√∫ng ta s·∫Ω c√≥ vector X v√† ch√∫ng ta s·∫Ω l·∫ßn l∆∞·ª£c nh√¢n h√†ng ho·∫°t v·ªõi l·∫°i vector A, ..\\n', '00:10:49 - 00:10:53, A1, A2, A3, cho ƒë·∫øn An.\\n', '00:10:53 - 00:10:57, V√† c√°c c√°i gi√° tr·ªã t√≠ch v√¥ h∆∞·ªõng ƒë√≥ s·∫Ω ƒë∆∞·ª£c ƒë∆∞a v√†o c√°i vector nh∆∞ th·∫ø n√†y.\\n', '00:10:57 - 00:11:02, ƒê√≥ l√† A1 x A2 chuy·ªÉn v·ªÅ x, A n chuy·ªÉn v·ªÅ x.\\n', '00:11:02 - 00:11:09, Th√¨ ƒë√¢y ch√∫ng ta ƒëang th·ª±c hi·ªán nh√¢n h√†ng ho·∫°t x v·ªõi l·∫°i c√°c c√°i vector h√†ng c·ªßa ma tr·∫≠n A.\\n', '00:11:09 - 00:11:16, V√† c√°i ph√©p n√†y th√¨ hi·ªÉu n√¥m na n√≥ c≈©ng l√† m·ªôt c√°i ph√©p chi·∫øu ho·∫∑c l√† m·ªôt c√°i ph√©p chuy·ªÉn kh√¥ng gian.\\n', '00:11:17 - 00:11:19, Chuy·ªÉn kh√¥ng gian.\\n', '00:11:19 - 00:11:24, V√≠ d·ª• nh∆∞ ch√∫ng ta c√≥ m·ªôt c√°i kh√¥ng gian l√† nh∆∞ th·∫ø n√†y.\\n', '00:11:24 - 00:11:31, V√† m·ªôt c√°i ƒëi·ªÉm trong c√°i kh√¥ng gian th√¨ khi ch√∫ng ta chi·∫øu xu·ªëng d∆∞·ªõi m·ªôt c√°i m·∫∑t ph·∫≥ng n√†o ƒë√≥, v√≠ d·ª• v·∫≠y.\\n', '00:11:32 - 00:11:44, R·ªìi, th√¨ khi ch√∫ng ta chi·∫øu xu·ªëng, ƒë·ªÉ ƒë·∫°i di·ªán cho c√°i m·∫∑t ph·∫≥ng n√†y, ƒë·∫°i di·ªán cho c√°i m·∫∑t ph·∫≥ng n√†y, m·∫∑t ph·∫≥ng P ƒëi,\\n', '00:11:44 - 00:11:49, th√¨ n√≥ s·∫Ω c√≥ c√°i tham s·ªë v√† tham s·ªë ƒë√≥ ch√≠nh l√† c√°i ma tr·∫≠n A c·ªßa m√¨nh.\\n', '00:11:49 - 00:11:59, V√† khi chi·∫øu, khi chi·∫øu xu·ªëng c√°i kh√¥ng gian c·ªßa A th√¨ ch√∫ng ta s·∫Ω chuy·ªÉn t·ª´ m·ªôt c√°i vector l√† 3 chi·ªÅu,\\n', '00:11:59 - 00:12:07, V√≠ d·ª• v·∫≠y, thu·ªôc R3, chuy·ªÉn xu·ªëng m·ªôt c√°i kh√¥ng gian 2 chi·ªÅu th√¨ l√∫c ƒë√≥ l√† ch√∫ng ta ch·ªâ c√≤n l√† m·ªôt c√°i ƒëi·ªÉm k√≠ch s·ªüi,.\\n', '00:12:07 - 00:12:11, nh∆∞ng m√† n·∫±m trong c√°i kh√¥ng gian ch·ªâ c√≥ 2 chi·ªÅu, ch·ª© kh√¥ng gian m·∫∑t ph·∫≥ng.\\n', '00:12:11 - 00:12:20, ·ªû ƒë√¢y l√† m·ªôt c√°i ph√©p chi·∫øu t·ª´ m·ªôt c√°i kh√¥ng gian l·ªõn chi·ªÅu l√† 3 chi·ªÅu xu·ªëng m·ªôt c√°i kh√¥ng gian m·∫∑t ph·∫≥ng l√† 2 chi·ªÅu.\\n', '00:12:20 - 00:12:25, V√† ng∆∞·ª£c l·∫°i, ch√∫ng ta c≈©ng s·∫Ω c√≥ c√°i ph√©p chi·∫øu t·ª´ kh√¥ng gian th·∫•p chi·ªÅu l√™n kh√¥ng gian cao chi·ªÅu.\\n', '00:12:26 - 00:12:29, V√≠ d·ª• t·ª´ kh√¥ng gian 2 chi·ªÅu l√™n kh√¥ng gian 3 chi·ªÅu.\\n', '00:12:35 - 00:12:39, V√† ti·∫øp theo th√¨ ch√∫ng ta s·∫Ω c√≥ c√°i ph√©p nh√¢n gi·ªØa ma tr·∫≠n v·ªõi ma tr·∫≠n,.\\n', '00:12:39 - 00:12:45, t·ª©c l√† ch√∫ng ta c≈©ng s·∫Ω th·ª±c hi·ªán h√†ng lo·∫°t gi·ªØa c√°c c√°i c·∫∑p AI v·ªõi l·∫°i BZ.\\n', '00:12:45 - 00:12:51, A s·∫Ω l√† ch·∫°y theo, t·ª´ tr√™n xu·ªëng d∆∞·ªõi l√† theo h√†ng, c√≤n B s·∫Ω l√† theo c·ªôt, l√† t·ª´ tr√°i sang ph·∫£i.\\n', '00:12:51 - 00:12:55, V√† khi l·∫•y hai c√°i ma tr·∫≠n n√†y nh√¢n v·ªõi nhau th√¨ ch√∫ng ta s·∫Ω ra m·ªôt c√°i ma tr·∫≠n,.\\n', '00:12:55 - 00:13:00, Trong ƒë√≥ c√°c c√°i ph·∫ßn t·ª≠ c·ªßa m√¨nh th√¨ s·∫Ω ƒë∆∞·ª£c nh√¢n l·∫ßn l∆∞·ª£t t·ª´ tr√°i sang ph·∫£i v√† t·ª´ tr√™n xu·ªëng d∆∞·ªõi.\\n', '00:13:00 - 00:13:04, Th√¨ ch√∫ng ta s·∫Ω ra ƒë∆∞·ª£c c√°i ma tr·∫≠n nh∆∞ th·∫ø n√†y.\\n', '00:13:04 - 00:13:08, V√† ƒë√¢y c≈©ng ki·ªÉu m·ªôt c√°ch n√¥n na, ƒë√≥ l√† m·ªôt c√°i ph√©p chi·∫øu,\\n', '00:13:08 - 00:13:13, nh∆∞ng m√† chi·∫øu h√†ng lo·∫°t tr√™n r·∫•t nhi·ªÅu ma tr·∫≠n v√† c√°c c√°i kh√¥ng gian.\\n', '00:13:14 - 00:13:20, V√† t√≠nh ch·∫•t c·ªßa c√°i vi·ªác m√† nh√¢n ma tr·∫≠n v·ªõi ma tr·∫≠n ƒë√≥ l√†,\\n', '00:13:20 - 00:13:26, Th·ª© nh·∫•t, c√°i ph√©p nh√¢n ma tr·∫≠n n√†y kh√¥ng c√≥ t√≠nh giao ho√°n, t·ª©c l√† A nh√¢n B th√¨ kh√¥ng c√≥ b·∫±ng B nh√¢n A.\\n', '00:13:26 - 00:13:33, C√°i th·ª© hai ƒë√≥ l√† c√°i k·∫øt h·ª£p, c√°i t√≠nh ch·∫•t k·∫øt h·ª£p l√† A nh√¢n B, t·∫•t c·∫£ nh√¢n C th√¨ s·∫Ω l√† b·∫±ng A nh√¢n BC.\\n', '00:13:34 - 00:13:44, T√≠nh ch·∫•t k·∫øt h·ª£p theo ki·ªÉu l√†, c√°i n√†y gi·ªëng nh∆∞ ph√¢n ph·ªëi t·ª´ A, B c·ªông C th√¨ s·∫Ω l√† A nh√¢n B v√† A c·ªông C.\\n', '00:13:44 - 00:13:50, C√°i n√†y l√† c√°i ph√©p t√≠nh ch·∫•t ph√¢n ph·ªëi.\\n', '00:13:50 - 00:13:58, V√† ch√∫ng ta s·∫Ω chuy·ªÉn v·ªã, th√¨ ma tr·∫≠n A m√† chuy·ªÉn v·ªã, r·ªìi chuy·ªÉn v·ªã l·∫°i th√¨ n√≥ s·∫Ω b·∫±ng A.\\n', '00:13:58 - 00:14:05, Th√¨ khi ƒë√≥ l√† A, B, t·∫•t c·∫£ chuy·ªÉn v·ªã s·∫Ω b·∫±ng l√† B chuy·ªÉn v·ªã nh√¢n v·ªõi A chuy·ªÉn v·ªã.\\n', '00:14:05 - 00:14:10, R·ªìi A c·ªông B chuy·ªÉn v·ªã s·∫Ω l√† b·∫±ng A chuy·ªÉn v·ªã c·ªông cho B chuy·ªÉn v·ªã.\\n', '00:14:10 - 00:14:16, V√† m·ªôt s·ªë c√°i ma tr·∫≠n ƒë·∫∑c bi·ªát, v√≠ d·ª• nh∆∞ l√† ma tr·∫≠n ƒë∆°n v·ªã, th√¨ ƒë√¢y l√† m·ªôt c√°i ma tr·∫≠n m√† c√≥ t√≠nh ch·∫•t ƒë√≥, l√†.\\n', '00:14:16 - 00:14:21, X m√† nh√¢n v·ªõi l·∫°i m·ªôt c√°i ma tr·∫≠n chuy·ªÉn v·ªã th√¨ n√≥ s·∫Ω b·∫±ng 9X.\\n', '00:14:21 - 00:14:27, Hi·ªÉu m·ªôt c√°ch n√¥ng d√¢n, ƒë√¢y gi·ªëng nh∆∞ l√† s·ªë 1, 1 nh√¢n v·ªõi c√°i g√¨ th√¨ c≈©ng b·∫±ng c√°i s·ªë 9.\\n', '00:14:28 - 00:14:36, ma tr·∫≠n ngh·ªãch ƒë·∫£o l√† m·ªôt c√°i ma tr·∫≠n vu√¥ng, l√† m·ªôt c√°i ma tr·∫≠n vu√¥ng m√† m·∫°ng t√≠nh ch·∫•t ƒë√≥ l√† A nh√¢n v·ªõi l·∫°i A.\\n', '00:14:36 - 00:14:39, Ngh·ªãch ƒë·∫£o th√¨ n√≥ s·∫Ω l√† b·∫±ng ma tr·∫≠n ƒë∆°n v·ªã.\\n']\n"
     ]
    }
   ],
   "source": [
    "print(corrected_lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Phuc1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
