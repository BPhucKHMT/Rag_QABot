0:00:00 - 0:00:12, Chủ đề tập trung vào dự án mô hình seq2seq, sequence-to-sequence và cơ chế attention.
0:00:14 - 0:00:24, Trong bài hôm nay thì chúng ta sẽ cùng tìm hiểu về bài toán dịch máy, mô hình seq2seq, sequence-to-sequence và cơ chế attention.
0:00:24 - 0:00:31, Bài này, chúng ta sẽ dựa trên ý tưởng của mạng RNN trước đây.
0:00:31 - 0:00:36, Và mạng RNN có bao gồm 2 bước biến đổi.
0:00:36 - 0:00:40, Bước số 1, chúng ta sẽ đi tính trạng thái ẩn.
0:00:40 - 0:00:45, Trạng thái ẩn sẽ tổng hợp thông tin của quá khứ và thông tin của hiện tại.
0:00:45 - 0:00:50, Sau đó, từ trạng thái ẩn này, chúng ta sẽ đi tính toán ra giá trị output.
0:00:50 - 0:00:55, Và một số biến thể của mạng RNN mà chúng ta đã được thảo luận trước đây
0:00:55 - 0:00:59, Đó là biến thể về Long Short-Term Memory LSTM
0:00:59 - 0:01:03, Biến thể về Bidirectional RNN, RNN 2 chiều
0:01:03 - 0:01:06, Và Stacked RNN
0:01:06 - 0:01:15, Thì trong nội dung đầu tiên mà chúng ta sẽ học bài hôm nay đó chính là tìm hiểu về bài toán dịch máy Machine Translation
0:01:15 - 0:01:26, Tiếp theo, đó là chúng ta sẽ tìm hiểu về cơ chế Attention trong việc giải quyết bài toán dịch máy này và một số biến thể của Attention
0:01:26 - 0:01:32, và tại sao chúng ta lại tìm hiểu về dịch máy mà không phải là các bài toán khác
0:01:32 - 0:01:44, Ví dụ như bài toán tóm tắt văn bản, bài toán phân loại văn bản hoặc là bài toán part-of-speech tagging (gán nhãn từ loại)
0:01:44 - 0:01:48, Bài toán dịch máy có 1 vai trò rất quan trọng
0:01:48 - 0:01:52, Đầu tiên là nó có độ khó
0:01:52 - 0:02:04, Cái độ khó của nó nó thể hiện ở chỗ là dịch máy, chúng ta phải làm việc trên hai cái domain, hai cái không gian khác nhau, đó chính là hai cái ngôn ngữ mà chúng ta cần phải dịch
0:02:04 - 0:02:10, Thế thứ hai, đó là dịch máy là một cái bài toán mà nó tổng quát
0:02:10 - 0:02:20, Cái kiểu tổng quát của dịch máy đó chính là nó biến đổi từ một cái chuỗi về một cái chuỗi khác
0:02:20 - 0:02:28, Và từ cái dạng chuỗi sang chuỗi này thì nó cũng có thể tương tự để giải quyết cho các bài toán như là
0:02:28 - 0:02:36, bài toán về tóm tắt văn bản, bài toán về chuyển đổi lại, tức là paraphrase một cái văn bản
0:02:36 - 0:02:44, Thậm chí cả bài toán phân loại văn bản thì cái chuỗi đầu ra của mình mới có thể hiểu là một cái giá trị
0:02:44 - 0:02:48, Cái chuỗi này có độ dài là một, như vậy thì tính tổng quát của nó nó là cao
0:02:48 - 0:02:54, Và đó là lý do tại sao chúng ta nghiên cứu về bài toán dịch máy
0:02:54 - 0:02:57, và dùng nó như là một kiến thức tổng quát
0:02:57 - 0:03:00, để có thể sau này áp dụng những kiến thức về attention
0:03:00 - 0:03:03, vào cho các bài toán khác
0:03:03 - 0:03:07, Thì định nghĩa bài toán dịch máy là một bài toán
0:03:07 - 0:03:13, cho phép chuyển đổi từ một câu từ ngôn ngữ nguồn sang một ngôn ngữ khác
0:03:13 - 0:03:16, Ví dụ như ở đây chúng ta có đầu vào
0:03:16 - 0:03:21, đầu vào sẽ là một câu thuộc cái văn bản là một ngôn ngữ đó là tiếng Anh
0:03:21 - 0:03:25, và đầu ra của mình, đây chính là cái giá trị output
0:03:25 - 0:03:30, thì nó sẽ ra là một cái văn bản tiếng Pháp
0:03:30 - 0:03:37, thì cái hệ thống dịch máy là làm sao có thể thực hiện được việc chuyển đổi một câu từ ngôn ngữ tiếng Anh sang tiếng Pháp
0:03:37 - 0:03:40, đây là một cái ví dụ ngôn ngữ nó còn hoàn toàn có thể chuyển đổi qua lại
0:03:40 - 0:03:45, giữa tiếng Anh, tiếng Việt, tiếng Pháp, tiếng Tây Ban Nha, v.v.
0:03:45 - 0:03:49, và thậm chí là các hệ thống sau này có khả năng dịch đa ngôn ngữ
0:03:49 - 0:03:52, tức là chúng ta có thể từ một ngôn ngữ bất kỳ
0:03:52 - 0:03:55, có thể chuyển sang một ngôn ngữ bất kỳ khác
0:03:55 - 0:03:59, thì đó là tầm nhìn về thiết kế các mô hình
0:03:59 - 0:04:02, để cho phép mô hình máy học
0:04:02 - 0:04:05, để có thể dịch được rất nhiều ngôn ngữ qua lại với nhau
0:04:07 - 0:04:11, Tiếp theo chúng ta sẽ nói về hướng tiếp cận Neural Machine Translation
0:04:11 - 0:04:16, Nó là hướng tiếp cận sử dụng RNN
0:04:16 - 0:04:19, là Recurrent Neural Network
0:04:19 - 0:04:24, và Machine Translation chính là tên của bài toán của mình
0:04:24 - 0:04:31, Đây là phương pháp sử dụng một mạng neural network chính là RNN của mình
0:04:31 - 0:04:36, từ đầu đến cuối tức là end to end
0:04:36 - 0:04:55, Chúng ta sẽ đưa vào mạng Neural Network này, mạng RNN này và nó sẽ tạo ra các giá trị output của mình mà không thực hiện những thao tác, thực hiện trung gian qua những loại mô hình khác
0:04:55 - 0:05:03, mà tất cả mọi thứ đều chỉ được thực hiện bởi duy nhất một mô hình đó là Neural Network, Recurrent Neural Network
0:05:03 - 0:05:12, Và kiến trúc sử dụng ở đây chính là kiến trúc sequence to sequence hay gọi tắt là seq2seq
0:05:12 - 0:05:18, thì đây sẽ bao gồm hai cái thành phần, hai cái vai trò là encoder và decoder
0:05:18 - 0:05:25, trong đó encoder thực hiện việc đọc và hiểu cái thông tin input đầu vào
0:05:25 - 0:05:32, decoder là tạo sinh ra cái kết quả trả về
0:05:32 - 0:05:42, Tạo sinh ra kết quả và chúng ta sẽ đến với mô hình seq2seq theo animation như sau
0:05:42 - 0:05:50, Bên trái sẽ là ký hiệu encoder RNN, tức là nguyên những cái phần màu xanh ở đây
0:05:50 - 0:05:57, chính là hidden state, các trạng thái ẩn mà mình sẽ encode câu đầu vào của mình
0:05:57 - 0:06:03, Ví dụ như đây chúng ta có đưa một câu của một văn bản nguồn là I'm not sure
0:06:03 - 0:06:15, Thì tại cái mũi tên này, nó là tổng hợp thông tin của toàn bộ nội dung của câu đầu vào
0:06:15 - 0:06:21, Và nó sẽ bắt đầu tiến hành quá trình decode, tức là giải mã
0:06:21 - 0:06:26, Rồi nó sẽ phải truyền vào một cái ký tự đặc biệt
0:06:26 - 0:06:30, Nó sẽ truyền vào một ký tự đặc biệt thì ở đây mình dùng là từ <SOS>
0:06:30 - 0:06:34, <SOS> là để cho hàm ý là có cái ý nghĩa của nó là bắt đầu thôi
0:06:34 - 0:06:40, Còn trong thực tế lúc thực hành chúng ta có thể sử dụng những cái ký tự đặc biệt
0:06:40 - 0:06:42, Ví dụ như là ký tự <SOS>
0:06:42 - 0:06:44, Ký tự <SOS>
0:06:44 - 0:06:46, hoặc là ký tự đô la
0:06:46 - 0:06:48, dấu thăng
0:06:48 - 0:06:50, đô la
0:06:50 - 0:06:52, rồi dấu thăng
0:06:52 - 0:06:54, Nhưng mà lưu ý, đó là phải
0:06:54 - 0:06:56, có sự đồng nhất
0:06:56 - 0:06:58, Ví dụ như trong toàn bộ những văn bản
0:06:58 - 0:07:00, trong cặp dữ liệu X, Y của mình
0:07:00 - 0:07:02, và trong quá trình huấn luyện
0:07:02 - 0:07:04, và thậm chí là trong quá trình
0:07:04 - 0:07:06, inference, tức là mình
0:07:06 - 0:07:08, thực hiện
0:07:08 - 0:07:12, khi thực hiện mình test mô hình và triển khai thực tế
0:07:12 - 0:07:15, thì chúng ta đều phải thống nhất sử dụng chung một hệ thống ký hiệu này
0:07:15 - 0:07:19, chứ không phải là lúc chúng ta sử dụng là alpha, <SOS>
0:07:19 - 0:07:21, lúc chúng ta sử dụng là dollar thì không được
0:07:21 - 0:07:26, rồi khi chúng ta truyền vô một ký tự đặc biệt để đánh dấu
0:07:26 - 0:07:31, cái này là đánh dấu quá trình decode
0:07:31 - 0:07:43, Đánh dấu quá trình decode và nó sẽ tạo ra dự đoán y hat, đó chính là từ đầu ra
0:07:43 - 0:07:47, Và ở đây chúng ta sẽ dùng hàm argmax
0:07:47 - 0:07:53, Tại vì đầu ra của mình sẽ ra là một cái vector
0:07:53 - 0:07:58, Rồi sau đó mình sẽ tìm coi trọng số nào là trọng số lớn nhất
0:07:58 - 0:08:01, tương ứng với lại cái từ trong tiếng Pháp của mình
0:08:01 - 0:08:03, trong cái embedding tiếng Pháp của mình
0:08:03 - 0:08:04, rồi
0:08:04 - 0:08:07, sau khi chúng ta có được cái từ r
0:08:07 - 0:08:09, thì chúng ta sẽ truyền
0:08:09 - 0:08:12, nối tiếp chúng ta sẽ đưa cái từ r này
0:08:12 - 0:08:14, như là một cái đầu vào
0:08:14 - 0:08:16, cho cái bước tiếp theo
0:08:16 - 0:08:18, cái bước decode tiếp theo
0:08:18 - 0:08:20, truyền cái từ r này và đương nhiên
0:08:20 - 0:08:22, là sử dụng cái thông tin của quá khứ
0:08:22 - 0:08:24, để mà
0:08:24 - 0:08:27, thực hiện cái hàm
0:08:27 - 0:08:29, tính giá trị output
0:08:29 - 0:08:31, nó sẽ tạo ra từ NE
0:08:31 - 0:08:33, rồi từ NE này
0:08:33 - 0:08:35, nó sẽ là đầu vào cho mạng
0:08:35 - 0:08:37, của mình và nó sẽ tạo ra từ suy
0:08:37 - 0:08:39, từ suy này sẽ truyền vào
0:08:39 - 0:08:41, để tạo ra từ PA
0:08:41 - 0:08:43, cứ như vậy
0:08:43 - 0:08:45, và đến từ kết thúc quá trình decode
0:08:45 - 0:08:47, thì hệ thống này nó phải trả ra
0:08:47 - 0:08:49, một từ đặc biệt
0:08:49 - 0:08:51, đó là <EOS>
0:08:51 - 0:08:53, và cũng tương tự như <SOS>
0:08:53 - 0:08:55, thì <EOS> này là để đánh dấu
0:08:55 - 0:09:02, Kết thúc quá trình decode
0:09:04 - 0:09:06, Và chúng ta sẽ phải sử dụng một ký tự đặc biệt
0:09:06 - 0:09:11, Ví dụ như là nếu ở đây đã sử dụng <SOS> rồi thì ở đây chúng ta có thể sử dụng là
0:09:11 - 0:09:14, Ký tự là <EOS>
0:09:16 - 0:09:18, Và phải có sự đồng nhất từ đầu đến cuối
0:09:18 - 0:09:29, Ở bên tay trái, encoder thực hiện công việc đó là tổng hợp thông tin của toàn bộ câu văn nguồn của mình
0:09:29 - 0:09:35, Còn ở bên tay phải là decoder đóng vai trò như là một mô hình ngôn ngữ
0:09:35 - 0:09:40, Là một language model để tạo ra cái văn bản đích, tạo ra câu văn đích
0:09:40 - 0:09:48, Và dựa trên decoder này nó thực hiện được là dựa trên thông tin đã được tổng hợp từ câu văn nguồn
0:09:48 - 0:09:58, Rồi, như vậy thì tính linh hoạt của Seq2Seq nó sẽ thể hiện ở những cái ví dụ sau.
0:09:58 - 0:10:10, Đầu tiên đó là bất cứ cái văn bản ở dạng chuỗi, input nào mà ở dạng chuỗi và output nào ở dạng chuỗi thì chúng ta đều có thể sử dụng được Seq2Seq này.
0:10:10 - 0:10:16, Thì cái ý đầu tiên này nó minh họa cho cái việc là tính linh hoạt của Seq2Seq
0:10:16 - 0:10:22, Và do đó thì một cái mạng Neural Network mà nhận cái input và tạo ra một cái vector biểu diễn
0:10:16 - 0:10:22, Một cái mạng Neural Network thì nhận cái input và tạo ra một cái vector biểu diễn
0:10:22 - 0:10:28, Một cái mạng Neural Network thì nhận cái input và tạo ra một cái vector biểu diễn
0:10:28 - 0:10:34, Rồi mạng Neural Network khác nó sẽ sinh ra cái output và từ cái vector biểu diễn trên
0:10:34 - 0:10:37, Thì thực ra đây chính là cái quá trình encode
0:10:37 - 0:10:42, Quá trình encode và đây là quá trình decode
0:10:42 - 0:10:48, Mạng Neural sẽ nhận toàn bộ nội dung đầu vào, nội dung input
0:10:48 - 0:10:53, Sau khi đã đọc hết toàn bộ thông tin đó thì nó sẽ tạo ra 1 vector
0:10:53 - 0:11:00, Và vector này sẽ tích hợp toàn bộ thông tin của input
0:11:00 - 0:11:09, Và với thông tin của input này, sẽ sinh ra output từ vector biểu diễn trên, tức là vector này.
0:11:09 - 0:11:14, Nó sẽ sinh ra output và đây chính là quá trình decode.
0:11:16 - 0:11:26, Seq2Seq không chỉ hiệu quả cho bài toán dịch máy, mà nó còn hiệu quả cho cả những bài toán khác.
0:11:26 - 0:11:29, Bài toán tóm tắt văn bản, summarization
0:11:29 - 0:11:31, Bài toán tóm tắt văn bản là gì?
0:11:31 - 0:11:35, Đầu vào của mình cũng sẽ là một chuỗi, một đoạn văn rất là dài
0:11:35 - 0:11:37, Và đầu ra của mình sẽ là một đoạn văn ngắn
0:11:37 - 0:11:41, Mô tả lại toàn bộ nội dung của đoạn văn dài
0:11:41 - 0:11:43, Tóm tắt lại nội dung chính của đoạn văn dài này
0:11:43 - 0:11:47, Cho bài toán hội thoại hay dialogue
0:11:47 - 0:11:53, Input của mình sẽ là lời thoại trước
0:11:53 - 0:11:57, Và output của mình sẽ là cái lời thoại sau, giống như là khi chúng ta đặt cái...
0:11:57 - 0:12:00, cái... cái... mình sẽ chat với lại một cái chatbot
0:12:00 - 0:12:02, thì mình sẽ cung cấp cho nó một cái lời thoại trước
0:12:02 - 0:12:04, và nó sẽ trả lời mình
0:12:04 - 0:12:06, và nó sẽ trò chuyện với mình
0:12:06 - 0:12:08, thì đây chính là cái lời thoại sau
0:12:08 - 0:12:11, và thì cái này phù hợp cho bài toán là chatbot
0:12:12 - 0:12:14, Bài toán phân tích cú pháp
0:12:14 - 0:12:17, thì đầu vào của mình sẽ là một cái chuỗi
0:12:17 - 0:12:19, đoạn văn, một cái câu văn
0:12:19 - 0:12:22, và đầu ra của mình cũng sẽ là một cái chuỗi
0:12:22 - 0:12:26, để mô tả cái cây cú pháp tương ứng với các đoạn văn trên
0:12:27 - 0:12:30, rồi bài toán Code Synthesis, Code Generation
0:12:30 - 0:12:34, thì đầu vào của mình sẽ là mô tả cái chức năng của một chương trình
0:12:34 - 0:12:37, hoặc là chức năng của một thuật toán mà mình đang muốn cài đặt
0:12:37 - 0:12:43, và đầu ra là nó sẽ tạo ra một cái mã nguồn theo một cái ngôn ngữ nào đó
0:12:43 - 0:12:47, mà chúng ta đã được chọn lựa ví dụ như mã nguồn cho ngôn ngữ Python
0:12:47 - 0:12:56, Tại sao, toàn bộ nội dung của Slide này thể hiện là tính linh hoạt của sequence to sequence
0:12:56 - 0:13:03, có thể xử lý được bất cứ bài toán nào đầu vào của mình là ở dạng chuỗi và đầu ra của mình cũng ở dạng chuỗi