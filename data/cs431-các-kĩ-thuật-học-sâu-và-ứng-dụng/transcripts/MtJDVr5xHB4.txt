0:00:00 - 0:00:10, Chúng ta sẽ dần làm quen với việc tổng quát hóa và vector hóa mô hình máy học.
0:00:10 - 0:00:18, Để tổng quát hóa và vector hóa, chúng ta sẽ đưa đến các khái niệm sử dụng các vector và ma trận.
0:00:18 - 0:00:20, thì ở đây chúng ta sẽ có hai cái vector
0:00:21 - 0:00:23, Đối với dữ liệu là một mẫu
0:00:23 - 0:00:30, tức là gồm nhiều biến x1, x2, xm
0:00:30 - 0:00:32, thì ở đây chúng ta sẽ cho một cái ví dụ
0:00:32 - 0:00:36, đây là một cái mẫu dữ liệu được ký hiệu bởi một cái vector
0:00:37 - 0:00:40, và vector này chúng ta lưu ý là ký hiệu bởi
0:00:40 - 0:00:45, một cái ký tự viết in đậm và viết thường
0:00:45 - 0:01:00, Với x0, thành phần đầu tiên là bias, đại diện cho tất cả mô hình độc lập với các biến đầu vào
0:01:00 - 0:01:07, Các biến đầu vào không còn là một biến đầu vào nữa mà nó có thể gồm nhiều biến đầu vào
0:01:07 - 0:01:09, Lấy ví dụ bài toán dự đoán giá nhà
0:01:10 - 0:01:14, thì x1 này của mình nó có thể sẽ là diện tích
0:01:17 - 0:01:19, x2 này của mình có thể là số phòng
0:01:22 - 0:01:27, và xm này có thể là khoảng cách đến trung tâm thành phố
0:01:28 - 0:01:34, Thì đây chính là các cái biến số để giúp cho chúng ta đưa ra được cái dự đoán
0:01:34 - 0:01:39, Cái nhãn của một mẫu dữ liệu sẽ là cái giá trị Y.
0:01:39 - 0:01:45, Và các biến số để giúp mình dự đoán cái nhãn này là x1, x2, xm.
0:01:45 - 0:01:50, Như vậy, chúng ta đã tổng quát hóa cho trường hợp nhiều biến,
0:01:50 - 0:01:52, nhưng lưu ý là mới chỉ có một mẫu dữ liệu thôi.
0:01:52 - 0:01:54, Với một mẫu dữ liệu thôi.
0:01:54 - 0:01:59, Trong phần tiếp theo, chúng ta sẽ tổng quát hóa cho nhiều mẫu dữ liệu.
0:01:59 - 0:02:06, Và tham số của mô hình dự đoán của mình trong trường hợp này đó chính là một vector theta
0:02:06 - 0:02:11, bao gồm nhiều thành phần, thì theta 0, theta 1 và theta m
0:02:11 - 0:02:17, thì tương ứng theta 0 sẽ được nhân với bias, theta 1 sẽ được nhân với x1
0:02:17 - 0:02:19, và theta m sẽ nhân với xm
0:02:19 - 0:02:23, Như vậy thì lúc này hàm dự đoán của mình f theta x
0:02:23 - 0:02:27, nó sẽ được viết bằng tích vô hướng của theta và x
0:02:27 - 0:02:32, và khi nhân tích vô hướng thì nó sẽ lấy từng thành phần nhân với nhau
0:02:32 - 0:02:34, xong rồi cộng lại
0:02:34 - 0:02:36, và hàm lỗi trong trường hợp này
0:02:36 - 0:02:41, nó sẽ là lấy theta x trừ y tất cả bình
0:02:41 - 0:02:44, đây chính là cái giá trị dự đoán
0:02:44 - 0:02:46, hay còn gọi là y ngã
0:02:46 - 0:02:50, và giá trị dự đoán này mình mong muốn xấp xỉ Y
0:02:50 - 0:02:51, thì chúng ta sẽ lấy cái thằng này
0:02:51 - 0:02:53, trừ nhau và bình phương
0:02:53 - 0:02:56, đó chính là cho trường hợp một mẫu dữ liệu
0:02:56 - 0:02:58, một mẫu dữ liệu
0:02:58 - 0:03:02, còn trong trường hợp mà dữ liệu của mình là toàn bộ n mẫu
0:03:02 - 0:03:05, thì mình sẽ vector hóa nó như thế này
0:03:05 - 0:03:08, chúng ta sẽ ký hiệu bằng một cái ma trận
0:03:08 - 0:03:10, chúng ta sẽ ký hiệu bằng một cái ma trận
0:03:10 - 0:03:13, trong đó các cái cột của cái ma trận này
0:03:13 - 0:03:17, nó tương ứng là một cái vector biểu diễn cho một mẫu
0:03:17 - 0:03:19, vector
0:03:19 - 0:03:21, của một mẫu
0:03:21 - 0:03:26, Vector này sẽ thể hiện như vậy là dạng cột
0:03:26 - 0:03:29, Các vector này thể hiện như dạng cột
0:03:29 - 0:03:33, Các cột này ráp lại với nhau, thì nó sẽ tạo ra thành một cái ma trận
0:03:33 - 0:03:37, Và cái chỉ số ở phía trên tương ứng là chỉ số thứ tự của mẫu
0:03:37 - 0:03:48, Nếu như chúng ta có n mẫu và từng cái x1, x2, cho đến xn là bao gồm m biến
0:03:48 - 0:03:57, Cái ma trận X này sẽ có kích thước m cộng 1 nhân n
0:03:57 - 0:04:08, Tại sao lại có cái m cộng 1 này? Đó chính là do thành phần bias, m biến, thêm một cái thành phần bias nữa là m cộng 1
0:04:08 - 0:04:17, Đối với nhãn, nhãn của toàn bộ n mẫu sẽ ký hiệu bằng một ma trận Y
0:04:17 - 0:04:21, trong đó mỗi giá trị Y1, Y2, Yn chính là nhãn của mình
0:04:21 - 0:04:29, và chúng ta sẽ có Y thuộc một ma trận kích thước đó là 1 nhân cho n
0:04:29 - 0:04:35, hay còn gọi là một vector dạng nằm ngang
0:04:35 - 0:04:39, X trong trường hợp này là một cái ma trận
0:04:39 - 0:04:43, số dòng của mình sẽ là m cộng 1
0:04:43 - 0:04:49, và số cột của mình trong trường hợp này là n
0:04:49 - 0:04:55, và như vậy thì cái hàm dự đoán của mình là
0:04:55 - 0:04:59, bằng, chúng ta sẽ lấy tham số theta
0:04:59 - 0:05:04, nhân cho từng mẫu dữ liệu theta nhân với x1, theta x2, theta xn
0:05:04 - 0:05:07, Và khi này thì chúng ta sẽ giống như là rút thừa số chung như vậy đó
0:05:07 - 0:05:09, Chúng ta sẽ rút thừa số chung theta ra
0:05:10 - 0:05:15, Rồi sau đó đưa các cái x1, x2 vào bên trong cái ngoặc
0:05:21 - 0:05:25, Và toàn bộ cái x1, x2 cho đến xn này nó chính là ma trận X
0:05:25 - 0:05:28, Như vậy thì chúng ta sẽ có cái công thức cho cái hàm dự đoán
0:05:28 - 0:05:30, Đó là theta chuyển vị nhân với X
0:05:30 - 0:05:35, Đối với hàm lỗi thì chúng ta cũng hoàn toàn làm tương tự như vậy
0:05:35 - 0:05:40, Hàm lỗi thì chúng ta cần phải lấy giá trị dự đoán trừ cho giá trị thực tế
0:05:40 - 0:05:48, Giá trị dự đoán trong trường hợp này là y ngã
0:05:48 - 0:05:54, Theta chuyển vị nhân với X này sẽ là một dạng vector dạng nằm ngang
0:05:54 - 0:05:59, Theta chuyển vị nhân với X là một vector dạng nằm ngang
0:05:59 - 0:06:08, Y cũng là vector dạng nằm ngang, là 1 nhân n
0:06:08 - 0:06:11, Tại vì có n mẫu, nên có n giá trị dự đoán
0:06:11 - 0:06:15, Đây là y ngã, đây là y
0:06:15 - 0:06:22, Lấy 2 cái này trừ cho nhau, thì sẽ có theta chuyển vị nhân X trừ cho Y
0:06:22 - 0:06:26, Kết quả của mình cũng sẽ ra vector dạng nằm ngang như thế này
0:06:26 - 0:06:34, Và để tính tổng trung bình cộng của các cái sai số bình phương
0:06:34 - 0:06:37, thì chúng ta sẽ lấy các cái vector nằm ngang này nhân với nhau
0:06:37 - 0:06:41, Rồi sau đó cộng lại thì đây chính là hai cái vector nhân với nhau
0:06:41 - 0:06:46, thì đây chính là (theta chuyển vị nhân với X trừ Y)
0:06:46 - 0:06:50, và cái vector dạng dọc như thế này
0:06:50 - 0:06:54, (Theta chuyển vị nhân với X trừ Y)
0:06:54 - 0:06:59, Tất cả chuyển vị sẽ tạo ra một vector dạng nằm dọc
0:06:59 - 0:07:07, Khi nhân 2 cái này lại với nhau, chúng ta sẽ nhân từng phần tử
0:07:07 - 0:07:13, Sau đó cộng lại, bản chất đây chính là sai số bình phương
0:07:13 - 0:07:19, Đây chính là sai số trong một mẫu, mẫu thứ 2, mẫu thứ 3, mẫu thứ n
0:07:19 - 0:07:25, Đây chính là các cái sai số và khi chúng ta nhân theo cái kiểu là hai cái vector nằm ngang, nhân với vector nằm dọc
0:07:25 - 0:07:30, thì nó sẽ tạo ra tổng các sai số bình phương sau đó chúng ta sẽ chia trung bình cộng
0:07:30 - 0:07:36, Và để tính đạo hàm cho hàm lỗi thì ở trong trường hợp này chúng ta lưu ý
0:07:36 - 0:07:43, theta của mình nó không còn là một cái tham số dạng scalar mà nó là một vector
0:07:43 - 0:07:46, do đó chúng ta phải dùng một cái ký hiệu nabla
0:07:46 - 0:07:52, Bản chất đây là một vector gradient, vector đạo hàm
0:07:52 - 0:07:59, Cho một vector, chúng ta tính đạo hàm cho cái hàm ở trên
0:07:59 - 0:08:02, 1 phần 2n chúng ta kéo xuống
0:08:02 - 0:08:08, Và số 2 này, ở đây nó sẽ tương đương là (theta chuyển vị nhân X trừ Y) tất cả bình phương
0:08:08 - 0:08:13, Bình phương chúng ta tính đạo hàm thì nó sẽ triệt tiêu đi con số 2
0:08:13 - 0:08:20, Thành phần này đem xuống dưới và đạo hàm bên trong ruột này theo theta
0:08:20 - 0:08:23, Đạo hàm của (theta chuyển vị nhân X) theo theta, còn Y
0:08:23 - 0:08:25, Đối với theta nó chính là hằng số
0:08:25 - 0:08:29, Do đó chúng ta sẽ xem như thằng này đạo hàm là bằng 0
0:08:29 - 0:08:36, Đạo hàm của theta chuyển vị nhân X theo biến theta thì chúng ta sẽ có
0:08:36 - 0:08:37, Chúng ta sẽ còn X
0:08:37 - 0:08:41, Do đó đạo hàm của theta chuyển vị nhân X nó chính là X
0:08:41 - 0:08:53, Nếu như chúng ta nhìn ở dưới dạng góc độ là đại số, đương nhiên cách này không chính thống, không đúng về mặt ký hiệu
0:08:53 - 0:09:00, Nhưng nếu chúng ta nhìn theo cách này, đó là (theta chuyển vị nhân X trừ Y) tất cả bình phương
0:09:00 - 0:09:06, Khi lấy đạo hàm của nó ra, chúng ta sẽ đem số 2 xuống, rồi thành phần này chúng ta sẽ kéo xuống
0:09:06 - 0:09:09, Đó là (theta chuyển vị nhân X trừ Y)
0:09:09 - 0:09:13, sau đó chúng ta sẽ nhân đạo hàm của phần ruột
0:09:13 - 0:09:17, đạo hàm của phần ruột theo theta thì thành phần Y là 0
0:09:17 - 0:09:20, thành phần (theta chuyển vị nhân X) nó sẽ còn là X
0:09:20 - 0:09:23, số 2 này nó sẽ triệt tiêu với này, nó sẽ bị mất
0:09:23 - 0:09:27, như vậy nó sẽ còn là X nhân cho (theta chuyển vị nhân X trừ Y)
0:09:27 - 0:09:29, thì đây là cái chứng minh cho công thức đạo hàm này
0:09:29 - 0:09:35, và khi chúng ta đã tính được đạo hàm rồi
0:09:35 - 0:09:38, thì thuật toán gradient descent rất là đơn giản
0:09:38 - 0:09:40, Đó là chúng ta sẽ khởi tạo ngẫu nhiên cái vector
0:09:40 - 0:09:43, Lúc này theta của mình là vector rồi
0:09:43 - 0:09:48, Tại vì cái x của mình bao gồm m biến
0:09:48 - 0:09:52, Theta của mình thì nó sẽ là bao gồm m cộng 1 thành phần
0:09:53 - 0:10:00, Theta sẽ là bao gồm theta 0, theta 1 cho đến theta m
0:01:00 - 0:01:02, m cộng 1 thành phần
0:01:03 - 0:01:05, Và các cái thành phần này chúng ta sẽ khởi tạo ngẫu nhiên
0:01:05 - 0:01:11, Hai siêu tham số alpha và epsilon cũng khởi tạo các con số nhỏ
0:01:11 - 0:01:15, chúng ta sẽ lặp và theta sẽ được cập nhật bằng theta
0:01:15 - 0:01:22, trừ cho alpha nhân đạo hàm gradient của L theo theta
0:01:22 - 0:01:26, nó sẽ có công thức là 1 phần n nhân X nhân (theta chuyển vị nhân X trừ Y)
0:01:26 - 0:01:31, do đó chúng ta chép nó qua đây và chúng ta sẽ có công thức cập nhật
0:01:31 - 0:01:45, Điều kiện dừng là nếu giá trị sai số đủ nhỏ thì chúng ta sẽ dừng lặp
0:01:45 - 0:01:53, Chúng ta lưu ý là cái nabla của L theo theta nó là cái vector gradient hay nói cách khác nó là bằng các đạo hàm thành phần theo theta 0, đạo hàm của L theo theta 1, theo theta m.
0:01:53 - 0:02:02, Thì đây là một cái vector, do đó chúng ta hoàn toàn có thể sử dụng cái giá trị độ lớn của cái vector này để làm cái điều kiện dừng.
0:02:02 - 0:02:09, khi giá trị độ lớn của vector đạo hàm này, vector gradient này đủ nhỏ, chúng ta sẽ kết thúc vòng lặp.
0:02:09 - 0:02:16, Đây chính là tổng quát hóa và vector hóa cho mô hình Linear Regression.
0:02:16 - 0:02:25, Trong phần tiếp theo, chúng ta sẽ tiến hành cài đặt bằng hai phương pháp vector hóa và phương pháp không vector hóa.
0:02:25 - 0:02:32, Và cuối cùng cho phần Linear Regression này, chúng ta sẽ biểu diễn mô hình dưới dạng là đồ thị
0:02:32 - 0:02:40, Đầu vào chúng ta sẽ có thành phần là bias, rồi các cái biến x1, x2, cho đến xm
0:02:40 - 0:02:48, Và tương ứng từng đầu vào này chúng ta sẽ có các tham số theta 0, theta 1, theta 2 và theta m
0:02:48 - 0:02:54, Và khi từng thành phần này nhân vô, chúng ta sẽ qua một hàm tính tổng
0:02:54 - 0:02:58, Tại sao lại tổng? Tại vì tổng của từng các tích này
0:02:58 - 0:03:05, 1 x theta 0, x1 x theta 1, x2 x theta 2, xm x theta m
0:03:05 - 0:03:09, sau đó chúng ta cộng lại, chúng ta sẽ ra được giá trị dự đoán
0:03:09 - 0:03:12, và đây là cái dạng viết với dạng vector hóa
0:03:14 - 0:03:19, Với một đồ thị này chúng ta có thể hiểu được cách mà chúng ta lan truyền thông tin
0:03:19 - 0:03:28, Và cái độ dài của các cạnh này tương ứng cho trọng số của thông tin.
0:03:28 - 0:03:34, Thông tin này đưa vào có trọng số theta 0, thông tin này đưa vào có trọng số theta 1.
0:03:34 - 0:03:36, Mỗi cái này sẽ có một cái trọng số.
0:03:36 - 0:03:48, Rồi, tổng hợp thông tin để đưa ra giá trị dự đoán.
0:03:48 - 0:03:52, thì đó là toàn bộ nội dung của phần Linear Regression.