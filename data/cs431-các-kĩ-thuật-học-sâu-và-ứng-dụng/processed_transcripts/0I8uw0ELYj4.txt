0:00:00 - 0:00:08, Cuối cùng, chúng ta sẽ cùng tìm hiểu về các cách thức để sử dụng một mạng huấn luyện sẵn.
0:00:08 - 0:00:16, Thông thường, các mạng CNN được huấn luyện trên những tập dữ liệu rất lớn.
0:00:16 - 0:00:23, Và việc huấn luyện này có thể kéo dài tính bằng ngày hoặc thậm chí tính bằng tháng.
0:00:23 - 0:00:26, Tháng, nó có thể kéo dài đến hàng tháng
0:00:26 - 0:00:32, Và không phải ai cũng có khả năng có thể đủ tài nguyên tính toán để mà có thể thực hiện được công việc huấn luyện này
0:00:32 - 0:00:40, Do đó thì chúng ta sẽ có một kỹ thuật đó là sử dụng những mô hình huấn luyện sẵn để đi giải quyết những bài toán của riêng mình
0:00:40 - 0:00:44, Ở đây chúng ta sẽ gọi là kỹ thuật sử dụng các pre-trained model
0:00:44 - 0:00:46, Ở đây có 3 cách chính
0:00:46 - 0:00:50, Cách đầu tiên đó là chúng ta sẽ sử dụng trực tiếp
0:00:50 - 0:00:53, Chúng ta sẽ sử dụng trực tiếp, nghĩa là sao?
0:00:53 - 0:00:58, Nếu như tập data set của mình, đây là tập data set của mình
0:00:58 - 0:01:06, Nó có các cái nhãn, ví dụ như là máy bay, xe, mèo, con ngựa, con chó
0:01:06 - 0:01:14, Và đối tượng cho bài toán mà mình đang quan tâm, đó là cat, dog, horse, tức là mèo, chó, và ngựa
0:01:14 - 0:01:22, Vô tình 3 đối tượng này trùng với lại các đối tượng trong tập dataset mà chúng ta đã huấn luyện trước đó
0:01:22 - 0:01:27, Nó đã trùng thì chúng ta sẽ sử dụng trực tiếp luôn
0:01:27 - 0:01:30, Chúng ta sẽ lấy chính cái model đó ra để sử dụng trực tiếp luôn
0:01:30 - 0:01:33, Thì đây là cách ngây thơ nhất, đơn giản nhất để sử dụng
0:01:33 - 0:01:35, Tuy nhiên nó sẽ có 1 vấn đề đó là
0:01:35 - 0:01:45, Đó là, dữ liệu của mình, Cat, Dog và Horse này, nó có khả năng là nó đi theo những cái giống loài mà ở cái khu vực mà mình đang sinh sống.
0:01:45 - 0:01:54, Còn tập Dataset này thì đó là những tập Dataset chung. Do đó thì có khả năng khi chúng ta sử dụng những cái mô hình mà đã huấn luyện trên tập dữ liệu lớn này,
0:01:54 - 0:02:00, áp dụng trên chính dữ liệu của mình có khả năng là độ chính xác không đạt như chúng ta kỳ vọng.
0:02:00 - 0:02:06, Nhưng mà đây là cách ngây thơ nhất, đơn giản nhất đầu tiên khi chúng ta sử dụng với một cái mạng huấn luyện sẵn
0:02:07 - 0:02:16, Rồi, cách thứ hai, đó là chúng ta sẽ sử dụng cái mạng CNN mà đã được huấn luyện sẵn như là một cái bộ rút trích đặc trưng
0:02:16 - 0:02:22, Thì ở đây chúng ta sẽ lấy ra một cái hình ảnh ví dụ thôi ha, đó là một cái mạng ResNet 50
0:02:23 - 0:02:28, Và cái ResNet 50 này nó sẽ có cái phần đầu là cái phần rút trích đặc trưng
0:02:30 - 0:02:39, Nó sẽ là rút trích đặc trưng. Cái phần sau là phần liên quan đến phần lớp.
0:02:39 - 0:02:50, Các nhà khoa học mới phát hiện ra rằng các đặc trưng mà được huấn luyện với những tập dữ liệu lớn trước đây thì khá tổng quát.
0:02:50 - 0:03:01, sau này chúng ta đưa vô một tập dữ liệu bất kỳ hoặc là đưa vô một đối tượng khác thì các đặc trưng này đâu đó vẫn có khả năng sử dụng, tái sử dụng lại được
0:03:01 - 0:03:05, và chúng ta sẽ kết hợp nó, kết hợp với lại một mô hình máy học khác
0:03:05 - 0:03:11, như vậy thì ở đây chúng ta sẽ loại bỏ đi, chúng ta sẽ loại bỏ đi phần lớp cuối cùng
0:03:11 - 0:03:17, và ở đây, đúng không, kết thúc bước feature extraction này chúng ta sẽ ra một feature
0:03:17 - 0:03:19, Chúng ta sẽ ra feature
0:03:19 - 0:03:24, và chúng ta sẽ sử dụng feature này để đi kết hợp với một bộ phân lớp khác
0:03:24 - 0:03:28, ví dụ ở đây chúng ta có thể sử dụng bộ phân lớp là k-nearest neighbors
0:03:28 - 0:03:31, chúng ta có thể sử dụng bộ phân lớp là SVM
0:03:31 - 0:03:34, thì feature này nếu mà chiếu trong không gian
0:03:34 - 0:03:38, chúng ta sẽ có các feature như thế này
0:03:38 - 0:03:51, và khi có một cái feature mới cần phải phân loại
0:03:51 - 0:03:53, ví dụ ở đây chúng ta sẽ có một cái feature mới
0:03:53 - 0:03:56, thì chúng ta sẽ chạy cái thuật toán k-nearest neighbors
0:03:56 - 0:03:59, ví dụ trong trường hợp này k là bằng 3
0:03:59 - 0:04:03, chúng ta sẽ lấy ra 3 cái feature gần với lại cái điểm
0:04:03 - 0:04:05, mà mình cần phân loại này nhất
0:04:05 - 0:04:09, sau đó chúng ta sẽ xem xem cái nhãn của 3 cái feature này
0:04:09 - 0:04:16, đó là gì? Ví dụ như nếu đây là Dog, đây là Dog, đây là Cat
0:04:16 - 0:04:21, như vậy chúng ta sẽ kết luận cái nhãn của cái điểm này, đó chính là Dog
0:04:21 - 0:04:28, Đây chính là ý tưởng của thuật toán k láng giềng gần nhất, k-nearest neighbors
0:04:28 - 0:04:31, tương tự như vậy cho thuật toán SVM là thuật toán phân lớp nhị phân
0:04:31 - 0:04:39, thì chúng ta sẽ có hai tập, ví dụ feature ở đây, tương ứng là cái điểm này
0:04:39 - 0:04:42, và chúng ta sẽ có hai tập là tròn và cộng
0:04:42 - 0:04:51, Rồi, chúng ta sẽ nhờ cái máy phân lớp để tìm ra cái đường biên tốt nhất để phân loại ra hai tập dữ liệu này
0:04:51 - 0:04:54, thì đó là thuật toán Support Vector Machine
0:04:54 - 0:04:57, Thì đây là cách sử dụng thứ hai
0:04:57 - 0:04:59, Và cách sử dụng thứ ba
0:04:59 - 0:05:01, Đó là
0:05:01 - 0:05:05, Chúng ta sẽ sử dụng Transfer Learning hay gọi là Học chuyển tiếp
0:05:05 - 0:05:08, Thì học chuyển tiếp ở đây là gì?
0:05:08 - 0:05:11, Như đã đề cập đó là
0:05:11 - 0:05:17, Các lớp đầu tiên đó đóng vai trò là Feature Extraction
0:05:17 - 0:05:23, Còn lớp cuối đó đóng vai trò là Phân lớp
0:05:23 - 0:05:35, Lớp phân lớp này dùng cho dữ liệu cũ
0:05:35 - 0:05:41, Dùng để phân lớp cho dữ liệu cũ, do đó mình sẽ không thể tái sử dụng nó được
0:05:41 - 0:05:48, Chúng ta sẽ phải bỏ đi và thay bằng một cái tầng mạng Fully Connected các FC khác
0:05:48 - 0:05:53, và chúng ta lưu ý là chúng ta ở lớp cuối cùng là chúng ta phải điều chỉnh nha
0:05:53 - 0:06:00, ví dụ như ở đây chúng ta có 1000 lớp thì cái output FC này nó sẽ là 1 vector 1000 chiều
0:06:00 - 0:06:04, nhưng mà giả sử như tập dữ liệu của mình nó chỉ có 3 object thôi
0:06:04 - 0:06:11, 3 nhãn thôi thì lúc đó lớp đầu ra của mình nó sẽ phải ra cái vector nó chỉ có 3 chiều thôi
0:06:11 - 0:06:18, chú ý cái chỗ đó là nó sẽ tùy thuộc vào số lượng các loại object của mình
0:06:18 - 0:06:28, sau khi chúng ta bỏ phần lớp ở đây đi và nối với lại lớp FC hay là Neural Network ở đây
0:06:28 - 0:06:34, và lưu ý là phải chỉnh lại tầng cuối cùng sao cho phù hợp với lại kích thước của data set của mình
0:06:34 - 0:06:42, thì mình sẽ tiến hành cách đầu tiên, cách 3.1, đó là chúng ta sẽ đóng băng các lớp đầu này đi
0:06:42 - 0:06:47, Tức là chúng ta sẽ không huấn luyện
0:06:50 - 0:06:56, Chúng ta sẽ không huấn luyện trên phần rút trích đặc trưng mà chúng ta chỉ huấn luyện ở đây
0:06:58 - 0:07:03, Huấn luyện cái lớp phân lớp, huấn luyện cái việc phân lớp
0:07:03 - 0:07:11, ở đây chúng ta sẽ có một thuật ngữ tinh chỉnh
0:07:11 - 0:07:16, tinh chỉnh hoặc gọi là fine tune
0:07:16 - 0:07:22, các tham số ở những lớp cuối này thôi thì đây là cái cách 3.1
0:07:22 - 0:07:25, tuy nhiên nếu như dữ liệu của mình đủ lớn
0:07:25 - 0:07:31, cái cách 3.1 này nó chỉ phù hợp cho trường hợp data mới của mình
0:07:31 - 0:07:34, data mới này của mình là nhỏ thôi
0:07:34 - 0:07:39, còn khi mà data mới của mình rất là lớn
0:07:39 - 0:07:44, thì chúng ta không cần phải đóng băng cái lớp này
0:07:44 - 0:07:48, chúng ta không cần đóng băng cái lớp rút trích đặc trưng
0:07:48 - 0:07:52, mà chúng ta sẽ huấn luyện luôn trên toàn bộ mạng này luôn
0:07:52 - 0:07:56, tức là chúng ta sẽ huấn luyện trên cả những phần feature extraction
0:07:56 - 0:07:58, lẫn phần mới thêm vào
0:07:58 - 0:08:01, thì đây là hai cái cách thức để học chuyển tiếp
0:08:01 - 0:08:04, và cái cách này nó sẽ phù hợp cho cái trường hợp data của mình
0:08:04 - 0:08:07, data mới của mình nó rất là lớn
0:08:07 - 0:08:08, data mới
0:08:08 - 0:08:10, hờ
0:08:10 - 0:08:14, rồi, như vậy thì hy vọng là
0:08:14 - 0:08:19, qua cái phần số 3 này chúng ta sẽ được giới thiệu
0:08:19 - 0:08:22, chúng ta hiểu qua các cái cách thức để mà
0:08:22 - 0:08:24, sử dụng một cái mạng huấn luyện sẵn
0:08:24 - 0:08:26, trên những tập dữ liệu rất là lớn
0:08:26 - 0:08:30, để đi giải quyết cho các bài toán của cá nhân mình
0:08:30 - 0:08:31, trên những dữ liệu của mình
0:08:31 - 0:08:33, thì trong cách số 1
0:08:33 - 0:08:35, đó là nếu như dữ liệu của mình
0:08:35 - 0:08:41, mà trùng với lại cái đối tượng mà mình quan tâm mà trùng với lại tập dữ liệu mà mình đã huấn luyện trước đó
0:08:41 - 0:08:44, thì chúng ta sử dụng trực tiếp
0:08:44 - 0:08:47, trong trường hợp mà dữ liệu của mình
0:08:47 - 0:08:49, nó không giống với dữ liệu
0:08:49 - 0:08:51, mà đã được huấn luyện trước đó
0:08:51 - 0:08:53, nhưng mà đồng thời hoặc là
0:08:53 - 0:08:58, dữ liệu của mình nó giống nhưng mà nó rất khác về cái thể loại
0:08:58 - 0:09:01, ví dụ như chó ở phương Tây nó sẽ khác với chó ở Việt Nam
0:09:01 - 0:09:04, thì chúng ta sẽ sử dụng đến cái cách số 2 và cách số 3
0:09:04 - 0:09:08, cái cách số 2, đó là chúng ta sẽ sử dụng kết hợp
0:09:08 - 0:09:10, cái cách 2
0:09:10 - 0:09:12, là chúng ta sẽ kết hợp
0:09:14 - 0:09:16, với các cái mô hình
0:09:18 - 0:09:20, khác
0:09:20 - 0:09:22, và cái cách số 3
0:09:22 - 0:09:26, đó là chúng ta sẽ học chuyển tiếp Transfer Learning
0:09:26 - 0:09:33, chúng ta sẽ thực hiện gọi là Transfer Learning
0:09:33 - 0:09:37, tức là chúng ta sẽ thiết kế phần sau của kiến trúc mạng CNN của mình
0:09:37 - 0:09:39, sao cho nó phù hợp với lại dữ liệu mới
0:09:39 - 0:09:42, sau đó chúng ta sẽ tinh chỉnh và có 2 cách tinh chỉnh
0:09:42 - 0:09:50, đó là cách 3.1, đó là chúng ta sẽ tinh chỉnh phần cuối
0:09:50 - 0:09:53, đó là phần phân lớp
0:09:53 - 0:10:01, và cách 3.2 đó là chúng ta sẽ đi tinh chỉnh toàn bộ
0:10:01 - 0:10:04, toàn bộ cả mạng
0:10:04 - 0:10:11, thì đây là 3 cái cách thức để chúng ta có thể sử dụng một cái mạng huấn luyện sẵn
0:10:11 - 0:10:17, thì hy vọng là qua cái bài học này giúp chúng ta có cái góc nhìn toàn diện hơn
0:10:17 - 0:10:22, về những thành tựu của mạng CNN và nắm bắt được một trong những cách thức
0:10:22 - 0:10:30, mà các nhà, các engineer đang sử dụng để ứng dụng trong các công việc của mình
0:10:30 - 0:10:33, là chính là sử dụng phương pháp sử dụng mô hình huấn luyện sẵn.