0:00:00 - 0:00:03, Bước tiếp theo, chúng ta sẽ khởi tạo các mô hình.
0:00:06 - 0:00:09, Rồi, CNN.build.
0:00:11 - 0:00:16, Và ở đây chúng ta sẽ copy xuống các tham số để tránh bị sơ suất.
0:00:17 - 0:00:20, Đầu tiên input dimension thì ảnh này của mình,
0:00:20 - 0:00:26, nếu thông thường chúng ta sẽ để là 28,28.
0:00:26 - 0:00:36, Tuy nhiên, mô hình convolution chỉ có thể thực hiện được khi nó phải là 1 tensor 3 chiều
0:00:36 - 0:00:40, Do đó ở đây chúng ta sẽ để là 28,28,1
0:00:40 - 0:00:43, Activation thì chúng ta sẽ để là sigmoid
0:00:43 - 0:00:46, Conv1 chúng ta sẽ để là 6
0:00:46 - 0:00:49, Conv2 chúng ta sẽ để là 16
0:00:49 - 0:00:51, FC chúng ta sẽ để là 1
0:00:51 - 0:00:54, FC Lớp số 1 chúng ta sẽ để là 120
0:00:54 - 0:00:57, FC số 2 thì chúng ta sẽ để là 84
0:00:57 - 0:01:01, và hàm activation ở đây thì chúng ta sẽ để là hàm sigmoid
0:01:01 - 0:01:04, rồi, bây giờ chúng ta sẽ chạy thử
0:01:04 - 0:01:07, và chương trình chạy được rồi
0:01:07 - 0:01:11, bây giờ chúng ta sẽ xem coi là cái mạng CNN này trong summary
0:01:11 - 0:01:13, xem có thể thực hiện được hay không
0:01:13 - 0:01:17, để xem cái kích thước, cái kiến trúc của cái mạng CNN này
0:01:17 - 0:01:20, thì chúng ta có thể thấy là trong cái mạng CNN này
0:01:20 - 0:01:24, nó thỏa mãn được đúng như kiến trúc mà chúng ta mong muốn
0:01:24 - 0:01:27, là bao gồm thực hiện phép convolution số 1
0:01:27 - 0:01:30, với 6 filter thực hiện convolution số 2
0:01:30 - 0:01:31, với 16 filter
0:01:31 - 0:01:37, rồi và cái kích thước của các tensor cũng giảm dần
0:01:37 - 0:01:41, đó là từ 28 xuống 14 xuống 7 giống như trong thiết kế ở đây
0:01:41 - 0:01:45, và số neuron của mình
0:01:45 - 0:01:51, Số tham số của mình sẽ là 100.000 tham số
0:01:51 - 0:01:53, Bây giờ chúng ta sẽ tiến hành Train
0:01:53 - 0:01:59, Chúng ta sẽ truyền vào 2 tham số, đó là X_train và y_train
0:01:59 - 0:02:01, Tuy nhiên y_train phải ở dạng là One-Hot
0:02:01 - 0:02:06, Rồi, thì việc Train này đâu đó nó có thể tốn
0:02:06 - 0:02:08, Ồ, ở đây chúng ta quên mất một cái việc
0:02:08 - 0:02:14, Đó là sau đây để mà có thể vẽ được cái
0:02:14 - 0:02:17, train loss, vẽ được cái giá trị loss
0:02:17 - 0:02:22, theo số epoch, chúng ta sẽ phải gán vào một cái biến, đó là History
0:02:22 - 0:02:28, Rồi sau đó thì ở đây ta mới có thể thực hiện được cái việc trực quan hóa này
0:02:28 - 0:02:36, Rồi, để trực quan hóa cho cái mô hình
0:02:36 - 0:02:40, thì chúng ta sẽ phải lấy ra các filter
0:02:40 - 0:02:45, thì ở đây chúng ta sẽ lấy ra filter ở lớp đầu tiên
0:02:45 - 0:02:48, đó chính là cnn.get_weights
0:02:48 - 0:02:52, get_weights ở đây chúng ta sẽ để layer số 1
0:02:52 - 0:02:55, tại vì layer số 0 là input rồi
0:02:55 - 0:02:57, layer số 1 là lớp convolution
0:02:59 - 0:03:01, rồi chúng ta sẽ cùng quan sát
0:03:01 - 0:03:10, Nhưng mà đương nhiên là phải chờ cái mô hình này huấn luyện xong thì chúng ta mới có thể thấy được cái train loss này chạy như thế nào
0:03:10 - 0:03:17, Ở đây thì chúng ta quan sát thấy là cái train loss của mình đã giảm từ 0.18 trong cái epoch đầu tiên
0:03:17 - 0:03:27, Giảm xuống còn 0.13, giảm xuống còn 0.10 và đến cái epoch thứ 25, 26 thì giảm xuống còn 0.01
0:03:27 - 0:03:35, và hi vọng là đến epoch số 30 thì loss của mình đã giảm xuống 0.007
0:03:35 - 0:03:42, và accuracy cho tập dữ liệu train đã lên đến 99.85%
0:03:42 - 0:03:46, và chúng ta quan sát thì loss giảm rất tốt
0:03:46 - 0:03:49, chúng ta quan sát train loss này giảm rất tốt
0:03:49 - 0:03:55, bây giờ chúng ta sẽ xem my_W có giá trị là bao nhiêu
0:03:55 - 0:04:03, thì ở đây chúng ta sẽ thấy là cái W này sẽ là một Array, xin lỗi là một List
0:04:03 - 0:04:12, bao gồm hai phần tử, thì phần tử đầu tiên chính là cái số, trọng số, số filter của phép convolution đầu tiên
0:04:12 - 0:04:17, và thành phần thứ hai chính là bias, tại vì chúng ta có sử dụng bias
0:04:17 - 0:04:21, W[0] chính là trọng số của mình
0:04:21 - 0:04:25, Để xem trọng số này có kích thước bao nhiêu
0:04:25 - 0:04:31, Chúng ta là .shape, trong đó 3, 3, 1, 6
0:04:31 - 0:04:33, 3, 3 chính là kích thước của kernel
0:04:33 - 0:04:37, và 1 chính là input, dimension của input
0:04:37 - 0:04:41, của đầu vào chỉ có 1 kênh, nó sẽ là 1
0:04:41 - 0:04:45, output của mình sẽ là 6, 6 filter
0:04:45 - 0:04:49, để trực quan, chúng ta sẽ có số filter là 6
0:04:49 - 0:04:53, Rồi chúng ta sẽ duyệt qua i từ 0 đến 5 để truyền vô đây.
0:04:55 - 0:05:00, Rồi đây là W[0], W[0].shape, 3,3,1,6.
0:05:00 - 0:05:03, Thì chúng ta sẽ lấy cái chỉ số i chạy ở đây trước,
0:05:03 - 0:05:05, rồi sau đó lấy chỉ số j chạy ở đây.
0:05:05 - 0:05:09, Thì ở đây một cách tổng quát, trong lớp convolution số 2,
0:05:09 - 0:05:12, thì số 1 này sẽ chuyển thành là số 16.
0:05:12 - 0:05:14, Vì vậy ở đây chúng ta sẽ để là,
0:05:14 - 0:05:18, i là chạy cho 1 cái range, range này thì ở đây chúng ta để là 1,
0:05:18 - 0:05:20, nhưng mà sắp tới có thể để là 16.
0:05:24 - 0:05:27, Đây chính là 6 filter ở cái lớp đầu tiên.
0:05:27 - 0:05:33, Thì chúng ta có thể hiểu ý nghĩa của filter này là chúng ta lấy sai số,
0:05:33 - 0:05:36, sự chênh lệch của vùng phía bên phải, phía dưới,
0:05:36 - 0:05:40, so với lại vùng phía trái, bên trên.
0:05:40 - 0:05:46, Ý nghĩa của filter này là lấy sự chênh lệch giữa hàng ở giữa
0:05:46 - 0:05:48, so với lại hai cái hàng ở phía trên và phía dưới
0:05:48 - 0:05:52, thì mỗi một cái filter này sẽ thể hiện một đặc trưng mà nó học được
0:05:52 - 0:06:00, rồi tiếp theo là chúng ta sẽ tiến hành thử nghiệm với một số biến thể khác nhau
0:06:00 - 0:06:04, nhưng trước khi qua thử nghiệm một số biến thể khác nhau thì chúng ta sẽ thử cái hàm predict
0:06:04 - 0:06:08, cái hàm predict thì cnn.predict
0:06:08 - 0:06:19, Chúng ta sẽ truyền vào X_test và mẫu dữ liệu thứ 300
0:06:19 - 0:06:21, Rồi
0:06:21 - 0:06:31, Ở đây thì hàm predict, chúng ta sẽ xem lại hàm predict của mình
0:06:31 - 0:06:36, truyền vào cnn.X_test
0:06:36 - 0:06:38, bây giờ chúng ta sẽ xem tiếp
0:06:38 - 0:06:43, X_test đã được reshape rồi
0:06:43 - 0:06:46, và đã được chuẩn hóa rồi
0:06:46 - 0:06:47, đúng không?
0:06:47 - 0:06:48, rồi
0:06:56 - 0:06:58, bây giờ chúng ta sẽ thử
0:06:58 - 0:07:00, truyền vào như thế này
0:07:00 - 0:07:10, Với mẫu test thì xét một hình ảnh
0:07:10 - 0:07:18, reshape và xét các dimension
0:07:18 - 0:07:24, Số kênh đầu vào thì xét một kênh
0:07:24 - 0:07:29, và 28 x 1
0:07:29 - 0:07:34, rồi sau đó chúng ta mới đưa vào để cho cái mô hình mình có thể predict được
0:07:34 - 0:07:37, cnn.predict
0:07:37 - 0:07:39, rồi
0:07:39 - 0:07:44, ồ, cũng chưa được nè
0:07:44 - 0:07:58, Rồi, ở đây, cái số này sẽ phải để lên trước, cái này sẽ phải để lên trước là 1,28
0:07:58 - 0:08:12, Ok, được rồi. Tức là nó sẽ phải để cái chỉ số của cái batch size lên trước, nó sẽ hơi ngược
0:08:12 - 0:08:18, bây giờ chúng ta sẽ thử xem cái nhãn này nó sẽ ra cái giá trị là bao nhiêu
0:08:18 - 0:08:21, tại vì ở đây nó chỉ trả ra một cái vector One-Hot
0:08:21 - 0:08:29, chúng ta sẽ phải có thêm một cái hàm nữa đó là argmax là np.argmax
0:08:35 - 0:08:37, rồi nó sẽ là 4
0:08:37 - 0:08:41, Bây giờ chúng ta sẽ xem mẫu thứ 300 này
0:08:41 - 0:08:43, y_test của mình
0:08:43 - 0:08:44, thứ 300
0:08:44 - 0:08:46, đó là bằng bao nhiêu?
0:08:46 - 0:08:47, đó là 4
0:08:49 - 0:08:52, Rồi, bây giờ chúng ta sẽ thử những mẫu khác
0:08:52 - 0:08:54, chúng ta sẽ thử những mẫu khác
0:08:54 - 0:08:55, ở đây chúng ta sẽ để là
0:08:55 - 0:08:56, predict
0:09:02 - 0:09:03, Dự đoán
0:09:03 - 0:09:11, nhãn dự đoán là 4
0:09:11 - 0:09:18, còn ở đây sẽ là nhãn thực tế
0:09:18 - 0:09:24, và ở đây chỉ số này chúng ta sẽ thêm số idx nữa
0:09:24 - 0:09:30, idx là bằng 100 dù vậy
0:09:30 - 0:09:32, Và chúng ta sẽ để đây là idx
0:09:36 - 0:09:40, Rồi, thì đại đa số chúng ta thấy là cái độ chính xác rất là cao
0:09:40 - 0:09:44, Chúng ta thử rất nhiều những cái nhãn khác nhau
0:09:45 - 0:09:49, Thì nó đều ra là dự đoán và thực tế không phải nhau
0:09:49 - 0:09:54, Bây giờ, trong cái mạng CNN thì chúng ta thấy nó có rất nhiều những cái module khác nhau
0:09:54 - 0:10:00, Và tại thời điểm hiện tại thì chúng ta sẽ chưa hiểu rõ cái vai trò của từng module này
0:10:00 - 0:10:04, Do đó thì chúng ta sẽ làm một cái thí nghiệm, nó gọi là Ablation Study
0:10:04 - 0:10:07, với các biến thể khác nhau bằng cách
0:10:07 - 0:10:13, đó là chúng ta sẽ lần lượt thay đổi một số cái cấu hình của chương trình của mình
0:10:13 - 0:10:15, chúng ta sẽ thay đổi một số cái cấu hình
0:10:15 - 0:10:19, Thì cái phiên bản, cái biến thể đầu tiên
0:10:19 - 0:10:25, đó là chúng ta sẽ bỏ đi cái thay cái hàm sigmoid bằng ReLU
0:10:25 - 0:10:27, chúng ta sẽ thay cái sigmoid bằng ReLU
0:10:27 - 0:10:30, Vì vậy chúng ta sẽ copy tập code ở đây đem xuống
0:10:36 - 0:10:40, Chúng ta sẽ đem phần hàm này thay sigmoid bằng ReLU
0:10:41 - 0:10:47, Vì vậy bản chất là biến thể này chúng ta không cần phải khởi tạo lại
0:10:49 - 0:10:52, Mà chúng ta chỉ sửa tham số của mình thôi
0:10:52 - 0:10:58, Chúng ta chỉ sửa tham số khi gọi hàm build thôi
0:10:58 - 0:11:03, Rồi, và đây là ReLU
0:11:03 - 0:11:10, Rồi, sau đó chúng ta sẽ tiến hành là cnn.train
0:11:10 - 0:11:18, và X_train, rồi y_train oh
0:11:18 - 0:11:23, Và lưu ý ở đây chúng ta sẽ để cái History là History số 2
0:11:24 - 0:11:29, Rồi, bây giờ chúng ta sẽ tiến hành build cái này
0:11:30 - 0:11:34, Và tranh thủ trong thời gian chờ đợi thì chúng ta sẽ thử
0:11:35 - 0:11:38, viết code trước cho cái phần là
0:11:40 - 0:11:42, vẽ cái giá trị loss
0:11:42 - 0:11:47, Chúng ta sẽ thêm một cái đường nữa, đó là History số 2
0:11:47 - 0:11:53, Và ở đây sẽ là train_loss v1
0:11:53 - 0:11:58, Đây sẽ là train_loss v2
0:11:58 - 0:12:01, Trong đó v2 đó là dùng ReLU
0:12:01 - 0:12:06, Dùng ReLU
0:12:06 - 0:12:10, Rồi, tương tự như vậy, bây giờ chúng ta sẽ chờ đợi
0:12:10 - 0:12:15, Chúng ta sẽ viết trước cái code cho các biến thể
0:12:15 - 0:12:23, Tiếp theo, biến thể bỏ hết các lớp Pooling thì chúng ta làm cũng rất là nhanh
0:12:23 - 0:12:32, Pooling chúng ta sẽ bỏ đi và lưu ý là phải để gối đầu các cái biến
0:12:32 - 0:12:36, Ví dụ như ở đây C1 thì sẽ được truyền trực tiếp sang đây
0:12:36 - 0:12:39, rồi C3 thì sẽ truyền trực tiếp sang đây
0:12:39 - 0:12:44, Vì vậy chúng ta đã xong biến thể số 3, chúng ta sẽ để là CNNv3