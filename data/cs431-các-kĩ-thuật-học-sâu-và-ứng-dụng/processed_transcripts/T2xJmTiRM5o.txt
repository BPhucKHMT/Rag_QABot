0:00:00 - 0:00:06, Mô hình logistic regression cũng được phát triển từ môn học tổng quát.
0:00:06 - 0:00:13, Chúng ta nhắc lại, đầu vào là chúng ta sẽ có dữ liệu x và đầu ra chúng ta sẽ có dữ liệu y.
0:00:13 - 0:00:26, Tùy vào tính chất của cặp dữ liệu xy này, mình sẽ thiết kế các hàm dự đoán f(theta, x) và hàm lỗi L(theta, x_i).
0:00:26 - 0:00:35, Còn công việc số 3 là tìm theta sao cho hàm lỗi nhỏ nhất này, thì chúng ta cũng đã có công cụ đó là thuật toán gradient descent.
0:00:35 - 0:00:47, Đối với mô hình logistic regression, chúng ta sẽ phải đi giải quyết một bài toán, trong đó chúng ta phải phân ra làm 2 lớp, xanh và cam ở đây.
0:00:47 - 0:00:55, x1 và x2 chính là đặc trưng đầu vào, trong trường hợp này chúng ta sẽ lấy mặt phẳng 2 chiều
0:00:55 - 0:01:02, và chúng ta sẽ phải phân tách 2 tập điểm xanh và một cái cam này ra làm 2 phần
0:01:02 - 0:01:09, và trong trường hợp này thì dữ liệu của mình nó gọi là phân tách được một cách tuyến tính hay còn gọi là linear separable
0:01:09 - 0:01:13, thì ở đây chúng ta sẽ có được một cái đường thẳng tách ra làm 2
0:01:13 - 0:01:17, thì theo như kiến thức toán cấp 2, cấp 3 mà chúng ta đã học
0:01:17 - 0:01:20, thì với phương trình đường thẳng này chúng ta có thể viết nó dưới dạng là
0:01:20 - 0:01:28, ax1 cộng cho bx2 cộng cho c bằng 0
0:01:28 - 0:01:31, và tất cả những điểm nào mà nằm trên đường thẳng này
0:01:31 - 0:01:35, thì khi thế vào các điểm x1, x2 nằm trên đường thẳng này
0:01:35 - 0:01:38, thế vào thì chúng ta sẽ có giá trị bằng 0
0:01:38 - 0:01:52, Bây giờ chúng ta sẽ làm quen với bộ tham số theta 1 x 1 cộng theta 2 x 2 cộng theta 0.
0:01:52 - 0:01:58, Thì nếu những điểm nào nằm trên đường thẳng này thì nó sẽ ra bằng không.
0:01:58 - 0:02:04, Còn những điểm nào nằm về phía bên trên, ví dụ như ở đây,
0:02:04 - 0:02:08, Thế vô thì sẽ ra giá trị lớn hơn 0.
0:02:08 - 0:02:14, Còn những điểm nằm dưới như vậy thì sẽ là nhỏ hơn 0.
0:02:14 - 0:02:21, Như vậy dựa trên quan sát, kiến thức toán cấp 2, cấp 3 mà chúng ta đã học được
0:02:21 - 0:02:27, thì chúng ta sẽ thiết kế hàm dự đoán bằng dạng như trên.
0:02:27 - 0:02:33, Đó là f(theta, x1, x2), x là dữ kiện đầu vào, hai đặc trưng đầu vào.
0:02:33 - 0:02:39, Nó sẽ bằng 1, tức là cái nhãn y này là bằng 1.
0:02:39 - 0:02:50, Nếu theta0 cộng theta1 x1 cộng theta2 x2
0:02:50 - 0:02:57, lớn hơn hoặc bằng 0, tức là nó thuộc về một nửa cái mặt phẳng này, thì nó sẽ được gán giá trị là 1.
0:02:57 - 0:03:02, và nó sẽ bằng 0, cái nhãn dự đoán của mình sẽ bằng 0
0:03:02 - 0:03:06, nếu như theta 0 cộng cho theta 1 x 1 cộng theta 2 x 2
0:03:06 - 0:03:11, nó bé hơn 0, tức là nó nằm về một nửa phía bên này
0:03:11 - 0:03:16, thì nếu như chúng ta thiết kế cái hàm dự đoán như thế này thì điều gì sẽ xảy ra?
0:03:16 - 0:03:20, điều gì sẽ xảy ra?
0:03:20 - 0:03:26, đó là hàm này là hàm không liên tục
0:03:26 - 0:03:28, Hàm này là hàm không liên tục.
0:03:28 - 0:03:31, Hàm không liên tục thì sau này khi chúng ta đến cái bước số 3,
0:03:31 - 0:03:35, chúng ta tính đạo hàm sẽ rất là khó.
0:03:35 - 0:03:39, Do đó chúng ta phải cố gắng thiết kế hàm f(theta, x)
0:03:39 - 0:03:41, sao cho nó phải là một cái hàm liên tục.
0:03:41 - 0:03:43, Chúng ta sẽ tìm cách thiết kế
0:03:43 - 0:03:46, bằng cách dựa trên quan sát
0:03:46 - 0:03:51, đó là miền giá trị của biểu thức tuyến tính,
0:03:51 - 0:03:54, tức là miền giá trị của giá trị này,
0:03:54 - 0:03:56, của cái phép tính này nè
0:03:56 - 0:03:58, nó sẽ thuộc cái đoạn là từ
0:03:58 - 0:04:00, trừ vô cùng cho đến cộng vô cùng
0:04:00 - 0:04:02, trong khi đó
0:04:02 - 0:04:04, trong khi đó cái giá trị
0:04:04 - 0:04:06, mà mình mong muốn
0:04:06 - 0:04:08, dự đoán nó sẽ nhận hai giá trị là
0:04:08 - 0:04:10, 0 và 1
0:04:10 - 0:04:12, trong khi đó cái miền giá trị của
0:04:12 - 0:04:14, theta 0 cộng cho theta 1 x1
0:04:14 - 0:04:16, cộng cho theta 2 x2 đó là trừ vô cùng
0:04:16 - 0:04:18, cho đến cộng vô cùng
0:04:18 - 0:04:20, thì để ép các
0:04:20 - 0:04:22, cái giá trị này
0:04:22 - 0:04:30, Tổng này về miền giá trị từ 0 cho đến 1, chúng ta sẽ sử dụng hàm sigmoid.
0:04:31 - 0:04:34, Hàm sigmoid sẽ có công thức như sau.
0:04:35 - 0:04:41, Sigmoid của x, chúng ta sẽ viết là bằng 1 phần 1 cộng cho e mũ trừ x.
0:04:41 - 0:04:46, Và dạng đồ thị hàm số của hàm sigmoid sẽ có dạng như sau.
0:04:52 - 0:04:57, Từ trừ vô cùng cho đến cộng vô cùng
0:04:57 - 0:05:02, Với giá trị đầu vào của mình là từ trừ vô cùng cho đến cộng vô cùng thì qua hàm sigmoid
0:05:02 - 0:05:10, Nó sẽ ép về miền giá trị là từ 0 cho đến 1
0:05:10 - 0:05:15, Đây là sơ đồ, đồ thị của hàm sigmoid
0:05:15 - 0:05:24, Vì vậy, giá trị đầu vào của biểu thức theta 0 cộng theta 1 x1 cộng theta 2 x2 từ trừ vô cùng cho đến cộng vô cùng
0:05:24 - 0:05:29, qua hàm sigmoid, thì nó đã đưa về miền giá trị từ 0 cho đến 1.
0:05:32 - 0:05:33, Ta sẽ thuộc đoạn từ 0 đến 1.
0:05:33 - 0:05:38, Đây chính là cách để chúng ta thiết kế hàm dự đoán.
0:05:38 - 0:05:42, Với hàm sigmoid này, đây là một hàm liên tục.
0:05:45 - 0:05:50, Hàm sigmoid là một hàm liên tục và giá trị bên trong này cũng là một hàm liên tục
0:05:50 - 0:05:53, Do đó thì f của mình sẽ là một hàm liên tục
0:05:54 - 0:05:57, Và như vậy đó thì thỏa mãn được tiêu chí đó là
0:05:57 - 0:06:01, Hàm của mình liên tục để sau này đến bước số 3 chúng ta tính đạo hàm, quá dễ
0:06:02 - 0:06:06, Rồi, ở cái dạng vector hóa
0:06:06 - 0:06:09, Tức là nếu như dữ liệu của chúng ta là một mẫu
0:06:09 - 0:06:12, Trong trường hợp này chúng ta sẽ có x1 và x2
0:06:12 - 0:06:17, Nhưng một cách tổng quát, x của mình sẽ bao gồm m thành phần,
0:06:17 - 0:06:23, x bao gồm m thành phần chứ không chỉ có hai thành phần, còn một chính là thành phần bias.
0:06:24 - 0:06:33, Tham số của mình là theta, theta sẽ bao gồm theta 0, theta 1, theta 2, ..., theta m tương ứng với x đầu vào.
0:06:33 - 0:06:40, Vì vậy, hàm dự đoán của mình sẽ là, viết gọn lại, f(theta, x) sẽ là bằng sigmoid của theta chuyển vị nhân x.
0:06:40 - 0:07:03, Đối với việc vector hóa cho dữ liệu toàn mẫu tập hợp tất cả mẫu dữ liệu của tập dữ liệu của dữ liệu của mình, đây là 1 mẫu, đây là 1 mẫu thứ nhất, mẫu thứ hai và đây là mẫu thứ n.
0:07:03 - 0:07:08, Mỗi cái mẫu này sẽ đại diện như vậy là một cái cột
0:07:08 - 0:07:12, và tập hợp tất cả cái cột này sẽ tạo thành một cái ma trận
0:07:15 - 0:07:23, Và theta sẽ là một cái vector theta 0, theta 1 cho đến theta m
0:07:23 - 0:07:28, Vì vậy cái hàm dự đoán của mình sẽ được viết gọn lại cũng cùng một cái công thức như trên
0:07:28 - 0:07:36, Nếu như công thức ở trên đây, đó là theta chuyển vị nhân với x này là một mẫu
0:07:39 - 0:07:47, Thì qua cái công thức bên đây, x này đó là n mẫu, tức là toàn bộ, toàn bộ các mẫu dữ liệu của mình
0:07:47 - 0:07:54, Và theta chuyển vị nhân với x trong trường hợp này nó chính là một vector dạng nằm ngang
0:07:54 - 0:08:04, Cái vector sẽ là giá trị y ngã dự đoán.
0:08:04 - 0:08:13, Hàm sigmoid không phải là tính cho một giá trị scalar, mà là tính cho một vector.
0:08:13 - 0:08:22, Và kết quả của phép sigmoid này, kết quả của phép biến đổi sigmoid, hàm sigmoid này trên cái vector này sẽ ra một cái vector.
0:08:23 - 0:08:30, Nó sẽ ra một cái vector. Và từng phần tử trong đây tương ứng chính là sigmoid của phần tử ở phía trên.
0:08:31 - 0:08:34, Phần tử này qua hàm sigmoid nó sẽ tính ra cái giá trị ở đây.
0:08:34 - 0:08:44, Vì vậy ở đây nó sẽ là tính từng phần tử một, tức là tính trên từng phần tử và sigmoid của 1 vector nằm ngang, nó sẽ ra 1 vector nằm ngang.
0:08:44 - 0:08:49, Và chúng ta sẽ qua cái bước thứ 2, đó là chúng ta sẽ thiết kế hàm lỗi.
0:08:49 - 0:08:59, Và trong trường hợp này thì cái y, giá trị thực tế là nó sẽ nhận 2 giá trị là 1, y bằng 1 hoặc là y bằng 0, tương ứng là 2 cái lớp của mình.
0:08:59 - 0:09:07, Đối với hàm lỗi cho trường hợp một mẫu dữ liệu và không có vector hóa,
0:09:07 - 0:09:11, không vector hóa nghĩa là chúng ta sẽ tính trên từng phần tử riêng biệt,
0:09:11 - 0:09:12, thay vì tính hàng loạt.
0:09:12 - 0:09:17, Và ở đây chúng ta sẽ có công thức hàm lỗi như trên.
0:09:17 - 0:09:19, Ở đây chúng ta sẽ đặt một câu hỏi là,
0:09:19 - 0:09:23, tại sao công thức của hàm lỗi này có vẻ phức tạp quá?
0:09:23 - 0:09:26, Tại sao công thức này có vẻ phức tạp?
0:09:26 - 0:09:32, Nếu có cả hàm log của y ngã và log của 1 trừ y ngã thì hàm này quá phức tạp.
0:09:32 - 0:09:43, Tại sao chúng ta không sử dụng chính hàm Mean Squared Error (MSE) của hàm cho phần linear regression,
0:09:43 - 0:09:55, đó là công thức L(theta) là bằng 1 phần 2n trung bình cộng của y ngã trừ cho y tất cả bình phương.
0:09:55 - 0:10:02, Tại sao chúng ta không dùng công thức này mà lại sử dụng công thức ở trên?
0:10:02 - 0:10:07, Rồi, thì bây giờ trước tiên chúng ta phải kiểm tra xem công thức ở trên có tính đúng đắn hay không.
0:10:07 - 0:10:15, Thế thì, yêu cầu đặt ra của hàm lỗi đó là nếu chúng ta đoán đúng thì lỗi của mình phải bằng 0.
0:10:15 - 0:10:18, Nếu mà đúng thì lỗi của mình phải bằng 0.
0:10:18 - 0:10:22, Và nếu chúng ta đoán sai thì lỗi của mình phải lớn hơn 0.
0:10:22 - 0:10:29, Bây giờ chúng ta sẽ xét thử 1 trường hợp nếu y của mình là bằng 1
0:10:29 - 0:10:35, Đây là giá trị thực tế, nhưng giá trị dự đoán của mình là y bằng 0
0:10:35 - 0:10:41, Bây giờ chúng ta sẽ xét trường hợp y của mình là đoán đúng rồi đi
0:10:41 - 0:10:45, Y cũng bằng 1 đi, khi chúng ta thế vào công thức này
0:10:45 - 0:10:49, Loss của mình trong trường hợp này sẽ là giá trị bằng bao nhiêu?
0:10:49 - 0:10:57, Bằng trừ, y bằng 1, giữ nguyên, kéo 1 xuống
0:10:57 - 0:11:10, log y là log y ngã, y ngã là bằng 1, 1 trừ y trong trường hợp này, 1 trừ cho 1 là 0
0:11:10 - 0:11:13, Vì vậy, phần còn lại là không cần tính nữa.
0:11:13 - 0:11:16, Vì vậy, nó sẽ là bằng trừ log của 1.
0:11:16 - 0:11:27, Trừ log của 1, trong đồ thị của hàm log, đây là hàm log x.
0:11:27 - 0:11:30, Giá trị tại đây là bằng 1.
0:11:30 - 0:11:34, Log của 1 tại vị trí này tương ứng là bằng 0.
0:11:34 - 0:11:37, Vì vậy, trong trường hợp đoán đúng,
0:11:37 - 0:11:46, trong trường hợp đoán đúng, thì loss của mình sẽ là bằng 0
0:11:46 - 0:11:51, và tương tự như vậy nếu cho trường hợp y bằng 0 và y ngã bằng 0
0:11:51 - 0:11:54, tức là đây cũng đoán đúng nhưng trong trường hợp y bằng 0
0:11:54 - 0:11:58, thì các bạn thấy chúng ta cũng sẽ ra được giá trị sai số là bằng 0
0:11:58 - 0:12:01, Bây giờ chúng ta sẽ xem trong cái trường hợp
0:12:02 - 0:12:04, chúng ta sẽ xem trong cái trường hợp đó là
0:12:05 - 0:12:07, nếu chúng ta đoán sai
0:12:08 - 0:12:09, nếu chúng ta đoán sai
0:12:10 - 0:12:14, y bằng một và y ngã dự đoán là bằng không
0:12:14 - 0:12:16, thì thế vô cái công thức
0:12:16 - 0:12:18, chúng ta sẽ thế vô cái công thức ở trên đây
0:12:18 - 0:12:20, thì nó sẽ ra như thế nào
0:12:21 - 0:12:22, y bằng một
0:12:22 - 0:12:33, Loss của mình sẽ là bằng trừ 1 giữ nguyên nhân cho log y ngã trong trường hợp này là bằng 0
0:12:33 - 0:12:38, 1 trừ y sẽ là bằng 1 trừ 1, do đó là phần sau chúng ta bỏ qua
0:12:38 - 0:12:41, Vậy nó sẽ là bằng trừ log 0
0:12:41 - 0:12:47, Chúng ta thấy là với cái đồ thị hàm số này thì khi x của mình mà tiến về 0
0:12:47 - 0:12:54, Vì vậy, giá trị của log sẽ tiến về trừ vô cùng
0:12:54 - 0:12:59, Và đó sẽ là bằng trừ của trừ vô cùng, tức là bằng cộng vô cùng
0:12:59 - 0:13:08, Hay nói cách khác, nếu đoán sai thì mất mát của chúng ta là chúng ta sẽ mất nguyên 1 căn nhà
0:13:08 - 0:13:11, Tức là chính là 1 giá trị rất là lớn
0:13:11 - 0:13:19, Bây giờ chúng ta sẽ thử nghiệm trên giá trị MSE, trên công thức Mean Squared Error
0:13:19 - 0:13:24, Nếu như dự đoán sai, nếu như dự đoán đúng thì lỗi cũng sẽ bằng 0
0:13:24 - 0:13:26, Thế vô chúng ta cũng sẽ ra bằng 0
0:13:26 - 0:13:33, Nếu chúng ta đoán sai, tức là chúng ta sẽ có y trừ cho y ngã tất cả bình phương
0:13:33 - 0:13:40, Y mà trừ y ngã bình phương tức là bằng 1, trừ 0 tất cả bình phương sẽ là bằng 1
0:13:40 - 0:13:45, Nếu dùng công thức MSE này thì sự trừng phạt này nó quá bé
0:00:00 - 0:00:06, Mô hình logistic regression cũng được phát triển từ môn học tổng quát.
0:00:06 - 0:00:13, Chúng ta nhắc lại, đầu vào là chúng ta sẽ có dữ liệu x và đầu ra chúng ta sẽ có dữ liệu y.
0:00:13 - 0:00:26, Tùy vào tính chất của cặp dữ liệu xy này, mình sẽ thiết kế các hàm dự đoán f(theta, x) và hàm lỗi L(theta, x_i).
0:00:26 - 0:00:35, Còn công việc số 3 là tìm theta sao cho hàm lỗi nhỏ nhất này, thì chúng ta cũng đã có công cụ đó là thuật toán gradient descent.
0:00:35 - 0:00:47, Đối với mô hình logistic regression, chúng ta sẽ phải đi giải quyết một bài toán, trong đó chúng ta phải phân ra làm 2 lớp, xanh và cam ở đây.
0:00:47 - 0:00:55, x1 và x2 chính là đặc trưng đầu vào, trong trường hợp này chúng ta sẽ lấy mặt phẳng 2 chiều
0:00:55 - 0:01:02, và chúng ta sẽ phải phân tách 2 tập điểm xanh và một cái cam này ra làm 2 phần
0:01:02 - 0:01:09, và trong trường hợp này thì dữ liệu của mình nó gọi là phân tách được một cách tuyến tính hay còn gọi là linear separable
0:01:09 - 0:01:13, thì ở đây chúng ta sẽ có được một cái đường thẳng tách ra làm 2
0:01:13 - 0:01:17, thì theo như kiến thức toán cấp 2, cấp 3 mà chúng ta đã học
0:01:17 - 0:01:20, thì với phương trình đường thẳng này chúng ta có thể viết nó dưới dạng là
0:01:20 - 0:01:28, ax1 cộng cho bx2 cộng cho c bằng 0
0:01:28 - 0:01:31, và tất cả những điểm nào mà nằm trên đường thẳng này
0:01:31 - 0:01:35, thì khi thế vào các điểm x1, x2 nằm trên đường thẳng này
0:01:35 - 0:01:38, thế vào thì chúng ta sẽ có giá trị bằng 0
0:01:38 - 0:01:52, Bây giờ chúng ta sẽ làm quen với bộ tham số theta 1 x 1 cộng theta 2 x 2 cộng theta 0.
0:01:52 - 0:01:58, Thì nếu những điểm nào nằm trên đường thẳng này thì nó sẽ ra bằng không.
0:01:58 - 0:02:04, Còn những điểm nào nằm về phía bên trên, ví dụ như ở đây,
0:02:04 - 0:02:08, Thế vô thì sẽ ra giá trị lớn hơn 0.
0:02:08 - 0:02:14, Còn những điểm nằm dưới như vậy thì sẽ là nhỏ hơn 0.
0:02:14 - 0:02:21, Như vậy dựa trên quan sát, kiến thức toán cấp 2, cấp 3 mà chúng ta đã học được
0:02:21 - 0:02:27, thì chúng ta sẽ thiết kế hàm dự đoán bằng dạng như trên.
0:02:27 - 0:02:33, Đó là f(theta, x1, x2), x là dữ kiện đầu vào, hai đặc trưng đầu vào.
0:02:33 - 0:02:39, Nó sẽ bằng 1, tức là cái nhãn y này là bằng 1.
0:02:39 - 0:02:50, Nếu theta0 cộng theta1 x1 cộng theta2 x2
0:02:50 - 0:02:57, lớn hơn hoặc bằng 0, tức là nó thuộc về một nửa cái mặt phẳng này, thì nó sẽ được gán giá trị là 1.
0:02:57 - 0:03:02, và nó sẽ bằng 0, cái nhãn dự đoán của mình sẽ bằng 0
0:03:02 - 0:03:06, nếu như theta 0 cộng cho theta 1 x 1 cộng theta 2 x 2
0:03:06 - 0:03:11, nó bé hơn 0, tức là nó nằm về một nửa phía bên này
0:03:11 - 0:03:16, thì nếu như chúng ta thiết kế cái hàm dự đoán như thế này thì điều gì sẽ xảy ra?
0:03:16 - 0:03:20, điều gì sẽ xảy ra?
0:03:20 - 0:03:26, đó là hàm này là hàm không liên tục
0:03:26 - 0:03:28, Hàm này là hàm không liên tục.
0:03:28 - 0:03:31, Hàm không liên tục thì sau này khi chúng ta đến cái bước số 3,
0:03:31 - 0:03:35, chúng ta tính đạo hàm sẽ rất là khó.
0:03:35 - 0:03:39, Do đó chúng ta phải cố gắng thiết kế hàm f(theta, x)
0:03:39 - 0:03:41, sao cho nó phải là một cái hàm liên tục.
0:03:41 - 0:03:43, Chúng ta sẽ tìm cách thiết kế
0:03:43 - 0:03:46, bằng cách dựa trên quan sát
0:03:46 - 0:03:51, đó là miền giá trị của biểu thức tuyến tính,
0:03:51 - 0:03:54, tức là miền giá trị của giá trị này,
0:03:54 - 0:03:56, của cái phép tính này nè
0:03:56 - 0:03:58, nó sẽ thuộc cái đoạn là từ
0:03:58 - 0:04:00, trừ vô cùng cho đến cộng vô cùng
0:04:00 - 0:04:02, trong khi đó
0:04:02 - 0:04:04, trong khi đó cái giá trị
0:04:04 - 0:04:06, mà mình mong muốn
0:04:06 - 0:04:08, dự đoán nó sẽ nhận hai giá trị là
0:04:08 - 0:04:10, 0 và 1
0:04:10 - 0:04:12, trong khi đó cái miền giá trị của
0:04:12 - 0:04:14, theta 0 cộng cho theta 1 x1
0:04:14 - 0:04:16, cộng cho theta 2 x2 đó là trừ vô cùng
0:04:16 - 0:04:18, cho đến cộng vô cùng
0:04:18 - 0:04:20, thì để ép các
0:04:20 - 0:04:22, cái giá trị này
0:04:22 - 0:04:30, Tổng này về miền giá trị từ 0 cho đến 1, chúng ta sẽ sử dụng hàm sigmoid.
0:04:31 - 0:04:34, Hàm sigmoid sẽ có công thức như sau.
0:04:35 - 0:04:41, Sigmoid của x, chúng ta sẽ viết là bằng 1 phần 1 cộng cho e mũ trừ x.
0:04:41 - 0:04:46, Và dạng đồ thị hàm số của hàm sigmoid sẽ có dạng như sau.
0:04:52 - 0:04:57, Từ trừ vô cùng cho đến cộng vô cùng
0:04:57 - 0:05:02, Với giá trị đầu vào của mình là từ trừ vô cùng cho đến cộng vô cùng thì qua hàm sigmoid
0:05:02 - 0:05:10, Nó sẽ ép về miền giá trị là từ 0 cho đến 1
0:05:10 - 0:05:15, Đây là sơ đồ, đồ thị của hàm sigmoid
0:05:15 - 0:05:24, Vì vậy, giá trị đầu vào của biểu thức theta 0 cộng theta 1 x1 cộng theta 2 x2 từ trừ vô cùng cho đến cộng vô cùng
0:05:24 - 0:05:29, qua hàm sigmoid, thì nó đã đưa về miền giá trị từ 0 cho đến 1.
0:05:32 - 0:05:33, Ta sẽ thuộc đoạn từ 0 đến 1.
0:05:33 - 0:05:38, Đây chính là cách để chúng ta thiết kế hàm dự đoán.
0:05:38 - 0:05:42, Với hàm sigmoid này, đây là một hàm liên tục.
0:05:45 - 0:05:50, Hàm sigmoid là một hàm liên tục và giá trị bên trong này cũng là một hàm liên tục
0:05:50 - 0:05:53, Do đó thì f của mình sẽ là một hàm liên tục
0:05:54 - 0:05:57, Và như vậy đó thì thỏa mãn được tiêu chí đó là
0:05:57 - 0:06:01, Hàm của mình liên tục để sau này đến bước số 3 chúng ta tính đạo hàm, quá dễ
0:06:02 - 0:06:06, Rồi, ở cái dạng vector hóa
0:06:06 - 0:06:09, Tức là nếu như dữ liệu của chúng ta là một mẫu
0:06:09 - 0:06:12, Trong trường hợp này chúng ta sẽ có x1 và x2
0:06:12 - 0:06:17, Nhưng một cách tổng quát, x của mình sẽ bao gồm m thành phần,
0:06:17 - 0:06:23, x bao gồm m thành phần chứ không chỉ có hai thành phần, còn một chính là thành phần bias.
0:06:24 - 0:06:33, Tham số của mình là theta, theta sẽ bao gồm theta 0, theta 1, theta 2, ..., theta m tương ứng với x đầu vào.
0:06:33 - 0:06:40, Vì vậy, hàm dự đoán của mình sẽ là, viết gọn lại, f(theta, x) sẽ là bằng sigmoid của theta chuyển vị nhân x.
0:06:40 - 0:07:03, Đối với việc vector hóa cho dữ liệu toàn mẫu tập hợp tất cả mẫu dữ liệu của tập dữ liệu của dữ liệu của mình, đây là 1 mẫu, đây là 1 mẫu thứ nhất, mẫu thứ hai và đây là mẫu thứ n.
0:07:03 - 0:07:08, Mỗi cái mẫu này sẽ đại diện như vậy là một cái cột
0:07:08 - 0:07:12, và tập hợp tất cả cái cột này sẽ tạo thành một cái ma trận
0:07:15 - 0:07:23, Và theta sẽ là một cái vector theta 0, theta 1 cho đến theta m
0:07:23 - 0:07:28, Vì vậy cái hàm dự đoán của mình sẽ được viết gọn lại cũng cùng một cái công thức như trên
0:07:28 - 0:07:36, Nếu như công thức ở trên đây, đó là theta chuyển vị nhân với x này là một mẫu
0:07:39 - 0:07:47, Thì qua cái công thức bên đây, x này đó là n mẫu, tức là toàn bộ, toàn bộ các mẫu dữ liệu của mình
0:07:47 - 0:07:54, Và theta chuyển vị nhân với x trong trường hợp này nó chính là một vector dạng nằm ngang
0:07:54 - 0:08:04, Cái vector sẽ là giá trị y ngã dự đoán.
0:08:04 - 0:08:13, Hàm sigmoid không phải là tính cho một giá trị scalar, mà là tính cho một vector.
0:08:13 - 0:08:22, Và kết quả của phép sigmoid này, kết quả của phép biến đổi sigmoid, hàm sigmoid này trên cái vector này sẽ ra một cái vector.
0:08:23 - 0:08:30, Nó sẽ ra một cái vector. Và từng phần tử trong đây tương ứng chính là sigmoid của phần tử ở phía trên.
0:08:31 - 0:08:34, Phần tử này qua hàm sigmoid nó sẽ tính ra cái giá trị ở đây.
0:08:34 - 0:08:44, Vì vậy ở đây nó sẽ là tính từng phần tử một, tức là tính trên từng phần tử và sigmoid của 1 vector nằm ngang, nó sẽ ra 1 vector nằm ngang.
0:08:44 - 0:08:49, Và chúng ta sẽ qua cái bước thứ 2, đó là chúng ta sẽ thiết kế hàm lỗi.
0:08:49 - 0:08:59, Và trong trường hợp này thì cái y, giá trị thực tế là nó sẽ nhận 2 giá trị là 1, y bằng 1 hoặc là y bằng 0, tương ứng là 2 cái lớp của mình.
0:08:59 - 0:09:07, Đối với hàm lỗi cho trường hợp một mẫu dữ liệu và không có vector hóa,
0:09:07 - 0:09:11, không vector hóa nghĩa là chúng ta sẽ tính trên từng phần tử riêng biệt,
0:09:11 - 0:09:12, thay vì tính hàng loạt.
0:09:12 - 0:09:17, Và ở đây chúng ta sẽ có công thức hàm lỗi như trên.
0:09:17 - 0:09:19, Ở đây chúng ta sẽ đặt một câu hỏi là,
0:09:19 - 0:09:23, tại sao công thức của hàm lỗi này có vẻ phức tạp quá?
0:09:23 - 0:09:26, Tại sao công thức này có vẻ phức tạp?
0:09:26 - 0:09:32, Nếu có cả hàm log của y ngã và log của 1 trừ y ngã thì hàm này quá phức tạp.
0:09:32 - 0:09:43, Tại sao chúng ta không sử dụng chính hàm Mean Squared Error (MSE) của hàm cho phần linear regression,
0:09:43 - 0:09:55, đó là công thức L(theta) là bằng 1 phần 2n trung bình cộng của y ngã trừ cho y tất cả bình phương.
0:09:55 - 0:10:02, Tại sao chúng ta không dùng công thức này mà lại sử dụng công thức ở trên?
0:10:02 - 0:10:07, Rồi, thì bây giờ trước tiên chúng ta phải kiểm tra xem công thức ở trên có tính đúng đắn hay không.
0:10:07 - 0:10:15, Thế thì, yêu cầu đặt ra của hàm lỗi đó là nếu chúng ta đoán đúng thì lỗi của mình phải bằng 0.
0:10:15 - 0:10:18, Nếu mà đúng thì lỗi của mình phải bằng 0.
0:10:18 - 0:10:22, Và nếu chúng ta đoán sai thì lỗi của mình phải lớn hơn 0.
0:10:22 - 0:10:29, Bây giờ chúng ta sẽ xét thử 1 trường hợp nếu y của mình là bằng 1
0:10:29 - 0:10:35, Đây là giá trị thực tế, nhưng giá trị dự đoán của mình là y bằng 0
0:10:35 - 0:10:41, Bây giờ chúng ta sẽ xét trường hợp y của mình là đoán đúng rồi đi
0:10:41 - 0:10:45, Y cũng bằng 1 đi, khi chúng ta thế vào công thức này
0:10:45 - 0:10:49, Loss của mình trong trường hợp này sẽ là giá trị bằng bao nhiêu?
0:10:49 - 0:10:57, Bằng trừ, y bằng 1, giữ nguyên, kéo 1 xuống
0:10:57 - 0:11:10, log y là log y ngã, y ngã là bằng 1, 1 trừ y trong trường hợp này, 1 trừ cho 1 là 0
0:11:10 - 0:11:13, Vì vậy, phần còn lại là không cần tính nữa.
0:11:13 - 0:11:16, Vì vậy, nó sẽ là bằng trừ log của 1.
0:11:16 - 0:11:27, Trừ log của 1, trong đồ thị của hàm log, đây là hàm log x.
0:11:27 - 0:11:30, Giá trị tại đây là bằng 1.
0:11:30 - 0:11:34, Log của 1 tại vị trí này tương ứng là bằng 0.
0:11:34 - 0:11:37, Vì vậy, trong trường hợp đoán đúng,
0:11:37 - 0:11:46, trong trường hợp đoán đúng, thì loss của mình sẽ là bằng 0
0:11:46 - 0:11:51, và tương tự như vậy nếu cho trường hợp y bằng 0 và y ngã bằng 0
0:11:51 - 0:11:54, tức là đây cũng đoán đúng nhưng trong trường hợp y bằng 0
0:11:54 - 0:11:58, thì các bạn thấy chúng ta cũng sẽ ra được giá trị sai số là bằng 0
0:11:58 - 0:12:01, Bây giờ chúng ta sẽ xem trong cái trường hợp
0:12:02 - 0:12:04, chúng ta sẽ xem trong cái trường hợp đó là
0:12:05 - 0:12:07, nếu chúng ta đoán sai
0:12:08 - 0:12:09, nếu chúng ta đoán sai
0:12:10 - 0:12:14, y bằng một và y ngã dự đoán là bằng không
0:12:14 - 0:12:16, thì thế vô cái công thức
0:12:16 - 0:12:18, chúng ta sẽ thế vô cái công thức ở trên đây
0:12:18 - 0:12:20, thì nó sẽ ra như thế nào
0:12:21 - 0:12:22, y bằng một
0:12:22 - 0:12:33, Loss của mình sẽ là bằng trừ 1 giữ nguyên nhân cho log y ngã trong trường hợp này là bằng 0
0:12:33 - 0:12:38, 1 trừ y sẽ là bằng 1 trừ 1, do đó là phần sau chúng ta bỏ qua
0:12:38 - 0:12:41, Vậy nó sẽ là bằng trừ log 0
0:12:41 - 0:12:47, Chúng ta thấy là với cái đồ thị hàm số này thì khi x của mình mà tiến về 0
0:12:47 - 0:12:54, Vì vậy, giá trị của log sẽ tiến về trừ vô cùng
0:12:54 - 0:12:59, Và đó sẽ là bằng trừ của trừ vô cùng, tức là bằng cộng vô cùng
0:12:59 - 0:13:08, Hay nói cách khác, nếu đoán sai thì mất mát của chúng ta là chúng ta sẽ mất nguyên 1 căn nhà
0:13:08 - 0:13:11, Tức là chính là 1 giá trị rất là lớn
0:13:11 - 0:13:19, Bây giờ chúng ta sẽ thử nghiệm trên giá trị MSE, trên công thức Mean Squared Error
0:13:19 - 0:13:24, Nếu như dự đoán sai, nếu như dự đoán đúng thì lỗi cũng sẽ bằng 0
0:13:24 - 0:13:26, Thế vô chúng ta cũng sẽ ra bằng 0
0:13:26 - 0:13:33, Nếu chúng ta đoán sai, tức là chúng ta sẽ có y trừ cho y ngã tất cả bình phương
0:13:33 - 0:13:40, Y mà trừ y ngã bình phương tức là bằng 1, trừ 0 tất cả bình phương sẽ là bằng 1
0:13:40 - 0:13:45, Nếu dùng công thức MSE này thì sự trừng phạt này nó quá bé
0:13:45 - 0:13:50, so với lại công thức của hàm Loss ở đây
0:13:50 - 0:13:53, Cái này quá bé, còn cái này là rất lớn
0:13:53 - 0:13:56, Thì việc lớn bé này nó sẽ ảnh hưởng như thế nào
0:13:56 - 0:13:59, Khi chúng ta có hàm mất mát mà lớn
0:13:59 - 0:14:06, thì việc cập nhật đạo hàm, việc tính đạo hàm
0:14:06 - 0:14:19, theta, nếu như đạo hàm này theo theta hoặc Nabla của L theo theta, thì khi giá trị này có độ dốc lớn,
0:14:19 - 0:14:25, tức là độ dốc của hàm L này lớn, thì khi đó đạo hàm của mình sẽ lớn.
0:14:25 - 0:14:31, Ngược lại, nếu như MSE này giá trị của mình nhỏ,
0:14:31 - 0:14:36, Khi đó tính đạo hàm, độ dốc của đạo hàm sẽ nhỏ
0:14:37 - 0:14:40, Dẫn đến bước cập nhật sẽ chậm
0:14:40 - 0:14:49, Chúng ta có công thức theta là theta trừ cho alpha nhân cho đạo hàm của loss theo theta
0:14:49 - 0:14:55, Nếu như độ dốc của hàm L này, ví dụ có hai cái hàm
0:14:55 - 0:15:04, Đây là hàm thứ nhất, hàm L1 và hàm thứ 2.
0:15:04 - 0:15:14, Cả hai hàm này, trong đó hàm L1 chúng ta thấy có độ dốc rất là lớn, đúng không?
0:15:14 - 0:15:17, Khi đó, cái đạo hàm, giá trị đạo hàm của mình sẽ lớn.
0:15:17 - 0:15:22, Còn hàm L2, độ dốc của mình nó thoai thoải, do đó đạo hàm của nó bé.
0:15:22 - 0:15:31, Nếu như độ dốc lớn, bước nhảy của mình sẽ lớn, dẫn đến việc cập nhật theta sẽ nhanh.
0:15:31 - 0:15:43, Do đó, chúng ta sử dụng công thức trừ của (y nhân log y ngã cộng cho (1 trừ y) nhân log (1 trừ y ngã)),
0:15:43 - 0:15:48, thì nó sẽ giúp cho việc huấn luyện sẽ thực hiện rất là nhanh,
0:15:50 - 0:15:54, nhanh hơn so với việc dùng công thức MSE ở đây.
0:15:54 - 0:15:58, Đó là lý do tại sao mình lại không sử dụng công thức MSE.
0:16:01 - 0:16:06, Bây giờ chúng ta sẽ qua công thức cho trường hợp nhiều mẫu và có vector hóa.
0:16:07 - 0:16:11, Với từng mẫu dữ liệu, chúng ta ghép lại,
0:16:11 - 0:16:13, thì chúng ta sẽ có một cái ma trận x
0:16:13 - 0:16:17, và cái nhãn y của dữ liệu sẽ là một cái vector dạng nằm ngang
0:16:17 - 0:16:22, tham số của mình là theta 0, theta 1 và theta m
0:16:23 - 0:16:32, thì khi đó, cái hàm lỗi của mình sẽ có cái công thức đó là 1 phần n
0:16:32 - 0:16:41, Binary, cái chữ b-c-e này là viết tắt của chữ binary cross entropy
0:16:41 - 0:17:01, Thì đây chính là cái công thức mà hồi nãy mình đã liệt kê, mình đã trình bày đó là trừ của (y nhân cho log của y ngã cộng cho (1 trừ y) nhân cho log của (1 trừ y ngã))
0:17:01 - 0:17:05, Đây là công thức binary cross entropy và lưu ý là chúng ta sẽ tính trên từng phần tử.
0:17:05 - 0:17:06, Nghĩa là sao?
0:17:06 - 0:17:19, Khi chúng ta tính sigmoid của theta x, chúng ta sẽ có chuỗi các phần tử dạng vector dạng nằm ngang.
0:17:20 - 0:17:28, Y ngã là kết quả dự đoán, còn y của mình cũng sẽ có chuỗi các phần tử tạo thành một vector nằm ngang.
0:17:28 - 0:17:34, và chúng ta sẽ tính toán lỗi trên hai giá trị y,y ngã này
0:17:34 - 0:17:37, bằng cách nó sẽ lấy từng phần tử ở đây ra
0:17:37 - 0:17:40, từng phần tử của y ngã với từng phần tử của y
0:17:40 - 0:17:42, thế vào công thức này để tính
0:17:42 - 0:17:43, rồi sau đó nó lại cộng trung bình lại
0:17:43 - 0:17:45, nó sẽ cộng hết, cộng trung bình
0:17:45 - 0:17:50, nó sẽ thực hiện trên từng phần tử của y ngã và y này
0:17:52 - 0:17:54, để mà tính ra hàm lỗi
0:17:54 - 0:18:05, Và dạng đồ thị của hàm logistic regression của mình thì nó cũng tương tự như hàm linear regression.
0:18:05 - 0:18:13, Nếu như linear regression, chúng ta đến bước tổng này là xong, chúng ta sẽ qua tiếp một phép biến đổi nữa là hàm sigmoid.
0:18:13 - 0:18:23, sau khi thực hiện phép tổng này, chúng ta sẽ có công thức y ngã là bằng f(theta, x) là bằng sigmoid của theta chuyển vị nhân x.
0:18:23 - 0:18:30, Theta chuyển vị nhân x chính là cái kết quả sau khi thực hiện cái này. Qua hàm sigmoid thì nó sẽ ra cái y ngã.
0:18:30 - 0:18:36, Trong đó, công thức của sigmoid sẽ là bằng 1 phần 1 cộng cho e mũ trừ x.
0:18:36 - 0:18:44, Đây là dạng đồ thị của mô hình Logistic Regression.