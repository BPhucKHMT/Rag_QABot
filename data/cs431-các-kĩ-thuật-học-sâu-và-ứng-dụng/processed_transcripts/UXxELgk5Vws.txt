0:00:00 - 0:00:08, Ý tưởng của Transformer đó là nếu như trong các kiến trúc cũ là Recurrent Neural Network,
0:00:08 - 0:00:13, thì trong quá trình tính toán chúng ta sẽ phải lan truyền thông tin một cách tuần tự.
0:00:13 - 0:00:21, Nghĩa là tại cái từ ở đây, khi chúng ta xử lý tính toán đến cái từ cuối cùng trong giai đoạn encode,
0:00:21 - 0:00:27, thì chúng ta sẽ phải lan truyền tuần tự đi lên, đi qua, đi lên, đi qua.
0:00:27 - 0:00:31, Như vậy thì nó sẽ tốn rất nhiều bước xử lý tuần tự
0:00:31 - 0:00:38, Và thông tin của cái từ đầu tiên này khi đến được nơi này thì nó cũng đã bị mai một, cũng đã bị mất đi rất là nhiều
0:00:38 - 0:00:42, Thì đó chính là cái điểm yếu của Recurrent Neural Network
0:00:42 - 0:00:50, Và đồng thời cái việc thực hiện tuần tự này cũng sẽ khiến cho chúng ta không thể xử lý song song sử dụng các sức mạnh của GPU
0:00:50 - 0:00:53, Còn kiến trúc của Transformer
0:00:53 - 0:01:03, Cơ chế self-attention cho phép chúng ta xử lý song song
0:01:03 - 0:01:12, Tại vì khi chúng ta tính toán tại đây, chúng ta không cần phải phụ thuộc vào các giá trị được tính toán tại đây
0:01:12 - 0:01:17, Tức là các node ở trên cùng sẽ được thực hiện một cách độc lập
0:01:17 - 0:01:30, Chúng ta muốn tính toán tại vị trí này, tại hidden này, thì chúng ta sẽ phải tính toán ở đây trước, rồi sau đó mới đến đây, tính đến đây, xong chúng ta mới đến đây được.
0:01:30 - 0:01:35, Còn ở đây là các node ở đây là tính độc lập, mà độc lập thì có thể sử dụng GPU được.
0:01:35 - 0:01:42, Do đó thì một số phép tính song song của mình sẽ không phụ thuộc vào chiều dài của chuỗi.
0:01:42 - 0:01:48, Tức là khi cái chuỗi này, cái chuỗi này mà dài, rất là dài, thì nó vẫn có thể thực hiện song song được.
0:01:48 - 0:01:57, Và đồng thời, chúng ta thấy các kết nối dày đặc này, nó sẽ cho phép chúng ta có thể tương tác, tương tác giữa các cái từ.
0:01:57 - 0:02:05, Nếu như ở đây, để mà có thể tương tác được cái từ đầu tiên và cái từ cuối cùng, cái từ đầu tiên và cái từ cuối cùng này,
0:02:05 - 0:02:14, Tại đây thì chúng ta sẽ cần có rất nhiều bước tuần tự mới lan truyền để mà có thể tương tác được
0:02:14 - 0:02:20, Trong khi đó tại đây, cái từ đầu tiên, cái từ đầu tiên này, nó đã có thể tương tác được
0:02:20 - 0:02:27, Thông qua cái lớp trước đó là lớp số 1, lớp số 2 sẽ dùng thông tin của lớp số 1
0:02:27 - 0:02:33, và nó cũng dựa trên thông tin của từ cuối cùng của lớp.
0:02:33 - 0:02:43, Tại layer số 2, nó đã có thể truy xuất đến thông tin của từ đầu tiên và từ cuối cùng của lớp trước đó
0:02:43 - 0:02:47, một cách trực tiếp và không cần thực hiện cách tuần tự.
0:02:47 - 0:02:51, Đây chính là những ưu điểm của Transformer.
0:02:51 - 0:02:54, Và hình vẽ trên đây đó chính là
0:02:55 - 0:02:58, sơ đồ kiến trúc của Transformer
0:02:58 - 0:03:01, thì khi chúng ta mới bắt đầu chúng ta nhìn vào cái sơ đồ này
0:03:01 - 0:03:04, chúng ta sẽ rất là rối vì nó có quá nhiều cái module
0:03:04 - 0:03:08, và chúng ta cũng không biết tại sao nó lại có những cái module này
0:03:08 - 0:03:12, Thế thì bây giờ tại cái bước này, tại cái hình vẽ này
0:03:12 - 0:03:13, thì chúng ta chỉ cần hình dung
0:03:13 - 0:03:15, đó là Transformer bao gồm
0:03:15 - 0:03:18, hai thành phần đó là Encoder và Decoder
0:03:18 - 0:03:21, Đây là Encoder và đây là Decoder
0:03:21 - 0:03:23, Và đây là kiến trúc của Encoder.
0:03:23 - 0:03:25, Và đây là kiến trúc của Decoder.
0:03:25 - 0:03:30, Chúng ta sẽ cùng đến với từng thành phần của Transformer.
0:03:30 - 0:03:33, Đầu tiên đó chính là Encoder.
0:03:33 - 0:03:42, Thì cái module xử lý tính toán đầu tiên của Encoder đó chính là self-attention.
0:03:42 - 0:03:47, Self-attention chính là một cái module chính của Transformer.
0:03:47 - 0:03:49, Đó là một cái module chính.
0:03:49 - 0:03:57, Và với dữ kiện đầu vào là các từ của mình hoặc là các token của mình qua input embedding
0:03:57 - 0:04:03, Tức là chúng ta sẽ biến một từ trong một văn bản thành một vector biểu diễn
0:04:03 - 0:04:11, Và vector biểu diễn này sẽ đến module self-attention
0:04:11 - 0:04:15, Và self-attention nó dựa trên cơ chế của attention
0:04:15 - 0:04:23, Thế thì attention nó cũng na ná, nó cũng tương đương với lại một phép truy vấn trong bảng dữ liệu của mình
0:04:23 - 0:04:29, Có điều nếu như truy vấn trong bảng dữ liệu của mình, chúng ta có một query ở đây
0:04:29 - 0:04:33, Chúng ta sẽ trả trong cơ sở dữ liệu của mình các key và value
0:04:33 - 0:04:36, Thông qua chúng ta sẽ so khớp dựa trên các key
0:04:36 - 0:04:39, Và chúng ta lấy thông tin của key và value
0:04:39 - 0:04:45, Chúng ta sẽ hình dung các khái niệm là Query, Key và Value
0:04:45 - 0:04:51, Chúng ta sẽ hình dung các khái niệm liên quan đến một ứng dụng trong thực tế
0:04:51 - 0:04:56, đó chính là các hệ thống tìm kiếm về multimedia
0:04:56 - 0:04:59, Query của mình chính là các keyword
0:04:59 - 0:05:28, Chúng ta muốn tìm kiếm một video về Transformer thì query sẽ là Transformer Architecture và key là tiêu đề của các video trong kênh YouTube của chúng ta.
0:05:28 - 0:05:33, Và cái value của mình trả về, nó chính là những cái nội dung của video của mình.
0:05:33 - 0:05:37, Đó chính là nội dung video.
0:05:37 - 0:05:42, Rồi, ví dụ như là mô tả, mô tả cái video.
0:05:42 - 0:05:45, Thì đó chính là các cái value của mình.
0:05:45 - 0:05:53, Thì đây chính là sự liên tưởng đến cái hệ thống tìm kiếm các hệ thống truy vấn trong thế giới thực của mình.
0:05:53 - 0:05:59, Còn attention của mình đó sẽ khác so với lại truy vấn trong bảng dữ liệu của mình ở chỗ
0:05:59 - 0:06:04, đó là chúng ta sẽ phải trích xuất ra thông tin của rất nhiều
0:06:04 - 0:06:07, của rất nhiều những cái key và value
0:06:07 - 0:06:09, Ở đây là chúng ta trích xuất một lượt
0:06:09 - 0:06:11, chúng ta sẽ lấy ánh xạ
0:06:11 - 0:06:17, chúng ta sẽ lấy mỗi cái query của mình, nó sẽ ánh xạ đến một cặp key và value
0:06:17 - 0:06:25, Query Q sẽ ánh xạ một lượt đến một cái cặp key và value
0:06:25 - 0:06:31, Trong khi đó, ở Attention thì mỗi một cái query của mình
0:06:31 - 0:06:35, Nó sẽ khớp với mỗi key, nó sẽ so khớp với các cái key này của mình
0:06:35 - 0:06:39, Và nó sẽ trả về tổng tất cả các cái value
0:06:40 - 0:06:44, Có điều ở đây chúng ta sẽ thấy là nó sẽ có trọng số nha
0:06:44 - 0:06:53, Nếu có trọng số thì những key và value liên quan đến query này thì nó mới có trọng số lớn.
0:06:53 - 0:07:06, Còn những key và value có trọng số thấp, tức là ít có sự liên quan, thì khi nó cộng lại thì nó sẽ ít tham gia vào giá trị output của mình.
0:07:06 - 0:07:10, Trong hình này, chúng ta thấy đây là những cái giá trị mà có trọng số thấp.
0:07:11 - 0:07:13, Đây là những cái giá trị có trọng số thấp.
0:07:16 - 0:07:22, Còn những cái key và value mà chúng ta tô đỏ ở đây chính là những cái mà có trọng số cao.
0:07:22 - 0:07:30, Thì khi đó, tỷ trọng, trọng lượng thông tin của V1, V3, V4, khi chúng ta tổng hợp thông tin sẽ là nhiều nhất.
0:07:30 - 0:07:34, V0, V2, V5, V6
0:07:34 - 0:07:39, Hàm lượng thông tin tổng hợp sẽ rất thấp
0:07:39 - 0:07:45, Đó là sự khác nhau giữa Attention với truy vấn trong bảng dữ liệu của mình
0:07:45 - 0:07:52, Khi này, chúng ta sẽ có công thức cho self-attention trong encoder của mình
0:07:52 - 0:07:57, Bước số 1 là với mỗi một từ, cái này chính là embedding
0:07:57 - 0:07:59, Embedding vector của mình
0:07:59 - 0:08:03, Đây là embedding vector của một cái từ
0:08:03 - 0:08:14, Với mỗi từ nó sẽ chia ra thành ba cái giá trị, đó là query, key và value tương ứng là các cái màu
0:08:14 - 0:08:17, Chúng ta sẽ theo dõi dựa trên màu cho dễ hiểu
0:08:17 - 0:08:22, Query nó sẽ phải ánh xạ từ embedding vector đó về cái không gian
0:08:22 - 0:08:25, Về ánh xạ về cái không gian của query của mình
0:08:25 - 0:08:34, X_i sẽ nhân với ma trận W_Q để ánh xạ về không gian của query của mình. X_i sẽ nhân với ma trận W_K để ánh xạ về không gian của key của mình
0:08:34 - 0:08:39, X_i nhân với W_V để ánh xạ về không gian của value của mình
0:08:39 - 0:08:45, Sang cái bước thứ 2, chúng ta sẽ tính attention score giữa query và key
0:08:45 - 0:08:52, Trong trường hợp này query và key của mình đã có cùng một số chiều và phải đưa về cùng một số chiều
0:08:52 - 0:09:03, Chúng ta chỉ việc thực hiện cái phép tích vô hướng giữa một query và một key thứ j bất kỳ.
0:09:03 - 0:09:09, Chúng ta sẽ trả về là relation, tức là sự liên hệ giữa query và key này.
0:09:09 - 0:09:13, Query thứ i và key thứ j này.
0:09:13 - 0:09:19, Sau đó chúng ta sẽ chuẩn hóa, bước thứ 3 là chúng ta chuẩn hóa giá trị này về không gian xác suất.
0:09:19 - 0:09:27, Thông qua hàm softmax và công thức của softmax, chúng ta sẽ có alpha_ij,
0:09:27 - 0:09:35, chính là attention distribution hay attention score mà chúng ta đã được chuẩn hóa.
0:09:35 - 0:09:40, Bước số 4 là chúng ta sẽ tính tổng trọng số của các value,
0:09:40 - 0:09:48, Tức là các trọng số alpha_ij này sẽ nhân với value tương ứng để chúng ta trả kết quả về output_i
0:09:48 - 0:09:52, Tức là output cho query thứ i
0:09:52 - 0:09:56, Output cho query thứ i của mình
0:10:00 - 0:10:03, Và khi này thì chúng ta sẽ có
0:10:03 - 0:10:07, Nếu chúng ta thực hiện trên vector, vector hóa
0:10:07 - 0:10:13, Tức là chúng ta sẽ gom các query lại với nhau.
0:10:13 - 0:10:18, Thì bước số 1, các từ X_i sẽ được gộp lại thành X.
0:10:18 - 0:10:21, Ở đây chúng ta sẽ thực hiện theo batch, thực hiện theo khối.
0:10:21 - 0:10:28, Trong cái slide trước, là chúng ta tính trên từng từ.
0:10:28 - 0:10:35, Trong slide này, chúng ta sẽ gom tất cả các từ trong câu input của mình
0:10:35 - 0:10:38, Vào thành một cái ma trận là ma trận X
0:10:38 - 0:10:41, Khi đó chúng ta tính toán trên cái ma trận X này
0:10:41 - 0:10:45, Nó sẽ tính toán nhanh hơn và có thể thực hiện được một cách song song
0:10:45 - 0:10:47, Nhờ sức mạnh tính toán của GPU
0:10:47 - 0:10:51, X_i này sẽ là một vector dạng cột như thế này của mình
0:10:51 - 0:10:55, Chúng ta sẽ gom tất cả X_i này lại với nhau
0:10:55 - 0:10:59, X_i sẽ gom tất cả các X_i lại với nhau
0:10:59 - 0:11:02, thì chúng ta sẽ có được 1 ma trận
0:11:02 - 0:11:07, toàn bộ X_i gom lại sẽ là ma trận X
0:11:10 - 0:11:14, toàn bộ các X_i sẽ là ma trận X
0:11:14 - 0:11:18, và khi đó chúng ta cũng có công thức tương tự như vậy
0:11:18 - 0:11:21, X nhân với ma trận W_Q
0:11:21 - 0:11:27, X nhân với ma trận W_Q, thì chúng ta sẽ có X_Q, X_Q tương ứng trong không gian query
0:11:27 - 0:11:32, X_K tương ứng là X khi nhân với ma trận W_K
0:11:32 - 0:11:36, thì chúng ta sẽ có trong không gian key
0:11:36 - 0:11:40, và X_V, tức là trong không gian value
0:11:40 - 0:11:51, Bước thứ 2, chúng ta sẽ tính attention score giữa query và key của mình.
0:11:51 - 0:11:57, Chúng ta sẽ có ma trận là X_Q nhân với X_K chuyển vị
0:11:57 - 0:12:04, Khi này chúng ta sẽ tính giữa các query và các key
0:12:04 - 0:12:09, Chúng ta sẽ tính trên 1 chuỗi tất cả các cặp query và key với nhau
0:12:09 - 0:12:20, Nhưng lưu ý là ở bước self-attention này query và key của mình sẽ tự thân
0:12:20 - 0:12:33, Chúng ta sẽ có các vector sau khi chúng ta đã chiếu về không gian query, key và value
0:12:33 - 0:12:41, Mỗi từ này sẽ đi so với các từ và thậm chí so với cả chính nó nữa để tính ra cái score
0:12:41 - 0:12:44, X_Q nhân với X_K chuyển vị
0:12:44 - 0:12:49, và triển khai ra thì X_Q chính là X nhân với lại ma trận W_Q
0:12:49 - 0:12:54, X_K chính là X nhân với lại ma trận W_K
0:12:54 - 0:12:55, Tất cả chuyển vị
0:12:55 - 0:13:00, Khi chúng ta triển khai chuyển vị vào bên trong dấu ngoặc này
0:13:00 - 0:13:04, thì nó sẽ là X chuyển vị đem ra và W_K chuyển vị đem lên trước
0:13:04 - 0:13:10, Như vậy thì công thức của ma trận biến đổi, xin lỗi, của ma trận attention score của mình
0:13:10 - 0:13:13, đó chính là (X nhân với W_Q) nhân với W_K chuyển vị nhân với X chuyển vị
0:13:14 - 0:13:17, Và sang bước thứ 3, chúng ta sẽ tính
0:13:17 - 0:13:20, Attention Distribution với hàm Softmax
0:13:21 - 0:13:23, thì chúng ta sẽ có ma trận A
0:13:25 - 0:13:27, Và sang bước số 4, chúng ta sẽ tổng hợp
0:13:27 - 0:13:29, là Output, là bằng cái công thức ở đây
0:13:31 - 0:13:36, Rồi, thì đây là cái công thức ở dạng vector hóa cho Self-Attention
0:13:37 - 0:13:39, Và khi chúng ta triển khai hết
0:13:40 - 0:13:52, Chúng ta sẽ có output là bằng softmax của (X nhân với W_Q) nhân với W_K chuyển vị nhân với X chuyển vị
0:13:52 - 0:13:57, Qua hàm softmax xong để tính ra được, đây là cái attention score
0:13:57 - 0:14:06, Chúng ta sẽ nhân với lại X_V để tổng hợp thông tin, đây sẽ là trọng số
0:14:06 - 0:14:11, và toàn bộ cái này sẽ là tổng hợp thông tin
0:14:11 - 0:14:21, tổng hợp toàn bộ những cái thông tin của giai đoạn self-attention, tức là giai đoạn của encoder