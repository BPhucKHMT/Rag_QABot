00:00:00 - 00:00:22, Chào các bạn, chúng ta sẽ cùng đến với chủ đề đầu tiên trong môn máy học nâng cao, đó chính là mô hình dựa trên Gradient.
00:00:22 - 00:00:26, Tại sao chúng ta cần phải tìm hiểu mô hình dựa trên Gradient?
00:00:26 - 00:00:35, Thì hiện nay các mô hình hiện đại và có rất nhiều những ứng dụng trong thực tế, ví dụ như các mô hình ngôn ngữ lớn,
00:00:35 - 00:00:45, các GBT, các mô hình sinh ảnh như là Diffusion Model, thì đều được huấn luyện dựa trên một phương pháp dựa trên Gradient,
00:00:45 - 00:00:52, tức là chúng ta tính đạo hàm. Và cái phương pháp này, nguyên lý của nó là gì?
00:00:52 - 00:01:03, Và tại sao nó có những ưu điểm để các mô hình hiện đại đều tập trung để sử dụng Gradient làm nền tảng để huấn luyện?
00:01:03 - 00:01:06, Chúng ta sẽ cùng tìm hiểu trong bài học ngày hôm nay.
00:01:07 - 00:01:14, Đầu tiên chúng ta sẽ cùng tìm hiểu về ý tưởng của mô hình dựa trên Gradient.
00:01:14 - 00:01:19, Ý tưởng của mô hình dựa trên Gradient thì chúng ta sẽ nhận dữ liệu đầu vào là x.
00:01:19 - 00:01:26, Thì x này có thể là bất cứ loại dữ liệu nào. Ví dụ như nó có thể là dữ liệu dạng vector,
00:01:26 - 00:01:31, nó cũng có thể là dữ liệu dạng Matron hoặc là tensor.
00:01:31 - 00:01:44, Với hai loại là vector hoặc là Matron hoặc là tensor, chúng ta có thể biểu diễn rất nhiều dữ liệu khác nhau.
00:01:44 - 00:01:50, Ví dụ như có thể biểu diễn dữ liệu văn bảng, dữ liệu dạng hình ảnh, video.
00:01:50 - 00:01:59, Và khi chúng ta đưa qua mô hình thì mô hình của mình sẽ được ký hiệu bằng một cái hàm, đó là hàm x.
00:01:59 - 00:02:05, Nhưng mà chúng ta lưu ý là đối với hàm fx ở đây nó sẽ có một cái tham số, đó là tham số theta.
00:02:05 - 00:02:15, Cũng tương tự như trong cái toán phổ thông của chúng ta, chúng ta có cái việc đó là ví dụ như chúng ta dải tìm m,
00:02:15 - 00:02:27, tìm cái tham số m sau cho cái phương trình bật 2.
00:02:27 - 00:02:46, Ví dụ như là x bình trừ m x cộng 3 m bình trừ 1 bằng 0 có 2 nghìn.
00:02:46 - 00:02:54, Ví dụ vậy, thì đây là cái toán mà hồi phổ thông chúng ta được làm qua.
00:02:54 - 00:03:02, Tham số ở đây nó chính là cái m của mình và cái dạng thức của cái hàm của mình đó là một cái hàm bật 2.
00:03:02 - 00:03:13, Ở đây chúng ta có hoàn toàn có thể ký hiệu đó là f của m x bằng 0, trong đó fmx là cái công thức ở cái vế bên tay trái như thế này.
00:03:13 - 00:03:16, Thế thì ở đây cũng như tương tự như vậy.
00:03:16 - 00:03:32, Chúng ta xác định được cái dạng thù của cái hàm f rồi, tuy nhiên trong đó chúng ta sẽ có nhiều cái tham số để quyết định xem là cái việc dự đoán mô hình chính xác đến mức độ nào.
00:03:32 - 00:03:40, Thì chúng ta sẽ phải tìm cái tham số sau cho cái hàm fx nó thỏa mãn cái việc đó là dự đoán chính xác.
00:03:40 - 00:03:51, Thì cái đầu ra của cái hàm fx này nó chính là y ngã và đây sẽ là cái giá trị mà chúng ta được dự đoán từ cái mô hình.
00:03:51 - 00:03:59, Và đương nhiên là việc dự đoán cái một cái giá trị nào đó thì chúng ta luôn mong muốn là nó sẽ sắp xỉ với lại cái giá trị thực tế.
00:03:59 - 00:04:10, Thì giá trị thực tế thì chúng ta sẽ ký hiệu là bằng y và để cho cái giá trị dự đoán mà sắp xỉ được với cái giá trị thực tế thì chúng ta sẽ phải có một cái hàm.
00:04:10 - 00:04:14, Hàm đó gọi là hàm lỗi và ký hiệu là chữ j.
00:04:14 - 00:04:22, Rồi, hàm lỗi này thì nó sẽ có cái biến số, lúc này nó không phải là x nữa mà biến số của mình lúc này nó sẽ là theta.
00:04:22 - 00:04:28, Biến số của nó sẽ là theta và x,y của mình nó sẽ đóng vai trò giống như là tham số.
00:04:28 - 00:04:39, Nó sẽ khác so với lại hồi xưa, khi mà chúng ta đặt cái tên biến mà là x,y thì nó sẽ ngầm hiểu đó là biến số.
00:04:39 - 00:04:46, Còn trong trường hợp này thì cái hàm lỗi của mình x,y sẽ là tham số và nó chính là cái dữ liệu huấn luyện.
00:04:46 - 00:04:49, Đó chính là cái dữ liệu huấn luyện.
00:04:49 - 00:05:01, Rồi, và mình sẽ phải tìm cái biến số theta làm sao cho cái việc dự đoán này là chính xác nhất.
00:05:01 - 00:05:10, Và cái việc tìm cái giá trị theta cho cái này chính xác nhất thì nó sẽ tương đương với việc chúng ta đi tìm một cái hàm min.
00:05:10 - 00:05:15, Tìm theta sao cho hàm lỗi là đạt giá trị nhỏ nhất.
00:05:15 - 00:05:21, Thế thì ba cái công việc chúng ta cần phải làm khi xây dựng một cái mô hình dựa trên Gradient.
00:05:21 - 00:05:28, Một, đó là chúng ta sẽ xác định xem cái hàm fx của mình nó sẽ có cái giảng thức như thế nào.
00:05:28 - 00:05:38, Thứ hai, đó là chúng ta sẽ thiết kế cái hàm lỗi là g theta x sao cho cái việc mà dự đoán càng chính xác thì cái lỗi của mình thấp.
00:05:38 - 00:05:43, Nhưng đó chưa phải là một cái tiêu chí để thiết kế một cái hàm lỗi.
00:05:43 - 00:05:50, Càng chính xác nè thì càng nhỏ.
00:05:50 - 00:05:56, Nhưng cái đó chưa phải là một cái tiêu chí duy nhất mà chúng ta sẽ còn rất nhiều những cái tiêu chí khác.
00:05:56 - 00:06:06, Mình có thể kể một vài cái tiêu chí ví dụ như là nó có thể hoạt động tốt khi chúng ta làm việc hoạt động tốt.
00:06:07 - 00:06:16, Với dữ liệu mà mất cân bằng.
00:06:22 - 00:06:31, Tức là cái y này của mình nó sẽ có nhiều class ví dụ vậy và có những class thì xuất hiện rất nhiều nhưng có những cái class rất ít.
00:06:31 - 00:06:45, Thì cái hàm lỗi này nó phải làm sao để cho hướng cái mô hình đến cái việc là kể cả những mẫu dữ liệu mà ít thì vẫn có thể được cho cái vai trò ngang bằng với lại những cái mẫu như nhiều.
00:06:45 - 00:06:54, Rồi ngoài ra thì chúng ta sẽ có những cái tiêu chí nữa ví dụ như hàm lỗi như thế nào để cho cái việc huấn luyện nhanh, hội tụ, huấn luyện nhanh.
00:06:54 - 00:07:00, Tức là nó sẽ mau chóng để mà tìm ra được cái tham số tốt nhất.
00:07:00 - 00:07:08, Rồi cái việc thiết kế hàm mô hình của vậy nó cũng sẽ dựa trên cái tính chất của y, cái giá trị thực tế vx để mà chúng ta sẽ thiết kế.
00:07:08 - 00:07:14, Ví dụ như nếu y mà phụ thuộc một cách tuyến tính vx thì chúng ta sẽ có các cái hàm tuyến tính.
00:07:14 - 00:07:21, Nhưng nếu mà y phụ thuộc một cách phi tuyến với là x thì chúng ta sẽ có các cái hàm là phi tuyến tính.
00:07:22 - 00:07:34, Rồi sau này là tùy thuộc vx của mình, đó là dữ liệu dạng vector, dạng ma trận hay là dữ liệu như thế nào đó mà chúng ta cũng sẽ có những cái kiểu thiết kế khác nhau.
00:07:34 - 00:07:45, Và cái công việc cuối cùng khi chúng ta làm với một cái mô hình mà dựa trên variant, đó chính là chúng ta sẽ tìm một cái tham số tối ưu của hàm mô hình.
00:07:45 - 00:07:56, Tức là chúng ta sẽ đi tìm cái theta sao, sao cho cái lỗi ở đây là nhỏ nhất, thì lỗi mà nhỏ nhất là cái dự đoán càng chính xác.
00:07:58 - 00:08:04, Thì so với lại các cái mô hình khác, chúng ta sẽ có rất nhiều những cái mô hình mà không dựa trên variant.
00:08:04 - 00:08:12, Ví dụ như mô hình là k-nearest neighbor, thì thực sự mô hình này nó không phải là hữu nguyện mà nó sẽ dựa trên các cái mẫu gần nhất.
00:08:12 - 00:08:22, Để nó giống như là cái cơ chế là voting, tức là chúng ta sẽ lấy ra những cái mẫu gần nhất, rồi dựa trên nhãn của những cái mẫu gần nhất, chúng ta sẽ đưa ra cái phán đoán.
00:08:22 - 00:08:32, Cái mô hình theo đó là 9bs là dựa trên sát suất, đây là một cái hàm để mà ước lượng cái tham số một cách tường minh.
00:08:33 - 00:08:44, Rồi decision tree, ví dụ như thuật toán ID3, rồi C4.5 thì ở đây là sẽ dựa trên luật để chia các cái nhãnh quyết định.
00:08:44 - 00:08:53, Random forest sẽ kết hợp nhiều mô hình cây thành phần, tức là nhiều cái mô hình cây ở trên để tạo thành một cái random forest.
00:08:53 - 00:09:04, Các cái thuật toán mà họ không giám sát, ví dụ như là Kmin, DBScan, Quang Vân là lặp lại cái việc cập nhật cái tâm cụm, nhưng mà nó là lặp lại không có dựa trên Gradient.
00:09:04 - 00:09:13, Thế thì chúng ta sẽ so sánh những cái khía cạnh giữa cái mô hình dựa trên Gradient và mô hình không có dựa trên Gradient.
00:09:13 - 00:09:24, Thì đối với cái khía cạnh là về cái cơ chế tối ưu, thì mô hình dựa trên Gradient sẽ dựa trên cái thuật toán là Gradient descent.
00:09:24 - 00:09:41, Và đây sẽ là một trong những cái thuật toán chính mà chúng ta sẽ cùng tìm hiểu sắp tới, cũng như là những cái biến thể của Gradient descent, ví dụ như là Stochastic Gradient descent Adam, hoặc là IMS Rob.
00:09:42 - 00:09:56, Còn đối với cái thuật toán mà không dựa trên Gradient thì nó sẽ dùng công thức tường minh, hoặc là những chiến thuật như là Reddy, Chiến thuật Time-Lam, Heuristic, ví dụ như thuật toán IBS Decision Key và Ca-Nearest Neighbor.
00:09:56 - 00:10:03, Và cái khả năng diễn giải, tức là cái khả năng giải thích, cái khả năng giải thích của mô hình đó.
00:10:04 - 00:10:11, Thì ở thuật toán, cái mô hình dựa trên Gradient thì thường là ở dạng Black Box, thì đây có thể nói là một cái điểm yếu.
00:10:11 - 00:10:25, Đây có thể nói là một điểm yếu của các cái mô hình dựa trên Gradient là vì cái tính giải thích của nó sẽ là không cao, không cao bằng so với các cái mô hình dễ giải thích kết quả, ví dụ như là Decision Tree, Night Bias.
00:10:25 - 00:10:45, Và đối với các cái tác vụ, nó sẽ hiệu quả trên các tác vụ phức tạp nào, thì mô hình Gradient Descent rất phù hợp cho cái kết quả rất tốt khi mà chúng ta làm trên những lịch vực như là Thị giác Máy Tính, Sửa Lý Ngụng Nguyện Thường Nhiên, khi mà cái dữ liệu của mình nó là ở dạng không cấu trúc, không có cấu trúc.
00:10:45 - 00:10:55, Còn thuộc toán các mô hình mà không dựa trên Gradient thì nó sẽ tốt trên những cái loại dữ liệu là dữ liệu bảng và dữ liệu nhỏ.
00:10:55 - 00:11:07, Về chi phí huấn luyện thì cái chi phí huấn luyện của mô hình dựa trên Gradient thì thường là cao và cần phải có GPU hoặc TPU với các cái mô hình lớn.
00:11:07 - 00:11:12, Còn mô hình không dựa trên Gradient thì thường là ít tốn kém hơn.
00:11:12 - 00:11:24, Vì vậy thì chúng ta thấy là với cái bảng này, chúng ta thấy là điểm yếu của Gradient, mô hình dựa trên Gradient có vẻ như là nhiều hơn, ví dụ như là cần tài nguyên tính toán nhiều hơn.
00:11:24 - 00:11:30, Rồi và nó là ít có khả năng diễn dải hơn.
00:11:30 - 00:11:38, Tuy nhiên cái điểm mạnh của nó là chính là nằm ở việc là nó có thể làm được trên rất nhiều những cái loại dữ liệu không có cấu trúc.
00:11:38 - 00:11:45, Mà chúng ta biết rằng là hiện nay cái dữ liệu của mình đại đa số sẽ là ở dạng không có cấu trúc.
00:11:45 - 00:11:59, Do đó thì chính cái điểm mạnh này mà nó khiến cho cái mô hình Gradient gần đây được chú ý là vì nó khai thác được một cái lượng dữ liệu khổng lồ trên mạng internet.
00:12:02 - 00:12:09, Còn cái mô hình dựa trên Gradient muốn mà hoạt động được thì chúng ta sẽ phải cấu trúc nó.
00:12:10 - 00:12:18, Mà cái chi phí để mà cấu trúc khóa cái dữ liệu của mình thì rất là tốn kém và cái khối lượng dữ liệu của mình nó sẽ không nhiều.
00:12:18 - 00:12:24, Vì đó chính là lý do tại sao các cái mô hình mà dựa trên Gradient gần đây được quan tâm.
00:12:24 - 00:12:30, Và đây sẽ là một vài cái hình ảnh để minh họa.
00:12:30 - 00:12:39, Nếu như trước đây từ năm 2000 vào 12 cái khối lượng dữ liệu huấn luyện của các cái mô hình học sâu thì thường là giữa tầm vài triệu mẫu.
00:12:39 - 00:12:44, Nhưng mà trong hai năm, ba năm gần đây thì là đã lên đến hàng trăm triệu mẫu.
00:12:44 - 00:12:52, Và gần đây nhất thì là có tập dữ liệu Lion 5B thì nó đã lên đến năm tỷ ảnh. Đó là một cái khối lượng thật kì lớn.
00:12:52 - 00:12:59, Và với cái khối lượng mà dữ liệu càng nhiều như thế này thì để mà có thể huấn luyện được hiệu quả thì nó cũng cần phải có những cái tài nguy mạnh.
00:12:59 - 00:13:09, Thì cũng may mắn đó là các cái phần cứng, cụ thể là các cái GPU hiện nay nó đã ngày càng trở nên rẻ hơn và sức mạnh của nó ngày càng cao hơn.
00:13:09 - 00:13:16, Đồng thời là về mặt khoa học thì các cái mô hình học sâu hiện nay là nó đã ngày càng hoàn thiện hơn.
00:13:16 - 00:13:23, Khiến cho chúng ta có thể học tốt hơn và nhanh hơn, khai thác được những cái kho dữ liệu lớn và sức mạnh tính quan lớn.
00:13:29 - 00:13:37, Hãy subscribe cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video hấp dẫn.
