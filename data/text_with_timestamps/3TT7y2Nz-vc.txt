00:00:00 - 00:00:19, Chúng ta sẽ cùng đến với một cái cải tiến cũng theo hướng đó là ATAC-T Learning Rate, đó là ADAM.
00:00:19 - 00:00:27, ADAM đã cải tiến so với lại phiên bản đó là Root Mean Square Propagation và cũng dựa trên Momentum.
00:00:27 - 00:00:36, Trong Root Mean Square Propagation, chúng ta thấy là có thành phần alpha chia cho căng của epsilon cộng cho r.
00:00:36 - 00:00:40, Đây là thành phần chuẩn hóa của Learning Rate.
00:00:41 - 00:00:53, Nhưng sau đó chúng ta lại đi nhân với lại g là thành phần radian và g này thì lại bằng đạo hàm của g theo theta,
00:00:53 - 00:01:10, tức là một cái thành phần radian nguyên bản, nó chưa có áp dụng momentum vào g, chưa có momentum cho radian.
00:01:10 - 00:01:14, Thì chúng ta sẽ có một cái cải tiến cho ở chỗ này.
00:01:14 - 00:01:21, Cái thứ hai, nó sẽ có một cái hiện tượng đó là tại những cái giai đoạn đầu tiên.
00:01:22 - 00:01:31, Thì chúng ta thấy là cái thành phần momentum của mình là ví dụ như ở đây là r là bằng 0, đúng không?
00:01:31 - 00:01:32, Là bằng 0.
00:01:32 - 00:01:44, Rồi sau đó sang cái vòng lập tiếp theo thì r sẽ là bằng beta nhân với r cộng cho 1 trừ beta nhân với lại g.g.
00:01:45 - 00:01:54, Thì dẫn đến đó là ở đây beta là bằng 90% của r mà r của mình nó đang bằng 0.
00:01:54 - 00:02:01, Thì dẫn đến cái thằng này là bằng 0 và thằng này thì chỉ có là 10% của cái đạo hàm hiện tại thôi.
00:02:01 - 00:02:13, Tức là ở trong 4 cái vòng lập đầu tiên, 3 cho đến 4 vòng lập, thì cái r này rất nhỏ.
00:02:14 - 00:02:18, R này rất nhỏ, thì gây ra cái hiện tượng gì?
00:02:18 - 00:02:21, Là nó sẽ không phản ánh đúng cái đạo hàm của mình.
00:02:21 - 00:02:24, Nó không phản ánh đúng cái đạo hàm, tức là cái đội giốc của mình.
00:02:24 - 00:02:29, Dẫn đến là cái quá trình cập nhật rất là chậm.
00:02:30 - 00:02:33, Vì nó bé mà nên nó sẽ là chậm.
00:02:33 - 00:02:42, Do đó thì chúng ta sẽ tìm cách chuẩn hóa sau cho với 3-4 vòng lập đầu tiên thì nó sẽ boost cái momentum của mình lên.
00:02:42 - 00:02:54, Chúng ta sẽ có một cái biến thể là cải tiến ở chỗ là những vòng lập đầu tiên thì nó sẽ boost momentum lên để cho nó cập nhật nhanh hơn, không bị chậm ở những bước đầu.
00:02:56 - 00:03:01, Chi tiết của thuật toán ADAM Adaptive Moment Optimization là nằm ở đây.
00:03:01 - 00:03:05, Chúng ta đầu tiên cũng sẽ khởi tạo là alpha là bằng 0.1.
00:03:05 - 00:03:11, Đối với cái Decay Rate thì bình thường trong cái Root Mean Sphere Propagation chúng ta chỉ có duy nhất một cái beta.
00:03:11 - 00:03:15, Ở đây chúng ta sẽ có hai cái beta là beta 1 và beta 2.
00:03:15 - 00:03:30, Trong đó beta 1 là cái hệ số momentum là cái Decay Rate cho cái momentum của radian cho cái việc cập nhật cái momentum của radian.
00:03:30 - 00:03:38, Còn cái beta 2 sẽ là cho cái việc cập nhật cái hệ số chuẩn hóa, hệ thành phần chuẩn hóa.
00:03:44 - 00:03:46, Chuẩn hóa cái Learning Rate.
00:03:49 - 00:03:58, Rồi, và chúng ta sẽ, ngoài R thì chúng ta sẽ có thêm S. S chính là cái thành phần momentum cho radian.
00:03:58 - 00:04:00, Thành phần momentum cho radian.
00:04:00 - 00:04:14, Thì đây chính là cái momentum cho cái beta radian của mình và công thức của mình là bình thường là trong cái phần Root Mean Sphere Roar thì S của mình nó chính là chỉ bằng G thôi.
00:04:14 - 00:04:24, Còn bây giờ S của mình nó sẽ là bằng cái thành phần quá khứ, nhân với lại, cộng với lại cái thành phần radian hiện tại.
00:04:24 - 00:04:26, Đây là hiện tại.
00:04:26 - 00:04:28, Còn đây là cái thành phần quá khứ.
00:04:32 - 00:04:37, Rồi, và nó sẽ là bằng 90% của quá khứ cộng cho 10% của hiện tại.
00:04:37 - 00:04:45, Và như hồi nãy chúng ta đã lập luận thì cái việc mà lấy quá nhiều cho cái quá khứ nó sẽ khiến cho những cái bước cập nhật đầu tiên rất là chậm.
00:04:45 - 00:04:48, Thì chúng ta sẽ có cái thành phần chuẩn hóa ở phía sau.
00:04:48 - 00:04:50, Chúng ta sẽ giải thích sau.
00:04:50 - 00:04:54, S mũ chính là cái thành phần chuẩn hóa cho cái S ở phía trên.
00:04:54 - 00:05:03, Thế thì tại sao cái việc chuẩn hóa với cái công thức này thì những cái vòng lập đầu tiên của mình nó sẽ có cái giá trị không quá bé.
00:05:03 - 00:05:09, Thì bây giờ chúng ta giả sử S ban đầu của mình là một cái con số rất là bé.
00:05:09 - 00:05:13, Nhưng khi chúng ta giả sử S ban đầu của mình là một cái con số rất là bé.
00:05:13 - 00:05:19, Như đã giải thích S sẽ là bằng quá khứ là bằng 90% của cái S ban đầu là bằng 0.
00:05:19 - 00:05:21, Tức là cái thành phần này là bằng 0.
00:05:21 - 00:05:25, Tức là ở những cái vòng lập đầu tiên là ban đầu.
00:05:25 - 00:05:33, Thì S sẽ là bằng quá khứ là bằng 0 cộng cho 10% của G.
00:05:33 - 00:05:35, Thì cái thành phần này rất là bé.
00:05:36 - 00:05:43, Nhưng khi chúng ta chia cho căn của 1 trừ beta mũ T với T là số thứ tự.
00:05:43 - 00:05:45, T là cái bước lập của mình.
00:05:45 - 00:05:47, Thì ở cái vòng lập đầu tiên tức là T bằng 1.
00:05:47 - 00:05:49, Vòng lập đầu tiên T bằng 1.
00:05:49 - 00:05:57, Thì khi đó S sẽ là bằng 1 trừ cho beta 1 của mình là 0.9 mũ 1.
00:05:57 - 00:06:01, 0.9 mũ 1 tức là là 0.9.
00:06:01 - 00:06:05, Thì 1 chỉ 0.9 tức là 0.1.
00:06:05 - 00:06:10, Thì S mà chia cho 0.1 tương đương với S chúng ta sẽ nhân lên 10 lần.
00:06:10 - 00:06:12, Thì ban đầu S của mình rất là thấp.
00:06:12 - 00:06:15, Nó chỉ bằng khoảng 0.9 cái đạo hàm góc của mình thôi.
00:06:15 - 00:06:18, Nhưng mà chúng ta chia cho 0.1 tức là nhân 10 lên.
00:06:18 - 00:06:22, Thì có phải là S tức này của mình nó tương đương với cái đạo hàm tại cái thời điểm đó không?
00:06:22 - 00:06:25, Tại cái thời điểm ban đầu.
00:06:25 - 00:06:29, Thì nó đã được boost lên 10 lần.
00:06:29 - 00:06:34, Thì cái công thức này sẽ giúp chúng ta boost tại những cái thời điểm đầu tiên.
00:06:34 - 00:06:39, Thế thì khi T mà càng lớn, đương nhiên không thể nào mà T tiến đến vô cùng được.
00:06:39 - 00:06:42, T chỉ là khoảng 10, 20 ví dụ vậy.
00:06:42 - 00:06:45, Cỡ 10 cho đến 20 đi.
00:06:45 - 00:06:49, Thì khi đó là beta 1 mũ T.
00:06:49 - 00:06:54, Không phải beta 1 mũ T vì beta 1 là 1 con số bé hơn 1 lớn hơn 0.
00:06:54 - 00:06:57, Nên nó sẽ tiến đến, nó sẽ tiến về 0.
00:06:57 - 00:07:03, Do đó 1 trừ beta 1 mũ T, nó sẽ tiến về 1.
00:07:03 - 00:07:05, 1 trừ 0 tức là 1.
00:07:05 - 00:07:10, Tức là khi đó S mũ của chúng ta, nó sẽ sắp xỉ bằng S chia cho 1.
00:07:10 - 00:07:13, Tức là nó sẽ bằng cái nguyên bảng của cái momentum ban đầu của mình.
00:07:13 - 00:07:17, Thì khi T mà càng lớn thì gần như nó không cần boost lên nữa.
00:07:17 - 00:07:21, Còn khi T của mình nhỏ khoảng 1, 2 thì nó sẽ boost lên rất là nhiều lần.
00:07:21 - 00:07:25, Thì đó là ý nghĩa của công thức chuộng hóa này.
00:07:25 - 00:07:34, Tương tự như vậy, cho cái thành phần để cập nhật chuẩn hóa của learning rate,
00:07:34 - 00:07:38, chúng ta cũng sẽ dùng cái công thức này để giúp cho cái việc mà
00:07:38 - 00:07:41, tại những thời điểm đầu tiên nó không quá bé.
00:07:41 - 00:07:45, Nó sẽ sắp xỉ bằng với lại cái đạo hàm của mình luôn.
00:07:45 - 00:07:50, Và ý nghĩa của công thức này như tương tự như trong cái
00:07:50 - 00:07:54, Root Mean Square Propagation, mục tiêu của nó là để tách ra
00:07:54 - 00:07:59, T vì là một cái vector nên nó sẽ tách ra thành những cái learning rate riêng
00:07:59 - 00:08:02, khi chúng ta cập nhật vô cái thành phần đạo hàm.
00:08:02 - 00:08:07, Và nó làm theo cái nguyên tắc đó là thành phần đạo hàm nào ở bên đây,
00:08:07 - 00:08:13, của g mà càng nhỏ thì cái learning rate sẽ càng lớn.
00:08:13 - 00:08:18, Thành phần nào của cái g này mà lớn thì cái learning rate của nó sẽ nhỏ.
00:08:18 - 00:08:23, Như vậy là Radian, Adam nó đã có những cái cải tiến chính.
00:08:23 - 00:08:27, Đó là nó có thêm cái momentum cho cái vector Radian.
00:08:27 - 00:08:31, Nó có thêm cái thành phần chuẩn hóa để những cái vòng lập đầu tiên
00:08:31 - 00:08:35, nó sẽ không quá nhỏ, cái thành phần đạo hàm của mình
00:08:35 - 00:08:39, nó sẽ không quá bé hoặc là cái phần chuẩn hóa đạo hàm cũng không quá bé.
00:08:39 - 00:08:44, Nó bị sai lệch so với lại cái đạo hàm tại cái thời điểm đó.
00:08:45 - 00:08:53, Và trong cái sơ đồ này thì chúng ta sẽ có cái trực quan hóa để cho thấy cái tốc độ hội tụ của tương kỳ thúc toán.
00:08:53 - 00:08:57, Thì ở đây Ada, Denta, đó chính là cái Adam của mình.
00:08:57 - 00:09:01, Đó là cái đường màu vàng. Đó là cái đường màu vàng này.
00:09:01 - 00:09:09, Rồi, và chúng ta thấy là cái đường màu vàng thì nó sẽ rớt xuống rất là nhanh, nó sẽ hội tụ rất là nhanh.
00:09:10 - 00:09:17, Khi đến cái khu vực mà gọi là Set-A-Point và đồng thời là có cái valet, là cái thông lũn
00:09:17 - 00:09:24, chúng ta thấy là có hai cái thành, giảm đường giốc, đi ngang, xong rồi lại đi lên, đó gọi là valet
00:09:24 - 00:09:29, thì cái Adam của chúng ta rớt xuống nhanh nhất, nó rớt xuống rất là nhanh.
00:09:29 - 00:09:34, Còn cái thúc toán mà Root Mean Square Propagation là cái đường màu đen
00:09:34 - 00:09:38, thì chúng ta thấy là nó sẽ rớt chậm hơn.
00:09:39 - 00:09:45, Còn Stochastic Gradient Descent thì đối với Stochastic Gradient Descent là cái chấm màu đỏ nè
00:09:45 - 00:09:51, là chúng ta thấy nó bị giao động qua lại và nó đứng yên luôn, nó không thoát ra được cái chỗ này luôn.
00:09:51 - 00:09:55, Momentum thì khá hơn một chút xíu là cái đường, cái điểm màu xa lá.
00:09:55 - 00:10:00, Chúng ta thấy là khi Momentum nó rớt xuống nó cũng sẽ chao đảo qua lại.
00:10:00 - 00:10:05, Nhưng mà vì có một số kiểu tối nhiễu nên nó sẽ dần dần dần dần nó thoát ra được
00:10:05 - 00:10:07, và nó đến được cái rảnh này nó di chuyển.
00:10:07 - 00:10:14, Trong khi các cái phương pháp cải tiến khác thì nó cũng bị cái hiện tượng là giao động qua lại rất là nhiều.
00:10:14 - 00:10:17, Nó bị hiện tượng giao động qua lại, bật và bật lại.
00:10:17 - 00:10:21, Còn Adam là cái đường màu vàng thì nó sẽ rớt thẳng xuống luôn.
00:10:21 - 00:10:26, Nó sẽ đi theo cái đường cập nhật hoàn hảo, cái đường cập nhật mà tôi yêu ở đây.
00:10:27 - 00:10:33, Bên phải thì đó là cái sơ đồ về giá trị của hàm loss khi chúng ta sử dụng các thực toán khác nhau.
00:10:33 - 00:10:40, Thì Adam là cái đường màu tím, nó cho cái giá trị hàm loss hồi tụng nhanh hơn và nó thấp nhất.
00:10:40 - 00:10:44, Loss càng thấp càng tốt, thì training loss của mình càng thấp càng tốt.
00:10:44 - 00:10:50, Thì chúng ta thấy là nó hồi tụng nhanh hơn nhiều so với lại các thực toán như là Root Mean Square,
00:10:50 - 00:10:57, Ada Delta, Ada Rack v.v.
00:10:57 - 00:11:04, Thì kết luận đó là một số cái phương pháp tối ưu, môi nọc sâu bằng cách.
00:11:04 - 00:11:11, Trong cái phần này thì chúng ta đã được thảo luận qua những cái cách để mà tùy chỉnh learning rate cho từng cái tham số.
00:11:11 - 00:11:15, Và thực toán, câu hỏi là thực toán nào sẽ được chọn khi muốn luyện?
00:11:15 - 00:11:20, Thì câu trả lời đó là không chắc chắn. Nó sẽ tùy thuộc vào cái dữ liệu của các bạn như thế nào.
00:11:20 - 00:11:23, Nó phụ thuộc vào cái mô hình của mình nó có phức tạp hay không?
00:11:23 - 00:11:32, Ví dụ, đối với những cái mô hình phức tạp mà nhiều tham số, thì khi đó chúng ta sẽ phải dùng các cái thực toán
00:11:32 - 00:11:38, ví dụ như là Root Mean Square, Rock, Propagation, hoặc là Adam.
00:11:38 - 00:11:44, Nhưng đối với những cái mô hình mà ít tham số, thì khi đó Adam và Root Mean Square là không cần thiết.
00:11:44 - 00:11:48, Mà chúng ta chỉ cần Stochastic Radiant Descent là đủ.
00:11:48 - 00:11:54, Rồi nếu mà dữ liệu của mình không quá phức tạp, thì chúng ta có thể dùng Stochastic Radiant Descent.
00:11:54 - 00:11:58, Nhưng nếu mà phức tạp thì chúng ta sẽ dùng hai cái thực toán bên đây.
00:11:58 - 00:12:03, Thì đa số các cái thực toán đều có cái sự phổ biến nhất định của mình.
00:12:03 - 00:12:07, Và được lựa chọn tùy theo cái sự quen thuộc của người dùng.
00:12:08 - 00:12:14, Như vậy thì đến đây chúng ta đã tìm hiểu qua hai cái biến thể rất là nổi tiếng của Adaptive Learning Rate,
00:12:14 - 00:12:17, đó là Root Mean Square, Propagation và Adam.
00:12:17 - 00:12:24, Thì cái Root Mean Square, Propagation, nó là một cái tiền đề để cho Adam có thể cải tiến.
00:12:24 - 00:12:30, Và Adam nó có một cái cải tiến khá là quan trọng, đó là chuẩn hóa để giúp cho những cái bước cập nhật đầu tiên của mình
00:12:30 - 00:12:32, nó không quá chậm.
00:12:32 - 00:12:37, Và đây chính là những cái thực phán Optimizer được sử dụng trong rất nhiều những cái mô hình học sâu,
00:12:37 - 00:12:40, những cái mô hình mà dựa trên Radiant về sau.
00:12:40 - 00:12:43, Và nó sẽ là tiền đề cho chúng ta đi tiếp.
