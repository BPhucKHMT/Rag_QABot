00:00:00 - 00:00:20, Bây giờ chúng ta sẽ đến với một cái ví dụ cụ thể để chúng ta có thể dễ dàng hình dung hơn.
00:00:20 - 00:00:25, Thì ở đây chúng ta set một cái mạng Neural Network gồm cụ thể đó là 10 lớp biến đổi.
00:00:25 - 00:00:31, Và các cái hàm kích hoạt của mình ở đây chúng ta sẽ sử dụng cùng một hàm kích hoạt đó là hàm sigmoid.
00:00:31 - 00:00:42, Thì công thức của cái hàm sigmoid đó là bằng một phần một cộng e mũ trừ x.
00:00:42 - 00:00:45, Thì đây là cái công thức của sigmoid.
00:00:46 - 00:00:55, Và khi chúng ta thuấn luyện cái mạng Neural Network này thì chúng ta sẽ đi tính cái hàm loss. Đây là hàm lỗi.
00:00:55 - 00:01:00, Và chúng ta sử dụng cái binary cross entropy tại vì ở đây chúng ta dùng hàm kích hoạt là sigmoid.
00:01:00 - 00:01:05, Nên cái giá trị đầu ra của mình là con số từ 0 cho đến 1.
00:01:05 - 00:01:08, Và chúng ta sẽ đi so với cái giá trị e này.
00:01:08 - 00:01:21, Sau đó thì khi chúng ta tính đạo hàm, ở đây là chúng ta sẽ tính đạo hàm theo cái biến theta.
00:01:21 - 00:01:24, Thế thì cái công thức này chúng ta sẽ triển khai như thế nào?
00:01:24 - 00:01:37, Đối với cái mạng Neural Network mà có 10 lớp, thì ở cái lớp đầu tiên chúng ta sẽ có bộ hệ số đó là theta 1.
00:01:37 - 00:01:40, Theta 1.
00:01:40 - 00:01:45, Sau đó chúng ta sẽ nhân với cái x là cái dữ liệu đầu vào.
00:01:45 - 00:01:51, Và ngay sau đó chúng ta sẽ thực hiện cái hàm kích hoạt để tạo ra cái giá trị đầu ra.
00:01:51 - 00:01:57, Thì sigmoid của theta 1 x, thì đây chính là cái lớp biến đổi đầu tiên là layer số 1.
00:02:00 - 00:02:05, Sau đó chúng ta lại tiếp tục lan truyền đến cái layer thứ 2.
00:02:08 - 00:02:12, Và cũng tương tự như vậy chúng ta sẽ có cái tham số là theta 2.
00:02:12 - 00:02:20, Thì theta 2 sẽ nhân với cái đầu ra của cái lớp trước đó chính là, ở đây theta 2 sẽ nhân với lại cái đầu ra trước đó.
00:02:20 - 00:02:24, Và sau đó chúng ta qua cái hàm sigmoid.
00:02:24 - 00:02:27, Qua cái hàm sigmoid.
00:02:27 - 00:02:31, Thì đây chính là cái kết quả của layer số 2.
00:02:31 - 00:02:39, Cứ như vậy, cho đến cái sigmoid của theta 10, tức là chúng ta đang tiến đến theta 10.
00:02:39 - 00:02:46, Thì theta 1 tương ứng layer 1, theta 2 tương ứng layer số 2, và theta 10 tương ứng layer số 10.
00:02:46 - 00:02:53, Thì đây chính là cái công thức của cái hàm mà chúng ta cần phải tối ưu hóa hàm j, hàm lỗi.
00:02:54 - 00:03:01, Thì nhìn trong cái công thức này, chúng ta sẽ đi tính đạo hàm của j theo từng thành phần của tham số theta.
00:03:01 - 00:03:05, Lưu ý là theta là một tập hợp các cái tham số.
00:03:05 - 00:03:07, Là một tập hợp các cái tham số.
00:03:07 - 00:03:09, Nó sẽ bao gồm là theta 1.
00:03:09 - 00:03:12, Trong theta 1 nó lại gồm có nhiều cái thành phần con nữa.
00:03:12 - 00:03:17, Đó là cái trọng số tương ứng với cái cạnh nối của lớp trước lưu ý lớp sau.
00:03:17 - 00:03:21, Thì đây là tập hợp các cái tham số.
00:03:21 - 00:03:22, Rồi.
00:03:22 - 00:03:30, Và chúng ta sẽ đi tính đạo hàm từng phần là đạo hàm của delta j chia cho delta theta i.
00:03:30 - 00:03:37, Thì nó sẽ là bằng công thức của đạo hàm của bce theo cái hàm sigmoid.
00:03:37 - 00:03:40, Đạo hàm của sigmoid theo cái hàm p10.
00:03:40 - 00:03:46, Thì cái p10 này nó chính là cái thao tác nhân của theta 10 với lại cái thành phần còn lại.
00:03:46 - 00:03:48, Đây chính là cái phép nhân của theta 10.
00:03:48 - 00:03:50, Với lại cái thành phần còn lại.
00:03:50 - 00:03:51, Đây chính là cái phép nhân.
00:03:51 - 00:03:56, Hàm p là product, viết tát của chữ product.
00:03:56 - 00:04:02, Tức là phép nhân của theta 10, cái tham số 10 với lại cái kết quả đầu ra trước đó.
00:04:02 - 00:04:07, Thì p10 và p1 chính là cái phép này.
00:04:07 - 00:04:09, Đây chính là p1.
00:04:09 - 00:04:17, Thế thì cái phép theta 1 nhân vx thì đây chính là cái hàm p1.
00:04:17 - 00:04:19, Rồi.
00:04:19 - 00:04:25, Thế thì chúng ta triển khai cái chain rule ra thì chúng ta sẽ có cái công thức như trên.
00:04:25 - 00:04:32, Và chúng ta để ý đó là cái hàm đạo hàm của sigmoid được thực hiện đi thực hiện lại 10 lần.
00:04:32 - 00:04:38, Chúng ta có bao nhiêu cái lớp biến đổi thì sẽ có bấy nhiêu hàm sigmoid.
00:04:38 - 00:04:41, Thì ở đây chúng ta có tất cả là 10 lớp biến đổi.
00:04:41 - 00:04:45, Do đó nó sẽ có 10 lần cái đạo hàm của sigmoid.
00:04:47 - 00:04:50, P1 cho đến p10.
00:04:50 - 00:04:54, Và mặt khác thì chúng ta lại có cái công thức của đạo hàm.
00:04:54 - 00:04:59, Ở đây chúng ta có hàm sigmoid là cái công thức như trên.
00:04:59 - 00:05:08, Thì hoặc là chúng ta có thể tính nháp bằng tay để chứng minh được đạo hàm của sigmoid là bằng sigmoid nhân vật từ sigmoid.
00:05:08 - 00:05:16, Hoặc là chúng ta có thể tra cứu các tài liệu học thuật online để có được sát nhận công thức này.
00:05:16 - 00:05:19, Và cơ bản thì công thức này cũng rất dễ chứng minh.
00:05:19 - 00:05:25, Dí do đó là vì trong cái đạo hàm này, trong cái công thức của hàm sigmoid này nó có emux.
00:05:25 - 00:05:28, Mà emux là một cái hàm rất dễ tính đạo hàm.
00:05:28 - 00:05:35, Rồi thì bây giờ chúng ta xem như chúng ta đã biết cái đạo hàm của sigmoid bằng sigmoid nhân vật từ sigmoid.
00:05:35 - 00:05:37, Thì chúng ta áp dụng cái bức đẳng thức cosy.
00:05:37 - 00:05:39, Ở đây là dấu bé hơn hoặc bằng.
00:05:39 - 00:05:41, Chúng ta áp dụng cái bức đẳng thức cosy.
00:05:41 - 00:05:50, Đó là đạo hàm của sigmoid x nhân cho 1 trừ cho sigmoid x.
00:05:51 - 00:06:00, Thì nó sẽ bé hơn hoặc bằng tổng của 2 phần tử này.
00:06:00 - 00:06:01, Tất cả bình chia 4.
00:06:01 - 00:06:07, Tức là sigmoid x cộng cho 1 trừ sigmoid x.
00:06:07 - 00:06:10, Tất cả bình phương.
00:06:10 - 00:06:12, Rồi chia cho 4.
00:06:12 - 00:06:16, Thì sigmoid cộng 1 trừ sigmoid khử.
00:06:16 - 00:06:18, Thì chúng ta sẽ còn là 1.
00:06:18 - 00:06:20, Như vậy đó sẽ là bằng 1 phần tư.
00:06:20 - 00:06:26, Như vậy thì áp dụng cái bức đẳng thức cosy thì chúng ta thấy là đạo hàm của sigmoid sẽ bé hơn 1 phần tư.
00:06:26 - 00:06:28, Tức là 0.25.
00:06:28 - 00:06:34, Vậy thì với cái đạo hàm này mà bằng 0.25 thì điều gì xảy ra?
00:06:34 - 00:06:48, Với cái công thức đạo hàm của G theo theta y, nó có đến sự xuất hiện của đạo hàm sigmoid đến 10 lần.
00:06:48 - 00:07:02, Tức là ngoài các thành phần kia thì chúng ta sẽ thấy là thành phần đạo hàm sigmoid sẽ bé hơn hoặc bằng 0.25.
00:07:02 - 00:07:06, Vậy là 0.25 là mũ 10.
00:07:06 - 00:07:10, Rồi xong đó nhân với cái thành phần đạo hàm còn lại.
00:07:10 - 00:07:18, Vậy thì bây giờ chúng ta sẽ xem xét cái 0.25 mũ 10 này nếu chúng ta lấy máy tính bấm thì nó sẽ sắp xỉ là bằng 0.
00:07:18 - 00:07:20, Nó là 1 con số rất là bé.
00:07:20 - 00:07:22, 0.0001 ví dụ vậy.
00:07:22 - 00:07:30, Và với cái khả năng biểu diễn của máy tính thì thậm chí đó là cái con số này có thể tự động được làm tròn bằng số 0.
00:07:30 - 00:07:34, Là vì cái khả năng biểu diễn nó không có khả năng biểu diễn những con số quá bé.
00:07:34 - 00:07:36, Thì không nhân với bao nhiêu cũng sẽ là bằng 0.
00:07:36 - 00:07:40, Do đó thì cái đạo hàm này nó sẽ được tiến về việc 0.
00:07:40 - 00:07:46, Và đây chính là cái cốt lõi gây ra cái hiện tượng vanishing gradient.
00:07:46 - 00:07:54, Tức là đạo hàm của thằng G hàm lỗi theo theta y bằng 0.
00:07:54 - 00:08:02, Thì khi đó cái việc cập nhật theta y bằng theta y trừ cho alpha nhân cho đạo hàm.
00:08:02 - 00:08:06, Thì cái giá trị này sắp xỉ bằng 0.
00:08:06 - 00:08:10, Nên cái bước cập nhật này sẽ gần như sắp xỉ bằng y.
00:08:10 - 00:08:12, Tức là không cập nhật.
00:08:12 - 00:08:14, Gần như không cập nhật.
00:08:14 - 00:08:18, Kể tham số.
00:08:18 - 00:08:22, Gần như nó không cập nhật tham số.
00:08:22 - 00:08:26, Tì đây là một cái ví dụ trực quan.
00:08:26 - 00:08:30, Và đây là một cái ví dụ mà có số liệu tính toán.
00:08:30 - 00:08:38, Để cho chúng ta hình dung là tại sao cái công thức đạo hàm của mình nó sẽ tiến về 0.
00:08:38 - 00:08:44, Khi có quá nhiều cái hàm kích hoạt tập lại.
00:08:44 - 00:08:48, Và cái hàm kích hoạt này có cái đạo hàm bé hơn 1.
00:08:48 - 00:08:52, Vì vậy nó sẽ tiến cho kích tích này của chúng ta nó sẽ mau chống tiến về 0.
00:08:52 - 00:09:04, Và lưu ý, đó là cái việc tính đạo hàm này nó sẽ đặc biệt đó là dễ tiến về 0 khi cái theta y của mình gần lớp số 1.
00:09:04 - 00:09:12, Còn nếu như cái theta y của mình nằm trong cái nhóm theta 10 thì cái công thức này nó sẽ ít hơn.
00:09:12 - 00:09:14, Nó sẽ ít bị ảnh hưởng hơn.
00:09:14 - 00:09:20, Chúng ta có một cái theta z là thuộc cái tham số của theta 10.
00:09:20 - 00:09:26, Theta z là thuộc cái theta 10.
00:09:26 - 00:09:30, Tức là tập hợp các cái tham số ở lớp cuối cùng.
00:09:30 - 00:09:38, Thế thì cái công thức này của chúng ta theta z, delta z trên theta z.
00:09:38 - 00:09:44, Thế thì sẽ là bằng đạo hàm của binary cross entropy theo sigmoid.
00:09:44 - 00:09:52, Sau đó sẽ là đạo hàm của hàm sigmoid theo cái biến theta z này.
00:09:52 - 00:09:56, Tại vì chúng ta chỉ tính đến cái theta z này là chúng ta sẽ dừng.
00:09:56 - 00:10:02, Do đó thì chúng ta chỉ việc tính cái giá trị đạo hàm của sigmoid này đúng có một lần thôi.
00:10:02 - 00:10:08, Cho dù cái giá trị này nó bé hơn 0.25 nhưng mà chúng ta chỉ nhân có một lần thì nó sẽ không có ảnh hưởng.
00:10:08 - 00:10:12, Còn những cái theta y nào mà nó nằm ở cái lớp đầu tiên.
00:10:12 - 00:10:20, Lu ý là ở đây chúng ta đang xét cái theta y thuộc những cái layer đầu tiên.
00:10:20 - 00:10:30, Thì nó sẽ phải kéo từ cái thép biến đổi sigmoid thứ 10, thứ 9 và cho đến cái hàm sigmoid thứ 9.
00:10:30 - 00:10:34, Và cho đến cái hàm sigmoid số 1.
00:10:34 - 00:10:46, Rồi thì đây là một cái giải thích thêm cho cái việc đó là tại sao cái tham số mà càng gần cái layer đầu thì dễ bị hiền tượng vanishing gradient.
00:10:46 - 00:10:52, Tức là nó cập nhật chậm. Còn những cái tham số ở những cái lớp cuối cùng thì ít bị hiền tượng vanishing gradient hơn.
00:10:52 - 00:11:00, Vì nó đã cập nhật đến những cái tham số cuối cùng thì nó sẽ dừng ở đó.
00:11:00 - 00:11:06, Và nó không có thực hiện cái phép nhân nhiều lần. Nhân đào hàm nhiều lần.
