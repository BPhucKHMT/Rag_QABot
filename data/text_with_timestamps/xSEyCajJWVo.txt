00:00:00 - 00:00:23, Chúng ta sẽ hướng dẫn thực hành với hai kiến trúc là Autoencoder và Variational Autoencoder
00:00:23 - 00:00:33, Đầu tiên là kiến trúc Autoencoder, chúng ta sẽ kết nối với một cái máy
00:00:37 - 00:00:47, Kết nối đã được kết nối rồi, chúng ta sẽ tiến hành chạy đoạn code của mình
00:00:47 - 00:00:53, Đầu tiên là khởi tạo phần thư vị
00:00:53 - 00:01:13, Hàm chính sẽ bao gồm .nn, vết tắc của Neural Network, functional, kích hoạt như sigmoid, relu, và hàm khác
00:01:13 - 00:01:23, Tiếp theo là sẽ sử dụng collab với chế độ có GPU
00:01:23 - 00:01:27, Chúng ta phải kiểm tra lại xem có GPU hay chưa
00:01:27 - 00:01:31, Nếu có GPU RAM thì đã kết nối được rồi
00:01:31 - 00:01:39, Còn nếu không thì vào runtime, chúng ta chọn chain runtime, chọn T4
00:01:39 - 00:01:47, Kiểm tra xem device của mình, đã kết nối với CUDA, tức là nó đã có GPU rồi
00:01:47 - 00:01:59, Tiếp theo là kết nối kinh trúc Encode và Decoder, trong đó phần vàng vàng, tương ứng là encoder và phần xanh là decode
00:01:59 - 00:02:07, Trong giai đoạn Encode, ảnh đầu vào sẽ là ma trận kích thước 2828
00:02:07 - 00:02:15, Tại vì ảnh này được lấy từ tập dữ liệu mnits, và ảnh này sẽ được flatten để biến thành vector 784 chiều
00:02:15 - 00:02:21, Sau đó chúng ta giảm số chiều xuống là 512 và xuống z
00:02:21 - 00:02:39, Về mặt coding, chúng ta nên để 1 biến là Latent Demo để tùy chỉnh xem có thể là 2, 3, 4, 5, 1 cái kích thước bất kỳ
00:02:39 - 00:02:51, Đầu tiên là lớp constructor, chúng ta sẽ khởi tạo các lớp biến đổi cho encoder
00:02:51 - 00:03:13, Lớp đầu tiên là lợp linear để ánh sạ từ 784 về 512, chúng ta sẽ dùng là neural network.linear và 784 về 512
00:03:13 - 00:03:24, Và lớp này sẽ được gắn vào một thục tính cell.linear1, tức là lớp biến đổi đầu tiên này
00:03:24 - 00:03:36, Tương tự như vậy, chúng ta sẽ có lớp linear2, có điều, chúng ta sẽ ánh sạ từ 512 về 2 chiều
00:03:36 - 00:03:47, Tuy nhiên để tổn quát, chúng ta có thể thay đổi số chiều của bector latent, nên chúng ta phải truyền vào biến là Latent Demo
00:03:47 - 00:03:52, Thay vì để 2 hạt code, chúng ta sẽ để là Latent Demo
00:03:52 - 00:04:00, Tiếp theo là hàm forward, hàm forward sẽ nhận dữ kiện đầu vào là 1 ảnh x và đầu ra của mình
00:04:00 - 00:04:05, Đối với encoder, đầu ra của chúng ta chỉ cần ra độ d, là đủ
00:04:05 - 00:04:09, Do đó, bước đầu tiên là chúng ta phải flatten
00:04:10 - 00:04:24, Thì để flatten, chúng ta sẽ sử dụng cái hàm, đó là hàm, chúng ta sẽ sử dụng một cái hàm flatten của neural network, đó là hàm torch.
00:04:29 - 00:04:34, Và chúng ta sẽ truyền vào x, startDem, là bằng 1
00:04:35 - 00:04:44, Rồi, để cho độ phức tạp thì chúng ta sẽ giữ luôn chính cái biến x, đầu vào là x và đầu ra sẽ gán ngược trở lại vào biến x
00:04:44 - 00:04:56, Lúc này thì x của chúng ta là bector 784 chiều, sau đó chúng ta sẽ đưa về cái bector 512 chiều, cell.linear
00:04:56 - 00:05:05, Và để cho có cái sự khác biệt giữa các lớp biến độ tiến tính thì chúng ta phải có một cái hàm kích hoạt ở giữa
00:05:05 - 00:05:10, Thế thì, hàm kích hoạt ở đây chúng ta dùng là gì?
00:05:10 - 00:05:18, Thì trong cái lý thuyết về deep learning chúng ta đã được học là có thể dùng hàm sigmoid hoặc hàm relu
00:05:18 - 00:05:27, Tuy nhiên hàm sigmoid có khả năng gây ra hình tượng vanishing gradient, tức là tiêu biến đạo hàm nên cái tốc độ hội tổn chọn
00:05:27 - 00:05:38, Do đó thì chúng ta sẽ dùng f.relu, tức là hàm rectify linear unit, rồi chúng ta sẽ trả data để x
00:05:38 - 00:05:43, Lúc này x của mình sẽ là một cái bector 512 chiều
00:05:43 - 00:05:51, Tiếp tục như vậy, thì từ 512 xuống 2 thì chúng ta sẽ có cell.linear2
00:05:51 - 00:06:01, Và lúc này thì chúng ta có sử dụng cái hàm kích hoạt hay không?
00:06:01 - 00:06:09, Nếu chúng ta sử dụng hàm rectify linear unit thì cái giải giá trị của mình là từ 0 cho đến còn vô cùng
00:06:09 - 00:06:19, Nhưng cái bector z này chúng ta muốn biểu diễn nó xung quanh con số 0 trong cái không gian, do đó nó phải có số âm và số dương
00:06:19 - 00:06:29, Tức là các cái thành phần của bector z của mình nó phải có thành phần âm, thành phần dương, do đó thì chúng ta sẽ không có dùng hàm kích hoạt ở đây
00:06:29 - 00:06:37, Và thay vì tạo biến tạng thì chúng ta sẽ trả ra trực tiếp, thì lúc này đây chính là cái z của mình
00:06:43 - 00:06:49, Và cái kết quả mà return đây chính là cái latent variable z của mình
00:06:49 - 00:07:05, Tương tự như vậy cho hàm decode, thì hàm decode chúng ta sẽ từ 2 chiều đến 52 chiều, hoặc là từ latent deam đến 52, rồi từ 52 về 784
00:07:05 - 00:07:10, Thì chúng ta sẽ có cell.linear1
00:07:15 - 00:07:22, Và để cho tổng quát thì chúng ta sẽ để là latent deam và ra là 52
00:07:24 - 00:07:30, Tiếp theo đó là cell.linear1
00:07:35 - 00:07:45, Và hàm forward là nó sẽ nhận đầu vào bector z, sau đó nó sẽ biến đổi qua 2 lớp biến đổi kia
00:07:45 - 00:07:53, cell.linear1, nhận đầu vào là z, và đầu ra thì chúng ta cũng sẽ dùng 1 cái hàm kích hoạt, cụ thể ở đây đó là cell.reloog
00:07:57 - 00:08:06, Rồi, thế thì nhận đầu vào là z, thì ở đây chúng ta có thể trả về cái biến z luôn, tuy nhiên nếu đúng về mặt, thì chúng ta có thể trả về cái biến z
00:08:06 - 00:08:14, Tuy nhiên nếu đúng về mặt, concept về mặt nữ nghĩa, chúng ta đang decode để tạo ra cái x-mũ, do đó chúng ta sử dụng luôn cái biến x-mũ, thay vì chúng ta dùng bector z
00:08:26 - 00:08:35, Cái vị đặt bên biến là z cũng không có ảnh hưởng đến kết quả, tuy nhiên để dễ maintain về sao, chúng ta nên đặt biến đúng cái ý nghĩa của nó
00:08:36 - 00:08:41, Sau đó thì cell.linear2, xh
00:08:42 - 00:08:48, Thế thì ở cái lớp cuối cùng này, chúng ta có dùng activation function hay không?
00:08:49 - 00:08:55, Thì chúng ta phải xem coi cái kiểu dữ liệu của cái ảnh này, mỗi một cái điểm ảnh nó sẽ là kiểu gì?
00:08:55 - 00:09:09, Thì cái đầu ra của mình nó sẽ là một cái giá trị scaling từ 0 cho đến 1, do đó ở đây chúng ta sẽ phải dùng một cái hàm để app một cái giá trị bất kỳ về cái đoạn từ 0 đến 1, đó chính là app.sigmoid
00:09:10 - 00:09:18, Tại sao không dùng relu nữa? Tại vì cái hàm relu nó là giải giá trị từ 0 cho đến cọng vô cụt, còn chúng ta đang muốn từ 0 cho đến 1
00:09:18 - 00:09:26, Sau khi chúng ta đã có xh xong thì chúng ta sẽ gọi hàm reset
00:09:27 - 00:09:30, xh.reset
00:09:32 - 00:09:37, Và cái kích thước của cái chef của mình nó sẽ là trừ 1, 1, 2, 8
00:09:37 - 00:09:51, Trong đó hai cái thành phần cuối là cái kích thước của cái ảnh mà mình sẽ đi cốt ra, và 1 đó là cái số kênh màu của mình, tại vì đây là ảnh mức xám nên nó chỉ có một kênh màu thôi
00:09:52 - 00:10:04, Còn trừ 1 ở đây có nghĩa là chúng ta có thể hướng luyện theo bác truyền bật theo một khổ dữ liệu, thì ở đây trừ 1 thì hàm ý đó là nếu đồ vàng thì chúng ta có bao nhiêu ảnh để hướng luyện, cái bác xa là bao nhiêu thì ở đây sẽ là bao nhiêu
00:10:05 - 00:10:12, Rồi, và bây giờ chúng ta sẽ tiến hành chạy hai cái đạn cốt encoder và decoder
00:10:14 - 00:10:22, Rồi, thì chúng ta sẽ có một cái nấp tối tượng nữa đó là autoencoder, nó sẽ bao gồm hai thành phần đó là encoder và decoder
00:10:23 - 00:10:31, Rồi khi gọi hàm forward thì chúng ta sẽ nhận cái ảnh đồ vào và gọi cái cell.encoder để tạo ra cái vector latent g
00:10:31 - 00:10:39, Rồi từ vector latent g chúng ta gọi cell.decoder để trả ra cái x mũ, đây chính là cái x mũ của mình
00:10:46 - 00:10:47, là ring construct
00:10:51 - 00:10:53, là cái ảnh đã được hôi phục
00:10:53 - 00:11:04, Rồi, ở trong cái hàm trend này thì chúng ta sẽ có dùng cái optimizer là adam, mặc định khi chúng ta không rành được optimizer thì chúng ta cứ dùng adam
00:11:05 - 00:11:14, và chúng ta sẽ có nhiều epoch, rồi sau đó chúng ta sẽ chuyển qua dữ liệu, chúng ta thấy là cái dữ liệu này sẽ được lấy từ tập dữ liệu MNIST
00:11:14 - 00:11:22, và trong tập MNIST thì cái data của mình nó sẽ có dữ liệu x và y, trong đó y cho biết đó là kỹ tực nào
00:11:23 - 00:11:29, nhưng mà chúng ta lưu ý, chúng ta đang huấn luyện theo cái phong cách đó là unsupervised learning, tức là không giám sát
00:11:30 - 00:11:39, do đó thì ở đây chúng ta sẽ không có sử dụng cái y, chúng ta rê chuột vào đây chúng ta thấy không gây, nhưng khi chúng ta rê vụ x thì chúng ta thấy x biến x có sử dụng
00:11:39 - 00:11:53, và x sẽ được chuyển vào trong device đó là GPU và khởi tạo optimizer là Zero Grad, tức là Radiant Descent với mặc định băng đầu bằng 0
00:11:54 - 00:12:02, rồi x hat qua cái autoencoder nhận vào cái biến đồ vào x và nó sẽ ra cái x hat là cái đã được khôi phục lại
00:12:02 - 00:12:17, thì hàm loss ở đây là gì? sẽ là bằng x hat trừ cho x, sau đó chúng ta sẽ vụ 2 lên, rồi sau đó chúng ta sẽ đi tính tổng
00:12:17 - 00:12:32, sau khi tính tổng xong chúng ta sẽ gọi hàm backward để thực hiện cái thực toán back propagation và tiến hành update lại cái tham số
00:12:32 - 00:12:44, rồi thì đây chính là cái hàm trend, tiếp theo thì chúng ta sẽ gọi cái hàm và gọi cái đối tượng là autoencoder với latent deam là bằng 2
00:12:44 - 00:12:54, câu hỏi là tại sao chúng ta lại chọn latent deam là bằng 2? Đa bán chính là để dễ trực quan hóa, điều gì xảy ra nếu latent deam bằng 4 bằng 5
00:12:54 - 00:13:03, chúng ta sẽ rất khó để có thể vẽ lên trên cái vùa colab, sau đó chúng ta chọn deam bằng 2 là vừa đủ nhỏ để có thể vẽ lên trên cái không gian 2 chiều
00:13:03 - 00:13:21, tỉ lệ nén trong trường hợp này nó sẽ là 784 x 2, tức là nó nén khoảng 392 lần, rồi điều gì xảy ra nếu deam bằng 1 thì rõ ràng là nó sẽ bị mất mát rất nhiều thông tin
00:13:21 - 00:13:33, và có thể cái việc khôi phục nó khó đạt được đến cái hãnh gốc ban đầu, thì đây chúng ta sẽ phải thử, nhưng mà trước bắt chúng ta sẽ sử dụng cái latent deam là bằng 2
00:13:33 - 00:13:42, và autoencoder sẽ được đưa vào GPU, rồi chữ liệu wrap và chúng ta sẽ gọi hàm trend
00:13:42 - 00:13:52, nó sẽ có no attribute là relu, rồi chúng ta sẽ xem trong cái decoder
00:13:52 - 00:14:02, rồi ở đây nó không phải là cell.relu mà là f.relu, chúng ta sẽ chạy lại
00:14:22 - 00:14:27, Cảm ơn các bạn đã xem video hấp dẫn
