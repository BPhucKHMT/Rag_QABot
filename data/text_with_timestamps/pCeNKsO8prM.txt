00:00:00 - 00:00:18, Chúng ta sẽ cùng tìm hiểu về thuật toán Gradient Descent
00:00:18 - 00:00:23, Đây có thể nói là một trong những thuật toán rất là hiệu quả trong việc tối ưu hóa
00:00:23 - 00:00:25, Một cái hàm khả vi
00:00:25 - 00:00:29, Và các mô hình máy học mà dựa trên Radiant hiện đại
00:00:29 - 00:00:33, Nói dụ như là học sâu, các mô hình ngôn ngữ lớn
00:00:33 - 00:00:39, Đều dựa trên ý tưởng của thuật toán Gradient Descent
00:00:39 - 00:00:43, Như vậy, vai trò của thuật toán Gradient Descent là gì?
00:00:43 - 00:00:49, Và các bước thực hiện ra sao? Chúng ta sẽ cùng tìm hiểu trong những phần tiếp theo
00:00:49 - 00:00:53, Chúng ta sẽ nhắc lại một chút xíu về mô hình dựa trên Gradient Descent
00:00:53 - 00:00:59, Đó là chúng ta sẽ dựa trên dữ liệu x để đưa ra giá trị dự đoán
00:00:59 - 00:01:03, Và dựa trên giá trị thực tế chúng ta sẽ có đạo hàm
00:01:03 - 00:01:11, Mục tiêu của mình là chúng ta sẽ đi tìm tham số theta sao là một tham số tối ưu
00:01:11 - 00:01:15, Sau cho giá trị hàm lỗi này là nhỏ nhất
00:01:15 - 00:01:22, Tức là sai số giữa y ngã và dự đoán y ngã và thực tế y là thấp nhất
00:01:22 - 00:01:27, Vậy thì đây có lẽ là một trong những công việc chúng ta phải thực hiện thường xuyên
00:01:27 - 00:01:30, Khi chúng ta huấn luyện một mô hình máy học
00:01:30 - 00:01:33, Do đó chúng ta sẽ bắt đầu tại công việc này
00:01:33 - 00:01:38, Đó là làm sao chúng ta có thể tìm được giá trị nhỏ nhất của một hàm số
00:01:39 - 00:01:45, Thế thì ở đây là một hình tượng là giả sử chúng ta đang đứng trên một định núi
00:01:45 - 00:01:53, Và chúng ta đang muốn tìm đường để đến giá trị nhỏ nhất, tức là chân núi
00:01:53 - 00:01:59, Y tưởng đó là mình sẽ tìm cách để cập nhật và giảm độ cao
00:01:59 - 00:02:01, Chúng ta sẽ tiếp tục giảm độ cao
00:02:01 - 00:02:05, Rồi đến một những ngọn núi thấp hơn, chúng ta lại tiếp tục giảm độ cao
00:02:05 - 00:02:07, Đến những ngọn núi thấp hơn nữa
00:02:07 - 00:02:13, Và giảm cho đến khi nào chúng ta không thể giảm được nữa thì đó là nơi mà chúng ta cần đến
00:02:13 - 00:02:22, Đây là một hình minh họa vui để cho thấy chúng ta phải cập nhật liên tục
00:02:22 - 00:02:30, Đây là một quá trình lập để tìm được giá trị cực tiểu
00:02:30 - 00:02:34, Vậy thì Gradient Descent là gì?
00:02:34 - 00:02:46, Gradient Descent là một thuật toán nằm trong nhóm tươi u hóa để tìm giá trị cực tiểu của hàm khả vi
00:02:46 - 00:02:50, Hàm khả vi là một hàm có khả năng tính đạo hàm được
00:02:56 - 00:03:01, Như vậy thì thuật toán này chỉ có thể chạy được với những hàm có thể tính đạo hàm được
00:03:01 - 00:03:05, Và ở đây chúng ta sẽ có hai dạng hàm cơ bản
00:03:05 - 00:03:11, Đó là dạng hàm lỏng và hàm lòi
00:03:11 - 00:03:19, Cả hai dạng hàm này đều là hai dạng cơ bản trong giải tích
00:03:19 - 00:03:23, Ngoài ra chúng ta có những dạng hàm phối hợp
00:03:23 - 00:03:26, Tức là một hàm sẽ vừa có lỏng và vừa có phần lòi
00:03:26 - 00:03:33, Đối với phần lòi thì chúng ta sẽ tìm được giá trị cực đại
00:03:33 - 00:03:39, Còn đối với hàm lỏng thì chúng ta sẽ tìm được giá trị cực tiểu
00:03:39 - 00:03:43, Còn hàm lỏng thì chúng ta sẽ tìm được giá trị cực đại
00:03:43 - 00:03:47, Nó gọi là localMaximum
00:03:49 - 00:03:51, Còn đây là localMinimum
00:03:57 - 00:04:05, Sự khác biệt ở đây là đối với hàm lồi và hàm lỏng
00:04:05 - 00:04:11, Tại một vị trí này chúng ta thấy là hướng của mình đang hướng đi lên
00:04:11 - 00:04:19, Thì đạo hàm của mình sẽ hướng cho chúng ta để tìm được cái điểm cực tiểu
00:04:19 - 00:04:23, Cái điểm cực đại của bộ
00:04:23 - 00:04:28, Trong khi đó ngược lại, đối với giá trị localMinimum
00:04:28 - 00:04:34, Thì đạo hàm của mình sẽ hướng ngược lại
00:04:34 - 00:04:39, Hướng ngược lại, lẽ ra chúng ta sẽ phải đi xuống
00:04:39 - 00:04:42, Thì đạo hàm lại hướng chúng ta đi lên
00:04:42 - 00:04:46, Đạo hàm lại hướng chúng ta đi lên, giống như tình huống bên này
00:04:46 - 00:04:50, Vậy thì để tìm giá trị nhỏ nhất thì chúng ta sẽ phải
00:04:50 - 00:04:55, Cập nhật theo hướng ngược với hướng của đạo hàm
00:04:55 - 00:05:03, Sau đây là chi tiết cho các bước thực hiện trong thuật toán Gradient Descent
00:05:03 - 00:05:07, Đầu tiên là chúng ta sẽ khởi tạo tham số
00:05:07 - 00:05:15, Sát xuất để giá trị theta mà chúng ta khởi tạo được mà trùng
00:05:15 - 00:05:18, Theta này là theta mà chúng ta khởi tạo
00:05:18 - 00:05:22, Theta old, nó chùng với theta sao này
00:05:22 - 00:05:25, Sát xuất của chúng ta là cực kỳ thấp
00:05:25 - 00:05:29, Do đó chúng ta sẽ phải tìm cách dựa trên đạo hàm
00:05:29 - 00:05:34, Dựa trên đạo hàm giống như là một chỉ báo để cho chúng ta biết
00:05:34 - 00:05:39, Đi về hướng nào là sẽ tìm được đến điểm cực tiểu
00:05:39 - 00:05:43, Khi có tính đạo hàm thì chúng ta sẽ update và cập nhật từ từ
00:05:43 - 00:05:46, Theta old này bằng một theta new
00:05:46 - 00:05:49, Và cứ như vậy là update update
00:05:49 - 00:05:56, Và ở bên dưới sẽ là cái minh họa cho cái mô hình
00:05:56 - 00:05:59, Đây chính là cái mô hình của mình
00:05:59 - 00:06:06, Khi ban đầu khởi tạo tham số của mình thì mô hình của mình đoán rất tệ
00:06:06 - 00:06:09, Do đó nó sẽ không khớp với lại tập điểm này
00:06:09 - 00:06:12, Nhưng khi chúng ta bắt đầu tính đạo hàm
00:06:12 - 00:06:14, Chúng ta sẽ bắt đầu tính đạo hàm
00:06:14 - 00:06:17, Thì đạo hàm có định nghĩa
00:06:17 - 00:06:20, Đạo hàm của một hàm z theo biến theta
00:06:20 - 00:06:24, Thì nó sẽ có định nghĩa là bằng f của theta cộng h
00:06:24 - 00:06:29, Chừ cho f theta khi h tiến đến 0
00:06:29 - 00:06:32, Thì đây là đạo hàm một phía
00:06:32 - 00:06:34, Nhưng mà chúng ta làm như vậy để cho nó đơn giản
00:06:34 - 00:06:36, Đạo hàm một phía
00:06:36 - 00:06:39, Rồi chúng ta thấy là tại vị trí này
00:06:39 - 00:06:43, Đạo hàm của mình sẽ là dương
00:06:43 - 00:06:45, Tức là nó hướng lên
00:06:45 - 00:06:48, Hướng này là hướng dương hay là hướng đi lên
00:06:48 - 00:06:50, Tại sao nó có cái dấu mũi tên đi lên này
00:06:50 - 00:06:56, Là vì đạo hàm nó còn có một ý nghĩa là tiết tuyến tại vị trí của mình
00:06:56 - 00:06:58, Tại cái biến số của mình
00:06:58 - 00:06:59, Tiết tuyến tại đây
00:06:59 - 00:07:01, Nhưng mà nó sẽ có hướng
00:07:01 - 00:07:03, Thì hướng này sẽ là hướng đồng biến tức là hướng đi lên
00:07:03 - 00:07:07, Trong khi đó lẽ ra cái điểm cực tiểu của mình
00:07:07 - 00:07:09, Thì nó nằm ở phía ngược lại
00:07:09 - 00:07:11, Lẽ là chúng ta phải đi về cái hướng ngược lại
00:07:11 - 00:07:16, Thì đó là lý do tại sao ở đây chúng ta thấy là nó có cái dấu trừ
00:07:16 - 00:07:19, Có cái dấu trừ để đi ngược
00:07:19 - 00:07:21, Với lại cái hướng của đạo hàm
00:07:21 - 00:07:23, Đi ngược hướng
00:07:31 - 00:07:35, Rồi sau khi chúng ta đã tính được cái đạo hàm là cái
00:07:35 - 00:07:39, Thể hiện bởi cái mũi tên màu xanh là hướng lên như thế này
00:07:39 - 00:07:43, Còn nếu mà xét về cái việc mà cập nhật đi tới hay đi lui
00:07:43 - 00:07:45, Thì chúng ta dùng cái dấu mũi tên này
00:07:45 - 00:07:47, Tức là dấu cộng là dương
00:07:47 - 00:07:50, Thì chúng ta sẽ tiến hành cái bước số 3
00:07:50 - 00:07:52, Đó là chúng ta sẽ cập nhật lại cái tham số của mình
00:07:52 - 00:07:54, Cập nhật lại cái tham số
00:07:54 - 00:07:57, Và chúng ta sẽ đi ngược hướng
00:07:57 - 00:07:59, Chúng ta sẽ đi ngược hướng với cái hướng của đạo hàm
00:07:59 - 00:08:01, Tức là chúng ta sẽ đi xuống
00:08:01 - 00:08:04, Và ngược hướng thể hiện ở cái dấu dấu trừ
00:08:04 - 00:08:09, Ở đây là dùng cái hiệu nắp la cho nó tổn quát
00:08:09 - 00:08:12, Đí do đó là tại sao chúng ta dùng nắp la
00:08:12 - 00:08:14, Là vì đây là cái...
00:08:14 - 00:08:17, Một cách tổn quát thì theta của chúng ta
00:08:17 - 00:08:19, Có thể là một vector
00:08:19 - 00:08:22, Nó sẽ là một vector gồm nhiều cái đĩa thành phần
00:08:22 - 00:08:28, Rồi, và ở đây nó sẽ có một cái siêu tham số đó là alpha
00:08:28 - 00:08:30, Thì tên của cái siêu tham số này á
00:08:30 - 00:08:32, Nó gọi là learning rate
00:08:32 - 00:08:36, Nếu mà dịch xác nghĩa sang tiếng Việt đó là hệ số học
00:08:36 - 00:08:40, Tuy nhiên mình có suy nghĩ và đặt cho nó một cái tên
00:08:40 - 00:08:42, Mà mình nghĩ là phù hợp
00:08:42 - 00:08:44, Nó sẽ không xác nghĩa với cái từ learning rate
00:08:44 - 00:08:47, Nhưng mà nó thể hiện được cái bản chất của cái mô hình của mình
00:08:47 - 00:08:49, Đó chính là hệ số dò dõm
00:08:49 - 00:08:52, Hay biết gọn lại đó là hệ số dò
00:08:52 - 00:08:56, Tại sao được gọi là hệ số dò?
00:08:56 - 00:08:58, Khi chúng ta tính đạo hàm
00:08:58 - 00:09:01, Thì đạo hàm là một hệ số dò dõm
00:09:01 - 00:09:06, Đạo hàm nhìn chung là nó chỉ giúp cho chúng ta biết cái hướng đi thôi
00:09:06 - 00:09:08, Hướng đi tại cái vị trí này
00:09:08 - 00:09:12, Thì đối với cái vị trí này đạo hàm của mình là hướng đi lên
00:09:12 - 00:09:15, Tức là nó đang hướng dương
00:09:15 - 00:09:21, Nhưng chúng ta sẽ phải đi cái hướng ngược lại với nó để tìm được cái
00:09:21 - 00:09:26, Cái giá trị nhỏ nhất của mình để tìm được đến cái điểm cực tiểu của mình
00:09:26 - 00:09:29, Chúng ta sẽ phải đi theo cái hướng ngược lại là hướng ông
00:09:29 - 00:09:34, Nhưng cái độ lớn của đạo hàm thì không có lý thuyết nào chứng minh được rằng là
00:09:34 - 00:09:37, Khi chúng ta đi đúng một cái đại lượng bằng độ lớn của đạo hàm
00:09:37 - 00:09:44, Thì nó sẽ giúp cho chúng ta đến được ngay cái điểm theta sao ở đây
00:09:44 - 00:09:49, Tức là không có lý thuyết nào nói cái độ dài này đúng bằng đạo hàm
00:09:49 - 00:09:52, Do đó thì nó cũng có khả năng đạo hàm của mình
00:09:52 - 00:09:56, Nó sẽ kéo chúng ta đi lố qua cái theta sao này
00:09:56 - 00:09:58, Chúng ta biết là phải đi hướng ngược lại
00:09:58 - 00:10:01, Nhưng mà nếu đi đúng theo cái giá trị đạo hàm này
00:10:01 - 00:10:04, Thì có thể chúng ta sẽ đi lố qua bên này
00:10:04 - 00:10:08, Do đó thì chúng ta sẽ đi một cách từ tốn, đi một cách từ từ
00:10:08 - 00:10:13, Theta on ở đây sẽ được cập nhật một cách từ từ
00:10:13 - 00:10:17, Cho đến lúc nào mà chạm được đến cái theta sao
00:10:17 - 00:10:21, Thế thì để mà đi từ từ ở đây thì chúng ta sẽ phải nhân với một cái đại lượng
00:10:21 - 00:10:27, Một cái siêu thăm số alpha và alpha này thường là một con số rất là bé
00:10:27 - 00:10:33, Thì nếu chúng ta không biết alpha là bao nhiêu thì mặc định trong các cái mẹo
00:10:33 - 00:10:37, Đó là chúng ta sẽ chọn alpha đâu đó là khoảng 10 mũi trừ 4
00:10:37 - 00:10:41, Tại sao nó lại là cái con số rất là bé như thế này
00:10:41 - 00:10:45, Lý do đó là vì trong các cái mô hình mà phức tạp
00:10:45 - 00:10:49, Thì cái độ dốc của cái hàm của mình nó rất là cao
00:10:49 - 00:10:50, Độ dốc này rất là cao
00:10:50 - 00:10:54, Do đó cái giá trị đạo hàm của mình nó rất là lớn
00:10:54 - 00:10:59, Nó có thể là kéo đến đây dẫn đến là chúng ta sẽ đi lố
00:10:59 - 00:11:03, Khi chúng ta đi hướng ngược lại chúng ta sẽ đi lố qua cái điểm cực tiểu
00:11:03 - 00:11:11, Thì theta với alpha mà nhỏ thì nó sẽ có thể có một cái điểm yếu đó là nó sẽ đi rất là chậm
00:11:11 - 00:11:14, Nhưng bù lại đó là nó sẽ đi chắc chắn
00:11:14 - 00:11:16, Chúng ta cứ đi một chút rồi chúng ta cập nhật
00:11:16 - 00:11:18, Đi một chút rồi chúng ta sẽ cập nhật
00:11:19 - 00:11:23, Rồi và ở cái hướng ngược lại
00:11:23 - 00:11:29, Nếu như chúng ta ở bên tay trái thì đạo hàm của mình nó sẽ là hướng ong
00:11:29 - 00:11:33, Tức là nó đang đi xuống đúng không là đạo hàm của mình sẽ là hướng ong
00:11:33 - 00:11:37, Thì mình sẽ đi theo cái hướng ngược với nó đó là hướng dương
00:11:37 - 00:11:42, Thì cho dù là nằm bên trái hay bên phải cái điểm cực tiểu của bộ
00:11:42 - 00:11:50, Thì đạo hàm đều có chung một quyết nguyên tắc đó là chỉ ngược hướng với cái điểm cực tiểu của bộ của mình
00:11:50 - 00:11:55, Do đó thì dù bên trái hay bên phải chúng ta vẫn luôn để cái dấu ở đây là dấu trừ
00:11:55 - 00:11:57, Tức là đi ngược hướng với đạo hàm
00:11:57 - 00:12:03, Và khi chúng ta cập nhật cái tham số
00:12:03 - 00:12:09, Thì ở bước tiếp theo là cái điểm của chúng ta nó đã rớt xuống
00:12:09 - 00:12:13, Và khi đó cái mô hình của mình nó cũng tiến gần hơn
00:12:13 - 00:12:17, Nó sẽ có xu hướng tiến gần hơn về các điểm dữ liệu của mình
00:12:17 - 00:12:19, Sắp xỉ với điểm dữ liệu của mình
00:12:19 - 00:12:24, Thì chúng ta sẽ quay ngược trở lại để ti tính đạo hàm
00:12:26 - 00:12:34, Và khi chúng ta tính đạo hàm thì một lần nữa đạo hàm lại thể hiện cái hướng ngược với lại cái hướng có cái điểm cực tiểu
00:12:34 - 00:12:39, Chúng ta lại tiếp tục cập nhật tham số để đi cái hướng ngược lại
00:12:39 - 00:12:45, Và khi chúng ta cập nhật cái tham số mới
00:12:45 - 00:12:52, Thì cái hàm, cái đường quyết định, cái decision boundary
00:12:52 - 00:12:59, Đường bin quyết định nó sẽ càng tiện cận và nó sẽ sắp xỉ vào bên trong cái điểm dữ liệu của mình
00:12:59 - 00:13:07, Thì đây chính là mô phỏng cho thuộc tòa án Gradient Descent thực hiện từng bước lặp đi lặp lại
00:13:07 - 00:13:11, Và cái việc này chúng ta sẽ dừng khi nào
00:13:11 - 00:13:13, Thì chúng ta sẽ có hai cách
00:13:13 - 00:13:21, Một, đó là dừng khi cái F phải theta là bằng 0
00:13:21 - 00:13:26, Hoặc là đây là trường hợp là hàm đơn biến
00:13:26 - 00:13:33, Hoặc là trị tiết đối của NABLA của J
00:13:33 - 00:13:41, Theo biến theta là bằng 0
00:13:46 - 00:13:48, Hoặc là bé hơn một cái ngữ nào đó
00:13:48 - 00:13:51, Thông thường cái khả năng mà bằng 0 rất là thấp
00:13:51 - 00:13:54, Và chúng ta chỉ xét một cái ngữ nào đó thôi
00:13:54 - 00:13:56, Nhưng cái cách này thì rất là khó
00:13:56 - 00:14:00, Tại vì khi chúng ta theta mà bằng 0
00:14:00 - 00:14:03, Hoặc là rất là bé mà chúng ta dừng
00:14:03 - 00:14:05, Thì có khả năng là nó sẽ mắc kẹt ở những điểm cục bộ
00:14:05 - 00:14:07, Do đó nó có một kiểu dừng thứ hai
00:14:07 - 00:14:13, Đó là khi chúng ta lặp đủ nhiều
00:14:13 - 00:14:16, Khi chúng ta lặp đủ nhiều thì chúng ta sẽ dừng
00:14:16 - 00:14:20, Thì đây là cái giải pháp phổ biến hơn
00:14:20 - 00:14:24, Mà thường các mô hình máy học hay sử dụng
00:14:24 - 00:14:29, Ở đây chúng ta có một cái lưu ý là F ở đây là hàm G
00:14:29 - 00:14:32, Tức là hàm lỗi của mình
00:14:32 - 00:14:34, Rồi
00:14:34 - 00:14:37, Thì ở trong cái sô đồ này
00:14:37 - 00:14:39, Đó là chúng ta sẽ có các cái khái niệm
00:14:39 - 00:14:43, Khái niệm đầu tiên, đây chính là khái niệm radian
00:14:43 - 00:14:47, Tức là cái đạo hàm theo một cái vector tham số theta của mình
00:14:47 - 00:14:52, Đạo hàm của hàm lỗi theo cái tham số theta
00:14:52 - 00:14:55, Cái thứ hai, đó là cái initial way
00:14:55 - 00:14:57, Tức là cái điểm khởi tạo
00:14:57 - 00:15:01, Tham số mầm khởi tạo mặc định ban đầu
00:15:01 - 00:15:03, Rồi cái incremental step
00:15:03 - 00:15:05, Tức là cái bước nhảy của mình
00:15:05 - 00:15:09, Đó là một cái bước nhảy từng bước như thế này
00:15:09 - 00:15:11, Thì đây là một cái bước nhảy nhỏ
00:15:11 - 00:15:13, Và cái minimal cost
00:15:13 - 00:15:17, Tức là cái vị trí mà cái hàm lỗi của mình
00:15:17 - 00:15:20, Nó đạt được cái giá trị cực tiểu
00:15:20 - 00:15:24, Và trong cái này thì chúng ta sẽ thấy có hai cái trục
00:15:24 - 00:15:26, Một trục là trục về trọng số
00:15:26 - 00:15:28, Là cái tham số theta
00:15:28 - 00:15:30, Cái trọng số của mô hình
00:15:30 - 00:15:35, Tức là cái tham số theta F của cái hàm mô hình
00:15:37 - 00:15:39, Và một cái trục là chiều cao
00:15:39 - 00:15:41, Và cái trục tung của mình chính là cái cost
00:15:41 - 00:15:44, Là biểu diễn cho chi của theta
00:15:44 - 00:15:52, Là cái sai số của mô hình khi tại một cái vị trí theta
00:15:52 - 00:15:56, Thì đây chính là những cái khái niệm mà chúng ta cần phải nắm vững
00:15:56 - 00:15:59, Khi chúng ta làm việc với tụt toán gradient descent
00:15:59 - 00:16:03, Và nó sẽ có một cái tình huống đó là
00:16:03 - 00:16:05, Khi chúng ta cập nhật theta là bằng theta
00:16:05 - 00:16:09, Trừ cho alpha nhân đạo hàm của hàm chi
00:16:09 - 00:16:14, Thì cái hệ số learning rate này là alpha
00:16:14 - 00:16:18, Mặc quá thấp, low learning rate
00:16:18 - 00:16:20, Thì chúng ta sẽ thấy các cái bước đi của mình nó rất là chậm
00:16:20 - 00:16:22, Nó rất là ngắn
00:16:22 - 00:16:27, Mà ngắn tức là nó sẽ đi rất là nhiều lần
00:16:27 - 00:16:31, Nhiều lần, tức là rất là lâu
00:16:31 - 00:16:36, Trong khi đó, nếu như mà cái learning rate của mình nó quá cao
00:16:36 - 00:16:40, Thì có khả năng là nó sẽ đi và nó lố qua
00:16:40 - 00:16:42, Cái điểm cực tiểu ở đây
00:16:42 - 00:16:44, Nó sẽ đi lố qua luôn
00:16:44 - 00:16:48, Thế thì một cái learning rate mà phù hợp
00:16:48 - 00:16:51, Đó là một cái learning rate mà nó sẽ đi đủ nhanh
00:16:51 - 00:16:54, Nhưng mà sẽ không đi lố qua được cái điểm cực tiểu
00:16:54 - 00:16:57, Nhưng mà để mà có được cái learning rate đó
00:16:57 - 00:17:00, Thì chúng ta cũng sẽ có rất nhiều những cái phiên bản
00:17:00 - 00:17:02, Có những cái chiến thuật
00:17:02 - 00:17:06, Có những cái chiến thuật để chọn learning rate
00:17:08 - 00:17:12, Thì chúng ta sẽ tìm hiểu trong những cái phần tiếp theo
