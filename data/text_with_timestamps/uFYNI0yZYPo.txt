00:00:00 - 00:00:17, Chúng ta sẽ cùng đến với mô hình tiếp theo đó là mô hình Blip
00:00:17 - 00:00:23, Blip là vý tắc của chữ Boost Trapping Language Image Retraining
00:00:23 - 00:00:29, Nếu như các mô hình ngôn ngữ thì trước thì thường nó sẽ có hai hạn chế chính
00:00:29 - 00:00:34, Hạn chế đầu tiên xét ở khía cạnh mô hình đó đó là kiến trúc encoder
00:00:34 - 00:00:40, của các mô hình đó thì ít phù hợp cho bài tán tạo sinh văn bản hay là Text Generation
00:00:40 - 00:00:45, Text Generation có thể kể đến một số ngữ cảnh ứng dụng
00:00:45 - 00:00:52, Ví dụ như là VQI tức là Visual Question Answering tức là khỏi đáp về hình ảnh
00:00:52 - 00:00:55, hoặc cũng có thể là image captioning
00:00:56 - 00:01:04, Thế thì các mô hình mà đòi hỏi có sự tạo sinh ra văn bản sau khi chúng ta nhận dữ liệu đầu vào là hình ảnh
00:01:04 - 00:01:08, thì những mô hình trước đó là không có phù hợp
00:01:08 - 00:01:14, Về khía cạnh dữ liệu thì các mô hình trước đây được huấn luyện trên dữ liệu từ web
00:01:14 - 00:01:18, nên nó có rất nhiều dữ liệu bị nhỉu và sai lệch
00:01:18 - 00:01:24, Do đó thì từ những hạn chế này Blip đã đề xuất ra hai ý chính
00:01:24 - 00:01:32, Ý đầu tiên là Blip đã giới thiệu một mô hình multimodal mixture of encoder decoder
00:01:32 - 00:01:39, và với mục tiêu đó là có thể giải quyết được các bài toán liên quan đến loại hình sinh văn bản
00:01:39 - 00:01:41, tức là Text Generation
00:01:41 - 00:01:49, Và một cái Capsule, tức là bộ dữ liệu mới được tạo ra từ hai cái module nhỏ
00:01:49 - 00:01:55, Capsule là sinh ra cho câu mô tả cho hình ảnh
00:01:55 - 00:01:58, và Modern Fill là viết tắc chữ filter
00:01:58 - 00:02:03, thì là loại bỏ những câu mô tả sai hoặc là câu mô tả nhỉu
00:02:03 - 00:02:11, từ cả hai cái câu được mô tả được tạo sinh và câu mô tả được lấy từ web
00:02:11 - 00:02:17, tức là ở đây chúng ta sẽ có hai câu mô tả với một tấm ảnh
00:02:17 - 00:02:22, Với một tấm ảnh ở đây thì chúng ta sẽ có một câu mô tả lấy từ internet, tức là từ web
00:02:22 - 00:02:29, và một câu mô tả nữa lấy từ module caption, image caption ở đây
00:02:29 - 00:02:35, Thì qua cái module filter thì chúng ta sẽ lọc lại là
00:02:35 - 00:02:43, ví dụ như trong tình huống này là chúng ta sẽ bỏ đi câu blue sky, bakery in sunset path
00:02:43 - 00:02:46, tại vì những thông tin này không liên quan đến nội dung ảnh
00:02:46 - 00:02:57, Trong khi đó chocolate cake with cream frosting liên quan đến nội dung của hình ảnh này
00:02:57 - 00:02:59, và nó sẽ được giữ lại
00:02:59 - 00:03:02, thì đây có lẽ là một trong những đóng góp lớn của clip
00:03:02 - 00:03:05, Vậy thì kiến trúc và mô hình của clip đó là gì?
00:03:05 - 00:03:14, Đóng góp đầu tiên là multi-module mixture of encoder-decoder
00:03:14 - 00:03:18, là một mô hình có khả năng thực hiện cùng lúc 3 chức năng
00:03:18 - 00:03:21, Chức năng đầu tiên là để mã hóa những dữ liệu đơn lẽ
00:03:21 - 00:03:28, tức là chúng ta sẽ tạo ra các đặc trưng biểu diễn
00:03:28 - 00:03:33, các representation vector của ảnh và văn bản một cách độc lập
00:03:33 - 00:03:37, Thì nó thông qua cái module ITC
00:03:37 - 00:03:44, Và cái module ITC này thì nó sẽ lấy thông tin caption của hình ảnh
00:03:44 - 00:03:46, và đưa ra cái vector biểu diễn
00:03:46 - 00:03:51, Đây chính là cái vector biểu diễn của ảnh
00:03:55 - 00:04:00, Rồi, còn ở đây sẽ là cái vector biểu diễn của văn bản
00:04:08 - 00:04:11, Và ở đây chúng ta thấy trong cái sơ đồ này
00:04:11 - 00:04:13, chúng ta thấy có cái module cross-attention
00:04:13 - 00:04:16, nhưng mà được làm mờ và gạch sọc đi
00:04:16 - 00:04:19, thì hàm ý đó là chúng ta sẽ không có sự tương tác
00:04:19 - 00:04:22, giữa hình ảnh với văn bản
00:04:22 - 00:04:26, là hàm ý đó là để cho hai cái loại đặc trưng này độc lập nhau
00:04:26 - 00:04:31, để sau này khi chúng ta huấn luyện cái mô hình này xong
00:04:31 - 00:04:34, thì chúng ta có thể sử dụng hai cái module này
00:04:34 - 00:04:40, như là hai cái emitting module cho hình ảnh riêng và cho văn bản riêng
00:04:40 - 00:04:43, tức là chúng ta sẽ cho hình ảnh vào và nó sẽ ra vector biểu diễn
00:04:43 - 00:04:48, nó không cần có sự can thiệp của một cái module văn bản nào khác
00:04:48 - 00:04:51, và vượt lại chúng ta có thể đưa văn bản vào
00:04:51 - 00:04:53, nó sẽ ra cái vector biểu diễn
00:04:53 - 00:04:59, và chúng ta cũng không cần thiết phải đưa ảnh vào để có thể biểu diễn
00:04:59 - 00:05:06, thì cái module này, cái ITC này nó khá là tương đồng với lại cái mô hình clip
00:05:06 - 00:05:08, tức là nó độc lập
00:05:08 - 00:05:14, cái mà nó có sự fusion có chăng nó sẽ là nằm ở cái bước cuối cùng
00:05:14 - 00:05:15, đó là cái ITC
00:05:15 - 00:05:18, mục tiêu của cái ITC này là để mapping
00:05:18 - 00:05:21, để xem coi cái sự tương đồng giữa hình ảnh với văn bản này
00:05:21 - 00:05:23, là có giống nhau hay không
00:05:24 - 00:05:29, thì cái clip này, cái module ITC này nó tương đương với lại cái clip của mình
00:05:29 - 00:05:34, và cái module thứ 2 đó là mã hóa hình ảnh văn bản
00:05:34 - 00:05:42, thì ở đây chúng ta thấy là bên trái là cross attention là không có được sử dụng
00:05:42 - 00:05:45, nhưng mà trong cái module ITM
00:05:45 - 00:05:52, là image route text encoder thì chúng ta có cái sự đưa cái nội dung của hình ảnh vào
00:05:53 - 00:05:55, để thực hiện cái cross attention
00:05:55 - 00:06:00, thì mục tiêu của cái ITM này là cho cái bài toán đó là captioning
00:06:00 - 00:06:02, là cái bài toán captioning
00:06:02 - 00:06:07, tức là với một tấm ảnh nó sẽ có cái caption mô tả cái tấm ảnh đó
00:06:07 - 00:06:15, và module này thì cũng tương tự như cái ITC tức là nó sẽ nhận cái đầu vào là cái câu mô tả của mình
00:06:16 - 00:06:20, rồi sau đó nó có một cái module là B-Self Attention
00:06:20 - 00:06:23, bí tắc của chữ B-Directional Self Attention
00:06:23 - 00:06:30, mục tiêu đó là chúng ta có thể tương tác được cái dữ liệu text theo 2 chiều
00:06:30 - 00:06:34, tức là chúng ta sẽ đọc từ trái sang phải và từ phải sang trái
00:06:34 - 00:06:36, nó cũng giống giống như cái mô hình bird
00:06:36 - 00:06:38, nó cũng giống như cái mô hình bird
00:06:38 - 00:06:41, trong cái việc đó là encode dữ liệu văn bản
00:06:42 - 00:06:48, và module cuối cùng đó chính là cái giải mã hình ảnh văn bản Image-Row Text Decoder
00:06:48 - 00:06:53, phục vụ cho các cái bài toán mà có tính chất là tạo sinh ra văn bản
00:06:53 - 00:07:00, thế thì cái sự khác biệt giữa tạo sinh văn bản với lại cái Image-Row Text Decoder đó là gì
00:07:00 - 00:07:03, đối với cái module ở giữa tức là ITM
00:07:03 - 00:07:08, mục đích chính của nó đó chỉ là để hiểu cái nội dung hình ảnh ở đây
00:07:08 - 00:07:11, tức là toàn bộ những cái nội dung văn bản chúng ta sử dụng ở đây
00:07:11 - 00:07:15, thì nó sẽ là mô tả cho cái tấm ảnh ở đây
00:07:15 - 00:07:21, nhưng đối với cái bài toán tạo sinh văn bản thì cái mục đích của nó đó là nó hướng đến
00:07:21 - 00:07:24, là chúng ta sẽ trả lời câu hỏi
00:07:24 - 00:07:27, chúng ta có thể hỏi đáp trò chuyện với tấm ảnh
00:07:27 - 00:07:29, nói chung là các cái tác vụ nâng cao
00:07:29 - 00:07:33, không có đơn giản chỉ là mô tả tấm ảnh thôi
00:07:33 - 00:07:35, thì đây là cho Text Generation
00:07:36 - 00:07:43, và đây chính là cái giải pháp để giải quyết vấn đề chúng ta nói ở trước
00:07:43 - 00:07:45, đó là ở khi cạnh môn hình
00:07:45 - 00:07:49, thì kiến trúc encoder-decoder nó ít phù hợp cho bài toán sinh văn bản
00:07:49 - 00:07:53, thì để giải quyết cái đó thì cái module chính của chúng ta chính là cái module này
00:07:53 - 00:07:57, và cái chữ LLM này là viết chắc tắc của Language Model
00:07:57 - 00:07:59, tức là nó sẽ nằm trong cái nhánh là Decode
00:07:59 - 00:08:02, nó tương đương với một cái decoder
00:08:03 - 00:08:06, và ở trong cái hình bên tay phải ở đây
00:08:06 - 00:08:09, nó chính là cái sơ đồ Mask
00:08:09 - 00:08:13, để thể hiện cái việc là chúng ta có được phép thấy
00:08:13 - 00:08:15, cái nội dung ở đằng trước đó hay không
00:08:15 - 00:08:17, Mask tức là không được thấy
00:08:17 - 00:08:19, còn Unmask là được thấy
00:08:19 - 00:08:22, thì đối với cái Image Text Contrastive Learning
00:08:22 - 00:08:25, tức là cái module đầu tiên của mình
00:08:25 - 00:08:26, module đầu tiên là ITC
00:08:26 - 00:08:29, thì nó sẽ tương tự như clip
00:08:29 - 00:08:34, tức là hình ảnh
00:08:34 - 00:08:35, là nó nằm trong cái query
00:08:35 - 00:08:38, hình ảnh và text
00:08:38 - 00:08:41, cái query và cái text
00:08:41 - 00:08:45, thì nó sẽ không có được phép thấy nhau
00:08:45 - 00:08:46, nó sẽ không được phép thấy nhau
00:08:46 - 00:08:49, tức là query thì chỉ được thấy query
00:08:49 - 00:08:50, không được thấy text
00:08:50 - 00:08:53, text thì cũng tương tự như vậy
00:08:53 - 00:08:56, text thì chỉ được thấy text chứ không được thấy query
00:08:57 - 00:08:58, rồi
00:08:58 - 00:09:00, và đối với cái module là
00:09:00 - 00:09:02, Image Text Matching
00:09:02 - 00:09:04, tức là tương ứng cái ở giữa
00:09:04 - 00:09:06, thì chúng ta được phép thấy hết
00:09:06 - 00:09:08, tại vì cả hai cái module này
00:09:08 - 00:09:11, nó đều nằm trong cái nhóm là encoder
00:09:11 - 00:09:12, ở đây cũng là encoder
00:09:14 - 00:09:15, từ đây qua đúng không
00:09:15 - 00:09:16, thì đây cũng là encoder
00:09:16 - 00:09:17, Image Encoder
00:09:17 - 00:09:19, mà trong encoder thì
00:09:19 - 00:09:22, các cái module của mình là được phép thấy
00:09:22 - 00:09:23, được phép thấy
00:09:23 - 00:09:25, còn cái module ở giữa
00:09:25 - 00:09:26, ờm
00:09:27 - 00:09:30, là Image Rail Text Generation
00:09:30 - 00:09:32, thì tương ứng là cái module cuối
00:09:32 - 00:09:35, tức là chúng ta sẽ không được phép thấy
00:09:35 - 00:09:37, những cái từ phía sau
00:09:37 - 00:09:39, mà chúng ta chỉ được phép thấy những từ phía trước
00:09:39 - 00:09:41, chỉ được phép thấy những từ phía trước
00:09:41 - 00:09:44, thì đây chính là cái sự khác biệt
00:09:44 - 00:09:46, trong cái cách mà mình
00:09:46 - 00:09:48, nhìn thấy cái thông tin
00:09:48 - 00:09:51, ở trong cái kiến trúc của Transformer
00:09:51 - 00:09:52, rồi
00:09:52 - 00:09:54, các cái token ở đây thì được sử dụng
00:09:54 - 00:09:58, được sử dụng cho các cái mục đích khác nhau
00:10:00 - 00:10:03, đối với cái module về Image Encoder
00:10:03 - 00:10:06, thì ở đây chúng ta sẽ sử dụng là
00:10:06 - 00:10:08, mô hình VIT
00:10:08 - 00:10:12, còn đối với cái Text Encoder
00:10:12 - 00:10:14, thì chúng ta sẽ sử dụng là
00:10:14 - 00:10:15, mô hình BERT
00:10:15 - 00:10:17, thì cái quá trình huấn luyện
00:10:17 - 00:10:18, của mô hình BERT
00:10:18 - 00:10:19, đó là
00:10:19 - 00:10:20, huấn luyện được thực hiện đồng thời
00:10:20 - 00:10:22, với 3 hà mục tiêu
00:10:22 - 00:10:24, và
00:10:24 - 00:10:25, bộ dữ liệu của mình
00:10:25 - 00:10:26, Bootstrapping
00:10:26 - 00:10:28, thì sẽ sử dụng cho cái việc là
00:10:28 - 00:10:30, tiền huấn luyện
00:10:30 - 00:10:32, rồi module Captioner và Filter
00:10:32 - 00:10:32, thì được phai tune
00:10:32 - 00:10:35, trên cái tập dữ liệu gán nhãn thủ công
00:10:35 - 00:10:37, thì cái cách thực hiện đó là
00:10:37 - 00:10:38, chúng ta sẽ có
00:10:38 - 00:10:41, sau khi chúng ta đã huấn luyện cái mô hình
00:10:41 - 00:10:43, multimodal mixture of decoder
00:10:43 - 00:10:45, thì chúng ta sẽ tiến hành
00:10:45 - 00:10:47, sử dụng các cái bộ dữ liệu
00:10:47 - 00:10:49, nhưng mà cái bộ dữ liệu gán nhãn này
00:10:49 - 00:10:51, thì nó rất là hạn chế
00:10:51 - 00:10:53, nó rất là hạn chế so với lại những bộ dữ liệu
00:10:53 - 00:10:55, trên internet
00:10:55 - 00:10:57, là vi tắc cửa chữ T H H là Human
00:10:57 - 00:10:59, và sau đó thì chúng ta sẽ huấn luyện cho
00:10:59 - 00:11:01, dùng cái dữ liệu gán nhãn này
00:11:01 - 00:11:03, thì chúng ta sẽ huấn luyện cho Captioner và Filter
00:11:03 - 00:11:05, sau đó thì
00:11:05 - 00:11:07, chúng ta sẽ từ Captioner
00:11:07 - 00:11:09, chúng ta sẽ tạo ra
00:11:09 - 00:11:11, một cái dữ liệu là Synthesis
00:11:11 - 00:11:13, Synthesis
00:11:13 - 00:11:15, tức là S, vi tắc chữ S
00:11:15 - 00:11:17, thì đây là cái mô tả cho tấm ảnh này
00:11:17 - 00:11:19, nhưng mà được tạo ra
00:11:19 - 00:11:21, bởi module Captioner
00:11:21 - 00:11:23, và cái này thì chúng ta để màu đỏ là
00:11:23 - 00:11:25, có thể sai
00:11:25 - 00:11:27, chúng ta có thể sai
00:11:27 - 00:11:29, do đó thì chúng ta sẽ sử dụng
00:11:29 - 00:11:31, cái module Filter ở đây
00:11:31 - 00:11:33, với cái image
00:11:33 - 00:11:35, chúng ta sẽ đưa cái module Filter
00:11:35 - 00:11:37, bằng cách đó là
00:11:37 - 00:11:39, chúng ta sẽ sử dụng cái module Filter
00:11:39 - 00:11:41, bằng cách đó là đưa cái T W và T S
00:11:41 - 00:11:43, T W, tức là
00:11:43 - 00:11:45, W là vi tắc của chữ Web
00:11:45 - 00:11:47, là cái Caption
00:11:47 - 00:11:49, lấy từ mạng internet về
00:11:49 - 00:11:51, và có thể đúng có thể sai
00:11:51 - 00:11:53, cái Synthesis này
00:11:53 - 00:11:55, cái TextSynthesis này thì cũng có thể đúng thể sai
00:11:55 - 00:11:57, chúng ta sẽ đưa vào để Filter
00:11:57 - 00:11:59, để được là TextWeb và TextSynthesis
00:11:59 - 00:12:01, được gán màu xanh
00:12:01 - 00:12:03, tức là những cái nội dung mà đúng
00:12:03 - 00:12:05, những cái nội dung mà đúng với
00:12:05 - 00:12:07, nội dung hình ảnh
00:12:07 - 00:12:09, sau đó thì chúng ta sẽ tổng hợp
00:12:09 - 00:12:11, với lại cái dữ liệu Human
00:12:11 - 00:12:13, Human Annotation
00:12:13 - 00:12:15, dữ liệu Synthesis và dữ liệu trên Web
00:12:15 - 00:12:17, để chúng ta được cái Data Set D
00:12:17 - 00:12:19, thì cái Data Set D này nó sẽ vừa
00:12:19 - 00:12:21, có cái tính, đó là cái Scale
00:12:21 - 00:12:23, tỷ lệ của nó rất là lớn
00:12:23 - 00:12:25, số lượng rất là nhiều, đồng thời là dữ liệu
00:12:25 - 00:12:27, nó sạch hơn
00:12:27 - 00:12:29, và đây cũng là một trong những cái đóng gấp chính
00:12:29 - 00:12:31, của bài báo này để cho cái mô hình của mình
00:12:31 - 00:12:33, có thể huấn luyện một cách thiệu quả
00:12:33 - 00:12:35, và cái kết quả của module CarveFuel
00:12:35 - 00:12:37, ví dụ như ở trong hình đây
00:12:37 - 00:12:39, thì chúng ta thấy là cái dữ liệu
00:12:39 - 00:12:41, dữ liệu lấy từ Web về
00:12:41 - 00:12:43, thì là From Bridge Near My House
00:12:43 - 00:12:45, thì ở đây có thể là cái nội dung này
00:12:45 - 00:12:47, nó đúng
00:12:47 - 00:12:49, nó đúng về mặt, là Context là
00:12:49 - 00:12:51, cái này nó sẽ chụp
00:12:51 - 00:12:53, từ trên cầu gần
00:12:53 - 00:12:55, cái nhà của cái người này
00:12:55 - 00:12:57, nhưng mà trong cái tống hình này nó không có
00:12:57 - 00:12:59, thể hiện cái cây nào, cầu nào
00:12:59 - 00:13:01, và không có thể hiện cái ngôi nhà nào
00:13:01 - 00:13:03, do đó thì ở đây là Scythe
00:13:03 - 00:13:05, rồi, còn nếu mà Synthesize thì
00:13:05 - 00:13:07, nó sẽ là có màu xanh, tức là
00:13:07 - 00:13:09, Hopper, chúng ta thấy có chìm
00:13:09 - 00:13:11, Flying Over a Lake
00:13:11 - 00:13:13, at Sunset
00:13:13 - 00:13:15, thì đây là cái nội dung đúng
00:13:15 - 00:13:17, nó sẽ giữ lại
00:13:17 - 00:13:19, tương tự như vậy, ở phía cuối thì chúng ta sẽ thấy
00:13:19 - 00:13:21, cũng có tình huống đó là cái Synthesize Text này là
00:13:21 - 00:13:23, A Last Building of a Lot
00:13:23 - 00:13:25, with a Lot of Windows on it
00:13:25 - 00:13:27, thì ở đây là
00:13:27 - 00:13:29, Scythe, nó nhầm cái này là cái cửa sổ
00:13:29 - 00:13:31, thì cũng sẽ là
00:13:31 - 00:13:33, được bỏ đi
00:13:33 - 00:13:35, như vậy thì với cái phương pháp CarveFuel
00:13:35 - 00:13:37, nó đã giúp chúng ta tạo ra
00:13:37 - 00:13:39, một cái bộ dữ liệu vô cùng lớn
00:13:39 - 00:13:41, và được
00:13:41 - 00:13:43, lọc lại
00:13:43 - 00:13:45, một cách xả xẻ để phục vụ cho cái việc
00:13:45 - 00:13:47, huấn luyện nó thiệt vả hơn
00:13:47 - 00:13:49, và ứng dụng
00:13:49 - 00:13:51, cho các cái bài toán
00:13:51 - 00:13:53, cụ thể thì Blip
00:13:53 - 00:13:55, được sử dụng rất là nhiều cho các cái bài toán
00:13:55 - 00:13:57, ví dụ như là bài toán VQA
00:13:57 - 00:13:59, là hỏi đáp trên hình ảnh
00:13:59 - 00:14:01, thì cái kiến trúc chung chúng ta có thể sử dụng
00:14:01 - 00:14:03, đó là chúng ta đưa vào một cái tấm ảnh
00:14:03 - 00:14:05, và cái encoder
00:14:05 - 00:14:07, rồi chúng ta lấy cái vector biểu diễn
00:14:07 - 00:14:09, sau đó chúng ta lại tiếp tục encode
00:14:09 - 00:14:11, đưa vào cái module là
00:14:11 - 00:14:13, questionencoder
00:14:13 - 00:14:15, thì chúng ta sẽ ra được
00:14:15 - 00:14:17, cái vector biểu diễn cho
00:14:17 - 00:14:19, cả hình ảnh và câu hỏi
00:14:19 - 00:14:21, sau đó lấy cái vector biểu diễn
00:14:21 - 00:14:23, của cả hình ảnh câu hỏi này
00:14:23 - 00:14:25, chúng ta sẽ qua cái module decoder
00:14:25 - 00:14:27, đây chính là cái module là lm
00:14:27 - 00:14:29, trong cái slide trước của mình
00:14:29 - 00:14:31, thì nó sẽ tạo ra cái
00:14:31 - 00:14:33, cái câu trả lời
00:14:33 - 00:14:35, và ứng dụng trong cái bài toán là
00:14:35 - 00:14:37, hội thoại hình ảnh thì chúng ta sẽ
00:14:37 - 00:14:39, đưa vào cái imageencoder
00:14:39 - 00:14:41, và độc lập với lại cái
00:14:41 - 00:14:43, caption
00:14:43 - 00:14:45, rồi chúng ta sẽ đưa vào một cái
00:14:45 - 00:14:47, câu hỏi đáp
00:14:47 - 00:14:49, hoặc là một cái hội thoại
00:14:49 - 00:14:51, hoặc là một câu hỏi v.v.
00:14:51 - 00:14:53, và độc lập
00:14:53 - 00:14:55, và nó sẽ tạo ra một cái
00:14:55 - 00:14:57, vector biểu diễn sau đó đem cả hai
00:14:57 - 00:14:59, cái vector biểu diễn này qua cái module
00:14:59 - 00:15:01, dialogencoder thì cái này
00:15:01 - 00:15:03, có thể là một cái module chúng ta sẽ phải nguyên luyện thêm
00:15:03 - 00:15:05, để trả lời cho cái
00:15:05 - 00:15:07, tài là true hay là false
00:15:07 - 00:15:09, tức là cái image và cái caption này là đúng
00:15:09 - 00:15:11, có khớp hay nhau hay không
00:15:11 - 00:15:13, rồi đối với cái bài toán là
00:15:13 - 00:15:15, suy luận trên hình ảnh và ngôn ngữ
00:15:15 - 00:15:17, là
00:15:17 - 00:15:19, natural language visual reasoning
00:15:19 - 00:15:21, thì chúng ta
00:15:21 - 00:15:23, cũng sử dụng cái imageencoder
00:15:23 - 00:15:25, và
00:15:25 - 00:15:27, đã được học ở trong
00:15:27 - 00:15:29, cái mô hình clip
00:15:29 - 00:15:31, đã được huấn luyện sẵn trong mô hình clip
00:15:31 - 00:15:33, sau đó chúng ta sẽ đi kết hợp
00:15:33 - 00:15:35, với lại một cái module này
00:15:35 - 00:15:37, để huấn luyện với các cái
00:15:37 - 00:15:39, module cross attention
00:15:39 - 00:15:41, để mà huấn luyện cho cái
00:15:41 - 00:15:43, module cross attention
00:15:43 - 00:15:45, để mà trả lời cho cái câu hỏi
00:15:45 - 00:15:47, đúng sai, cái reasoning này là
00:15:47 - 00:15:49, đúng hay sai và nó có thể
00:15:49 - 00:15:51, kết hợp nhiều loại ảnh với nhau chứ không phải là
00:15:51 - 00:15:53, chỉ cho một hình ảnh
00:15:53 - 00:15:55, như vậy thì chúng ta đã qua
00:15:55 - 00:15:57, cái mô hình clip ở phiên bản đầu tiên
00:15:57 - 00:15:59, và
00:15:59 - 00:16:01, phiên bản số 2 thì clip
00:16:01 - 00:16:03, nó đã có cái cải tiến
00:16:03 - 00:16:05, chính, đó chính là
00:16:05 - 00:16:07, nó sử dụng các cái pre-trained model rất là mạnh
00:16:07 - 00:16:09, ví dụ như
00:16:09 - 00:16:11, module imageencoder
00:16:11 - 00:16:13, và languagemodel
00:16:13 - 00:16:15, languagemodel
00:16:15 - 00:16:17, thì đây chính là hai cái module mà chúng ta sẽ không
00:16:17 - 00:16:19, có đi file tune lại mà
00:16:19 - 00:16:21, chúng ta sẽ đóng băng, nó còn gọi là
00:16:21 - 00:16:23, frozen, chúng ta đóng băng nó đi
00:16:23 - 00:16:25, dì do đó là vì cái hai cái module này
00:16:25 - 00:16:27, nó rất là mạnh và nó được trend từ
00:16:27 - 00:16:29, những cái lag gọi là data set
00:16:29 - 00:16:31, một cái internet scale data set trước đó
00:16:31 - 00:16:33, nên cái việc huấn luyện lại
00:16:33 - 00:16:35, là không cần thiết và nếu có
00:16:35 - 00:16:37, thì cũng có thể gây ra cái hiện tượng là
00:16:37 - 00:16:39, overfitting do đó
00:16:39 - 00:16:41, chúng ta chỉ tập trung để huấn luyện
00:16:41 - 00:16:43, cái mô hình là
00:16:43 - 00:16:45, weakformer của mình thôi
00:16:45 - 00:16:47, thì ở đây nó sẽ có hai giai đoạn
00:16:47 - 00:16:49, giai đoạn 1, đó là chúng ta sẽ huấn luyện
00:16:49 - 00:16:51, cái module weakformer
00:16:51 - 00:16:53, tức là cái module này, thì ở bên đây
00:16:53 - 00:16:55, đó là cái mô hình phóng to
00:16:55 - 00:16:57, thì mục đích của cái module weakformer này
00:16:57 - 00:16:59, đó là để trích xuất cái thông tin ảnh
00:16:59 - 00:17:01, ở bên đây
00:17:01 - 00:17:03, nó sẽ trích xuất ra cái thông tin ảnh
00:17:03 - 00:17:05, thì đầu vào của mình chúng ta
00:17:05 - 00:17:07, sẽ phải đưa vào một cái learn query
00:17:07 - 00:17:09, và cái câu caption
00:17:09 - 00:17:11, cái câu caption
00:17:11 - 00:17:13, và sau đó nó sẽ huấn luyện
00:17:13 - 00:17:15, các cái module cell attention, growth attention
00:17:15 - 00:17:17, feedforward v.v.
00:17:17 - 00:17:19, để mục đích đó là chúng ta
00:17:19 - 00:17:21, làm cái image text matching
00:17:21 - 00:17:23, rồi image text contrastive learning
00:17:23 - 00:17:25, thì đây giống như là cái module clip của mình
00:17:25 - 00:17:27, rồi image ground text generation
00:17:27 - 00:17:29, và giống như là cái module để mà
00:17:29 - 00:17:31, VQI
00:17:31 - 00:17:33, các cái task liên quan đến VQI
00:17:33 - 00:17:35, và cái task này liên quan đến cái captioning
00:17:35 - 00:17:37, nó kế thừa cái ý tưởng của clip 1
00:17:37 - 00:17:39, và sang giai đoạn 2 thì chúng ta
00:17:39 - 00:17:41, sẽ có một cái phần
00:17:41 - 00:17:43, tập trung để huấn luyện
00:17:43 - 00:17:45, và phần tập trung để huấn luyện
00:17:45 - 00:17:47, và phần tập trung để huấn luyện
00:17:47 - 00:17:49, và sang giai đoạn 2
00:17:49 - 00:17:51, thì chúng ta sẽ huấn luyện cả Weformer
00:17:51 - 00:17:53, và một cái projection layer
00:17:53 - 00:17:55, để liên kết cái ảnh vào ngôn ngữ
00:17:55 - 00:17:57, thì cái projection layer
00:17:57 - 00:17:59, nó chính là cái module này
00:18:03 - 00:18:05, thế thì ở giai đoạn 1 là
00:18:05 - 00:18:07, chúng ta chỉ có huấn luyện
00:18:07 - 00:18:09, một cái module Weformer thôi
00:18:09 - 00:18:11, và sang giai đoạn 2 thì chúng ta sẽ huấn luyện cho
00:18:11 - 00:18:13, cả hai module
00:18:13 - 00:18:15, và cái cách thức chúng ta
00:18:15 - 00:18:17, huấn luyện đó là chúng ta đều
00:18:17 - 00:18:19, có thể tắt cái này ra
00:18:19 - 00:18:21, là 1, đó là chúng ta chỉ huấn luyện một cái
00:18:21 - 00:18:23, LM decoder
00:18:23 - 00:18:25, chúng ta chỉ sử dụng một cái LM decoder
00:18:25 - 00:18:27, một cái cách thứ 2 đó là chúng ta
00:18:27 - 00:18:29, sẽ phải kết hợp thêm cả cái
00:18:29 - 00:18:31, LM encoder
00:18:31 - 00:18:33, và cái module là LM decoder
00:18:33 - 00:18:35, thì đối với cái module
00:18:35 - 00:18:37, LM, cả hai cái
00:18:37 - 00:18:39, cái cách thức này nó sẽ giúp cho chúng ta
00:18:39 - 00:18:41, giải quyết trên những cái bài toán khác nhau
00:18:45 - 00:18:47, Hãy subscribe cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video hấp dẫn
