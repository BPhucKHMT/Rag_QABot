00:00:00 - 00:00:18, Chắc hồi trước thì chúng ta đã cùng tìm hiểu về kiến trúc autoencoder
00:00:18 - 00:00:25, Bây giờ thì chúng ta sẽ cùng tìm hiểu về một biến thể rất là quan trọng, đó chính là Variational Autoencoder
00:00:26 - 00:00:37, Trước hết thì chúng ta sẽ nhắc lại autoencoder là gì và chúng ta sẽ xem có những điểm gì cần phải cải tiến trong kiến trúc này
00:00:37 - 00:00:46, Đầu tiên đó là autoencoder thì sẽ bao gồm hai thành phần, đó là một encoder và một decoder
00:00:46 - 00:00:59, Trong đó, encoder, nhiệm vụ của nó là chúng ta sẽ nén dữ liệu và biểu diễn dữ liệu ở số chiều cao về một vector biểu diễn thấp chiều hơn
00:00:59 - 00:01:10, Nếu chỉ như vậy thì không được, chúng ta sẽ phải tái tạo lại được so với dữ liệu gốc ban đầu là x, chúng ta sẽ phải có một decoder
00:01:10 - 00:01:20, Với decoder này thì nó sẽ giúp chúng ta liên kết về mặt tử nghĩa giữa vector z và dữ liệu gốc ban đầu x
00:01:20 - 00:01:34, Chúng ta tưởng tượng là trong một không gian latent, giống như ở đây thì ảnh của mình sẽ được map vào một không gian và ở đây sẽ là vector z
00:01:34 - 00:01:40, Vector z này qua decoder sẽ khôi phục ngược lại về cảnh gốc ban đầu
00:01:40 - 00:01:46, Câu hỏi đặt ra đó là bây giờ chúng ta sẽ dùng autoencoder này để làm gì?
00:01:46 - 00:01:55, Rõ ràng các mô hình tạo sinh được sử dụng để sinh hình ảnh, do đó chúng ta sẽ tiến hành sinh hình ảnh
00:01:55 - 00:02:08, Ví dụ chúng ta có một vector ở đây và chúng ta sẽ qua hàm decoder, vi tắc chữ D, thì nó sẽ tạo ra thành một tấm hình
00:02:08 - 00:02:23, Với kinh trúc encoder thì nó không có gì đảm bảo rằng ảnh sau khi chúng ta tái tạo lại có thể là một ảnh có ý nghĩa
00:02:23 - 00:02:35, Đó là ý thứ nhất. Ý thứ hai là một điều rất quan trọng, một vector z phải nằm ở gần z trong không gian latent space
00:02:35 - 00:02:54, Khi chúng ta decoder ra thì liệu có gì đảm bảo rằng ảnh mà chúng ta qua decoder phải có tính chất gì đó giống với vector z ở đây không?
00:02:54 - 00:03:08, Tại vì hai vector diễn z và z phải ở trong không gian của mình nằm gần nhau, chúng ta kỳ vọng cái số giải mã ra được là giống như số 3
00:03:08 - 00:03:17, Chứ không thể nào nếu vector z phải gần z, nhưng nếu chúng ta decode mà nó ra số 7 chẳng hạn
00:03:18 - 00:03:27, Thì đây là điều mà chúng ta không mong muốn. Chúng ta mong muốn là z phải gần z, thì khi decode nó phải ra con số giống với con số z này
00:03:27 - 00:03:34, Thì đó mới đúng là một cái không gian latent. Với autoencoder nó không có đảm bảo được cái chuyện đó
00:03:34 - 00:03:44, Nếu như vector z sau khi tôi đã encode, thì khi tôi decode nó sẽ ra lại đúng ảnh ban đầu
00:03:44 - 00:03:54, Nó thiếu đi một tính chắc chắn và tính liên quan trong không gian của mình. Đó là những điểm gần nhau sẽ có cùng một nữ nghĩa giống nhau
00:03:54 - 00:04:04, Và đó chính là điểm yếu của autoencoder và variational autoencoder sẽ tìm cách giải quyết cái bến đề này
00:04:04 - 00:04:14, Đó là thay vì với mỗi một cái ảnh khi chúng ta encode thì cũng có một cái module là encode
00:04:14 - 00:04:30, Đó là trong không gian latent. Tuy nhiên thay vì chúng ta ánh sạ sang một điểm cố định giống như trong autoencoder
00:04:30 - 00:04:40, Thì ở đây cái mà chúng ta sẽ ánh sạ sang đó chính là một distribution, một cái phân bố
00:04:40 - 00:04:50, Thế thì tại sao lại là phân bố mà không phải là một điểm cố định? Tại vì nếu chúng ta ánh sạ sang một điểm cố định
00:04:50 - 00:04:58, Thì nó sẽ dễ khiến cái mô hình của mình là học thuộc cái vị trí, tức là cứ ảnh đó thì vị trí đó, ảnh đó thì vị trí đó
00:04:58 - 00:05:10, Mà nó không có cái tính chắc tổng quát. Thế thì dẫn đến là khi chúng ta có một cái điểm nào đó gần gần với điểm gì
00:05:10 - 00:05:16, Thì khi chúng ta tạo sinh ra nó không ra cái con số giống con số 3 này. Trong khi đó nếu chúng ta ánh sạ sang một cái phân bố
00:05:16 - 00:05:24, Vì nó là một cái phân bố nên khi chúng ta lấy mẫu ngộ nhiên một cái vector z nào đó để chúng ta thực hiện cái việc tái tạo lại
00:05:24 - 00:05:30, Thì cái z này nó có thể nằm ở đây, nó có thể nằm ở đây, nó có thể nằm ở đây, nó có thể nằm ở đây
00:05:30 - 00:05:40, Nói chung là xung quanh cái giá trị min này, thì cái vector z này khi chúng ta decode ra nó cũng đều có khả năng là tạo ra được cái số 3 như bạn đọc
00:05:40 - 00:05:54, Thì chính nhờ cái yếu tốt ngộ nhiên nó sẽ khiến cho cái mô hình của mình không có học thuộc
00:05:54 - 00:06:07, Và từ đó là nó sẽ tổng quát hơn và nó thoả mảng được tính chắc mà chúng ta mong muốn đó là hai vector gần nhau
00:06:07 - 00:06:10, Thì khi chúng ta decode nó sẽ giống nhau
00:06:10 - 00:06:17, Thì khi ra một cái phân bố như thế này thì chúng ta random, chúng ta bốc ngộ nhiên một cái vector z
00:06:17 - 00:06:23, Theo cái phân bố mà chúng ta đã encode được là bao gồm hai thăm số là mi và sigma
00:06:23 - 00:06:30, Thì cái đường màu vàng này, ý nghĩa của nó đó chính là cái phép sampling là lấy mẫu
00:06:30 - 00:06:42, Và lấy mẫu với cái phân bố được đại diện bởi hai thăm số của cái phân bố đó là mi và sigma
00:06:42 - 00:06:47, Thì đây chính là cái sự khác biệt lớn nhất của autoencoder và decoder
00:06:47 - 00:06:57, Đó là thay vì chúng ta encode về một cái vector z cố định thì chúng ta sẽ đưa về một cái phân bố
00:06:57 - 00:07:00, Và từ cái phân bố này chúng ta mới bắt đầu đi lấy mẫu nó
00:07:00 - 00:07:03, Rồi sau đó cái vector z ở đây chúng ta decode
00:07:03 - 00:07:09, Và cái mục tiêu của nó cũng hoàn toàn tương tự như autoencoder
00:07:09 - 00:07:14, Thì cái module này là hoàn toàn tương tự
00:07:19 - 00:07:22, Hoàn toàn tương tự cái autoencoder
00:07:22 - 00:07:30, Rồi thì đây chính là cái ý tưởng của VAE
00:07:30 - 00:07:34, Thế thì chi tiết chúng ta sẽ đi đến những cái khái niệm
00:07:34 - 00:07:38, Thì mi ở đây là cái min vector
00:07:38 - 00:07:43, Thì chúng ta biết là trong một cái phân bố chuẩn hoặc là phân bố gauss
00:07:43 - 00:07:49, Thì nó sẽ có hai cái thăm số để biểu diễn cho cái phân bố gauss này
00:07:49 - 00:07:52, Một đó là mi là min vector
00:07:52 - 00:07:55, Và hai đó là standard deviation vector là sigma
00:07:55 - 00:08:01, Với mi và sigma thì chúng ta có thể tạo ra được cái phân bố gauss của mình
00:08:01 - 00:08:06, Cái variational autoencoder nó chính là một cái biến thể
00:08:06 - 00:08:10, Và biến thể này là mang tính sát suất
00:08:10 - 00:08:12, Đây là một cái biến thể mang tính sát suất của autoencoder
00:08:12 - 00:08:15, Sự khác biệt nó nằm ở chỗ này
00:08:15 - 00:08:17, Thay vì chúng ta đưa đến một cái vector z cố định
00:08:17 - 00:08:22, thì chúng ta sẽ đến một cái cặp giá trị đại diện cho 1 phần bố, đó là mi và sigma
00:08:22 - 00:08:25, rồi sau đó chúng ta sẽ đi lấy mẫu ngộ nhiên
00:08:25 - 00:08:29, và xoay xung quanh cái time số mi và sigma này
00:08:32 - 00:08:36, rồi, và cái việc lấy mẫu này thì từ giá trị trung bình và độ lật chuẩn
00:08:36 - 00:08:39, để chúng ta lấy cái mẫu tìm ẩn z
00:08:39 - 00:08:43, thì vector z này sẽ là cái vector tìm ẩn trong cái không gian latent của mình
00:08:43 - 00:08:48, thế thì cái giai đoạn tiếp theo của autoencoder đó chính là
00:08:48 - 00:08:54, chúng ta sẽ tối ưu hóa cái VAR này như thế nào
00:08:54 - 00:08:57, tức là chúng ta hướng luyện cái VAR này như thế nào
00:08:57 - 00:09:00, thì ở đây, đối với cái dây quá trình mã hóa
00:09:00 - 00:09:05, thì chúng ta sẽ ký hiệu bởi cái phân bố sát xuất đó là quý
00:09:05 - 00:09:10, tức là cho trước x, x là cái hảnh đầu vào
00:09:10 - 00:09:16, thì cái phân bố của z khi cho trước x thì nó được time số hóa bởi phi
00:09:16 - 00:09:20, thì nhờ có cái time số phi này nè
00:09:20 - 00:09:22, chúng ta mới có thể tính toán
00:09:22 - 00:09:25, từ x chúng ta sẽ tính toán ra được
00:09:25 - 00:09:29, qua các cái bước mà encode chúng ta sẽ tính toán ra được cái mi và sigma
00:09:29 - 00:09:32, thì đây là chính là time số của nguồn
00:09:32 - 00:09:40, ở cái quá trình dạy mã thì chúng ta sẽ có cái phân bố là p theta
00:09:41 - 00:09:48, khi cho trước x thì chúng ta sẽ có được cái phân bố của z
00:09:48 - 00:09:50, rồi
00:09:50 - 00:10:00, và cái hàm loss cuối cùng của VAR đó là l phi theta và là x
00:10:00 - 00:10:05, thì phi theta và x thì phi chính là cái time số của encoder
00:10:06 - 00:10:10, và theta chính là cái time số của decoder
00:10:10 - 00:10:16, và cái hàm loss này, cái hàm lỗi này sẽ bao gồm 2 thành phần
00:10:16 - 00:10:19, đó là size số tái tạo và thành phần chính quy
00:10:19 - 00:10:21, cái thành phần chính quy này nè
00:10:21 - 00:10:27, cái thành phần để giúp cho chúng ta đưa cái phân bố của mi và sigma về
00:10:27 - 00:10:29, đúng như cái phân bố mà chúng ta mong muốn
00:10:29 - 00:10:32, đó gọi là phân bố ti ngềm
00:10:32 - 00:10:38, Tiếp theo thì chúng ta sẽ tính cái size số tái tạo này
00:10:38 - 00:10:42, đó là cái size số được tính từ
00:10:42 - 00:10:48, bằng cách đó là so sánh cái x ban đầu với lại cái x mũ sau khi chúng ta đã decode
00:10:48 - 00:10:56, thì chúng ta có thể tính bằng cách là dùng log likelihood hoặc là dùng hàm độ lỗi bình phương
00:10:57 - 00:11:03, chúng ta sẽ lấy x trừ cho x mũ
00:11:03 - 00:11:08, tức là cái x sau khi chúng ta đã decode trừ cho cái giá trị ban đầu
00:11:08 - 00:11:12, rồi tất cả là lấy tổng bình phương các cái size số
00:11:12 - 00:11:14, thì đây là tổng bình phương
00:11:14 - 00:11:30, Tiếp theo thì liên quan đến cái thành phần chính quy
00:11:30 - 00:11:38, thì chúng ta sẽ đảm bảo rằng là cái phân bố của quy
00:11:38 - 00:11:46, với cho trước x thì cái phân bố của vector z cho trước x là cái phân bố quy
00:11:46 - 00:11:52, và cái phân bố quy này thì được tạo bởi cái tham số đó là phi
00:11:52 - 00:11:54, thì cái phân bố quy này nè
00:11:54 - 00:11:58, tức là đại diện cho được đợt đại diện bởi hai cái tham số mi và sigma nè
00:11:58 - 00:12:04, thì chúng ta mong muốn nó sắp xỉ với lại một cái phân bố ẩn tiềm nghiện
00:12:04 - 00:12:06, một cái phân bố ẩn tiềm nghiện là px
00:12:06 - 00:12:10, thì chúng ta sẽ chính quy hóa thì px nó là cái gì
