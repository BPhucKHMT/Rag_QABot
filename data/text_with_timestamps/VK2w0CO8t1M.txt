00:00:00 - 00:00:17, Trong phần trước chúng ta đã được tìm hiểu về momentum
00:00:17 - 00:00:25, Thì đây có thể nói là một trong những lý thuyết rất là quan trọng để làm tiền đề cho tất những cái biến thể biến thể về sau
00:00:25 - 00:00:28, Của các cái Optimizer, các cái thuật toán về tối ưu hóa
00:00:28 - 00:00:38, ý tưởng chính của Momentum đó là nó sẽ tận dụng được động năng của quá khứ để giúp cho chúng ta thoát ra khỏi điểm cực tiểu của bộ.
00:00:38 - 00:00:44, Tức là tại vị trí này thì thế năng, tức là đạo hàng của mình nó mất, nhưng mà bù lại nó sẽ tận dụng được những thành phần
00:00:44 - 00:00:53, và động năng ở phía trước, nó sẽ giúp cho chúng ta thoát ra và hy vọng là đến được những khu vực có cực tiểu tốt hơn.
00:00:53 - 00:01:03, Và vấn đề của momentum đó là gì? Đối với những khu vực có đội dốc bất thường thì ví dụ như chúng ta thấy trong sơ độ này
00:01:03 - 00:01:17, Cái phát thảo của HempLoss, chúng ta thấy là cái đội dốc của mình tăng lên, nó dốc xuống rất là cao, sau đó lập tức nó đi ngang, rồi sau đó nó lại lập tức đi lên
00:01:17 - 00:01:27, Tức là nó thay đổi trạng thái, đi xuống, đi ngang và sau đó là đi lên một cách rất là ngắn như vậy, thì đó chính là cái đội dốc bất thường
00:01:27 - 00:01:35, và nó sẽ khiến cho nó có rất nhiều những cái giao động mà bật qua bật lại, bật qua bật lại giữa hai cái thành giốc này.
00:01:35 - 00:01:39, Cụ thể hơn, đó là chúng ta xét tại một cái địa ma ở đây.
00:01:39 - 00:01:45, Thì nếu như cái thành phần mà theo cái theta 1 của mình, mà đạo hàm của mình nó lớn,
00:01:45 - 00:01:50, tức là cái giá trị độ lớn này, nó lớn hơn so với lại cái thành phần theo theta 2,
00:01:50 - 00:01:59, khi chúng ta tổng hợp lại là cái vector tổng hợp thì nó sẽ bị thiên lạch về phía có thành phần radian lớn
00:01:59 - 00:02:01, tức là cụ thể đây là theta 1
00:02:01 - 00:02:06, khi đó là nó sẽ không có cân bằng mà nó sẽ bật qua bên tay phải
00:02:06 - 00:02:13, sau đó tại vị trí này chúng ta lại tiếp tục tính cái radian và chúng ta lại thấy cái hiện tượng mất cân bằng này lập lại
00:02:13 - 00:02:22, thì khi chúng ta cập nhật thì nó cũng sẽ bị thiên lạc về cái phía mà có cái thành phần lớn hơn, đó chính là thành phần theta 1
00:02:22 - 00:02:30, tức là một khi cái đạo hàm của mình theo thành phần, một cái thành phần nào đó mà lớn thì nó sẽ gây ra cái hiện tượng là bật qua bật lại
00:02:30 - 00:02:39, trong khi đó cái đường tối ưu, ideal path thì lẽ ra nó phải là đường đường màu xanh ở đây, nó sẽ phải đi theo cái trục này
00:02:39 - 00:02:44, Nó phải đi theo cái trục này, hay là đi theo cái trục của theo cái hướng của theta, theta2
00:02:44 - 00:02:47, Thì ở đây nó lại cứ bật qua trái, bật qua phải
00:02:47 - 00:02:51, Thì bây giờ chúng ta sẽ cải tiến cái momentum bằng cách nào
00:02:51 - 00:02:56, Thì muốn cải tiến thì chúng ta sẽ phải xem cái nguyên nhân của nó là gì
00:02:56 - 00:03:02, Nguyên nhân đó là vì khi chúng ta có hai cái thành phần radian theo theta1 và theta2
00:03:02 - 00:03:09, Nếu thành phần nào đó lớn thì lẽ ra chúng ta sẽ phải giảm learning rate của nó xuống
00:03:09 - 00:03:21, Nguyên nhân đó là do chúng ta dùng chung learning rate alpha
00:03:21 - 00:03:35, dùng cho d, d là đạo hàm của J theo theta 1 và đạo hàm của J theo theta 2
00:03:35 - 00:03:40, một cách tổng quát thì nó có thể là có theta 3 theta n
00:03:40 - 00:03:48, thì alpha a đang dùng chung cho radian theo theta 1 và radian theo theta 2
00:03:48 - 00:03:56, Bây giờ chúng ta mong muốn mỗi thành phần này sẽ có 1 cái alpha riêng
00:03:56 - 00:04:03, Và nó sẽ giúp chúng ta cân bằng lại, đó chính là ý tưởng của kệ tiến adaptive learning rate
00:04:03 - 00:04:10, Giảm giao động dựa trên độ lớn của radian cập nhật gần đây, chúng ta giảm sự giao động đó
00:04:10 - 00:04:19, Y tưởng chính đó là, giả sử chúng ta có vector g là bằng hai thành phần, là g1 và g2
00:04:19 - 00:04:27, Nếu như chúng ta lấy alpha nhân với g, thì alpha mà dùng chung
00:04:27 - 00:04:34, Thì không, bây giờ chúng ta sẽ dùng ring, mỗi cái thành phần này sẽ có một cái alpha ring
00:04:34 - 00:04:40, trong đó, ở đây là ý tưởng đầu tiên là tắt riêng ha, tắt learning rate cho từng thêm số
00:04:40 - 00:04:45, và cái ý tưởng tiếp theo, đó là cái radian mà lớn thì learning rate nó sẽ nhỏ
00:04:45 - 00:04:51, tức là nếu cái thành phần theo G1 mà lớn, G2 mà nhỏ, ví dụ vậy
00:04:51 - 00:04:57, thì chúng ta sẽ có cái hệ số alpha 1 cân bằng ngược trở lại
00:04:57 - 00:05:00, nó cân bằng ngược trở lại
00:05:00 - 00:05:11, và thành phần G2 mà nhỏ thì chúng ta sẽ có cái Alpha 2
00:05:11 - 00:05:20, và khi đó thì cái vector tổng hợp của mình nó sẽ đều hơn thay vì là nó bị thiên lập như thế này
00:05:20 - 00:05:24, Đường màu đỏ là đường mà nó bị thiên lạc.
00:05:28 - 00:05:32, Nó bị lạc về phía theta, phía theta 1.
00:05:32 - 00:05:37, Rồi, bằng ngược lại thì thành phần theta 2 nhờ có cái alpha 2 lớn,
00:05:37 - 00:05:41, nó sẽ kéo ra để cho nó cân bằng cả hai hướng.
00:05:42 - 00:05:46, Và chi tiết thục toán Root Mean Square Propagation,
00:05:46 - 00:05:56, đó là chúng ta sẽ khởi tạo với cái alpha là bằng 0.01 và beta thì đây là cái hệ số decay rate
00:05:59 - 00:06:03, tức là cái hệ số mà để cập nhật cho cái momentum
00:06:03 - 00:06:09, thì chúng ta nhìn một cái công thức của cái thuật toán root mean square là
00:06:09 - 00:06:15, V tức là cái vận tốc cập nhật tham số của mình là cái thành phần ở phía trên mà chúng ta khoanh vùng ở đây
00:06:15 - 00:06:25, Các làm cũ của anh Pha là theta trừ anh Pha nhân cho Nephla J
00:06:25 - 00:06:32, Anh Pha là đây và đạo hàm radian là G
00:06:32 - 00:06:43, Cách làm củ là đạo hàm quá lớn, thành phần radian lớn sẽ lớn hơn những thành phần còn lại
00:06:43 - 00:06:57, R là momentum để phục vụ chuẩn hóa learning rate
00:06:57 - 00:07:04, Chưởng hóa Learning Ray, tức là thành phần nào của radian này mà lớn thì cái alpha của nó sẽ nhỏ
00:07:04 - 00:07:09, và thành phần nào mà của radian này mà nhỏ thì cái alpha của mình nó sẽ lớn
00:07:09 - 00:07:13, Đó, thì cái cách làm của chúng ta là như vậy
00:07:13 - 00:07:19, R này nó là một cái vector, chúng ta thấy là R được in động, nó là một cái vector
00:07:19 - 00:07:23, thì khi alpha chia cho canh của một vector thì nó sẽ biến thành một vector
00:07:23 - 00:07:27, thì chút nữa chúng ta sẽ ghi rõ hơn cái công thức của nó
00:07:27 - 00:07:33, Đây là hệ số tỷ lệ tùy chỉnh, tức là Adaptive Scaling Factor
00:07:33 - 00:07:38, Với mỗi thành phần của G, chúng ta sẽ có một cái alpha riêng
00:07:38 - 00:07:43, Tại sao nó là như vậy? Tại sao alpha chỉ có một cái giá trị Scaler là một giá trị?
00:07:43 - 00:07:47, Tại sao khi chia giao căn R nó lại biến thành hệ số tùy chỉnh?
00:07:47 - 00:07:50, Chúc lại chúng ta sẽ chứng minh chi tiết hơn
00:07:50 - 00:07:54, Mỗi time số sẽ có một cái hệ số tỷ lệ khác nhau
00:07:54 - 00:08:01, và ở đây chúng ta sẽ thấy là nó có một cái thành phần hơi lạ đó là epsilon là bằng một cái con số rất là bé
00:08:01 - 00:08:07, thì ở đây đó là để chống cho cái việc là có một cái thành phần r nào đó mà bằng 0
00:08:07 - 00:08:14, có một cái thành phần trong r nào đó mà bằng 0 thì khi đó chúng ta sẽ bị cái lỗi là division by r chia cho xuống 0
00:08:14 - 00:08:16, thì chúng ta sẽ cộng thêm epsilon để chống cái hiện tượng đó
00:08:16 - 00:08:24, Và phép toán mà r chia cho căn của epsilon cộng r được thực hiện trên từng phần tử
00:08:24 - 00:08:27, Bây giờ chúng ta sẽ cùng xem xét
00:08:27 - 00:08:34, Giá sử như z là bằng 2 thành phần là g1 và g2
00:08:34 - 00:08:41, Rồi, khi đó g-r là phép tích hà ra math trên từng phần tử
00:08:41 - 00:08:51, d-r-g là d-1 bình phương, d-2 bình phương
00:08:51 - 00:09:02, sau đó nó sẽ được cộng với thành phần chuẩn hóa ở phía quá khứ
00:09:02 - 00:09:09, đây là beta của quá khứ, tức là 90% thành phần r chuẩn hóa
00:09:09 - 00:09:22, R là thành phần để chúng ta chuẩn hóa learning rate theo kiểu đạo hàm thành phần nào mà càng nhỏ thì learning rate sẽ càng nhỏ
00:09:22 - 00:09:30, Trong công thức thành phần chuẩn hóa này chúng ta sẽ chia cho cái căn, thế thì tại sao nó lại có cái căn
00:09:30 - 00:09:35, Nếu chúng ta lại bỏ đi thành phần epsilon đây để cho nó dễ hình dung
00:09:35 - 00:09:40, epsilon này chỉ may tính chất đó là chống cái việc chia cho 0 thôi
00:09:40 - 00:09:44, còn nó sẽ không có nhiều kỷ nghĩa lắm trong việc truyền hóa
00:09:44 - 00:09:53, thì khi chúng ta lấy alpha mà chia cho căn của epsilon cộng cho r
00:09:53 - 00:09:55, thì nó sẽ xấp xỉ
00:09:55 - 00:09:58, tại vì có cái epsilon này nên mình mới để cái dấu xấp xỉ
00:09:58 - 00:10:06, đó là alpha chia cho căn của vector g1 bình g2 bình
00:10:06 - 00:10:08, cộng cho thành phần quá khứ
00:10:08 - 00:10:11, nhưng tại thời điểm ban đầu chúng ta thấy là r bằng 0
00:10:11 - 00:10:18, nên xem như chúng ta tạm bỏ qua thành phần này để chúng ta dễ hình dung khái niệm của việc trộn hóa này là gì
00:10:18 - 00:10:24, thì là g1 bình phương và g2 bình phương
00:10:24 - 00:10:29, Thì ở đây là chúng ta thực hiện phép căn là phép căn trên ElementWise
00:10:29 - 00:10:32, do đó nó sẽ là căn của các thành phần bên trong
00:10:32 - 00:10:35, như vậy thì nó sẽ là trị thiệt đối của G1
00:10:35 - 00:10:38, và trị thiệt đối của G2
00:10:38 - 00:10:45, Rồi, và vì đây là cái phép chia trên phép áp dụng trên từng phần tử
00:10:45 - 00:10:48, do đó thì cái này nó sẽ là bằng một cái vector
00:10:48 - 00:10:52, trong đó alpha chia cho trị thiệt đối của G1
00:10:52 - 00:10:56, và α chia cho trị thiệt đối của d2
00:10:56 - 00:11:02, như vậy thì chúng ta nhìn lại nếu như d1 của chúng ta mà lớn
00:11:02 - 00:11:10, nếu d1 mà lớn thì khi đó α chia cho d1
00:11:10 - 00:11:14, đó chính là tương ứng cái α1 của mình đề cập trong slide trước
00:11:14 - 00:11:18, thì α1 là bằng α chia cho trị thiệt đối d1
00:11:18 - 00:11:26, G1 lớn thì trị tiết đối của G1 sẽ lớn và 1 phần trị tiết đối của G1 sẽ nhỏ do đó thằng này sẽ là nhỏ
00:11:26 - 00:11:36, Ngược lại, nếu như G2 mà nhỏ thì khi đó là 1 phần trị tiết đối của G2 sẽ lớn do đó thành phần này sẽ lớn
00:11:36 - 00:11:39, như vậy là nó đáp ứng được yêu cầu mà chúng ta đã thiết kế băng đầu
00:11:39 - 00:11:44, thành phần gradient nào mà lớn thì learning rate sẽ nhỏ và ngược lại
00:11:44 - 00:11:51, Thì khi đó, đây chính là thành phần chấp chơi cho chúng ta chuộng hóa
00:11:51 - 00:11:57, Thì khi đó cái công thức V này của mình sẽ biến thành là V là bằng
00:11:57 - 00:12:06, alpha 1, tức là alpha chi cho trị thiệt đối của G1
00:12:06 - 00:12:08, nhân với lại G1
00:12:08 - 00:12:16, Thành phần thứ 2 sẽ là alpha chia cho trị thiệt đối của G2
00:12:16 - 00:12:27, Ngoài việc chia trộn hóa này sẽ khiến cho 2 thành phần gradient cân bằng hơn
00:12:27 - 00:12:35, Đây là ý tưởng để giúp chúng ta thoát ra khỏi vấn đề thiên lạc
00:12:35 - 00:12:38, thì chúng ta xét trong hình ở bên tay phải
00:12:39 - 00:12:45, ở đây chúng ta thấy là tại vị trí A thì nó bị thiên lật về hướng theta 1
00:12:45 - 00:12:53, nhưng mà nhờ có cái thành phần alpha 1 và alpha 2 trực chuẩn hóa ở đây
00:12:53 - 00:12:55, đây là alpha 1, đây là alpha 2
00:12:57 - 00:12:58, thì khi đó
00:12:58 - 00:13:08, Khi đó, vector tổng hợp của mình thay vì nó bị lệch về phía này thì nó sẽ đi đều hơn.
00:13:08 - 00:13:15, Và khi đó thì chúng ta sẽ vẽ lại. Thay vì nó bật qua phải, rồi lại bật qua trái theo một cái góc rất là gắt như thế này,
00:13:15 - 00:13:19, thì nó sẽ bật qua một cái góc là 4,5 độ.
00:13:19 - 00:13:23, Rồi, qua đây nó sẽ bật một cái góc 4,5 độ.
00:13:23 - 00:13:28, Rồi, qua đây nó sẽ bật một cái góc bấn 5 độ.
00:13:28 - 00:13:34, Thì chúng ta thấy là chỉ số bước mà nó di chuyển ít hơn nhiều so với đường màu đỏ này.
00:13:34 - 00:13:40, Thì cái đường màu xanh lá đã giúp chúng ta giảm bớt những cái hiện tượng bật qua bật lại.
00:13:53 - 00:14:03, Hãy subscribe cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video hấp dẫn
