00:00:00 - 00:00:19, Dạy pháp để giải quyết hiện tượng vanishing radia này là gì?
00:00:19 - 00:00:23, Chúng ta sẽ phải xem xét đến nguyên nhân.
00:00:23 - 00:00:30, Trong nguyên nhân của mình là có những cái hàm được xuất hiện đi xuất hiện lại nhiều lần
00:00:30 - 00:00:33, và giá trị đạo hàm của nó nhỏ hơn một.
00:00:33 - 00:00:37, Thế thì khi chúng ta đang làm với các mô hình học sâu,
00:00:37 - 00:00:47, thì cái việc mà một cái hàm lặp đi lặp lại nhiều lần đó là chuyện bắt buộc phải xảy ra rồi.
00:00:47 - 00:00:58, Việc một cái hàm lặp đi lặp lại nhiều lần là một cái việc mà hoàn toàn là khó tránh khỏi.
00:01:02 - 00:01:07, Cho đó cái vế đầu tiên đó là cái hàm của mình lặp đi lặp lại nhiều lần thì mình sẽ không can thiệp vào
00:01:07 - 00:01:22, mà mình sẽ đưa đến cái giải quyết bằng cách đó là chúng ta sẽ chọn một cái hàm mà có cái giá trị đạo hàm không quá bé.
00:01:22 - 00:01:27, Thế thì ở đây chúng ta sẽ tấn công vô cái hàm kích hoạt tức activation.
00:01:28 - 00:01:33, Thì nếu như chúng ta không dùng cái hàm kích hoạt có đạo hàm nhỏ,
00:01:33 - 00:01:39, thì chúng ta sẽ chọn những cái hàm kích hoạt mà có cái đạo hàm nó có tính chất cân bằng hơn.
00:01:39 - 00:01:42, Cân bằng hơn tức là nó sẽ gần bằng một.
00:01:42 - 00:01:47, Hoặc là có thể lúc nó sẽ là số rất bé nhưng mà cũng có thể là số lớn,
00:01:47 - 00:01:50, nó sẽ phải giao động xung quanh con số một.
00:01:50 - 00:01:55, Còn với hàm sigmoid thì cho dù là hàm kích hoạt có đạo hàm nhỏ,
00:01:55 - 00:01:59, cho dù là hàm của mình nó có kiến trúc gì đi chăng nữa,
00:01:59 - 00:02:02, thì sigmoid của x luôn luôn là con số bé hơn một.
00:02:02 - 00:02:07, Trong khi đó chúng ta không có dùng cái hàm này, không dùng hàm sigmoid nữa,
00:02:07 - 00:02:10, mà chúng ta dùng hàm relu.
00:02:10 - 00:02:17, Thì đạo hàm của hàm relu này thì nó sẽ là bằng 0.
00:02:18 - 00:02:25, Nếu z bẻ hơn 0 và bằng 1 nếu z lớn hơn 0,
00:02:28 - 00:02:31, thì cái việc này nó sẽ giúp chúng ta cân bằng.
00:02:31 - 00:02:36, Và chúng ta lưu ý đó là không phải lúc nào z khi chúng ta gọi hàm activation function,
00:02:36 - 00:02:39, thì cái giá trị chúng ta truyền vô là đều bé hơn 0,
00:02:39 - 00:02:41, nó sẽ có lúc bé hơn 0, có lúc lớn hơn 0,
00:02:41 - 00:02:44, do đó nó tạo ra sự hài hòa và sự cân bằng cho mình.
00:02:45 - 00:02:48, Đó là cái biểu hiện của tính cân bằng,
00:02:48 - 00:02:49, là thể hiện cho đó.
00:02:49 - 00:02:51, Nó sẽ lúc không, lúc bằng một,
00:02:51 - 00:02:55, nhưng mà trung bình thì nó sẽ là xoay xung quanh con số một.
00:02:57 - 00:03:02, Và với cái thao tác là đổi cái hàm sigmoid thành relu,
00:03:02 - 00:03:06, thì AlexNet đã ghi một cái dấu ấn,
00:03:06 - 00:03:09, đó là vào năm 2012,
00:03:09 - 00:03:14, đó là khi lần đầu tiên trong một cuộc thi về phân loại hình ảnh,
00:03:14 - 00:03:16, trên tập dữ liệu là e-mailnet.
00:03:18 - 00:03:21, Thì cái kiến trúc mạng CNN đã chiến thắng
00:03:21 - 00:03:25, so với lại các giải pháp sử dụng đặc trưng handcrafted features,
00:03:25 - 00:03:27, tức là lần đầu tiên mà deep learning
00:03:27 - 00:03:30, có được cái đội chính xác vượt trội so với lại
00:03:30 - 00:03:35, các mô hình sử dụng đặc trưng được tuôn thủ công.
00:03:36 - 00:03:40, Và một trong những cái tạo nên sự khác biệt của AlexNet
00:03:40 - 00:03:42, đó chính là có sử dụng hàm relu,
00:03:43 - 00:03:45, thay vì sử dụng hàm sigmoid thì họ dùng relu.
00:03:45 - 00:03:47, Tuy nhiên họ sẽ còn những cái cải tiến khác,
00:03:47 - 00:03:51, nhưng mà ứng với lại cái vấn đề về vanishing radian thì
00:03:51 - 00:03:53, họ đã có những cái cập nhật này.
00:03:55 - 00:03:58, Cái giải pháp số 2, đó là chúng ta sẽ thêm các cái cách nối tác,
00:03:58 - 00:04:01, tức là chúng ta sẽ can thiệp vô cái kiến trúc,
00:04:01 - 00:04:08, thay đổi cái kiến trúc của cái mô hình, học sâu.
00:04:09 - 00:04:14, Thì đối với một cái mạng neural network bình thường,
00:04:14 - 00:04:17, thì chúng ta sẽ thực hiện cái phép biến đổi là
00:04:17 - 00:04:22, tính, weight layer, tức là chúng ta đang thực hiện phép biến đổi tuyến tính
00:04:22 - 00:04:24, để rút chết đặc trưng,
00:04:24 - 00:04:26, sau đó rồi chúng ta qua hàm relu,
00:04:26 - 00:04:28, để phi tiến hóa cái đặc trưng đó,
00:04:28 - 00:04:31, rồi sau đó lại qua weight layer,
00:04:31 - 00:04:33, tức là để rút chết đặc trưng,
00:04:33 - 00:04:35, rồi lại đi relu.
00:04:35 - 00:04:38, Thì đây là cái thao tác biến đổi một cách tuần tự.
00:04:39 - 00:04:41, Và chúng ta ký hiệu nó là hx.
00:04:41 - 00:04:44, Thì nếu như chúng ta dùng một cái kiến trúc mạng bình thường như thế này,
00:04:44 - 00:04:46, mà không có cái cách nối tác,
00:04:46 - 00:04:48, cái nối tác là gì thì chúng ta sẽ nói trong slide tiếp theo.
00:04:48 - 00:04:51, Nhưng mà với cái kiến trúc thông lệ này,
00:04:51 - 00:04:54, thì khi tác giả của cái bài báo
00:04:54 - 00:04:57, là Deep Residual Learning for Image Recognition,
00:04:57 - 00:05:01, bài này là viết tác của cái kiến trúc mạng là ResNet,
00:05:01 - 00:05:04, là tác giả Kaming Hair,
00:05:04 - 00:05:12, thì khi chúng ta tăng số layer lên từ 20 lên 32 lên 46 lên 56,
00:05:12 - 00:05:16, thì cái giá trị loss của mình nó lại càng lúc càng cao.
00:05:16 - 00:05:19, Khi mà tăng layer lên thì nó lại đi hội tụ chậm.
00:05:22 - 00:05:27, Nó hội tụ chậm hơn so với lại những cái layer x,
00:05:27 - 00:05:29, so với lại những cái mạng mà x layer hơn.
00:05:29 - 00:05:31, Thì đây là một cái điều vô lý,
00:05:32 - 00:05:36, tại vì nó sẽ khiến cho mô hình của mình không có tốt
00:05:36 - 00:05:40, đối với những cái mạng có kiến trúc sâu.
00:05:40 - 00:05:44, Mặc dù trước đó đã có một số công trình họ nghiên cứu
00:05:44 - 00:05:47, và họ tổn quát bá rằng khi mạng càng sâu,
00:05:51 - 00:05:54, thì nó sẽ càng tạo ra nhiều đặc trưng tốt,
00:05:54 - 00:05:57, tạo ra càng nhiều đặc trưng,
00:05:57 - 00:06:00, từ đặc trưng cấp thấp cho đến đặc trưng cấp cao
00:06:01 - 00:06:05, và nó sẽ bổ trợ cho cái việc phân biệt đối tượng.
00:06:05 - 00:06:07, Nhưng mà khi chúng ta tăng cái độ sâu lên,
00:06:07 - 00:06:09, thì nó lại không còn hiệu quả nữa.
00:06:09 - 00:06:15, Thì đây chính là cái quan sát đầu tiên của các tác giả của bài ResNet
00:06:15 - 00:06:19, và họ đã đưa ra một số lý giải
00:06:19 - 00:06:23, và đề xuất một cái giải pháp rất là đơn giản.
00:06:23 - 00:06:25, Đó là chúng ta thêm một cái kết nối tắc như thế này.
00:06:25 - 00:06:27, Thêm một cái kết nối tắc là cái đường này.
00:06:27 - 00:06:32, Và chúng ta ý nghĩa của cái hàm kết nối tắc này đó là gì?
00:06:32 - 00:06:35, Hx sẽ là bằng fx,
00:06:35 - 00:06:37, hx là bằng fx,
00:06:37 - 00:06:39, tức là các cái thao tác biến đổi tuần tự,
00:06:39 - 00:06:41, cộng thêm 9x đầu vào.
00:06:41 - 00:06:43, Chỉ là đơn giản như vậy thôi.
00:06:43 - 00:06:47, Thế thì tại sao một cái thao tác biến đổi đơn giản này
00:06:47 - 00:06:49, nó lại giúp cho chúng ta
00:06:49 - 00:06:53, dạy quyết được cái bấn đề balancing gradient?
00:06:53 - 00:06:55, Đó là vì chúng ta khi chúng ta tính đậu hàm,
00:06:56 - 00:07:00, thì bình thường là nếu như hx mà bằng fx,
00:07:00 - 00:07:04, tức là các cái phép biến đổi tuần tự ở đây,
00:07:04 - 00:07:06, thì khi chúng ta tính đậu hàm,
00:07:06 - 00:07:08, các cái activation function này mặc dù đã được thiết kế,
00:07:08 - 00:07:12, là đã có thể giảm thiểu được cái hiện tượng balancing gradient,
00:07:12 - 00:07:14, nhưng mà cái việc khuấn luyện thì các cái...
00:07:14 - 00:07:18, càng về sau thì các cái đậu hàm của mình nó sẽ có su hướng càng nhỏ
00:07:18 - 00:07:21, và nó sẽ tiến về là nhỏ hơn một.
00:07:21 - 00:07:23, Dẫn đến đó là cái mô hình của mình
00:07:24 - 00:07:28, sẽ càng về sau, nó sẽ càng khó hồi tụ hơn.
00:07:28 - 00:07:30, Có thể ban đầu nó sẽ hồi tụ tốt, nhưng về sau nó khó hồi tụ hơn.
00:07:30 - 00:07:36, Thì khi đó là đậu hàm của f này sẽ là...
00:07:36 - 00:07:38, không có... nó sẽ mau tiếng về không.
00:07:40 - 00:07:44, Thế thì với cái việc chúng ta cộng thêm xo thì đậu hàm...
00:07:44 - 00:07:48, bên đây là đậu hàm của x, của h,
00:07:48 - 00:07:50, là bằng đậu hàm của hàm f,
00:07:50 - 00:07:52, tức là chúng ta vẫn lấy những cái đặc trưng bình thường.
00:07:53 - 00:07:55, Đây là các cái đặc trưng bình thường.
00:08:01 - 00:08:03, Nhưng mà chúng ta sẽ có thêm cái vế cộng x này,
00:08:03 - 00:08:07, mà đậu hàm của x thì nó chính là bằng một.
00:08:07 - 00:08:13, Như vậy thì nhờ có cái thao tác mà cộng hợp một này,
00:08:13 - 00:08:17, thì kể cả sau này khi đậu hàm này có lớn hơn không,
00:08:17 - 00:08:21, hay nhỏ hơn không, thì cái đậu hàm của h sẽ là giao động.
00:08:23 - 00:08:29, Giao động quanh số 1,
00:08:29 - 00:08:31, nó sẽ khiến cho cái mô hình của mình,
00:08:31 - 00:08:33, nó sẽ càng cân bằng hơn.
00:08:35 - 00:08:37, Cái đậu hàm của mình nó cân bằng hơn.
00:08:37 - 00:08:43, Do cái h phải, tức là đậu hàm của h,
00:08:43 - 00:08:47, lúc thì nó sẽ nhận số lớn hơn một, lúc nhận số nhỏ hơn một,
00:08:47 - 00:08:51, tại vì nó tùy thuộc vô cái đậu hàm của f là có...
00:08:51 - 00:08:57, nhưng mà trung dung nhất thì là h phải x sẽ nhận các cái trái trị vừa giao động
00:08:57 - 00:08:59, xung quanh số 1.
00:08:59 - 00:09:01, Khi chúng ta tính cái chain rule,
00:09:01 - 00:09:05, các cái con số mà giao động quanh số 1 nó sẽ là cân bằng,
00:09:05 - 00:09:09, dẫn đến là đậu hàm của mình sẽ không có giảm quá nhanh.
00:09:09 - 00:09:13, Thì đó là cái lý thuyết lý giải về lý thiết toán.
00:09:13 - 00:09:19, Và đây là một cái phương pháp rất là đơn giản.
00:09:20 - 00:09:22, Nhưng mà cực kỳ hiệu quả.
00:09:22 - 00:09:24, Ở đây tôi nhấn mạnh đó là...
00:09:24 - 00:09:26, dùng cái từ là cực kỳ hiệu quả.
00:09:26 - 00:09:29, Và rất nhiều những cái bài báo về sau,
00:09:29 - 00:09:32, họ đều có sử dụng cái ý tưởng kết nối tác này.
00:09:32 - 00:09:34, Ví dụ như trong kiến trúc ResNet,
00:09:34 - 00:09:38, đối với kiến trúc VGG trước đó là VGG 19,
00:09:38 - 00:09:41, là bao gồm 19 cái layer biến đổi này,
00:09:41 - 00:09:45, thì nó đã đạt được cái ngưỡng bảo hòa về độ chính xác,
00:09:45 - 00:09:47, nó không thể tăng lên độc nữa.
00:09:47 - 00:09:51, Nó tăng lên là 20 lớp, 30 lớp là nó gần như nó bảo hòa và nó không có...
00:09:51 - 00:09:53, thậm chí là nó còn giảm.
00:09:53 - 00:09:57, Nhưng ResNet bằng cái cơ chế Skip Connection,
00:09:57 - 00:10:02, đây là cái Skip Connection, các cái kết nối tác,
00:10:02 - 00:10:08, thì ResNet đã có thể huấn luyện được với những cái mạng có số lượng layer lên đến là 34 lớp,
00:10:08 - 00:10:11, và thậm chí là lên đến trên 100 lớp.
00:10:11 - 00:10:16, Nhưng mà độ chính xác của mình nó vẫn càng lúc càng cải tiến, càng tốt.
00:10:16 - 00:10:19, Vậy thì nhờ có cái Skip Connection này,
00:10:19 - 00:10:22, thì khi chúng ta tính ra cái giá trị Loss ở đây,
00:10:22 - 00:10:24, nếu như bình thường,
00:10:24 - 00:10:27, nếu như bình thường chúng ta tính Loss,
00:10:27 - 00:10:30, và chúng ta muốn lan truyền đến những cái layer ở lớp đầu tiên,
00:10:30 - 00:10:33, những cái layer số 1, số 2,
00:10:35 - 00:10:39, thì khi chúng ta lan truyền cái độ đổi về đến cái layer đầu tiên,
00:10:39 - 00:10:41, thì cái Loss gần như đã bị trực tiêu.
00:10:42 - 00:10:50, Là cái đạo hàm của hàm j theo những cái tham số ở những cái lớp số 1 hoặc là số 2 gần như bằng 0.
00:10:50 - 00:10:58, Vì chúng ta phải thực hiện các cái con số nhân này với 34, 34 cái đạo hàm
00:11:00 - 00:11:02, của các cái con số,
00:11:02 - 00:11:06, ờ, của các cái đạo hàm thành phần là do của 34 lớp.
00:11:06 - 00:11:09, Nhưng nhờ có các cái Skip Connection này,
00:11:09 - 00:11:11, thì khi chúng ta lan truyền cái lỗi,
00:11:11 - 00:11:18, thì cái số bước nhảy này sẽ giúp chúng ta giảm bớt con đường để đi từ layer cuối về layer đầu.
00:11:18 - 00:11:20, Ví dụ như trên hình này,
00:11:20 - 00:11:27, chúng ta có là 1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16.
00:11:27 - 00:11:31, Tức là chúng ta chỉ còn 16 lần, 16 cái bước nhảy.
00:11:35 - 00:11:37, Như vậy là nó đã tăng gấp đôi,
00:11:37 - 00:11:41, nó đã tăng được tốc độ gấp đôi để lan truyền cái đổ lỗi đó xuống.
00:11:41 - 00:11:47, Đó thì chính là cái lý giải 1 cái góc nhìn khác về cái lý thuyết về mặt không tin.
00:11:47 - 00:11:52, Tức là cái sai số nếu bình thường chúng ta truyền qua các cái tuần tự,
00:11:52 - 00:11:54, thì về đến đây là nó đã bị tiêu biến dần.
00:11:54 - 00:11:59, Nhưng mà nhờ có các Skip Connection này thì cái sai số truyền qua cái đường tắt.
00:11:59 - 00:12:02, Thì đến đây nó chỉ có 16 bước thôi.
00:12:02 - 00:12:04, Nó sẽ ít hơn nhiều so với 0.34 bước.
00:12:04 - 00:12:10, Vì vậy là cái mô hình của mình sẽ đỡ hiện tượng vanishing gradient hơn.
00:12:12 - 00:12:14, Và với cái sự thay đổi đó,
00:12:14 - 00:12:22, thì ở phiên bản trước, đây là trước khi dùng Skip Connection.
00:12:22 - 00:12:24, Và đây là sau.
00:12:24 - 00:12:27, Sau khi dùng Skip Connection thì chúng ta thấy là
00:12:27 - 00:12:38, Cái layer số 50, 6 và thậm chí là cái layer số 110 là cho cái loss thấp hơn so với lại cái loss của cái layer số 20.
00:12:38 - 00:12:47, Trong khi đó bên đây là không có dùng thì 56, cái giá trị sai số của nó còn cao hơn cả cái sai số của layer số,
00:12:47 - 00:12:51, của cái mô hình mà có layer số 20 lớp.
00:12:51 - 00:12:58, Vì vậy chúng ta thấy là cái tác dụng của cái nối tắc rất là hiệu quả.
00:12:58 - 00:13:04, Đặc biệt là chúng ta vẫn có thể tiếp tục cải tiến được trên những cái mạng mà có số lớp lên đến hàng trăm.
00:13:04 - 00:13:11, Và không phải ngũ nhiên mà cái bài này được đánh giá là một trong những thành tựu lớn trong Deep Learning.
00:13:11 - 00:13:15, Vì đến tính từ thời điểm 2015 cho đến bây giờ là 10 năm.
00:13:15 - 00:13:23, Và đã có là hơn 200.000 và gần sắp xỉ 300.000 citation, là 300.000 cái trích dẫn.
00:13:23 - 00:13:32, Thì cách đây là hơn một năm thì mới chỉ khoảng là 210.000 nhưng mà sau một năm là nó nhảy lên 90.000.
00:13:32 - 00:13:42, Thì thế thì chúng ta thấy đó là cái tốc độ tăng citation rất là nhanh và chưa thấy có một cái dấu hiệu gì là dừng lại.
00:13:42 - 00:13:50, Thì điều đó chứng thỏ là cái kỹ thuật mà Skip Connection này rất là hiệu quả và được rất nhiều những cái nghiên cứu họ sử dụng gần đây.
00:13:52 - 00:14:01, Và với cái cách nối tắc này thì không chỉ ResNet mà những cái kiến trúc khác ví dụ như là DanceNet cũng sẽ có các cái cách nối tắc.
00:14:01 - 00:14:03, Đây là các cách nối tắc.
00:14:04 - 00:14:12, Rồi Transformer cũng vậy. Chúng ta thấy là Transformer là Attention Eon Unit.
00:14:16 - 00:14:19, Tức là là tất cả những gì bạn cần.
00:14:19 - 00:14:25, Nhưng mà các bạn nhìn vô đây thì chúng ta thấy cái kết nối tắc mới chính là những cái thao tác mà nhiều nhất.
00:14:25 - 00:14:27, Đây là một cái nối tắc.
00:14:27 - 00:14:29, Một kết nối tắc.
00:14:29 - 00:14:31, Rồi lại tiếp tục kết nối.
00:14:31 - 00:14:33, Kết nối tắc.
00:14:33 - 00:14:39, Đó thì không biết là Attention Eon Unit hay là Skip Connection Eon Unit. Cái này là một cái nai vui.
00:14:41 - 00:14:48, Và tương tự như vậy thì cũng sẽ có Deep Supervision là một cái kiến trúc mạng mà có các cái kết nối tắc.
00:14:48 - 00:14:53, Thì cái kết nối tắc này nó không có theo kiểu của DanceNet mà là nó sẽ tắt ra thành từng nhánh.
00:14:53 - 00:15:01, Và với mỗi nhánh thì chúng ta thấy nếu đi theo đúng cái đường ban đầu là cái đường kết trúc ở giữa thì sẽ biến đổi rất là nhiều.
00:15:01 - 00:15:07, Nhưng mà nhờ có cái nhánh nó tắt ra, tắt ra, tắt ra, tắt ra ở đây.
00:15:07 - 00:15:12, Thì chúng ta sẽ tính các cái loss ở những cái lớp mà có ít biến đổi hơn.
00:15:12 - 00:15:22, Thì những cái loss ở các lớp ít biến đổi này thì khi chúng ta lan truyền về thì chúng ta sẽ giúp cập nhật được trọng số của những lớp đầu tiên dễ dàng hơn.
00:15:22 - 00:15:28, Tại vì khoảng cách từ lớp đầu tiên này cho đến những cái... ví dụ như đến đây đi.
00:15:28 - 00:15:30, Đến cái chỗ loss này.
00:15:30 - 00:15:34, Chúng ta thấy là chỉ có 1, 2, 3, 4, 5, 6, 7 bước.
00:15:35 - 00:15:48, Trong khi đó nếu chúng ta đi hết nguyên 1 cái trục này thì có thể lên đến gấp 2, thậm chí là gấp 2,5 lần là có thể lên đến là 16, 17 bước.
00:15:48 - 00:16:04, Thì nhờ các cái kết nối đầu ra như thế này, nó sẽ giúp cho chúng ta lan truyền đến cái trọng... cái đồ lỗi đến những cái lớp đầu tiên và ít bị hình tượng vanishing radiant hơn.
00:16:18 - 00:16:22, Cảm ơn các bạn đã xem video hấp dẫn.
