00:00:15 - 00:00:25, Trước khi đến với những mô hình học sâu, thì chúng ta sẽ có một trong những nền tảng đầu tiên của Deep Learning, đó chính là mạng Neuro Network.
00:00:26 - 00:00:30, Đây có thể nói là một trong những mạng học sâu đời đầu.
00:00:30 - 00:00:41, Và ý tưởng của mạng Neuro Network là một biến thể cho rất nhiều kiến trúc về sau, cụ thể là CNN, RNN và thậm chí là Transformer.
00:00:45 - 00:00:47, Bài báo là Attention Ion Unit
00:00:54 - 00:00:57, Nhưng thực tế thì trong kiến thúc này
00:00:57 - 00:01:01, vai trò của mạng Neural Network
00:01:01 - 00:01:03, hay là thành phần MLP
00:01:03 - 00:01:05, trong bài báo đó họ dùng là MLP
00:01:05 - 00:01:07, thì vai trò này cực kỳ quan trọng
00:01:07 - 00:01:10, trong việc phi tuyến hóa bài toán của mình
00:01:10 - 00:01:13, để giúp giải quyết được bài toán phú tạp
00:01:15 - 00:01:22, Nó là một mô hình bước đầu để đánh dấu vào việc giải quyết các bài toán phi tuyến tính.
00:01:23 - 00:01:28, Trong các mô hình như CNN và ANN, chúng ta cũng có thể thấy có chữ NN ở đây.
00:01:28 - 00:01:30, Nó chính là chữ Neural Network.
00:01:30 - 00:01:35, Vậy thì chúng ta có thể thấy là vai trò của mạng Neural Network rất là quan trọng.
00:01:35 - 00:01:41, Sự khác biệt của mạng Neural Network so với những kiến trúc mạng thuộc nhóm tuyến tính
00:01:46 - 00:01:54, Nếu chúng ta bỏ 2 lớp ẩn này đi và bỏ dấu 3 chấm này đi thì nó chính là Softmax Reversal
00:01:56 - 00:01:59, Đây chính là một mô hình hội quy Softmax
00:02:03 - 00:02:09, Nhưng khi chúng ta trèng thêm các lớp ẩn, ở đây nó gọi là lớp ẩn
00:02:09 - 00:02:11, Hoặc thuộc ngữ tiếng Anh gọi là hidden layer
00:02:15 - 00:02:17, khi chúng ta thêm các hít đần layer này vào
00:02:18 - 00:02:22, thì nó sẽ giúp chúng ta giải quyết được các bài toán phi tuyến tính
00:02:24 - 00:02:29, và chúng ta cũng có một quy tắc đó là ngay sau lớp biến đổi tuyến tính là sigma
00:02:29 - 00:02:32, thì chúng ta phải có một cái hàm kích hoạt
00:02:32 - 00:02:35, hàm này bắt buộc phải là một cái hàm phi tuyến
00:02:35 - 00:02:39, tại vì nếu không thì hai lớp biến đổi tuyến tính liên tiếp
00:02:39 - 00:02:42, thì nó sẽ tạo ra thành một tổ hợp tuyến tính
00:02:45 - 00:02:51, các bài toán phi tuyến do đó chúng ta phải có những thành phần phi tuyên tính như thế này
00:02:52 - 00:02:59, Và tham số của mô hình của chúng ta sẽ được đánh số từ theta1, theta2, cho đến thetaL
00:02:59 - 00:03:02, tức là ở đây chúng ta có Llayer
00:03:03 - 00:03:08, Và ở cái mô đun cuối cùng đó là hàm Lost Function, hàm độ lỗi
00:03:08 - 00:03:11, thì chúng ta cũng sẽ sử dụng công thức Cross entropy
00:03:15 - 00:03:17, chúng ta không thay đổi cái chỗ này
00:03:18 - 00:03:20, và ở đây thì chúng ta sẽ minh họa xem
00:03:21 - 00:03:27, ý nghĩa hình học của mạng Neural Network như thế nào
00:03:27 - 00:03:32, thì ở bên trái chúng ta có 1 cái ví dụ là 2 tập điểm hình tròn và tam giác
00:03:32 - 00:03:34, là có mối quan hệ phi tiến tính
00:03:34 - 00:03:41, tại vì chúng ta không thể nào dùng 1 cái đường thẳng để chia 2 tập điểm này ra làm 2 phần
00:03:45 - 00:03:49, và dùng đường cong như thế này, thì mới có thể chia được.
00:03:49 - 00:03:52, Vậy thì làm sao mạng Neural Network này
00:03:52 - 00:03:58, có thể phân chia 2 tập tròn và tam giác này ra làm 2 phần?
00:03:59 - 00:04:02, Thì chúng ta đã biết trong mạng Logistic Regression
00:04:02 - 00:04:09, thì mỗi một cái node này tương ứng là một đường phân lớp tiến tính.
00:04:10 - 00:04:13, Và cụ thể là các trọng số mà nối đến Neural Network này
00:04:15 - 00:04:18, pham tham số của trường thẳng của mình. Như vậy, cái Neural đầu tiên
00:04:18 - 00:04:22, nó sẽ giúp cho chúng ta tạo ra được 1 cái
00:04:22 - 00:04:24, đường phân lớp, tách ra làm 2 phần
00:04:24 - 00:04:28, và đây là 1 cái bộ phân loại yếu. Nhưng nhiều cái bộ phân loại yếu
00:04:28 - 00:04:31, ráp lại với nhau. Chúng ta có nhiều cái bộ phân loại yếu
00:04:32 - 00:04:35, và ở những cái nodes tiếp theo, nó đã tổng hợp
00:04:35 - 00:04:37, có trọng số
00:04:37 - 00:04:41, các cái nodes ở đằng trước. Như vậy là nó đang tạo ra
00:04:41 - 00:04:42, các cái đặc trưng cấp cao
00:04:45 - 00:04:49, các đặc trưng cấp cao tổng hợp từ những đặc trưng trước đó
00:04:50 - 00:04:53, Mỗi đặc trưng ở phía trước sẽ là đường thẳng như thế này
00:04:53 - 00:04:57, Và khi chúng ta tính tổng động số lại
00:04:57 - 00:05:02, thì nó đã giúp chúng ta dần dần phân tách tập hình tròn
00:05:02 - 00:05:05, và hình tam giác ra làm hai phần tách biệt như thế này
00:05:05 - 00:05:10, Đó chính là ý nghĩa hình học của mạng Neural Network
00:05:10 - 00:05:14, Các layer đầu tiên sẽ tạo ra các đặc trưng cấp thấp
00:05:15 - 00:05:18, phân tách ra thành 2 phần như thế này
00:05:18 - 00:05:28, nhưng ở những đặc trưng phía sau sẽ tạo ra những đặc trưng cấp cao hơn, phi tuyến tính hơn và phức tạp hơn để giải quyết các bài tuyến phức tạp của chúng ta
00:05:28 - 00:05:33, Đó chính là sơ lượt về kiến trúc mạng Neural Network
00:05:45 - 00:05:47, Hãy subscribe cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video hấp dẫn
