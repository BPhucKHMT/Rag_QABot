00:00:00 - 00:00:21, Chúng ta sẽ cùng đến với mô hình tiếp theo đó là clip Routed Language Image Regaining
00:00:21 - 00:00:35, Routed Language Image Regaining là clip phát triển từ clip, clip cho thấy tình năng trong việc khai thác thông tin dựa trên một cặp dữ liệu huấn luyện, đó là hình ảnh và văn bản
00:00:35 - 00:00:39, và khai thác dự liệu này để cho bài toán, đó là Image Classification
00:00:39 - 00:01:03, Tuy nhiên, điều gì xảy ra nếu chúng ta khai thác nó cho những bài toán, ví dụ như là phân đoạn ngữ nghĩa, bài toán Post Ectimation xác định tư thế của người, gián người, bài toán phát hiện lối tượng
00:01:03 - 00:01:15, Chúng ta thấy trong thị giác máy tính có rất nhiều bài toán khác nhau và clip đã thể hiện được tiềm năng trong bài toán đầu tiên, đó là bài toán Classification
00:01:15 - 00:01:33, Vậy thì, làm sao chúng ta có thể khai thác nó cho các bài toán khác? Đó chính là câu hỏi chúng ta đặt ra là làm sao áp dụng được ý tưởng tương tự với clip, nhưng mà cho bài toán phân biệt, phát hiện đối tượng, phân đoạn ngữ nghĩa đối tượng và xác định tư thế Post Ectimation
00:01:33 - 00:01:50, Như vậy thì, nó đòi hỏi chúng ta phải nắm bắt được thông tin thị giác, nhưng mà nó không phải ở mức độ toàn cục, tại vì trong mô hình clip, chúng ta truyền vào nguyên một tấm ảnh và chúng ta sẽ ra được vét tơ biểu diễn thông qua image encoder, tắc chứ ic
00:01:50 - 00:02:16, Rồi câu mô tả của chúng ta, thì qua text image encoder là ic, text encoder là te, nó sẽ tạo ra một cái văn bạng, sau đó chúng ta sẽ xo hai cái này lại với nhau, thế thì nó chưa có cơ chế để mà chúng ta khai thác trên từng vùng của đối tượng
00:02:16 - 00:02:26, Vậy thì làm sao chúng ta có thể nắm bắt được thông tin thị giác ở mức độ chi tiết hơn và vùng cục bộ, thay vì là vùng toàn bộ, tấm ảnh
00:02:26 - 00:02:39, Thì cái kết quả chính mà clip đã đạt được đó là clip G-clip có thể học những cái không cần dữ liệu, tức là Zero Source Transfer cho bài tán phát hiện đối tượng
00:02:39 - 00:02:52, Trên hình đây chúng ta thấy là khi chúng ta đưa vào một cái code prompt là raccoon, thì nó đã chỉ ra được cái vị trí của cái đối tượng đó ở đâu, kèm theo cái score tương ứng của mình là bao nhiêu phần trăm
00:02:52 - 00:03:07, Khi chúng ta gõ cái prompt mà đa đối tượng, ở đây là một đối tượng, còn đây là một đối tượng, còn nếu chúng ta đưa vào nhiều hơn một đối tượng, ví dụ như là person, bicycle, car, motobike, motocycle
00:03:07 - 00:03:28, Và giữa các đối tượng này thì chúng ta để cách nhau bởi một cái dấu chấm để phân biệt, thì nó cũng khoanh vùng được Apple, rồi prompt là tương tự như vậy cho các ví dụ khác, thì chúng ta thấy là nó có thể khoanh vùng được cái bánh xe, có thể khoanh vùng được nguyên chiếc xe
00:03:28 - 00:03:43, Rồi vâng vâng, thì đây chính là thế mạnh của clip sau với lệch clip, đó là nó có thể học không cần dữ liệu Zero Source Transfer cho cái bài toán phát hiện đối tượng
00:03:43 - 00:03:59, Và clip thì chế tên bài báo của mình đó là Routed Language Image Retraining, thì chúng ta cũng sẽ có bắt gặp những cụm từ mới, thì cái cụm từ mà chúng ta đang cần quan tâm ở đây đó chính là Routed Language Image
00:03:59 - 00:04:12, Thì cái Routed Language Image đó là chúng ta sẽ tìm cách để liên kết giữa các cái từ hoặc là cụm từ trong câu với một phật thể hoặc là một vùng của tấm ảnh, ví dụ chúng ta có nguyên một cái tấm ảnh như thế này
00:04:12 - 00:04:32, Và chúng ta có một cái code prompt đó là ví dụ như một cái bus.reperson, thì giả sử như trong hình này thì nó có một chiếc xe bus, nó có một chiếc xe bus
00:04:32 - 00:04:51, Và có một người ở đây thì nó sẽ tìm cách liên kết cái từ bus này với lại cái phần, cái khu vực mà tương ứng có cái nội dung hình ảnh đó, rồi nó liên kết cái person với lại cái khu vực mà tương ứng có cái đối tượng trong hình ảnh đó, đó là cái vùng màu đen ở đây
00:04:51 - 00:05:12, Và như vậy thì nó đã liên kết được giữa văn bản và hình ảnh, thì Routed Language Image đó là một cái phương pháp hoặc là một cái bài toán mà giúp cho chúng ta khoanh vùng được cái đối tượng ảnh mà tương đương với lại cái nội dung ở bên trong tấm hình của mình
00:05:12 - 00:05:25, Thì cái ý tưởng chính đó là đưa bài toán phát hiện đối tượng Object Detection trở về thành một cái bài toán đó là phân lập hoặc là từ hoặc là cậm từ trong câu hay là phrase, routing
00:05:25 - 00:05:33, Và chúng ta sẽ phát hiện vật thể và phân lập cái cậm từ, hai đối tượng nó có cái sự tương đồng cao
00:05:33 - 00:05:44, Thì ở đây là một cái ví dụ, chúng ta thấy đó là khi chúng ta đưa vô một cái câu mô tả thì trong cái câu mô tả tương ứng với một tấm ảnh, đây là một cái cặp ảnh và câu mô tả
00:05:44 - 00:05:57, Thì nó sẽ có cái cậm từ là đi kèm với lại cái khu vực mà cái ảnh nó có nội dung đó, small, vile, op, vaccine
00:05:57 - 00:06:04, Thì đây là một cái hình ảnh tương ứng với cái mô tả của cái hình ảnh tương ứng với lại cái câu mô tả đó
00:06:04 - 00:06:13, Thế thì clip, clip có khả năng nhận diện được các đối tượng ít gặp và trù tượng
00:06:13 - 00:06:19, Tức là bên cạnh những đối tượng rất phổ biến như là car, dog, bus, person, v.v.
00:06:19 - 00:06:29, Thì ở đây chúng ta thấy là những cậu từ này là những cái từ ít gặp, hổng tim, vaccine, v.v. đó là những cái từ ít gặp
00:06:29 - 00:06:37, Thì thậm chí là những cái từ mà có tính trùa tượng cao ví dụ như là the view, là khoanh vùng nguyên cái này là the view
00:06:37 - 00:06:46, Rồi beautiful caribbean, caribe sea, thì ở đây chúng ta thấy là nó biết đây là một cái biển ở caribbean để mà nó khoanh vùng
00:06:46 - 00:06:56, Thì đây chính là những cái khái niệm rất là trù tượng và ít gặp trong thực tế mà clip nó vẫn có thể phát hiện bằng định vị được
00:06:56 - 00:07:02, Vậy thì cách thức hoạt động và các cái module chính của clip là thực hiện như thế nào
00:07:02 - 00:07:17, Thì đầu tiên đó là chúng ta sẽ tạo ra một cái code prom tổng quát là tên của 80 vật thể trong tập dữ liệu coco được phân cách bởi dấu chấm
00:07:17 - 00:07:26, Tức là mỗi một cái vật thể thì sẽ cách nhau với các vật thể khác, ví dụ chữ person, cách chữ bicycle là dấu chấm như thế này
00:07:26 - 00:07:39, Và kèm theo là một cái câu mô tả, một cái cặp câu mô tả giữa hình ảnh và chúng ta sẽ đưa các cái từ này, cái code prom
00:07:39 - 00:07:48, Và cái câu mô tả này qua textencoder thì chúng ta sẽ tạo ra được là P0, P1, P2 v.v.
00:07:48 - 00:08:01, Và tương tự như vậy thì cái ảnh chúng ta cũng sẽ khoanh vùng các cái đối tượng này và qua cái visualencoder để tạo ra cái O0, O1, O2
00:08:01 - 00:08:10, Lưu ý đó là trong cái mô hình clip thì chúng ta sẽ kết hợp hai cái loại dữ liệu là hình ảnh và văn bản ở bước trung gian
00:08:10 - 00:08:22, G clip thì là ở bước trung gian, trong khi đó với clip thì chúng ta chỉ kết hợp nó ở cái bước cuối cùng thôi
00:08:22 - 00:08:25, Còn ở đây chúng ta sẽ thực hiện ở bước trung gian
00:08:25 - 00:08:41, Thì tại sao lại có những cái việc như vậy? Đó là khi chúng ta tổng hợp ra cái đặc trưng thì ở phía trên là chỉ thuần nội dung cái đặc trưng về văn bản
00:08:41 - 00:08:54, Còn ở bên dưới là các cái đặc trưng mà chúng ta thuần về hình ảnh và qua cái Fusion này thì chúng ta sẽ kết hợp được với nhau
00:08:54 - 00:09:09, Đó là lấy cái đặc trưng của hình ảnh để bỏ vào văn bản và lấy nội dung của văn bản đưa vào hình ảnh để cho nó có thể tương tác để giúp cho hình ảnh nó có thể thực sự được thấy ở trong văn bản này
00:09:09 - 00:09:21, Và cái sự khác biệt lớn nhất đó chính là cái module thứ 2, đó là chúng ta sẽ đo cái độ tương đồng giữa mỗi vùng và từ mô tả thay thế cho cái module phần lớp truyền thống
00:09:21 - 00:09:31, Và ở trên đây chúng ta sẽ có m từ khóa là P1, P2, m tốc cần là P1, P2 và Pm
00:09:31 - 00:09:44, Thì chúng ta có một cái lưu ý ở đây là có những cái từ mà dài và có hai, gọi là giống như tiếng Việt của mình là từ F là hair rayer thì nó sẽ tắt ra làm hai, làm hai tốc cần
00:09:44 - 00:09:59, hoặc có những cái từ hoặc cụm từ nó sẽ đi một cái combo với nhau, ví dụ như là blue dot, nó là một cái cụm từ thì nó sẽ tắt ra làm hai tốc cần
00:09:59 - 00:10:15, Thì khi đó chúng ta thấy là tính cái độ tương đồng nó sẽ là hai cái phần tử trên cái ma trận mà có cái giá trị lớn, ví dụ như đây là chữ tính từ và đây là danh từ thì chẳng hạn
00:10:15 - 00:10:37, Hoặc trong cái ví dụ trên thì chúng ta thấy là hair rayer thì đây chính là hair và đây là rayer, hair và rayer này thì nó sẽ đi chung với nhau một cái combo khi chúng ta so sánh cái độ tương đồng với cái feature của cái hình ảnh là của cái máy xe tóc
00:10:37 - 00:11:03, Thì cái ý tưởng của nó cũng tương tự như clip nhưng mà clip thì sẽ tắt nó ra thành các cái khu vực, các cái mảnh hình ảnh nhỏ rồi sau đó nó sẽ ti tính độ tương đồng với những cái từ ở bên trong cái chuỗi prompt của mình
00:11:03 - 00:11:05, Cái tốc cần được tạo ra bởi cái prompt của mình
00:11:06 - 00:11:15, Sau khi chúng ta đã tính được cái ma trận này rồi thì chúng ta sẽ đi tính cái alignment loss giữa cái route root, đây là cái ma trận route root
00:11:17 - 00:11:31, Và ở trên đó là cái ma trận mà chúng ta được tính từ cái độ tương đồng giữa cái vùng ảnh, cục bộ với lại các cái token ở bên trong cái prompt của mình
00:11:31 - 00:11:45, Và alignment loss này thì mục đích là để xác định xem cái đối tượng đó là gì, bên cạnh đó thì chúng ta sẽ có cái region feature để tính cái localization loss
00:11:46 - 00:11:53, Mục tiêu đó là để cho chúng ta xác định xem cái vị trí, cái sai số khi dự đáng vị trí
00:11:53 - 00:12:02, Thế thì quá trình huấn luyện của clip thì nó sẽ bao gồm đầu tiên đó là cái phrase routing
00:12:03 - 00:12:14, Với mỗi một cái tấm ảnh ở đây, O1, O2, ON chúng ta sẽ truyền vào, truyền vào đây và chúng ta sẽ ra được O
00:12:15 - 00:12:21, O chính là cái feature hoặc là cái embedding của cái vùng ảnh
00:12:23 - 00:12:35, Và sau đó thì chúng ta sẽ lấy cái prompt, prompt của mình thì nó sẽ được hình thành từ cái câu là detect person bicycle car third brush v.v.
00:12:37 - 00:12:50, Rồi, và cái ở đây thêm một cái ý nữa đó là cái feature mà biểu diễn cho cái đặc trưng hình ảnh thì đó là một cái đặc trưng có kích thước đó là đê chiều
00:12:50 - 00:12:56, Còn cái ảnh của mình thì nó sẽ có N cái vùng, N vùng
00:12:57 - 00:13:09, Rồi tương tự như vậy thì prompt thì đặc trưng của mỗi từ hoặc là token thì chúng ta sẽ biểu diễn bởi các cái vector và đại diện nó là ma trận p
00:13:09 - 00:13:22, Ma trận p này thì cũng tương tự đó là nó sẽ có chiều của cái đặc trưng văn bản sẽ là d chiều, giống như bên đây để mà sau này chúng ta có thể nhân tích vô hướng được
00:13:23 - 00:13:33, Và số token của chúng ta thì sẽ có m token, và m này thì thường lớn hơn N rất là nhiều
00:13:33 - 00:13:42, Rồi sau khi chúng ta lấy cái O và P chúng ta nhân với nhau thì chúng ta sẽ ra được cái S routing
00:13:43 - 00:13:49, Thì cái S routing này nó sẽ có kích thước đó là N nhân m, trong đó N là số vật thể hoặc số vùng, m là số token
00:13:50 - 00:14:10, Và cuối cùng là chúng ta sẽ dựa trên cái S routing này để chúng ta đi tính cái loss của mình, trong đó loss của mình thì nó sẽ có cái T phải là được mở rộng ra từ T bao gồm là N đối tượng
00:14:10 - 00:14:22, Rồi, thế thì clip đó là cho phép chúng ta có thể tạo ra một cái câu prom thủ công
00:14:23 - 00:14:31, Ở trên ví dụ trên thì chúng ta có thể thấy những cái đối tượng hiếm và lạ thì ta có thể mô tả đối tượng đó bằng một cái nguồn tự nhiên
00:14:31 - 00:14:41, Bình thường thì chúng ta mô tả đó là Car, Bus, Car, Drop, đây là những cái đối tượng tương đối là phổ biến
00:14:41 - 00:14:56, Thì chúng ta sẽ không cần phải mô tả nhiều hơn, nhưng đối với cái từ lạ, ví dụ như là cá búi, là stingray, thì nếu chúng ta đưa vào cái mô hình clip để mà phát hiện đối tượng
00:14:56 - 00:15:05, thì chúng ta thấy là cái khả năng phát hiện của nó khá là thấp, nó chỉ phát hiện được một con và với cái Confidence là khoảng 0.21
00:15:06 - 00:15:19, Nhưng khi chúng ta mô tả nó dài dòng hơn, nhiều thông tin hơn, ví dụ như là stingray with a flat and round, thì ở đây chúng ta sẽ, mô hình của mình nó sẽ hiểu
00:15:19 - 00:15:32, và nó sẽ phát hiện ra cái đối tượng nhiều hơn và dài đặt hơn. Thì cái việc mà bổ sung thêm cái câu mô tả này nó gọi là manual prompt tuning
00:15:33 - 00:15:42, và cái việc này thì cũng hoàn toàn có thể làm tự động nếu như chúng ta có cái dữ liệu có gắn nhãn thì có thể tự động làm cái quá trình là prompting này
00:15:42 - 00:15:50, bằng cách đó là chúng ta sẽ hướng luyện một cái module nhỏ là một cái phép biến đổi tiến tĩnh thôi, là một cái linear mapping thôi để mà trực tiếp học ra cái prompt
00:15:51 - 00:16:07, cái prompt này sẽ được học và tạo tự động. Nếu như chúng ta có cái dữ liệu thì chúng ta có thể cho nó học để mà bổ sung thêm cái câu mô tả này
00:16:07 - 00:16:22, Với cái slide này, với cái bài về G-Lib thì chúng ta thấy là cái tiềm năng của mô hình ngôn ngữ hình thị giác nó có thể cho chúng ta giải quyết được những cái bài toán phức tạp hơn
00:16:23 - 00:16:31, so với cái bài toán phân đoạn so với bài toán phân loại đối tượng đó là các cái bài toán như là phát hiện đối tượng
00:16:37 - 00:16:42, Cảm ơn các bạn đã xem video hấp dẫn
