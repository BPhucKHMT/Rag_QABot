00:00:00 - 00:00:20, Tiếp theo chúng ta sẽ cùng tìm hiểu về một số biến thể của tổ toán Radiant Descent
00:00:20 - 00:00:26, Đầu tiên đó là batch Radiant Descent và vi tác của chữ BGD
00:00:26 - 00:00:34, Đó là chúng ta sẽ truyền toàn bộ dữ liệu huấn luyện vào mô hình của mình
00:00:34 - 00:00:41, Theta là bằng theta trừ alpha nhân cho đạo hàm của...
00:00:41 - 00:00:44, ở đây chúng ta sẽ có chữ kỳ vọng
00:00:44 - 00:00:48, tức là chúng ta đang tính trên toàn bộ dữ liệu của mình
00:00:48 - 00:00:52, kỳ vọng của hàm loss, hàm lỗi
00:00:52 - 00:00:59, Và bí dụ như ở đây chúng ta có một hình ảnh minh họa là một đoạn dữ liệu
00:00:59 - 00:01:10, thì expectation tức là E sẽ được tính trên full toàn bộ dữ liệu của mình
00:01:10 - 00:01:17, Và trong đoạn code mả giả chúng ta thấy là chúng ta sẽ lập Y chạy trên số lượng Epoch
00:01:17 - 00:01:23, Mỗi một Epoch là một lượng chúng ta hướng luyện trên toàn bộ dữ liệu của mình
00:01:23 - 00:01:29, Chúng ta sẽ dụy qua Y với toàn bộ số Epoch
00:01:29 - 00:01:38, Và với mỗi Epoch chúng ta sẽ tính Radiant trên full toàn bộ dữ liệu
00:01:38 - 00:01:41, Vậy là chúng ta đưa toàn bộ data của chúng ta vào
00:01:41 - 00:01:47, Và chúng ta sẽ có được Radiant cho từng tham số
00:01:47 - 00:01:51, Và ở đây thì tham số sẽ là bằng tham số trừ cho Learning Ray Alpha
00:01:51 - 00:01:54, Nhân cho Vector Radiant của mình
00:01:54 - 00:01:58, Thì ở đây chính là cái mả giả của Batch Radiant Descent
00:02:00 - 00:02:04, Viên bản thứ hai, biến hệ thứ hai đó là Stochastic Radiant Descent
00:02:04 - 00:02:06, Ví tác của chữ SGD
00:02:06 - 00:02:09, Thì ở đây chúng ta sẽ truyền trên từng mẫu 1
00:02:09 - 00:02:14, Chúng ta sẽ truyền từng mẫu 1 để hướng luyện cho cái mô hình của mình
00:02:14 - 00:02:20, Thì công thức của chúng ta sẽ là theta là bằng theta trừ cho Alpha
00:02:20 - 00:02:23, Nhân cho NapLa của theta
00:02:23 - 00:02:26, Và ở đây chúng ta sẽ tính trên 1 mẫu dữ liệu Y
00:02:30 - 00:02:34, Và 1 cái mẫu dữ liệu Y này thì sẽ được lấy ngộ nhiên
00:02:38 - 00:02:43, Thế thì nếu chúng ta minh họa nguyên 1 cái đoạn dữ liệu của chúng ta như thế này
00:02:43 - 00:02:50, Thì phương pháp Stochastic Radiant Descent là chúng ta bốc ra ngộ nhiên 1 cái mẫu dữ liệu
00:02:51 - 00:02:54, Để chúng ta đưa vào và chúng ta hướng luyện
00:02:54 - 00:02:58, Thì cái mẫu dữ liệu này là mồn cập XE và EA
00:02:58 - 00:03:01, Và trong cái mả giả này thì chúng ta sẽ
00:03:01 - 00:03:06, Có là Y cũng sẽ lặp qua tất cả số epoch của mình
00:03:06 - 00:03:09, Với mỗi epoch thì chúng ta sẽ suffer
00:03:10 - 00:03:12, Cái data này sẽ xáo
00:03:14 - 00:03:19, Tức là nó sẽ được sắp xếp của cách ngộ nhiên
00:03:19 - 00:03:21, Thay đổi cái thứ tượng ngộ nhiên
00:03:21 - 00:03:24, Và với mỗi 1 cái example in data
00:03:24 - 00:03:29, Thì cái example của chúng ta chính là mẫu dữ liệu này
00:03:30 - 00:03:32, Chính là cái mẫu dữ liệu này
00:03:32 - 00:03:35, Thì khi đó chúng ta sẽ đưa cái mẫu dữ liệu này
00:03:35 - 00:03:39, Vào bên trong cái hàm EVALUA RADIANT để tính đầu hàm
00:03:39 - 00:03:43, Như vậy thì params rad là cái radiant
00:03:43 - 00:03:46, Chỉ được tính trên 1 mẫu dữ liệu ngộ nhiên ở đây
00:03:46 - 00:03:49, Và công thức cập nhặt thì cũng tương tự như là
00:03:49 - 00:03:51, Thực toán radiant descent nguyên bản
00:03:51 - 00:03:53, Đó là params bằng params trừ cho alpha
00:03:53 - 00:03:56, Nhân cho cái thành phần nát la này
00:03:58 - 00:04:00, Và đến với biến thể
00:04:00 - 00:04:02, Có cái sự light gap
00:04:02 - 00:04:04, Nó có sự ngộ nhiên
00:04:04 - 00:04:07, Của cái stochastic radiant descent
00:04:07 - 00:04:11, Nhưng đồng thời là nó sẽ không có tính trên full toàn mũ dữ liệu
00:04:11 - 00:04:13, Thì đó chính là mini-bar
00:04:13 - 00:04:17, Radiant descent vítac của chữ m-g-d
00:04:17 - 00:04:20, Thì chúng ta sẽ truyền vào 1 khối
00:04:20 - 00:04:22, Chúng ta sẽ truyền vào 1 khối dữ liệu
00:04:22 - 00:04:23, Để huấn luyện
00:04:23 - 00:04:25, Và ở đây chúng ta sẽ có cái khái niệm
00:04:25 - 00:04:28, Nó gọi là bar size tức là kích thước của 1 khối
00:04:28 - 00:04:31, Thì kích thước của 1 khối
00:04:31 - 00:04:33, Thì thường là lĩa thừa cổ 2
00:04:33 - 00:04:35, Lý dụ như là 1, 2, 4, 8
00:04:35 - 00:04:38, Thì tại sao người ta hay chọn cái lĩa thừa cổ 2
00:04:38 - 00:04:40, Là vì trong máy tính của chúng ta
00:04:40 - 00:04:42, Kiến trúc của mình là kiến trúc nhị phân
00:04:42 - 00:04:46, Nên thường là các cái bộ chẳng của số 2
00:04:46 - 00:04:49, Thì nó sẽ vừa fit với lại cái dữ liệu của mình
00:04:49 - 00:04:51, Nên tính toán nó sẽ nhanh hơn
00:04:51 - 00:04:54, Thế thì cái công thức của mini-bar radiant descent
00:04:54 - 00:04:58, Theta sẽ là bằng theta trừ cho alpha nhưng cho nát la
00:04:58 - 00:05:01, Thế thì ở đây chúng ta thấy là nó sẽ đi lấy trên mũ dữ liệu
00:05:01 - 00:05:04, Từ y cho đến y cộp n
00:05:04 - 00:05:06, Thì đây là 1 bar
00:05:06 - 00:05:10, Tương tự như vậy cho y chúng ta sẽ lấy trên 1 bar
00:05:10 - 00:05:12, Để chúng ta huấn luyện
00:05:12 - 00:05:17, Thế thì nếu chúng ta vẽ cái khối này là toàn bộ dữ liệu
00:05:17 - 00:05:20, Thì chúng ta sẽ chia cái dữ liệu của mình ra
00:05:20 - 00:05:23, Thành từng bar mỗi cái này là 1 bar
00:05:23 - 00:05:28, Rồi thì cái mạng giả của chúng ta là như sau
00:05:28 - 00:05:33, Với mỗi y chạy từ nằm trong cái range của số lượng epoch
00:05:33 - 00:05:36, Thứ là chúng ta cũng dượt qua tất cả các cái số epoch của mình
00:05:36 - 00:05:41, Và tại 1 cái thời điểm này thì chúng ta sẽ xáo ngộ nhiên
00:05:41 - 00:05:44, Tức là chúng ta sẽ xúc phồ cái mũ dữ liệu của chúng ta
00:05:44 - 00:05:46, Để cho nó có cái tính ngộ nhiên trong đó
00:05:46 - 00:05:49, Và tại 1 thời điểm thì chúng ta sẽ lấy ra 1 bar
00:05:49 - 00:05:51, Ví dụ như ở đây là 1 bar
00:05:51 - 00:05:56, Chúng ta sẽ dượt qua tất cả cái bar của data của mình
00:05:56 - 00:06:00, Và chúng ta sẽ truyền nó vào cái hàm evaluate radiant
00:06:01 - 00:06:04, Thì ở đây chúng ta sẽ tính được cái radiant
00:06:04 - 00:06:09, Chúng ta cũng sẽ cập nhật tương tự như thuật toán stochastic radiant descent
00:06:12 - 00:06:18, Như vậy thì chúng ta sẽ có 1 cái bảng so sánh về các cái biến thể này
00:06:18 - 00:06:22, Đầu tiên đó là cái bar radiant descent
00:06:22 - 00:06:29, Thì chúng ta thấy là vì nó được tính trên full toàn bộ cái dữ liệu
00:06:29 - 00:06:32, Nên rõ ràng là cái chi phí tính toán lớn
00:06:32 - 00:06:39, Và đồng thời là cái bộ nhớ của chúng ta có thể sẽ là sẽ phải cần rất là nhiều
00:06:39 - 00:06:43, Trong khi đó stochastic radiant descent thì ngược lại
00:06:43 - 00:06:46, Là tại 1 thời điểm chúng ta chỉ tính trong duy nhất 1 mũ dữ liệu thôi
00:06:46 - 00:06:53, Do đó thì cái bộ nhớ, cái chi phí về bộ nhớ của chúng ta
00:06:55 - 00:06:58, Thì sẽ tiết giảm rất là đáng kể
00:06:58 - 00:07:03, Nhưng mà bù lại thì có thể cái thời gian tính toán của chúng ta sẽ lâu
00:07:03 - 00:07:09, Ở trên là cái chi phí tính toán bao gồm là cái tài nguyên tính toán là CPU
00:07:09 - 00:07:11, Rồi RAM là tốn
00:07:11 - 00:07:14, Nhưng ở phía dưới thì chúng ta lại liên quan điểm tối tính toán
00:07:14 - 00:07:20, Thay vì chúng ta tính hết 1 khối lớn thì chúng ta lại tính từng mẫu từng mẫu
00:07:20 - 00:07:23, Do đó cái thời gian của chúng ta sẽ lâu hơn
00:07:23 - 00:07:26, Và millibar thì đâu đó nó sẽ là nằm ở giữa
00:07:26 - 00:07:32, Tức là chúng ta sẽ tính trên 1 khối lớn là n mẫu dữ liệu mà thôi
00:07:32 - 00:07:37, Thì ở đây nó sẽ là cân bằng hơn giữa 2 phương pháp ở trên
00:07:37 - 00:07:44, Và cái landscape tức là cái đồ họa của hàm lỗi của chúng ta cho từng biến thể này
00:07:44 - 00:07:49, Đối với Batch Radiant Descent thì chúng ta sẽ thấy cái đường đi của nó rất là mượt
00:07:49 - 00:07:52, Nó rất là mượt
00:07:52 - 00:07:55, Là vì nó được tính trên tổng thể toàn mũ dữ liệu của mình
00:07:55 - 00:07:57, Nên cái đường đi của nó sẽ mượt
00:07:57 - 00:08:04, Còn Stochastic Radiant Descent thì chúng ta sẽ thấy nó rất là dích sắc, rất là nhấp nhô
00:08:04 - 00:08:06, Thì cái này đó là do có cái ưu tố ngẫu nhiên
00:08:09 - 00:08:13, Sẽ có lúc chúng ta gặp những mẫu dễ thì cái loss của mình nó thấp
00:08:13 - 00:08:15, Nhưng gặp những mẫu cao thì loss nó cao
00:08:15 - 00:08:19, Nhưng mà nhìn chung sau 1 số lần lập nhiều đủ lớn
00:08:20 - 00:08:26, Thì nó sẽ hội tụ và tiến về cái giá trị hàm lỗi của mình nó sẽ càng lúc càng thấp
00:08:26 - 00:08:32, Và trong nhiều lý thuyết người ta đã chứng minh đó là Stochastic Radiant Descent
00:08:32 - 00:08:36, Nó sẽ giúp cho cái mô hình của mình học bền vững và ổn định hơn
00:08:36 - 00:08:38, Nhờ có những cái bước nhảy như thế này
00:08:38 - 00:08:41, Những cái bước nhảy dập lên dập xuống như thế này
00:08:41 - 00:08:46, Nó sẽ giúp cho mô hình của chúng ta thoát ra những cái điểm thực tiểu của bộ
00:08:50 - 00:08:57, Để hy vọng nó có thể tiến đến được những cái điểm tối ưu, những điểm cực tiểu toàn cục
00:08:57 - 00:09:00, hoặc là những điểm cực tiểu tối ưu hơn
00:09:00 - 00:09:02, Thì nhờ có những cái dập như thế này
00:09:02 - 00:09:10, Và Minibar Radiant Descent thì nó sẽ ở mức lương trần ở giữa nó cũng tương đối smooth
00:09:10 - 00:09:12, nhưng mà nó cũng sẽ có những cái dích sắc nhất định
00:09:12 - 00:09:15, Thì đây là Minibar Radiant Descent
00:09:15 - 00:09:19, Và thông thường thì người ta hay sử dụng Minibar Radiant Descent
00:09:19 - 00:09:23, vì nó cân bằng được yếu tố về thời gian và chi phí tính toán
