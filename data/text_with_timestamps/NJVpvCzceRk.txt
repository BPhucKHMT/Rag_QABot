00:00:00 - 00:00:21, Rồi trong những phần trước thì chúng ta đã cùng tìm hiểu về sự khác nhau giữa VIR, Variational Autonomous Cutter và Diffusion
00:00:21 - 00:00:28, Cả hai mô hình VAE và Diffusion đều dựa trên lý thuyết
00:00:28 - 00:00:35, đó là chúng ta sẽ cực đại hóa log của VX này, tức là làm sao cho kỷ ảnh X của mình giống thật nhất.
00:00:35 - 00:00:46, Thay vì chúng ta cực đại hóa log VX này, chúng ta sẽ cực đại hóa chặn dưới, tức là evidence lower bound ELMO của kỳ vọng này.
00:00:46 - 00:00:57, Nếu như trong công thức của VAE, thì cái G này là một vector ẩn, thì ở đây chúng ta sẽ có nhiều vector ẩn,
00:00:57 - 00:01:01, tại vì mô hình Diffusion của mình sẽ thực hiện nhiều bước để encoding.
00:01:01 - 00:01:08, Nguyên lý của nó đó là chia thành những bước nhỏ, thì nó sẽ giúp chúng ta đơn giản hóa bài toán của mình.
00:01:08 - 00:01:14, Thay vì chúng ta làm một bước lớn thì nó sẽ khó để huấn luyện và bài toán của chúng ta phức tạp hơn.
00:01:14 - 00:01:22, Thay vì chúng ta có một biến Z thì bây giờ nó sẽ là T biến là X1 cho đến XT,
00:01:22 - 00:01:28, và do đó thì thành phần Z này sẽ biến thành từ X1 cho đến XT.
00:01:28 - 00:01:34, Còn đối với biến X này của mình thì nó sẽ là X0, tức là cái giá trị ký hiệu ở đây.
00:01:34 - 00:01:42, X0 sẽ là ảnh đầu vào được sampling theo phân bố của data X.
00:01:42 - 00:01:49, Vậy thì công thức ở trên sẽ được đưa về và bài toán mô hình Diffusion model
00:01:49 - 00:01:57, sẽ đưa về việc cực đại hóa kỳ vọng của x1 cho đến XT cho trước X0.
00:01:57 - 00:02:00, Thì đây chính là lây tần của mình.
00:02:00 - 00:02:08, Và xZ sẽ là gồm x0 cho đến XT, nó mở, trộn lại với nhau.
00:02:08 - 00:02:11, Còn công thức ở đây thì nó đem qua từ đây.
00:02:11 - 00:02:15, Như vậy thì đây chính là mô hình của Diffusion model.
00:02:15 - 00:02:21, Vậy thì bây giờ chúng ta sẽ đến với bước đầu tiên, đó là quá trình khuyết tấn thuận hay còn gọi là encoding.
00:02:21 - 00:02:27, Ở bước encoding này, công thức của phân bố sát xuất x1,
00:02:27 - 00:02:35, tức là phân bố sát xuất của các vector ẩn khi cho trước ảnh gốc đầu vào lấy mẫu x0,
00:02:35 - 00:02:39, thì nó sẽ được biểu diễn bằng sơ đồ ở đây.
00:02:39 - 00:02:45, Và công thức phân bố sát xuất của các viễn ẩn cho trước x0,
00:02:45 - 00:02:50, thì nó sẽ là bằng tích của t từ 1 cho đến t lớn của quý X1,
00:02:50 - 00:02:53, xT cho trước xT từ 1.
00:02:53 - 00:02:58, Thế thì các quý phân bố của xT cho trước xT từ 1,
00:02:58 - 00:03:00, nó chính là các dấu mũi tài này.
00:03:00 - 00:03:06, Tức là từ x0, chúng ta sẽ ra được phân bố sát xuất của x1.
00:03:06 - 00:03:10, Từ xT cho trước x1, chúng ta sẽ ra được phân bố sát xuất của xT.
00:03:10 - 00:03:14, Từ xT, chúng ta sẽ ra được phân bố sát xuất của xT cộng 1.
00:03:14 - 00:03:20, Từ xT lớn cho trước x1, chúng ta sẽ ra được phân bố sát xuất của xT lớn.
00:03:20 - 00:03:24, Thì đây là một quá trình các bước để encoding.
00:03:25 - 00:03:31, Và phân bố sát xuất của x1 cho đến xT sẽ là bằng tích phân bố sát xuất này.
00:03:31 - 00:03:37, Tiếp theo, chúng ta sẽ tham số hóa hàm phân bố sát xuất.
00:03:37 - 00:03:44, Thế thì ở đây chúng ta sẽ tham số hóa dùng công thức thêm nhiễu.
00:03:44 - 00:03:48, Để đại diện cho thêm nhiễu, chúng ta sẽ dùng một phân bố Gauss.
00:03:48 - 00:03:53, Trong đó, min là bằng căng của alphaT nhưng xT cho 1.
00:03:53 - 00:03:59, Và variance là bằng 1 trừ alphaT nhưng với y.
00:03:59 - 00:04:01, Y là cái ma trận đơn vị.
00:04:01 - 00:04:07, Thế thì alphaT của mình sẽ là một con số lớn hơn 0 và nhỏ hơn 1.
00:04:07 - 00:04:11, Nếu alphaT mà càng tiến về 0,
00:04:11 - 00:04:17, tức là cái thao tác mà căng của alphaT nhưng với xT cho 1 là chúng ta đang làm yếu.
00:04:17 - 00:04:21, Làm yếu cái tín hiệu của xT đi.
00:04:21 - 00:04:24, Làm yếu cái tín hiệu của xT cho 1.
00:04:24 - 00:04:28, Sau đó chúng ta thêm bố đã làm yếu cái thằng xT cho 1.
00:04:28 - 00:04:34, Và cái thành phần mà độ lệch nó tương ứng sẽ là cái nhiễu mà chúng ta muốn thêm vào.
00:04:35 - 00:04:42, Thế thì nếu alphaT mà càng nhỏ, thì cái xT cho 1 càng yếu và 1 trừ alphaT càng lớn,
00:04:42 - 00:04:44, tức là cái nhiễu của mình càng mạnh.
00:04:44 - 00:04:47, Thì cái quá trình khói táng này nó sẽ càng nhanh.
00:04:47 - 00:04:53, Còn khi alphaT của mình mà càng tiến về 1, tức là xT này chỉ bị, xT cho 1 này chỉ bị làm yếu một ít.
00:04:53 - 00:04:57, Còn cái phần, phần nhiễu này thì chúng ta cũng thêm vô một ít.
00:04:57 - 00:05:01, Như vậy thì kỷ ảnh của mình nó sẽ ít bị thêm nhiễu.
00:05:01 - 00:05:06, Thế thì cũng là tham số nhưng mà ở đây, cái quá trình encoding này,
00:05:06 - 00:05:10, thì cái tham số alphaT này của mình là cố định.
00:05:11 - 00:05:13, Và cố định theo từng bước T.
00:05:13 - 00:05:17, Tức là với mỗi cái bước T chúng ta sẽ có một cái alpha riêng.
00:05:18 - 00:05:20, Và nó sẽ khác so với lại VAE.
00:05:20 - 00:05:24, Đó là cái quá trình encoding này không có tham số huấn luyện phi.
00:05:24 - 00:05:28, Tức là chúng ta không có huấn luyện cái tham số alphaT này.
00:05:28 - 00:05:30, Mà alphaT này là một con số cố định.
00:05:31 - 00:05:34, Vậy thì chúng ta sẽ ghi lại cái công thức ở đây.
00:05:34 - 00:05:37, Đó là qi xT cho trước xT.
00:05:37 - 00:05:39, Quy của xT cho trước xT cho 1.
00:05:39 - 00:05:44, Thì đây chính là một cái phân bố sát xuất trong cái không gian mà chúng ta mạ hóa.
00:05:45 - 00:05:49, Phân bố gauss với min là bằng căng của alphaT xT cho 1.
00:05:49 - 00:05:53, Chúng ta đang làm yếu thông tin của cái ảnh trước đó đi.
00:05:53 - 00:05:57, Rồi thêm vô một cái thành phần nhiễu là 1 trừ alphaT.
00:05:57 - 00:06:01, Thì đây chính là cái công thức của quýt tán thuận.
00:06:02 - 00:06:05, Thế thì chúng ta sẽ đến với một cái kỹ thuật.
00:06:05 - 00:06:09, Đó là kỹ thuật Reparameterization hay gọi là tái xam số hóa.
00:06:09 - 00:06:13, Thì cái này cũng tương tương tương tự như là cái VAE.
00:06:14 - 00:06:18, Nếu như cái thao tác x mà chúng ta xam link, cái ký hiệu này là xam link.
00:06:19 - 00:06:23, Thì cái thao tác xam link này là một cái thao tác không tính đạo hàm được.
00:06:23 - 00:06:26, Đây là một cái thao tác không tính đạo hàm được.
00:06:26 - 00:06:33, Dẫn đến là khi chúng ta sử dụng cái kỹ thuật tán Gradient Ascent, chúng ta sẽ không thể huấn luyện được cái mô hình Diffusion Mode này.
00:06:33 - 00:06:39, Do đó chúng ta sẽ viết lại x dưới dạng là một cái ký hàm số khác.
00:06:39 - 00:06:44, Và thay vì chúng ta lấy mẫu trên x, thì chúng ta sẽ lấy mẫu trên cái biến epsilon.
00:06:44 - 00:06:48, Ví dụ epsilon này sẽ tuân theo cái phân bố chuẩn.
00:06:48 - 00:06:51, Ví dụ như epsilon là nằm ở đây.
00:06:51 - 00:06:58, Thì khi chúng ta lấy epsilon nhân với sigma, rồi cộng cho mi, thì nó sẽ ra cái x nằm ở đây.
00:06:58 - 00:07:08, Nếu chúng ta lấy epsilon nằm ở đây, thì khi chúng ta nhân sigma và cộng epsilon, thì nó sẽ ra cái biến x nằm ở đây.
00:07:08 - 00:07:13, Thế thì đây là một cái thao tác hoàn toàn tương đương.
00:07:13 - 00:07:20, Thay vì chúng ta lấy mẫu trong cái không gian phân bố là gauss, với min là bằng mi và sigma.
00:07:20 - 00:07:24, Bằng mi và sigma, baryon là bằng sigma.
00:07:24 - 00:07:31, Thì chúng ta sẽ lấy trong cái không gian gauss là không phân bố chuẩn, không một.
00:07:31 - 00:07:39, Thì khi chúng ta hướng luyện, thì chuyển đến cái tham số epsilon này rồi, thì nó sẽ không có chuyển đi đâu nữa.
00:07:39 - 00:07:47, Nên cho dù epsilon này là một cái thao tác, là một cái sampling variable từ phân bố chuẩn,
00:07:47 - 00:07:50, nhưng mà chúng ta sẽ không có cập nhật tham số ở đây.
00:07:50 - 00:07:56, Mà chúng ta chỉ đi cập nhật các cái tham số nếu có ở các cái bước mi và sigma này thôi.
00:07:56 - 00:08:04, Vậy thì tại sao chúng ta lại dùng cái công thức này và cái quá trình khích tán thuận,
00:08:04 - 00:08:13, chúng ta không có dùng các cái tham số hướng luyện được, mà chỉ dùng cái alpha và một trừ alpha là những cái tham số mà cố định.
00:08:13 - 00:08:21, Đó là vì cái quá trình encoding, thì chúng ta đơn giản hóa cái mô hình của mình.
00:08:21 - 00:08:25, Nó không có tham số để hướng luyện nên nó sẽ đỡ bị hiện tượng overfitting hơn.
00:08:25 - 00:08:34, Và cái mô hình của mình, decoding về sau, nó bám theo cái công thức này để mà nó cố gắng khôi phục lại cái phân bố sát suất này.
00:08:34 - 00:08:41, Sao cho nó decode giống với cái giá trị này nhất. Thế thì nó giảm được, thứ nhất là nó giảm được cái lỗi.
00:08:41 - 00:08:49, Vì cái này nó không có tham số, tức là cái độ cố định của nó sẽ cao. Mặc dù nó là một cái phân bố sát suất nhưng mà nó sẽ cố định.
00:08:49 - 00:08:56, Còn nếu như cái này mà có tham số vào thì cái việc khuấn luyện của mình nó sẽ vừa phụ thuộc cả encoding mà vừa phụ thuộc cả decoding.
00:08:56 - 00:09:00, Nên cái size số của mình nó sẽ rất là lớn và nó khó hồi tụn.
00:09:00 - 00:09:10, Vậy thì chúng ta sẽ free tham số cái bước mà khuết tán thuận, nó có nhiều cái lợi chống overfitting và cái việc khuấn luyện về sau nó sẽ dễ hơn.
00:09:10 - 00:09:26, Và đồng thời nó có một cái lợi thứ ba đó là với cái cách mà chúng ta samling như thế này thì xt với t bất kỳ chúng ta có thể tính được theo x0, tức là cái ảnh ban đầu mà công thức nó rất là đơn giản.
00:09:26 - 00:09:38, Bây giờ chúng ta sẽ thử với những cái x1 và x2 trước. Thì với x1 thì nó sẽ là bằng căn của alpha 1 nhân với x0 cộng cho căn của alpha 1 nhân với epsilon.
00:09:38 - 00:09:42, Với epsilon đó là một cái biến theo cái phần bố chuẩn.
00:09:43 - 00:09:52, Sau đó chúng ta thé cái công thức x2 thì sẽ là bằng căn của alpha 2 nhân x1 cộng cho căn của 1 nhân alpha 2 nhân epsilon 1.
00:09:52 - 00:09:56, epsilon 1 cũng là một cái biến theo phần bố chuẩn luôn.
00:09:56 - 00:10:13, Thì chúng ta thé cái công thức x1 ở trên đem xuống đây thì lúc này nó sẽ là bằng căn của alpha 2 nhân cho căn của alpha 1 x0 cộng cho căn của 1 trừ alpha 1 epsilon 0.
00:10:13 - 00:10:21, Tất cả cộng cho căn của 1 trừ alpha 2 epsilon 1.
00:10:21 - 00:10:31, Rồi thì chúng ta thé vào đây thì nó sẽ là bằng căn của alpha 1, alpha 2, x0 cộng cho căn của...
00:10:31 - 00:10:44, Thé vào đây thì nó sẽ là alpha 2 1 trừ alpha 1 epsilon 0 cộng cho căn của 1 trừ alpha 2 tất cả nhân cho epsilon 1.
00:10:44 - 00:10:49, Thế thì cái công thức này nó sẽ đưa về đây.
00:10:49 - 00:10:52, Những cái biến đổi của chúng ta hồi nãy nó chính là ở đây.
00:10:52 - 00:11:02, Và ở đây chúng ta sẽ có một chú ý 2 cái biến epsilon 0 và epsilon 1 là 2 cái biến lấy mẫu với cùng một phân bố.
00:11:02 - 00:11:07, Thì khi đó chúng ta cộng lại nó cũng sẽ ra cùng một cái phân bố Gaussian.
00:11:07 - 00:11:16, 2 cái phân bố Gaussian mà có cùng một cái min là bằng 0 thì khi chúng ta cộng lại nó sẽ ra một phân bố Gaussian.
00:11:16 - 00:11:24, Và khi đó cái công thức của cái sigma của mình nó sẽ là bằng 1 trừ alpha 1 nhưng alpha 2.
00:11:24 - 00:11:28, Hay chúng ta biết gọi là truyển khai.
00:11:28 - 00:11:39, Đó là bằng epsilon của căn của cái vế bên tay trái là alpha 2 trừ cho alpha 1 alpha 2.
00:11:39 - 00:11:44, Rồi cộng cho 1 trừ cho alpha 2.
00:11:44 - 00:11:53, Thì đây là một cái công thức mà khá là kinh điển của cái việc mà chúng ta cộng 2 cái biến ngẫu nhiên theo phân bố chuẩn.
00:11:53 - 00:12:00, Thế thì trừ alpha 2 cộng alpha 2 là mất do đó thì chúng ta sẽ còn là 1 trừ alpha 1 alpha 2 epsilon.
00:12:00 - 00:12:11, Và một cách tương tự thì cái xt bất kỳ nó sẽ được tính dưới từ cái x0 và một cái biến epsilon theo phân bố chuẩn.
00:12:11 - 00:12:13, Và công thức của nó sẽ là như đây.
00:12:13 - 00:12:18, Thế thì alpha ở đây sẽ là alpha gạch ngang trên đầu.
00:12:18 - 00:12:21, Đó công thức của cái alpha t gạch ngang trên đầu.
00:12:21 - 00:12:28, Nếu mà viết truyển khai ra thì alpha gạch ngang t sẽ bằng alpha 1 alpha 2... cho đến alpha t.
00:12:28 - 00:12:33, Và epsilon ở đây thì vẫn là một cái biến ngẫu nhiên theo phân bố chuẩn.
00:12:33 - 00:12:37, Thì cái công thức này chúng ta thấy khá là gọn.
00:12:37 - 00:12:41, Và như vậy thì chúng ta đã tìm hiểu xong cái quá trình khích táng thuận.
00:12:41 - 00:12:44, Bây giờ chúng ta sẽ sang cái quá trình khích táng nghịch.
00:12:44 - 00:12:48, Với cái thao tác encoding là chúng ta đang phun thêm nhiễu vào.
00:12:48 - 00:12:53, Vậy thì cái thao tác encoding là chúng ta sẽ tìm cách để đi khử nhiễu.
00:12:55 - 00:12:57, Chúng ta sẽ tìm cách để đi khử nhiễu.
00:12:57 - 00:13:01, Và cái hàm khử nhiễu này nó sẽ là có tham số theta.
00:13:01 - 00:13:08, Hay nói cách khác, encoding theta là một cái hàm số truyền vào cái xt để ra xt triệu một.
00:13:08 - 00:13:11, Sao cho nó bớt đi một cái lượng nhiễu nào đó.
00:13:11 - 00:13:14, Như vậy thì làm sao chúng ta có thể khử nhiễu được?
00:13:14 - 00:13:23, Vì dựa trên cái công thức của cái elbow mà chúng ta đã có ở những slide trước.
00:13:23 - 00:13:28, Qua một số cái phép biến đổi, thì ở đây là do số lượng phép biến đổi quá nhiều.
00:13:28 - 00:13:31, Nên chúng ta chỉ ghi cái kết quả cuối cùng ở đây thôi.
00:13:31 - 00:13:33, Thì nó sẽ có ba cái thành phần số hạng.
00:13:33 - 00:13:40, Số hạng đầu tiên đó là cái kỳ vọng của phân bố sát xuất của x1 cho trước x0.
00:13:40 - 00:13:44, X0 của mình chính là cái hạng gốc ban đầu.
00:13:44 - 00:13:50, Nó sau đó thêm một ít nhiễu để tạo ra thành x1.
00:13:50 - 00:13:56, Thì chúng ta sẽ lấy kỳ vọng trên toàn bộ kết phân bố của x1.
00:13:56 - 00:14:01, Và log của Ptheta x0 cho trước x1.
00:14:01 - 00:14:04, Thì cái ý nghĩa của cái công thức này đó là gì?
00:14:04 - 00:14:08, Ý nghĩa của cái công thức này đó là từ x1 cho trước x1.
00:14:08 - 00:14:11, Chúng ta khôi phục được trở lại x0.
00:14:11 - 00:14:16, Và phân bố sát xuất của Ptheta x0 này là phải cực đại.
00:14:16 - 00:14:21, Tại vì trong công thức của diffusion model là chúng ta cực đại cái kỳ vọng này.
00:14:21 - 00:14:25, Tương đương chúng ta cực đại hóa cái kỳ vọng này.
00:14:25 - 00:14:30, Ở đây vì có dấu trừ nên chúng ta sẽ là minimize, tức là chúng ta tối thiệu hóa cái công thức này.
00:14:30 - 00:14:34, Ở đây có dấu trừ nên chúng ta sẽ minimize cái công thức này.
00:14:35 - 00:14:40, Đối với số hàng đầu tiên đó là reconstruction.
00:14:40 - 00:14:44, Tức là làm sao chúng ta có thể tái tạo lại được cái x0 ban đầu.
00:14:44 - 00:14:47, Sau cho cái sát xuất này là cao nhất.
00:14:47 - 00:14:51, Tức là chúng ta tái tạo lại được cái x0 ban đầu từ cái x1.
00:14:51 - 00:14:54, Tái tạo lại được.
00:14:54 - 00:15:00, Rồi trong cái công thức prior matching, thì ở đây là chúng ta cũng...
00:15:00 - 00:15:05, Nếu như mà các công thức trước thì chúng ta nhớ lại cái công thức của VAR
00:15:05 - 00:15:09, thì cái công thức reconstruction này cũng tương tự như VAR.
00:15:09 - 00:15:11, Prior matching này cũng tương tự như VAR.
00:15:11 - 00:15:17, Đó là chúng ta làm sao để chính quy hóa với một cái prior distribution.
00:15:17 - 00:15:22, Tức là chúng ta mong muốn cái phân bố PXT này của mình, đó là một phân bố chuẩn.
00:15:23 - 00:15:31, Vậy thì làm sao để cho cái quy của XT, cho trước XT từ 1
00:15:31 - 00:15:35, là nó tương theo một cái phân bố chuẩn.
00:15:35 - 00:15:38, Giống như cái PXT này.
00:15:38 - 00:15:41, Thì đây chính là cái thành phần regularization.
00:15:43 - 00:15:48, Rồi, và thành phần cuối cùng, đó chính là cái kỳ vọng này.
00:15:48 - 00:15:52, Rồi, chạy IT chạy từ 1 cho đến T lớn.
00:15:52 - 00:15:57, Thì cái ý nghĩa của cái tổng kỳ vọng này đó là cái sự consistency.
00:15:57 - 00:16:05, Là cái consistency, tức là cái sự nhất quán giữa cái kỳ vọng của cái hàm chúng ta
00:16:05 - 00:16:11, denoise, giải mã, chúng ta khưởng nhiễu.
00:16:11 - 00:16:16, Với lại cái phân bố của quy XT, XT từ 1.
00:16:16 - 00:16:19, Tức là cái phân bố của cái quá trình encode.
00:16:21 - 00:16:25, Dicode nó sẽ giống với cái phân bố mà chúng ta đã từng encode.
00:16:25 - 00:16:30, Thì trong cái slide tiếp theo, chúng ta sẽ có cái mình minh họa rõ hơn cái chỗ này.
00:16:30 - 00:16:35, Các cái thành phần như là reconstruction và prior matching nó tương tự như VE.
00:16:35 - 00:16:39, Và cái sự khác biệt ở đây chính là cái consistency, chúng ta đã nhắc như hồi nãy.
00:16:39 - 00:16:43, Đây chính là cái sự đặng thù riêng của cái diffusion model.
00:16:43 - 00:16:48, Như vậy thì làm sao để có thể huấn luyện được cái hàm P này.
00:16:48 - 00:16:52, Sao cho nó khớp với lại cái quy.
00:16:52 - 00:16:56, Thì ở trong cái hình này, nó minh họa một cái trực quan.
00:16:56 - 00:17:02, Đó là cái quá trình, cái màu tím ở đây, tương ứng là cái màu hồng ở đây.
00:17:02 - 00:17:06, Là cái phân bố của cái XT.
00:17:07 - 00:17:14, Phân bố của XT khi chúng ta được encode từ XT-1, được encode từ XT-1.
00:17:14 - 00:17:17, Tức là chúng ta phun nhiễu và thêm nhiễu từ XT-1.
00:17:17 - 00:17:26, Thì cái phân bố này nó phải khớp với lại cái phân bố của cái Pθ, tức là cái màu xanh của XT, cho trước XT-1.
00:17:26 - 00:17:31, Tức là chúng ta khử nhiễu từ XT-1 để về cái XT.
00:17:31 - 00:17:38, Thì cái kết quả của cái việc thêm nhiễu, nó phải khớp với kết quả của chúng ta khử nhiễu từ cái ảnh phía sau.
00:17:38 - 00:17:44, Thì đây chính là cái ConsistencyLoss.
00:17:44 - 00:17:53, Và chúng ta sẽ biến đổi một chút xíu để chuyển từ cái công thức ở bên Slide này về cái dạng.
00:17:53 - 00:18:00, Đó là chúng ta sẽ tính cái phân bố của cái quy XT.
00:18:00 - 00:18:10, Khi chúng ta biết cả XT và X0, thì tại sao lại như vậy?
00:18:10 - 00:18:21, Trong cái quá trình mà chúng ta add-denoit và denoit, thì nhờ có cái X0, nó sẽ giúp cho chúng ta định hướng được cái quá trình denoit.
00:18:21 - 00:18:27, Và đảm bảo được cái XT của mình, đó là một cái phân bố đúng.
00:18:27 - 00:18:32, Thì khi chúng ta sửa cái công thức bên đây trái lại, viết theo một cái cách khác thì nó sẽ ra như thế này.
00:18:32 - 00:18:37, Và chúng ta chú ý có một số cái sự khác biệt được highlight bởi cái màu tím.
00:18:37 - 00:18:46, Nếu như trong công thức trước, là chúng ta làm sao từ XT-1, chúng ta thêm nhiễu vào, chúng ta thêm nhiễu vào XT.
00:18:46 - 00:18:54, Thì bây giờ, cái công thức của mình là từ X0, chúng ta thêm nhiễu vào để tạo ra cái XT.
00:18:54 - 00:18:59, Và cái XT này nó vẫn theo cái phân bố Gaussian, ờ, thông phân theo cái phân bố chuẩn.
00:18:59 - 00:19:10, Rồi ở cái công thức bên dưới, cái thành phần consistency nó sẽ chuyển thành là cái phân bố của cái hàm denoit.
00:19:10 - 00:19:16, Chúng ta denoit cái XT về cái XT-1.
00:19:16 - 00:19:18, Ờ, denoit.
00:19:19 - 00:19:26, Từ XT về XT-1, thì cái phân bố này, nó phải giống với lại cái phân bố round truth.
00:19:26 - 00:19:31, Round truth là quy của XT-1 cho trước XT và X0.
00:19:31 - 00:19:41, Thì khi chúng ta biết được cái X0 và chúng ta biết được cái XT, thì chúng ta có thể dễ dàng chúng ta nội suy và tính được cái XT-1.
00:19:41 - 00:19:48, Còn nếu mà chúng ta không có cái X0 này, thì chúng ta có thể bị lạch hướng và chúng ta sẽ không biết được là cái XT-1 nó nằm ở đâu.
00:19:48 - 00:19:55, Do đó nhờ có cái thành phần X0 này, nó sẽ giúp chúng ta xác định được cái XT-1.
