00:00:00 - 00:00:22, Chúng ta sẽ cùng đến với một trong những mô hình rất quan trọng trong phần này, đó chính là mô hình GDT4V
00:00:22 - 00:00:37, Chữ V là bí tắc của chữ Vision, đó có tham gia của một language model, là Generative Retrained Transformer
00:00:37 - 00:00:57, Mô hình ngôn ngữ lớn có sự phối hợp với dữ địa về hình ảnh, tuy nhiên đây là một mô hình close-up, nên chúng ta không bàn nhiều về kiến trúc của nó
00:00:57 - 00:01:03, Nhưng tương tưởng và chắc thức bận hành của nó là vấn đề quan trọng mà chúng ta cần phải thảo luận
00:01:03 - 00:01:09, Sau này thì có những mô hình tương tương với GDT4V
00:01:09 - 00:01:23, Ví dụ như là mô hình Lava, chúng ta sẽ tìm hiểu Lava trong những phần tiếp theo đã sử dụng ý tưởng của GDT4V để phát triển
00:01:23 - 00:01:29, GDT4V có cái gì đặc biệt so với mô hình trước đây
00:01:29 - 00:01:37, Đầu tiên chúng ta sẽ xem các tình huống sử dụng của GDT4V, đó là Interleaf Image Text Pair
00:01:37 - 00:01:44, Với cái prompt đầu vào của mình, nó có thể mix với nhiều thể thức khác nhau
00:01:44 - 00:01:52, Ví dụ như ở đây là chúng ta không chỉ có một câu hỏi mà thậm chí chúng ta có đến hai câu hỏi là nó đã phức tạp hơn bình thường rồi
00:01:52 - 00:01:57, Cái tiếp theo đó là cái tấm ảnh, nó cũng sẽ có ba tấm ảnh
00:01:57 - 00:02:08, Và cái câu prompt dạng text ở đây nó sẽ phải đi trương tác với lại các dự điều ảnh này để mà nó đi tìm ra câu trả lời
00:02:08 - 00:02:15, Và thậm chí là trong cái ví dụ bên tay phải nó còn phải có cái sự trương tác giữa hai ảnh với nhau
00:02:15 - 00:02:24, Ví dụ với cái câu prompt này thì nó chỉ đơn giản là nó đi tìm cái con số nào để thể hiện số đó là thuế phải trả
00:02:24 - 00:02:31, Thì in the first receipt thì mình đã trả 3,75 tiền thuế
00:02:31 - 00:02:35, Nó nằm ở đâu? Nó nằm ở bớt tầm, tại khái vậy
00:02:35 - 00:02:39, Nằm ở phía dưới của cái hóa đơn này
00:02:39 - 00:02:44, Rồi in the second receipt thì nó lại có tương tự như vậy, nó nằm ở đâu?
00:02:44 - 00:02:47, Mô tả cái chi tiết cái vị trí của nó
00:02:47 - 00:02:54, Thế thì ở đây là một cái ví dụ mà cái câu prompt của mình nó tương tác lần lược với lại các cái ảnh
00:02:54 - 00:02:56, Thì cái này nó vẫn chưa đủ cái độ phức tạp
00:02:56 - 00:03:01, Cái ví dụ cái bên nó là nó có sự tương tác giữa hai ảnh với nhau
00:03:01 - 00:03:07, Còn ở trên là các cái ảnh là độc lập nhau, nó chỉ đi trả lời cái câu hỏi là bao nhiêu tax?
00:03:07 - 00:03:09, Thế phải đóng thôi
00:03:09 - 00:03:17, Còn ở đây là how much should I pay for the beer on the table according to the price on the menu?
00:03:17 - 00:03:19, Thì ở đây nó có hai cái tấm hình
00:03:19 - 00:03:25, Tấm hình đầu tiên là cái chai bia mà họ hậu uống, cái người chụp hậu uống
00:03:25 - 00:03:31, Và tấm thứ hai đó là cái tấm hình của cái photo và chúng ta thấy cái tấm hình này cũng khá là xấu
00:03:31 - 00:03:34, Nó chụp nghiêng rồi nó tối
00:03:34 - 00:03:39, Thì GPT4V nó đã trả lời đó là nó sẽ lục vào bên trong cái...
00:03:39 - 00:03:44, Đầu tiên là nó sẽ xem cái tấm hình đầu tiên và nó lấy ra được cái tên của cái loại beer
00:03:44 - 00:03:47, Đó là magna
00:03:47 - 00:03:53, Rồi sau đó nó sẽ đi tra vào bên trong đây là cái beer magna thì nó nằm ở đâu?
00:03:53 - 00:03:55, Để mà từ đó là nó...
00:03:55 - 00:03:59, Vì dụ đây, nó tìm ra được cái magna nằm ở đây chẳng hạn
00:03:59 - 00:04:02, Thì nó sẽ trả lời là 6 đô la
00:04:02 - 00:04:06, Và sau đó nó còn lập lụn và thực hiện cái thao tác đếm
00:04:06 - 00:04:12, Là trong cái đây có hai chai bia, giờ đó nó sẽ lấy 6 dân tra 2 là bằng 12
00:04:12 - 00:04:17, Như vậy thì ở đây chúng ta thấy cái mô hình này nó còn có cái sự gọi là reasoning
00:04:19 - 00:04:21, Tức là cái sự suy lụn
00:04:21 - 00:04:27, Chứ nó không chỉ đơn giản là information extraction, tức là rút chức thông tin ra từ tấm ảnh
00:04:27 - 00:04:29, Mà nó có cái sự reasoning ở đây
00:04:29 - 00:04:37, Thì để đạt được cái reasoning này thì nó sẽ phải nhờ đến cái kết quả hoặc là những thành tựu của GBT
00:04:37 - 00:04:50, Là cái mô hình mà retrain dành cho cả decoder để phục vụ cho việc là triệu bán cái văn bản
00:04:50 - 00:04:52, Hoặc là generate tạo sinh ra văn bản
00:04:52 - 00:05:01, Vậy thì GBT4V là một cái mô hình cho phép xử lý đa dạng các dữ liệu đầu vào hay còn gọi là multimodality là đa thể thức
00:05:07 - 00:05:11, Và nó có thể xử lý dữ liệu đầu vào là dạng văn bản thông thường
00:05:11 - 00:05:13, Nó có thể gồm một hoặc là nhiều ảnh
00:05:13 - 00:05:18, Ví dụ như trong cái ví dụ này ta thấy là có thể nâng đến vài ảnh, ba ảnh
00:05:18 - 00:05:23, Rồi văn bản trong ảnh, tức là trong tấm ảnh nó lại có văn bản
00:05:23 - 00:05:26, Bình thường là mình sẽ có văn bản riêng và ảnh riêng
00:05:26 - 00:05:29, Bây giờ trong ảnh nó lại có văn bản
00:05:29 - 00:05:34, Đó, thì đây là một cái ví dụ trong ảnh là có văn bản
00:05:34 - 00:05:41, Là trong tấm hình menu nó sẽ có các cái tên của các loại đủ và giá tiên
00:05:41 - 00:05:48, Rồi visual pointer tức là một cái dạng thức để cho chúng ta tương tác
00:05:48 - 00:05:53, Đối lại là một cái dạng prompt, nó là một cái dạng prompt mới
00:05:53 - 00:05:56, Bình thường mình có prompt là dạng text và ảnh
00:05:56 - 00:06:00, Bây giờ cái prompt của mình có thể là giấu mũi tên giống như chúng ta đang vẽ ở đây
00:06:00 - 00:06:03, Cái mũi tên này nó cũng được gọi là một cái prompt
00:06:04 - 00:06:11, Nếu như cái mô đồ này giả xử như nó giải không được thì chúng ta có thể chỉ vô đây
00:06:11 - 00:06:17, Giá của cái beer magna nó là nằm ở đây, mình chỉ vô
00:06:17 - 00:06:22, Mô hình của mình sẽ hiểu được cái visual pointer này như là một cái loại prompt
00:06:22 - 00:06:29, Vậy thì GPT-4V nó có một vài thật chứng chính, đó là gì
00:06:29 - 00:06:34, Đây là một cái bài toán có thể được thực hiện với GPT-4V
00:06:34 - 00:06:39, Đây là những cái bài toán mà GPT thực hiện được bao gồm những task rất là lơn giản
00:06:39 - 00:06:49, Ví dụ như là mô tải hình ảnh, image description, image captioning, rồi nhận dạng ảnh, recognition on different domains
00:06:49 - 00:06:54, Rồi kết hợp kiến thức đa thể thức, multimodal knowledge
00:06:54 - 00:07:02, Trong cái ví dụ ở trên chúng ta thấy là nó có sử dụng cái knowledge của văn bảng là text
00:07:02 - 00:07:06, Đồng thời nó cũng có sử dụng cái knowledge của tấm ảnh
00:07:09 - 00:07:12, Rồi nó có sử dụng cái text trong ảnh
00:07:17 - 00:07:19, Thì đó là multimodality
00:07:19 - 00:07:23, Và có thể tương tác với các kinh tức tận quát
00:07:23 - 00:07:27, Ví dụ như nó có thể hiểu về những người nổi tiếng như là David Beckham
00:07:27 - 00:07:29, Người nổi tiếng này
00:07:29 - 00:07:35, Rồi các cái địa danh như là Paris, Hà Nội v.v.
00:07:35 - 00:07:41, Địa danh nổi tiếng thì đó là những cái kiến thức tận quát
00:07:41 - 00:07:50, Rồi và đồng thời nó có khả năng quan trọng là hiểu và suy luận
00:07:50 - 00:07:53, hay gọi là reasoning trên cái văn bảng đơn thuần
00:07:53 - 00:07:58, hoặc là văn bảng trong ảnh syntax understanding hoặc document reasoning
00:07:58 - 00:08:06, Thì đây chính là những cái khả năng nổi trội của GPT-4V so với những mô hình ngôn ngữ thị giác
00:08:06 - 00:08:08, mà chúng ta đã tìm hiểu ở phía trước
00:08:09 - 00:08:13, Tiếp theo thì chúng ta sẽ cùng tìm hiểu về khái niệm visual pointer
00:08:13 - 00:08:19, Ở bên tay phải là một hình ảnh ví dụ về visual pointer
00:08:19 - 00:08:25, Bên cạnh câu mô tả là display the pointed reason in the image
00:08:25 - 00:08:27, Và chúng ta sẽ có một tấm ảnh
00:08:27 - 00:08:30, Trong tấm ảnh này, nó sẽ có một đường màu đỏ
00:08:30 - 00:08:34, Đây chính là một ví dụ của visual pointer
00:08:38 - 00:08:44, Vì visual pointer sẽ hướng dẫn cho mô hình tập trung vào những phần quan trọng của tấm ảnh
00:08:44 - 00:08:48, Thay vì chúng ta nhìn vô, tấm ảnh này sẽ có cả rừng chữ và số
00:08:48 - 00:08:56, Với đường khoanh màu đỏ này, mô hình của mình biết là sẽ tập trung vào đây để phân tích số liệu của mình
00:08:56 - 00:09:02, Với đường màu đỏ này, mô hình GPT-4V đã nêu được
00:09:02 - 00:09:07, Và chúng ta đã đưa ra các phần tích tương ứng của tấm ảnh
00:09:07 - 00:09:12, Và chúng ta đã đưa ra các phần tích tương ứng của tấm ảnh
00:09:12 - 00:09:18, Vậy thì ngoài đường khoanh màu đỏ, nó sẽ còn những dạng visual pointer nào
00:09:18 - 00:09:23, Ví dụ như chúng ta có thể đưa vào tỏa độ dạng số, hoặc là coordinate
00:09:23 - 00:09:31, Hoặc chúng ta có thể đưa vào blockbox, trong đó chúng ta loại bỏ hết tất cả những phần ảnh không liên khoan
00:09:31 - 00:09:37, Và chỉ chừa cái vùng ảnh có liên quan đến việc suy luận hoặc là cái việc trả lời câu hỏi của chúng ta
00:09:37 - 00:09:44, Hoặc là nó có thể ở dạng là một cái mũi tên, chỉ vào những đối tượng mà chúng ta đang muốn quan tâm
00:09:44 - 00:09:53, Thì đây có thể là một trong những dạng khá là thú vị và gần với cách thoại người tương tác khi mà trao đổi với nhau trên hình ảnh
00:09:54 - 00:10:04, Rồi cái dạng nữa đó là chúng ta có thể dùng một cái box, một cái hồn kèm cái ảnh gốc thì nó sẽ có thêm một cái đường màu đỏ để khoanh mùa cái đối tượng chúng ta quan tâm
00:10:04 - 00:10:13, Và có những cái dạng mà freestyle hơn, ví dụ như là hình oval, hoặc là hand drawing, tức là một cái đường net tự do
00:10:14 - 00:10:22, Với cái visual pointer, nó đã giúp cho cái việc tương tác giữa người và máy tính trở nên thuyện tiện hơn
00:10:22 - 00:10:34, Và đây có lẽ là một trong những cái thể thức quan trọng đặc biệt mà GPT-4V nó khác biệt so với lại những cái mô hình vision language, cái mô hình thị giác ngôn ngữ trước đây
00:10:35 - 00:10:46, Vậy thì một vài cái ví dụ nữa để cho chúng ta thấy cái tính hiệu quả của GPT-4V liên quan đến cái việc là reasoning
00:10:46 - 00:10:56, Nếu như chúng ta đưa vào một cái code prompt đó là count number of apples in the image thì nó sẽ đếm sai là có 12 quả táo
00:10:56 - 00:11:06, Nâng cấp hơn một chút xíu thì mình sẽ chỉ dẫn cho nó, đó là thêm một cái câu vết đằng sau đó là suy nghĩ step by step
00:11:06 - 00:11:18, Thì nó sẽ đưa ra là có 4 step, rồi step 1 là tính như thế nào, step 2 là làm gì, step 3 là làm thế nào, step 4 thậm chí đã đúng project lại
00:11:18 - 00:11:29, Nhưng cuối cùng nó vẫn ra sai và chỉ đến khi chúng ta đưa ra một cái chỉ dẫn đầy đủ và các cái bước đủ đơn giản
00:11:37 - 00:11:41, Thì nó mới có thể làm đúng ví dụ, câu đầu giống như nó khen
00:11:41 - 00:11:45, You are an expert in counting things in the image
00:11:45 - 00:11:57, Rồi, hãy đến những số lượng apple trong tám ảnh này row by row và đảm bảo rằng là nó ra được cái kết quả chính xác
00:11:57 - 00:12:07, Thế thì nó sẽ set trong tổng hình này thì nó sẽ có 4 dòng và với mỗi dòng nó sẽ lần lượt luyện kê ra, nó sẽ đưa ra cái con số đếm
00:12:07 - 00:12:17, Vì dù như là 4 apple, dòng số 2 là có 4 apple, dòng số 3 là có 3 apple và cuối cùng nó sẽ ra được con số đúng là 11 apple
00:12:17 - 00:12:31, Rồi, thì như vậy, GBT4V ở đây là một cái ví dụ cho chúng ta thấy là nó có thể đưa ra, chúng ta có thể đưa vào các cái chỉ dẫn cộng với cái tám ảnh
00:12:31 - 00:12:45, Và cái chỉ dẫn này thì biết nó giống như là một cái code prom mà chúng ta trò chuyện với JackBot, chỉ dẫn cho nó biết là phải làm như thế nào để mà suy lận
00:12:45 - 00:12:56, Thì đây chính là một cái ví dụ khi sử dụng GBT4V giống như là chúng ta sử dụng với cái mô hình GBT4O hoặc GBT4 của nguyên dựng JackGBT
00:12:57 - 00:13:04, Rồi, cái Incontext FuelShot Learning thì ở đây là một cái ví dụ ZeroShot
00:13:04 - 00:13:19, Đại đa số mọi người khi mà làm việc thì đều hay sử dụng ZeroShot, tại vì thứ nhất là họ nghĩ rằng cái mô hình của mình là tốt, cái mô hình của mình là xịn
00:13:19 - 00:13:25, Giờ đến đây là cái gì cũng biết, cái gì cũng biết
00:13:25 - 00:13:34, Cái thứ hai là bản thân mình là cái người sử dụng thì mình cũng lười, mình có nhiều hướng dẫn cho nó nhiều thì mình lười
00:13:34 - 00:13:40, Thì đó là hai cái ế tố khiến cho ZeroShot là một trong những cái kỹ thuật được sử dụng rất là phổ biến
00:13:40 - 00:13:50, Ví dụ trong ví dụ này là chúng ta đưa vào cái prom là In the Paragraph with years had the highest average gas price ở trong tháng 6
00:13:50 - 00:13:55, Thì ở đây là mô hình trả lời sai là 3,3 đô
00:13:55 - 00:14:04, Nếu mà chúng ta dùng là ZeroShot nhưng mà chúng ta kêu nó là thing step by step thì kết quả của mình cũng sai
00:14:04 - 00:14:12, Chỉ khi chúng ta đưa vào cái Incontext FuelShot, cụ thể ở đây là hai shot
00:14:12 - 00:14:17, Thì cái Incontext FuelShot này có nghĩa là gì? Chúng ta sẽ cho nó một cái cặp câu hỏi
00:14:17 - 00:14:27, Và ví dụ, câu hỏi ví dụ, cái đáp án thì nó sẽ bám theo cái cặp suy luận đó để mà nó trả lời cho những câu hỏi mới
00:14:27 - 00:14:37, Ví dụ như ở đây là chúng ta hỏi, ở đây chúng ta sẽ đưa cho nó là chỉ dẫn thêm là cái đô thị này
00:14:37 - 00:14:49, Plot the National Gas Price, tức là plot giá gas ở trong nước là từ 2016 cho đến 2019
00:14:49 - 00:14:57, Rồi nó mô tả ra chi tiết là màu đỏ là gì, màu xanh là gì, màu xanh lá là gì, v.v.
00:14:57 - 00:15:10, Rồi sau đó thì nó sẽ đưa ra tính toán là năm mà có cái High Gas Price là vào tháng 6 năm 2018
00:15:10 - 00:15:19, Thì ở đây là nó đưa ra những cái FuelShot, tức là cái kết quả
00:15:19 - 00:15:26, Vậy thì GPT nó đã dựa trên cái lập luận tương tự ở phía trên là với hai shot, đây là shot số 1
00:15:26 - 00:15:30, Đây là shot số 2 ở phía bên dưới, nó sẽ còn một cái câu nữa
00:15:30 - 00:15:39, Ví dụ thì khi chúng ta đưa vào một số liệu mới, ở trên là chúng ta cho ví dụ 2019, 2018
00:15:39 - 00:15:50, Và với số liệu mới này là 2023 thì nó sẽ tự động lập luận y trang như thế này để tìm ra được giá gas mà cao nhất là bao nhiêu
00:15:50 - 00:15:55, Thì nó bắt trước hai cái shot ở phía trên cho một cái loại dự liệu mới
00:15:55 - 00:16:08, Và cái kết quả nó ra được đó là tháng 6 năm 2020, thì đó chính là cái hướng dẫn cho cái mô hình, cái cách thức để mà nó lập luận
00:16:08 - 00:16:13, Và với FuelShot hay còn gọi là In-context learning
00:16:14 - 00:16:24, Như vậy tổng kết lại chúng ta đã cùng tìm hiểu qua các cái mô hình, mô hình đầu đầu như là clip
00:16:24 - 00:16:32, Và cho đến bây giờ thì vẫn được dùng nhiều, clip thì dùng nhiều cho bài toán là Zero Shot Image Classification
00:16:32 - 00:16:41, Sau đó chúng ta có phiên bản là G-Clip, với cái sự cũng là Zero Shot nhưng mà cho bài toán Detection
00:16:41 - 00:16:51, Rồi nâng lên là có mô hình clip, sau đó là sẽ có Visual Programming
00:16:54 - 00:16:59, Rồi cái mô hình mà chúng ta vừa mới tìm hiểu đó chính là GPT-4V
00:17:02 - 00:17:10, Vậy thì cái việc mà chúng ta sẽ có cái nhận định gì khi chúng ta đã tìm hiểu qua các cái mô hình này
00:17:10 - 00:17:19, Đó là cái việc mà chúng ta huấn luyện một cái mô hình từ đầu cho cái mô hình mô mụ thị giác thì nó cần rất nhiều tài nguyên tính toán
00:17:19 - 00:17:22, Cũng như là cái dữ liệu của chúng ta rất là lớn
00:17:22 - 00:17:30, Nói vui đó là nhiều khi cái dữ liệu của mình nó lớn đến nỗi mà chúng ta không đủ cưỡng để chứa chứ đừng nói đến cái chuyện là chúng ta huấn luyện mô hình
00:17:30 - 00:17:34, Tại vì quy mô nó lên đến Internet Scale
00:17:37 - 00:17:39, Internet Scale Dataset
00:17:39 - 00:17:43, Và do đó thì thông thường chúng ta sẽ sử dụng cái mô hình đã huấn luyện sẵn
00:17:43 - 00:17:49, Nhưng mà quan trọng là chúng ta sẽ phải biết được cái công năng của từng mô hình, của từng phần trong mô hình là gì
00:17:49 - 00:18:00, Ví dụ như khi chúng ta nhìn vào cái mô hình clip thì chúng ta biết cái mô hình nào là mô hình chúng ta sẽ sử dụng để cho cái công việc gọi là Unimodo encoding
00:18:00 - 00:18:18, Và khi nào thì chúng ta sẽ sử dụng cái mô đồ, mà cross model hay là image route, image route là text matching
00:18:18 - 00:18:31, Và khi nào thì chúng ta sẽ sử dụng cái language model ở phía sau để cho cái tác vụ là text generation
00:18:31 - 00:18:36, Thì chúng ta biết được cái công năng của từng mô hình và dùng mô hình nào là phù hợp cho cái bài toán của chúng ta
00:18:36 - 00:18:42, Cái thứ hai đó là phối hợp các cái thành phần huấn luyện sẵn đó cho cái bài toán của mình
00:18:42 - 00:18:48, Ví dụ như chúng ta khai thác cái module để mả hóa hình ảnh hoặc module mả hóa văn bản
00:18:48 - 00:19:06, Tại vì cái việc mà chúng ta cho hình ảnh và văn bản tương tác với nhau để học ra được encoding thì nó sẽ giúp cho chúng ta có cái tính tổng quát, có cái tính tương tác và phân biệt được ngữ nghĩa một cách rõ ràng hơn so với việc chúng ta chỉ học dựa trên văn bản không hoặc chỉ học dựa trên hình ảnh không
00:19:06 - 00:19:11, Và sử dụng các kỹ thuật Prom Engineer một cách hiệu quả
00:19:11 - 00:19:17, Ví dụ như chúng ta sử dụng cái In-contact learning với kỹ thuật FieldShop Prompting
00:19:17 - 00:19:30, Chúng ta sẽ cho nó khoảng hai, ba ví dụ về cái instruction và cái instruction và cái kết quả của mình
00:19:30 - 00:19:41, Thì kết quả, cái Resol, tờn thì nó sẽ bắt trước cái instruction và cái Resol này
00:19:41 - 00:19:50, Khi chúng ta có một cái new instruction thì nó sẽ giúp cho chúng ta dự đoán ra được cái kết quả
00:19:50 - 00:19:54, Nó sẽ đưa ra một cái kết quả dự bán
00:19:54 - 00:20:00, Thì đó là ý tưởng của In-contact learning và đây là một trong những kỹ thuật dùng cũng rất là phổ biến
00:20:00 - 00:20:09, Rồi hướng dẫn mô hình suy luận một cách có hệ thống, chúng ta sẽ cho nó suy nghĩ theo kiểu step by step
00:20:09 - 00:20:15, Rồi có thể chỉ dẫn cho nó chi tiết hơn là chúng ta sẽ chia nó ra thành bước 1, bước 2, bước 3 như thế nào
00:20:15 - 00:20:22, Bước 1 chi tiết là sao? Càng đơn giản thì mô hình sẽ dễ thực hiện theo
00:20:22 - 00:20:26, Rồi hướng dẫn cho nó cách suy luận
00:20:26 - 00:20:35, Và cuối cùng đó là sử dụng Visual Pointer thì đây là một kỹ thuật để giúp cho chúng ta có thể hỗ trợ cho người dùng
00:20:35 - 00:20:38, Tạo ra một cái prompt một cách đơn giản và tự nhiên
00:20:38 - 00:20:48, Chúng ta có thể tạo ra một cái visual pointer là dạng mũi tên, nó có thể là một cái box hoặc là một cái script ball như thế này
00:20:48 - 00:20:50, Một cái đường mà zigzag
00:20:50 - 00:20:58, Vậy thì trên đây đó là một vài cái mô hình đầu tiên khi nói về mô hình ngôn ngữ thì giác
00:20:58 - 00:21:04, Trong những phần tiếp theo thì chúng ta sẽ nói về những cái mô hình hiện đại hơn, được trend trên những dữ liệu lớn hơn
00:21:04 - 00:21:12, Và phục vụ cho các cái bài toán mà chúng ta đã đề cập trước đây, ví dụ như bài toán segmentation
00:21:12 - 00:21:19, Và bài toán sinh ngôn ngữ text generation
00:21:19 - 00:21:24, Cụ thể đó là cái mô hình là lava
00:21:24 - 00:21:32, Còn đối với cái segmentation thì chúng ta có thể sử dụng hai cái mô hình, đó là Sam routing Dino
00:21:32 - 00:21:41, Đây là hai cái mô hình ZeroSort, Segmentation
00:21:41 - 00:21:49, Và mô hình SIM là một cái mô hình tương tác đa thể thức
00:21:49 - 00:21:54, Trong những phần tiếp theo chúng ta sẽ tìm hiểu về các cái mô hình này
