00:00:00 - 00:00:21, Chúng ta sẽ đến với thuật toán x-propiation sau khi chúng ta đã thực hiện được thuật toán lang truyền thuận feedforward rồi
00:00:21 - 00:00:29, Chúng ta sẽ đến với thuật toán lang truyền ngược và vẫn sẽ sử dụng những ví dụ như đã nói ở trước
00:00:29 - 00:00:32, đó là fx, ez sẽ là x, y
00:00:32 - 00:00:44, Về nguyên lý, chúng ta sẽ sử dụng chen rule, tức là quy tắc để tính đạo hàm của hàm hợp
00:00:44 - 00:00:54, Chúng ta sẽ áp dụng chen rule trên đồ thị tính toán và tính tại những giá trị cụ thể, đó là x bằng 3, y bằng trường 4 và z bằng 5
00:00:54 - 00:01:01, Bước đầu tiên là đồ thị tính toán, đây là bước số 1
00:01:01 - 00:01:03, Chúng ta sẽ tạo ra đồ thị tính toán
00:01:04 - 00:01:10, Rồi, thì nó cũng tương tự như trong lang truyền thuận, chúng ta sẽ có các cái node là node cộng và node nhân ở đây
00:01:10 - 00:01:18, và tương ứng đầu ra sẽ là f1 và f2, cái f2 này bản chất nó chính là cái f của mình
00:01:18 - 00:01:24, Sau đó thì chúng ta sẽ chạy cái thuật toán lang truyền thuận
00:01:24 - 00:01:30, Khi chúng ta lang truyền thuận lên thì chúng ta sẽ có cái giá trị f đầu ra chính là bằng trường 5
00:01:30 - 00:01:33, Sau đó thì chúng ta sẽ áp dụng cái nguyên tắc của chen rule
00:01:37 - 00:01:41, Thì chúng ta sẽ... cái bước này là bước số 3 ha
00:01:41 - 00:01:46, Bước số 3, còn cái bước số 2 chính là lang truyền thuận
00:01:52 - 00:01:56, Rồi, thì chúng ta áp dụng chen rule trên cái cấu trúc đồ thị tính toán như thế này
00:01:57 - 00:02:01, Đầu tiên, đó là chúng ta sẽ khởi tạo một cái con số 1
00:02:01 - 00:02:11, Tại vì trong cái nguyên tắc của chen rule thì chúng ta sẽ phải tính các effect nhân rất là nhiều
00:02:11 - 00:02:15, Ví dụ như d của f thứ n, nhân cho d của f thứ n trừ 1
00:02:15 - 00:02:22, Sau đó d của f n trừ 1, d của d của f n trừ 2, v.v.
00:02:22 - 00:02:25, Thì chúng ta sẽ phải thực hiện một chuỗi các effect nhân này
00:02:25 - 00:02:31, Do đó khởi tạo nó sẽ là giá trị 1 để có thể nhanh, tiếp tục nhanh
00:02:31 - 00:02:34, Còn nếu chúng ta khởi tạo bằng 0 thì không nhanh với bao nhiêu cũng bằng 0, không được
00:02:34 - 00:02:41, Do đó thì khởi tạo đối với cái effect tích thì chúng ta sẽ sử dụng cái số 1 ở đây
00:02:41 - 00:02:47, Sau đó chúng ta sẽ đi tính đạo hàm của f2 theo f1
00:02:47 - 00:02:50, Chúng ta sẽ tính đạo hàm của f2 theo f1
00:02:50 - 00:02:53, Sau đó sẽ nhanh với 1, sẽ là như vậy kết quả ở đây
00:02:53 - 00:02:55, Đó là đạo hàm của f2 theo f1
00:02:55 - 00:03:01, Đạo hàm của f2 theo f1 chúng ta đã biết đó là nó chính là bằng z
00:03:01 - 00:03:05, Đạo hàm của f2 theo f1 tức là đạo hàm của cái công thức này
00:03:05 - 00:03:11, Và đạo hàm của cái công thức f2 ở đây đó chính là bằng z
00:03:11 - 00:03:15, Z của chúng ta lúc này nó đang là bằng 5
00:03:15 - 00:03:19, Do đó chúng ta sẽ lấy cái giá trị này vào và thế vào vào đây
00:03:19 - 00:03:25, Là 1 nhân với 5 là bằng 5
00:03:25 - 00:03:29, Sau đó chúng ta sẽ lấy cái công thức này
00:03:29 - 00:03:38, Đạo hàm của f theo x nó sẽ là bằng 1 nhân với đạo hàm của f2 theo f1 dựa trên cái công thức này
00:03:38 - 00:03:45, Rồi nhân cho đạo hàm của f1 theo x thì đạo hàm của f2 theo f1 chúng ta đã biết nó là bằng z
00:03:46 - 00:03:53, Còn đạo hàm của f1 theo x nó chính là bằng 1 ở đây
00:03:53 - 00:03:56, Đạo hàm của f1 theo x nó sẽ là bằng 1
00:03:56 - 00:04:00, Như vậy thì nó sẽ là bằng 1 nhân z nhân với lại 1
00:04:00 - 00:04:03, 1 nhân z nhân 1 tức là bằng 5 nhân 1 là bằng 5
00:04:03 - 00:04:08, Như vậy đạo hàm của f theo x sẽ là bằng 5
00:04:08 - 00:04:13, Và chúng ta hoàn toàn làm tương tự như vậy cho các biến y và biến z
00:04:13 - 00:04:20, Thì đây là thuật toán để tính sử dụng chen rule trên cấu trúc đồ thị tính toán
00:04:20 - 00:04:24, Nhưng ở đây thực tế thì sao?
00:04:24 - 00:04:26, Thực tế nếu chúng ta làm như vậy
00:04:26 - 00:04:30, Khi tính đến từ đạo hàm của f cuối cùng
00:04:30 - 00:04:36, Đạo hàm của f theo x thì nó sẽ có rất nhiều những theo tác tính toán thừa
00:04:36 - 00:04:42, Nó bị lặp lại do đó chúng ta sẽ che đi chen rule
00:04:42 - 00:04:50, Mà chúng ta sẽ trực tiếp kế thừa những cái giá trị đã tính toán ở những bước trước đó để trọc nhật cho phía sau
00:04:50 - 00:04:54, Để không làm tiết kiệm chi phí tính toán
00:04:56 - 00:04:59, Không có tính lại
00:04:59 - 00:05:04, Không tính lại những cái theo tác thừa
00:05:07 - 00:05:09, Thì tại sao là thừa?
00:05:09 - 00:05:13, Tại vì ở trong quá trình tính toán thì chúng ta sẽ tính đạo hàm của f2 theo f1
00:05:13 - 00:05:15, Chúng ta đã có cái giá trị ở đây rồi
00:05:15 - 00:05:20, Nhưng mà sang đây chúng ta lại đi tính lại đạo hàm của f2 theo f1
00:05:20 - 00:05:22, Thì đó là thừa
00:05:22 - 00:05:26, Chúng ta chỉ cần copy cái giá trị từ bên đây qua đây
00:05:26 - 00:05:28, Thì nó sẽ không phải đi tính lại
00:05:28 - 00:05:31, Thì chi tiết cái thực toán back propagation
00:05:31 - 00:05:36, Với cái phiên bản đó là chúng ta che đi cái hàm rule, chen rule như thế nào
00:05:36 - 00:05:43, Bước đầu tiên là chúng ta sẽ xây dựng đồ thị tính toán và thực hiện cái thực toán feed forward
00:05:43 - 00:05:48, Sau khi feed forward xong thì chúng ta sẽ nhận được các cái giá trị tại các cái node
00:05:52 - 00:05:56, Sau đó thì khởi tạo cho cái thực toán back propagation
00:05:56 - 00:05:58, Chúng ta sẽ bắt đầu bằng số 1
00:05:58 - 00:06:01, Sau đó các bước của thực toán rất đơn giản là như sau
00:06:01 - 00:06:09, Đầu tiên chúng ta sẽ copy cái giá trị đạo hàm ở cái node cha
00:06:09 - 00:06:11, Tức là cái node trước ngay đằng sau đó
00:06:11 - 00:06:13, Kéo về
00:06:13 - 00:06:14, Kéo về đây
00:06:14 - 00:06:21, Sau đó chúng ta sẽ nhân với đạo hàm của cái thành phần f2
00:06:21 - 00:06:23, Chia cho đạo hàm của f1
00:06:23 - 00:06:26, Tức là lấy cái đạo hàm của cái node phía sau
00:06:26 - 00:06:29, Theo cái đạo hàm của cái node hiện tại
00:06:29 - 00:06:31, Cái node hiện tại của mình chính là f1
00:06:31 - 00:06:33, Tức là đạo hàm của f2 theo f1
00:06:33 - 00:06:36, Thì đạo hàm của f2 theo f1 chính là bằng z
00:06:36 - 00:06:40, Z của mình trong trường hợp này là bằng 5
00:06:40 - 00:06:42, Do đó nó sẽ là bằng 5
00:06:42 - 00:06:45, Sau đó thì quay trở lại
00:06:45 - 00:06:48, Chúng ta sẽ đến cái x
00:06:48 - 00:06:50, Thay vì chúng ta tính lại f2 theo f1
00:06:50 - 00:06:54, Đạo hàm của f2 theo f1 thì ở đây chúng ta sẽ copy khí số 5 này qua
00:06:54 - 00:07:00, Chúng ta sẽ copy khí số 5 và sau đó chúng ta chỉ cần tính đạo hàm của cái node ngay phía sau
00:07:01 - 00:07:03, Với cái đạo hàm của cái node hiện tại đó là x
00:07:03 - 00:07:05, Tức là đạo hàm của f1 theo x
00:07:05 - 00:07:08, Đạo hàm của cái node ngay phía sau
00:07:08 - 00:07:12, Và cái đạo hàm của cái node hiện tại đó là đạo hàm của f1 theo x
00:07:12 - 00:07:16, Và đạo hàm của f1 theo x thì tức là bằng 1
00:07:16 - 00:07:20, Đạo hàm của f1 theo x
00:07:20 - 00:07:24, Do đó nó sẽ là bằng 5
00:07:24 - 00:07:30, Rồi, như vậy thì tương tự như vậy chúng ta sẽ làm cho cái biến y
00:07:30 - 00:07:33, Chúng ta cũng sẽ xây dựng cái cấu trúc đồ thị tính toán
00:07:33 - 00:07:36, Và thực hiện cái tục toán feedforward
00:07:36 - 00:07:38, Hoàn toàn tương tự như với biến x
00:07:38 - 00:07:40, Nhưng mà khi chúng ta lan truyền ngược
00:07:40 - 00:07:42, Thì tại đây chúng ta sẽ bắt đầu bằng 1
00:07:42 - 00:07:46, Sau đó chúng ta sẽ chép cái số 1 này qua cái node này
00:07:46 - 00:07:48, Vì chúng ta muốn quay về y
00:07:48 - 00:07:51, Thì để đi về y bắt buộc chúng ta phải đi qua cái node f1
00:07:51 - 00:07:53, Bắt buộc phải qua cái node f1
00:07:53 - 00:07:56, Rồi, sau đó chúng ta sẽ tính đạo hàm của f2 theo f1
00:07:57 - 00:08:02, Đạo hàm của f2 theo f1 chính là bằng đạo hàm của f2 theo f1
00:08:02 - 00:08:04, Thì đây là cái phép nhân
00:08:04 - 00:08:06, Đây là phép nhân nên đạo hàm của nó sẽ là bằng gì?
00:08:06 - 00:08:10, Chúng ta sẽ lấy số 5 qua là bằng 5
00:08:10 - 00:08:13, Sau đó chúng ta sẽ cộp y cái số 5 này
00:08:13 - 00:08:15, Đến cái vị trí của cái node y
00:08:15 - 00:08:18, Đó là cái node mà chúng ta cần tính đạo hàm về
00:08:18 - 00:08:21, Sau đó chúng ta sẽ lấy đạo hàm của cái node tiếp theo đó là f1
00:08:21 - 00:08:24, Chi cho cái node hiện tại đó là y
00:08:24 - 00:08:28, Tức là d của f1 theo di trên di
00:08:28 - 00:08:30, d của f1 trên di
00:08:30 - 00:08:33, Đạo hàm của hàm này theo biến y
00:08:33 - 00:08:35, Tức là bằng 1
00:08:37 - 00:08:39, Như vậy nó sẽ là bằng 5 nhân 1 là bằng 5
00:08:42 - 00:08:45, Rồi, chúng ta làm tương tự như vậy cho biến z
00:08:45 - 00:08:47, Chúng ta sẽ khởi tạo bằng 1
00:08:47 - 00:08:51, Thì để đi về z chúng ta sẽ không có đi qua cái con đường này
00:08:51 - 00:08:53, Mà chúng ta đi trực tiếp về đây luôn
00:08:53 - 00:08:55, Do đó chúng ta sẽ cộp y cái số 1 vào đây
00:08:55 - 00:09:00, Và sau đó chúng ta sẽ tính đạo hàm của cái node tiếp theo
00:09:00 - 00:09:04, trên cái node hiện tại tức là d của f2 theo z
00:09:04 - 00:09:06, d của f2 theo z
00:09:06 - 00:09:08, Thì d của f2 theo z
00:09:10 - 00:09:13, thì nó chính là bằng x cộng y
00:09:13 - 00:09:15, nó chính là bằng x cộng y
00:09:15 - 00:09:19, d của f2 theo z nó chính là x cộng y tức là f1
00:09:19 - 00:09:22, Như vậy chúng ta sẽ cộp y cái giá trị f1 xuống
00:09:22 - 00:09:24, cộp y giá trị f1 xuống
00:09:24 - 00:09:26, thì nó sẽ là bằng 1 nhân quay lại trừ 1
00:09:26 - 00:09:28, tức là bằng trừ 1
00:09:28 - 00:09:30, thì đây chính là thuộc toán backpropagation
00:09:30 - 00:09:34, và chúng ta sẽ tiến hành
00:09:34 - 00:09:36, tại sao chúng ta ở cái bước này
00:09:36 - 00:09:38, chúng ta suy nghĩ là mình nói rất nhiều
00:09:38 - 00:09:42, nhưng mà tại sao chúng ta lại đang ở đây
00:09:42 - 00:09:44, thì quay trở lại
00:09:44 - 00:09:46, thuộc toán backpropagation
00:09:46 - 00:09:50, mục tiêu của mình đó là chúng ta sẽ đi tính đạo hàm
00:09:50 - 00:09:54, theo những cái biến x, y, z
00:09:54 - 00:09:56, tức là những cái biến đồ vào
00:09:56 - 00:09:58, và mục tiêu của mình đó chính là để đi
00:09:58 - 00:10:02, chạy cái thuộc toán gradient descent
00:10:02 - 00:10:06, chúng ta sẽ chạy cái thuộc toán gradient descent
00:10:06 - 00:10:08, và thuộc toán gradient descent
00:10:08 - 00:10:12, thì nó sẽ có cái công thức đó là
00:10:12 - 00:10:14, ở trong công thức gốc thì x theta
00:10:14 - 00:10:16, sẽ là bằng theta trừ cho alpha
00:10:16 - 00:10:20, x theta là alpha nhân với đạo hàm
00:10:20 - 00:10:24, của chạy theo theta
00:10:24 - 00:10:30, ở đây nếu mà chúng ta dùng theta là một vector
00:10:30 - 00:10:34, thì nó sẽ là nabla của chạy theo theta
00:10:34 - 00:10:36, trong đó theta này sẽ bao gồm các thành phần
00:10:36 - 00:10:38, thì ở trong cụ thể theta của mình
00:10:38 - 00:10:40, nó sẽ là bao gồm x, y và z
00:10:40 - 00:10:44, x, y và z
00:10:44 - 00:10:48, và khi chúng ta thực hiện cái back propagation
00:10:48 - 00:10:50, chúng ta thực hiện cái gradient descent
00:10:50 - 00:10:52, chúng ta cũng sẽ cập nhật trên từng tham số thành phần
00:10:52 - 00:10:58, thì x sẽ là bằng x trừ cho alpha nhân đạo hàm của f theo x
00:10:58 - 00:11:02, nó cũng lấy từ công thức này ra bỏ xuống
00:11:02 - 00:11:06, rồi y sẽ là bằng y trừ alpha nhân cho đạo hàm
00:11:06 - 00:11:08, của f theo y
00:11:08 - 00:11:12, x sẽ là bằng z trừ cho alpha nhân đạo hàm của f theo z
00:11:12 - 00:11:14, thì tại sao chúng ta lại ở đây?
00:11:14 - 00:11:16, đó là vì chúng ta phải đi tính đạo hàm
00:11:16 - 00:11:20, mà để tính đạo hàm đối với những cái hàm phức tạp
00:11:20 - 00:11:24, thì chúng ta phải sử dụng cái thực toán back propagation
00:11:24 - 00:11:26, thì đó là lý do tại sao
00:11:26 - 00:11:28, thì ở đây chúng ta sẽ có một cái bài tập
00:11:28 - 00:11:30, đó là xây dựng đồ thị tính toán
00:11:30 - 00:11:34, và chạy thực toán lang truyền thuận cũng như là lang truyền ngược
00:11:34 - 00:11:36, trên cái hàm như sau
00:11:36 - 00:11:46, đó là f x theta 0 theta 1 bằng 1 trên cho 1 cộng e mũ trừ theta 0 cộng theta 1 nhân x
00:11:46 - 00:11:50, trong đó chúng ta sẽ đi tính lang truyền thuận và lang truyền ngược
00:11:50 - 00:11:56, tại cái vị trí đó là x bằng 1 theta 0 bằng trừ 3 và theta 1 bằng 3
00:11:56 - 00:12:02, thì đây sẽ là cái bài tập để chúng ta có thể luyện tập thực toán lang truyền ngược back propagation
00:12:02 - 00:12:10, kết luận đó là back propagation là một trong những thực toán rất phổ biến để mạng neural network
00:12:10 - 00:12:14, đây là tiền đề của các mô hình học sâu
00:12:14 - 00:12:18, các mô hình học sâu có thể phát triển
00:12:18 - 00:12:26, đây là một cái thực toán phổ biến để neural network có thể cập nhập được các trạng số
00:12:26 - 00:12:30, sau khi đã thực hiện được bước lang truyền thuận
00:12:30 - 00:12:38, và đây là một bước quan trọng của các thực toán tối ưu hóa hay optimizer
00:12:38 - 00:12:42, tại vì như chúng ta đã đề cập trong những slide đầu tiên của mục này
00:12:42 - 00:12:50, các thực toán như Adam, Root Mean Square, Propagation, Stochastic Gradient Descent, Momentum v.v
00:12:50 - 00:13:00, thì cái thao tác tính toán mà khó nhất chính là tính radian của G theo theta
00:13:00 - 00:13:05, và nhờ có back propagation thì cái công việc này đã trở nên dễ dàng hơn
00:13:05 - 00:13:10, và thậm chí đó là có thể hoàn toàn thực hiện một cách tự động
00:13:10 - 00:13:14, tức là chúng ta sẽ thực hiện một cách tự động bằng thực toán
00:13:14 - 00:13:24, chúng ta sẽ không cần phải đi ngồi tính tay tính đạo hàm của hàm G theo theta
00:13:24 - 00:13:29, mà chúng ta sẽ dùng máy để có thể tính một cách dễ dàng với thực toán back propagation
00:13:29 - 00:13:34, và khi đó thì cái optimizer này sẽ là một cái bộ công cụ hoàn thiện
00:13:34 - 00:13:39, để cho chúng ta có thể phục vụ cho các mô hình học dưới trên radian
00:13:39 - 00:13:45, như vậy thì chúng ta quay trở lại mô hình học dưới trên radian
00:13:45 - 00:13:50, chúng ta có ba cái công việc đó là thiết kế mô hình, thiết kế hàm lỗi
00:13:50 - 00:13:55, và cuối cùng đó là tối ưu tìm theta sao cho hàm lỗi này là nhỏ nhất
00:13:55 - 00:14:02, như vậy thì đối với công việc số 3 là chúng ta đã có thể giải được rồi
00:14:02 - 00:14:06, nhờ một bộ công cụ đó là radian descent
00:14:09 - 00:14:14, kết hợp với lại thuộc tán back propagation
00:14:14 - 00:14:23, thì hai cái bộ công cụ này hiện nay đã được cài đặt rất là thuận tiện
00:14:23 - 00:14:29, trong các deep learning framework ví dụ như là PyTorch hoặc là TensorFlow
00:14:29 - 00:14:35, thì hai cái deep learning framework này đã được cung cấp
00:14:35 - 00:14:41, và thậm chí là cả những cái optimizer hiện đại như là Aram, Root Means Web Propagation
00:14:41 - 00:14:44, cũng đã được cài đặt trong các deep learning framework này rồi
00:14:44 - 00:14:50, do đó công việc số 3 từ nay về sau chúng ta sẽ không còn phải quan tâm nữa
00:14:50 - 00:14:54, mà sử dụng những cái bộ công cụ các hàm optimizer
00:14:54 - 00:14:59, chúng ta sẽ chỉ tập trung vào việc thiết kế hàm mô hình và thiết kế hàm lỗi
00:14:59 - 00:15:03, sau cho mô hình của mình có thể học một cách tốt nhất
00:15:03 - 00:15:08, thì đó chính là cái động lực tại sao nó có back propagation
00:15:08 - 00:15:11, và cái vai trò của back propagation là cực lớn
00:15:11 - 00:15:17, vì nó giúp cho chúng ta giải quyết được một cái khối lượng đáng kể trong cái công việc
00:15:17 - 00:15:20, khi mà chúng ta làm việc trên cái mô hình máy học đó là hỗn luyện mô hình
00:15:20 - 00:15:25, chúng ta chỉ còn tập trung vào cái việc thiết kế mô hình và thiết kế hàm lỗi mà thôi
00:15:29 - 00:15:34, Cảm ơn các bạn đã xem video hấp dẫn
