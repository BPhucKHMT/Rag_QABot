00:00:00 - 00:00:23, Chúng ta sẽ mở rộng kẻ đặt của Autoencoder cho Variational Autoencoder
00:00:23 - 00:00:30, Trong phần trước, chúng ta đã cài hai cái module đó là Encode và Decode
00:00:30 - 00:00:41, Trong phần Variational Autoencoder, chúng ta nhận thấy cái kiến trúc này, phần Encode thì nó khác một chút ở cái bước cuối
00:00:41 - 00:00:45, Còn phần Decode thì cũng từ một cái vector z
00:00:45 - 00:00:52, Chúng ta giải nén ra và biến thành cái vector từ 2,5,12 lên 784
00:00:52 - 00:01:00, Hoàn toàn giống với cái kiến trúc từ 2,5,2,7,84 của Autoencoder
00:01:00 - 00:01:04, Do đó chúng ta sẽ tái sử dụng lại cái hàm này
00:01:04 - 00:01:07, Ở đây chúng ta sẽ chạy lại
00:01:07 - 00:01:09, Rồi
00:01:09 - 00:01:14, Và chúng ta sẽ tái sử dụng lại cái hàm Decoder này
00:01:15 - 00:01:28, Đối với phần Encode thì chúng ta sẽ thấy, thay vì chúng ta tạo ra một vector z gọi là Tắc Định, tức là Cố Định
00:01:28 - 00:01:33, Thì ở đây chúng ta sẽ tạo ra hai cái giá trị đó là My và Sigma
00:01:33 - 00:01:40, Từ My và Sigma này thì chúng ta sẽ samling để tạo ra một vector z theo phân bố
00:01:41 - 00:01:46, Đó là z là bằng phân bố Gaussian của My và Sigma
00:01:46 - 00:01:52, Và ở đây chúng ta cũng sẽ sử dụng một cái kỹ thuật đó là Reparameterization, thật là tái tham số
00:01:52 - 00:01:56, Thế thì trước tiên chúng ta sẽ cài cái Variational Encoder
00:01:56 - 00:02:03, Thì đối với cái Variational Autoencoder này thì chúng ta sẽ sử dụng tương tự như vậy
00:02:03 - 00:02:08, Chúng ta sẽ có cell.linear
00:02:09 - 00:02:16, Thì cái linear này là linear1 và sẽ là bằng Neural Network.linear
00:02:16 - 00:02:20, Đầu vào là 784 và đầu ra sẽ là 512
00:02:20 - 00:02:28, Rồi tương tự như vậy chúng ta sẽ có cái kỹ thuật đó là từ 512 về...
00:02:28 - 00:02:33, Ở đây là lấp linear số 2
00:02:33 - 00:02:39, Thì cái lấp linear số 2 này là từ 512 về cái phía vector My
00:02:39 - 00:02:42, Thì My của mình sẽ có kỹ thuật là 2
00:02:42 - 00:02:47, Tuy nhiên ở đây chúng ta sẽ không có hạt cốt mà chúng ta sẽ để Layton Dim ở đây
00:02:47 - 00:02:52, Rồi tương tự như vậy chúng ta sẽ có cái linear số 3
00:02:52 - 00:02:58, Thì ở đây chúng ta sẽ tương tự nhưng có điều là chúng ta phải đặt tên lấp là lớp khác
00:02:58 - 00:03:01, Tại vì đây sẽ là dành cho My
00:03:01 - 00:03:05, Còn đây sẽ là dành cho Sigma
00:03:05 - 00:03:12, Rồi và đối với cái phần mà forward thì chúng ta cũng sẽ có các cái bước tương tự
00:03:12 - 00:03:14, Một đó là chúng ta sẽ flatten
00:03:14 - 00:03:23, Thì chúng ta sẽ để là x sẽ được gán ngược trở lại là bằng torch.flatten
00:03:23 - 00:03:31, X và stack Dim là bằng một
00:03:31 - 00:03:37, Rồi sau đó chúng ta sẽ gọi đến cái bước đầu tiên
00:03:37 - 00:03:42, Bước biến đổi đầu tiên đó chính là chúng ta sẽ đưa qua cái lấp linear
00:03:42 - 00:03:45, Chúng ta sẽ đưa qua cái lấp linear và có một cái activation function
00:03:45 - 00:03:52, Thì ở đây activation function chúng ta có thể sử dụng đó là một cái hàm là relu
00:03:52 - 00:04:00, Thì đây là f.relu của cái self.linear1
00:04:00 - 00:04:06, Rồi và chúng ta sẽ truyền vào cái biến x và nó sẽ trả ra là x
00:04:06 - 00:04:10, Rồi tiếp theo thì chúng ta sẽ qua cái lấp thứ 2
00:04:10 - 00:04:13, Sau cái lấp 6 thì nó đã biến thành một cái vector 512
00:04:13 - 00:04:16, Bây giờ chúng ta sẽ trẻ nó ra làm hai nhánh
00:04:17 - 00:04:24, Thì my sẽ là bằng self.linear2 của x
00:04:24 - 00:04:27, Rồi và sẽ tạo ra là my
00:04:27 - 00:04:35, Và trong trường hợp này thì chúng ta cũng không có sử dụng cái activation function
00:04:35 - 00:04:37, Lý do đó là vì cái my của mình
00:04:37 - 00:04:42, Thì nó có thể từ giá trị âm cho đến giá trị dương
00:04:43 - 00:04:49, Trong khi nếu chúng ta dùng relu thì nó bị bó hẹp lại cái output của mình là phải từ không trở lên
00:04:49 - 00:04:54, Trong khi my của mình nó hoàn toàn có thể là nó là tâm của một cái phân bố sắc xuất
00:04:54 - 00:04:56, Nó có thể là số âm hay số dương được
00:04:56 - 00:05:01, Do đó ở đây chúng ta sẽ không có để cái activation function relu
00:05:01 - 00:05:10, Về sigma thì sigma của chúng ta là chúng ta sẽ để là self.linear
00:05:11 - 00:05:14, Và truyền vào x
00:05:14 - 00:05:18, Thế thì chúng ta muốn cái sigma của mình nó phải là một cái con số dương
00:05:18 - 00:05:20, Con sigma của mình là con số dương
00:05:20 - 00:05:23, Thì chúng ta có thể để relu ở đây cũng được
00:05:23 - 00:05:26, Còn nếu chúng ta muốn một cách khác
00:05:26 - 00:05:29, Ví dụ chúng ta để một cái hàm miễn sao nó trả ra là số dương
00:05:29 - 00:05:36, Ví dụ như hàm torch.asp
00:05:36 - 00:05:42, Thì nó sẽ ép cái giá trị có thể là từ âm vô cùng đến dư vô cường về một con số dương
00:05:42 - 00:05:45, Sigma thì bắt buộc phải là con số dương
00:05:45 - 00:05:48, Bước tiếp theo chúng ta sẽ sampling
00:05:48 - 00:05:54, Thế thì như trong slide lý thuyết chúng ta không thể nào mà sampling trực tiếp z
00:05:54 - 00:06:00, Với cái phân bố Gaussian mà có my và sigma
00:06:00 - 00:06:01, Giống như ở trên được
00:06:01 - 00:06:06, Tại vì nếu làm như vậy thì nó sẽ không có back propagation được
00:06:06 - 00:06:10, Giờ ở đây chúng ta sẽ dùng cái kỹ thuật parameterize descent trick
00:06:10 - 00:06:15, Thì để là my cộng cho cái code ở đây nó cũng đã gợi ý ra nè
00:06:15 - 00:06:20, Nhân với lại cái n sample
00:06:20 - 00:06:25, Thế thì ở đây cái n sample nó là cái gì
00:06:25 - 00:06:34, Nó chính là cái các cái giá trị đã được lấy theo cái phân bố chuẩn
00:06:34 - 00:06:39, Rồi sau đó thì chúng ta sẽ nhân với lại cái thành phần sigma này
00:06:39 - 00:06:42, Và kích thước của mình thì nó sẽ là bằng my.set luôn
00:06:42 - 00:06:46, Tức là my và sigma này sẽ cùng kích thước nhau và cụ thể luôn
00:06:46 - 00:06:48, Trong cái ví dụ này thì set của nó là bằng 2
00:06:48 - 00:06:52, Thì đây chính là parameterize descent
00:06:52 - 00:07:03, Tiếp theo thì chúng ta sẽ tính cái kl diversion
00:07:03 - 00:07:08, Thì sau khi chúng ta đã có được cái phân bố my và sigma rồi
00:07:08 - 00:07:11, Chúng ta sẽ làm một cái thành phần nó gọi là chính quy hóa
00:07:11 - 00:07:16, Thế thì dựa trên cái công thức của cái trang web
00:07:16 - 00:07:17, Ở đây chúng ta có thể tham khảo ha
00:07:17 - 00:07:20, Thì cái công thức của cái kl diversion
00:07:20 - 00:07:23, Nó sẽ là bằng sigma bình phương cộng cho my bình phương
00:07:23 - 00:07:25, Trừ cho lóc của sigma
00:07:25 - 00:07:26, Trừ 1 phần 2
00:07:33 - 00:07:41, Rồi trừ cho tả chấm lóc của sigma
00:07:41 - 00:07:44, Tất cả trừ cho 1 phần 2
00:07:44 - 00:07:48, Rồi sau đó tất cả cái thành phần này chúng ta sẽ đi lấy tổng
00:07:49 - 00:07:52, Tại vì nó có rất nhiều mẫu du lịch
00:07:52 - 00:07:53, Thì chúng ta sẽ lấy tổng
00:07:53 - 00:07:58, Rồi sau đó chúng ta sẽ chỉ return z thôi
00:07:58 - 00:08:04, Còn k return z để cho cái bước sau là qua decode
00:08:04 - 00:08:08, Còn kl diversion thì chúng ta sẽ để lại
00:08:08 - 00:08:11, Để chút nữa chúng ta sẽ đưa vào bên trong cái hầm loss
00:08:11 - 00:08:15, Rồi như vậy thì chúng ta sẽ tạo cái variational
00:08:15 - 00:08:17, Để chúng ta chạy cái code này
00:08:18 - 00:08:22, Rồi sau đó chúng ta sẽ tạo cái lớp variational autoencoder
00:08:22 - 00:08:27, Trong đó encoder thì lấy từ variational encoder chúng ta vừa mới lập trình xong
00:08:27 - 00:08:33, Còn decoder thì chúng ta sẽ lấy lại lớp mà chúng ta đã cài đặt
00:08:33 - 00:08:36, Ở trong cái phần autoencoder ở phía trước
00:08:36 - 00:08:39, Tức là cái class này
00:08:39 - 00:08:43, Rồi thì bây giờ chúng ta sẽ tạo
00:08:43 - 00:08:47, Và z là cái kết quả của encode
00:08:47 - 00:08:50, Sau đó z được truyền vào cho decode để trả kết quả cuối cùng
00:08:50 - 00:08:52, Và đây chính là cái x mũ
00:08:52 - 00:08:56, Thì chúng ta luôn mong muốn cái x mũ sắp x với cái x đầu vào bắt đầu
00:08:56 - 00:08:57, Thế thì để cái chuyện đã xảy ra
00:08:57 - 00:09:01, Thì chúng ta sẽ có một cái hầm gọi là hầm loss
00:09:01 - 00:09:08, Thì hầm loss này nó sẽ là bằng sai số của cái x hat
00:09:08 - 00:09:13, Trừ cho x tất cả mũ 2
00:09:14 - 00:09:20, Rồi, sau đó thì chúng ta sẽ có thêm cái phần là tính tổng nữa
00:09:20 - 00:09:27, Rồi cái này thì để cho chắc thì chúng ta sẽ để cái dấu mọi mặt đúng không ạ
00:09:27 - 00:09:29, Tức là cái tổng sai số
00:09:29 - 00:09:34, Rồi sau đó chúng ta sẽ đi cộng cho cái thành phần KL diversion
00:09:34 - 00:09:39, Thì nó sẽ lấy ra từ cái auto, cái mô hình là autoencoder này
00:09:39 - 00:09:44, Tại vì nó chỉ nằm trong encoder thôi
00:09:44 - 00:09:49, Chứ không nằm trong cái lớp trà là autoencoder
00:09:49 - 00:09:55, KL thì nó nằm ở trong cái encoder
00:09:55 - 00:09:59, Encoder là con của KL
00:09:59 - 00:10:02, Rồi bây giờ chúng ta sẽ chạy cái thúc toán này
00:10:02 - 00:10:05, Chạy cái hầm này và hỗn luyện
00:10:05 - 00:10:08, Latent deam thì chưa được defy
00:10:08 - 00:10:12, Rồi
00:10:12 - 00:10:18, Ok, thì cái Latent deam này là bằng 2
00:10:18 - 00:10:21, Rồi thì chúng ta sẽ khởi tạo trực tiếp ở đây
00:10:21 - 00:10:28, Chúng ta tắt 2 cái đoạn code này ra
00:10:28 - 00:10:30, Data is not defied
00:10:30 - 00:10:35, Thì cái code của cái phần data là chúng ta sẽ lấy trong cái tập kỹ liệu mnits
00:10:35 - 00:10:42, Do đó thì chúng ta gọi cái hầm của scikitlearn để lấy cái data về
00:10:42 - 00:10:44, Đây
00:10:44 - 00:10:48, Autoencoder is not defied
00:10:48 - 00:10:54, Nó sẽ lấy từ torch, không hại từ scikitlearn
00:10:54 - 00:10:56, Rồi util.latch
00:10:56 - 00:10:58, Và đưa về cái thư mục data
00:10:58 - 00:11:02, Và data lúc này của mình thì sẽ chứa dữ liệu
00:11:02 - 00:11:04, Cả nhãn và không có nhãn
00:11:04 - 00:11:09, Thế thì là có cái phần trend nhưng mà chưa được gọi
00:11:15 - 00:11:19, Rồi thì ở đây chúng ta không cần phải chạy lại cái hầm trend của autoencoder
00:11:19 - 00:11:22, Cái chúng ta cần đó là cái hầm lấy data ở đây
00:11:22 - 00:11:25, Chúng ta sẽ mượn lại đem xuống
00:11:31 - 00:11:34, Và chờ cái mô hình này nó trend xong
00:11:34 - 00:11:37, Thì có thể nó tốn của chúng ta là khoảng 2 phút
00:11:37 - 00:11:41, Thế thì chúng ta sẽ cùng xuống dưới để xem cái kết quả
00:11:44 - 00:11:45, Rồi
00:11:45 - 00:11:49, Thì ở đây chúng ta sẽ tạm xóa cái kết quả cũ đã chạy trước đây
00:11:49 - 00:11:53, Để xem tại vì với mỗi lần random có thể ra những cái con số khác nhau
00:11:53 - 00:11:56, Thì cái phần trực quan hóa, cái latent space này
00:11:56 - 00:11:58, Thì chúng ta sẽ dùng cái hầm
00:11:58 - 00:12:00, Là
00:12:00 - 00:12:01, Blot latent
00:12:01 - 00:12:04, Tương tự như của autoencoder
00:12:04 - 00:12:08, Nhưng có điều cái mô hình ở đây chúng ta truyền sẽ là cái mô hình của VAE
00:12:08 - 00:12:13, Và cái data của mình sẽ là lấy cái data của toàn bộ data set
00:12:13 - 00:12:23, Và chúng ta sẽ in ra cái biểu đồ của cái không gian latent
00:12:23 - 00:12:28, Thì ở đây chúng ta sẽ có một cái thí nghiệm đó là chúng ta sẽ interpolate
00:12:28 - 00:12:32, Tức là sẽ quét trong toàn bộ cái không gian của ảnh
00:12:32 - 00:12:39, Và sau đó thì chúng ta sẽ xác định xem là cái kết quả sau khi chúng ta đi cố ra
00:12:39 - 00:12:41, Là cái ảnh của mình nó nhìn như thế nào
00:12:41 - 00:12:44, Thì nó sẽ nằm trong cái interpolate
00:12:44 - 00:12:50, Và cái interpolate này thì nó sẽ chạy từ x1 cho đến x2
00:12:50 - 00:12:53, Với x1, x2 là một cái điểm nào đó mà chúng ta lấy
00:12:53 - 00:13:01, Ví dụ như là x1 thì được lấy từ một cái điểm ảnh có cái nhãn là 1 là số 1
00:13:01 - 00:13:06, x2 thì tương ứng là một cái x có cái y là bằng 0
00:13:06 - 00:13:07, Tức là chúng ta lấy một cái con số 0 ra
00:13:07 - 00:13:13, x1 chính là số 1 và x2 chính là một cái hình của con số 0
00:13:13 - 00:13:20, Và sau đó chúng ta sẽ plot trên toàn bộ các cái vector z
00:13:20 - 00:13:25, Vector ẩn từ x1 cho đến x2
00:13:25 - 00:13:30, Thì chúng ta sẽ bàn chi tiết hơn về cái cách thức mà nó chạy
00:13:30 - 00:13:37, Ngoài ra thì chúng ta có cung cấp thêm một cái ảnh gif để cho tạo cái hình động
00:13:37 - 00:13:46, Và cuối cùng chúng ta có thể thử nghiệm với lại các đồ dài của các latent space khác nhau
00:13:51 - 00:13:56, Rồi thì sau khoảng 2 phút thì đã đã kết thúc cái quá trình trend
00:13:56 - 00:14:02, Và chúng ta sẽ plot cái hàm plot này chúng ta chưa lấy từ ở phía trên xuống
00:14:16 - 00:14:20, Chúng ta có một cái câu hỏi đó là nhận xét gì về cái kết quả này
00:14:20 - 00:14:25, Thì chúng ta thấy nếu như ở trong cái không gian latent ở trên
00:14:25 - 00:14:28, thì chúng ta thấy là nó bị hở khá là nhiều
00:14:28 - 00:14:33, Và cái đường đi của mình nó không phải là dạng hình tròn một cái phân buộc Gaussian
00:14:33 - 00:14:36, mà nó đi một cái dạng vòng cung như thế này
00:14:36 - 00:14:41, Trong khi đó ở bên dưới thì chúng ta thấy là nó tròn hơn
00:14:41 - 00:14:45, Đương nhiên nó sẽ có một số cái khu vực
00:14:45 - 00:14:52, Ví dụ như màu số 8 thì nó sẽ là nó hơi dẹp
00:14:52 - 00:14:53, Thực ra là dẹp hay không chúng ta cũng không biết
00:14:53 - 00:14:56, Tại vì nó sẽ ẩn nó bị trồng lên ở đằng sau
00:14:56 - 00:14:59, Lý do tại sao nó bị trồng lên ở khu vực này
00:14:59 - 00:15:03, Đó là vì cái không gian của chúng ta là chọn là 2 chiều
00:15:03 - 00:15:08, Nên nó khá là chật chội dẫn đến là cái mô hình của mình nó có thể bị trồng lấp
00:15:08 - 00:15:15, Nhưng riêng số 1 và số 7 thì nó ít bị trồng lấp hơn
00:15:15 - 00:15:18, Chúng ta thấy là nó nằm ngoài ra ngoài rìa và ít bị trồng lấp hơn
00:15:18 - 00:15:23, Thì số 1 nó có hình thù khá là khác biệt so với những cái con số còn lại
00:15:23 - 00:15:24, Tương tự như vậy số 7
00:15:24 - 00:15:30, Còn các con số ví dụ như là số 8 hoặc là số 3 chẳng hạn
00:15:30 - 00:15:33, Thì chúng ta thấy là nó có những cái nét trùng nhau
00:15:33 - 00:15:38, Vì vậy như số 8 chúng ta bỏ đi cái phần bên tay trái thì sẽ ra số 3
00:15:38 - 00:15:44, Sau đó chúng ta sẽ tìm cách trực quan hóa
00:15:44 - 00:15:51, Thế thì trong cái hình này chúng ta sẽ trực quan hóa trong đoạn từ trừ 3 cho đến 3
00:15:51 - 00:15:58, Thì chúng ta sẽ dùng công cụ snip để cắt cái màn hình này ra
00:15:58 - 00:16:05, Rồi, và trong cái sơ đồ này chúng ta sẽ thấy là
00:16:05 - 00:16:10, Cái khu vực là từ trừ 3 cho đến 3
00:16:10 - 00:16:18, Rồi, thì cái trục hoành của mình là từ trừ 3 cho đến 3
00:16:18 - 00:16:20, Thì đâu đó là khoảng giá trị ở đây
00:16:20 - 00:16:25, Giá trị ở đây cho đến 3, giá trị là ở đây
00:16:26 - 00:16:33, Còn trục tung là từ trừ 3 cho đến 3 thì nó cũng sẽ là từ đây cho đến đây
00:16:33 - 00:16:36, Rồi thì chúng ta chiếu lên
00:16:39 - 00:16:41, Thì nó sẽ là nằm ở đây
00:16:48 - 00:16:52, Thì đây là cái ô vuông mà chúng ta sẽ trực quan
00:16:52 - 00:16:59, Rồi, và chúng ta cũng sẽ chia lưới ra là 12 đường, 12 phần
00:16:59 - 00:17:06, Và với mỗi điểm trên cái mắt lưới này thì chúng ta sẽ đi vẽ ra để xem cái ảnh của nó là như thế nào
00:17:06 - 00:17:10, Thì đó là cái ý tưởng của cái hàng plot reconstructed
00:17:11 - 00:17:23, Thì chúng ta thấy là phía trên bên trái nó sẽ gần với lại số 8 là đúng rồi
00:17:23 - 00:17:26, Tại vì đây là cái khu vực màu vàng lá mạ
00:17:26 - 00:17:31, Thì ở trong cái sơ đồ này là nó gần với lại số 8
00:17:31 - 00:17:34, Nên nó sẽ vẽ ra là số 8
00:17:35 - 00:17:41, Rồi, từ trái sang phải thì chúng ta sẽ thấy là đi vào cái khu vực nó khá là nhỡ loạn
00:17:41 - 00:17:46, Nó vừa có màu vàng lá mạ, vừa có màu nâu của số 5 và màu đỏ của số 3
00:17:46 - 00:17:49, Nên chúng ta thấy là nó có bóng dáng của số 3
00:17:49 - 00:17:54, Nhưng mà một cách trực quan thì chúng ta thấy là từ trái sang phải nó đã có cái sự biến đổi
00:17:54 - 00:18:02, Cái đường nét của số 8 nó dần mất 2 cái nét cong ở bên tay trái để chuyển dần thành số 3
00:18:02 - 00:18:07, Rồi ra đây nó sẽ dần dần hòa lại để thành số 0
00:18:07 - 00:18:09, Thì số 0 của mình là màu xanh bên tay phải đây
00:18:11 - 00:18:18, Rồi, và từ trên xuống thì chúng ta thấy là nó đã dần chuyển hóa từ số 8 sang số 1
00:18:20 - 00:18:22, Từ số 8 sang số 1
00:18:22 - 00:18:30, Thì nó đang là đi vào từ vùng màu vàng xuống cái vùng màu cam, màu cam tướng vào số 1
00:18:30 - 00:18:32, Và nó đi cũng khá là mượt
00:18:32 - 00:18:44, Rồi, tiếp theo thì chúng ta sẽ tìm cách trực quan hóa một chuỗi các con số mà được lấy từ 2 cái vector
00:18:44 - 00:18:47, Từ 1 cho đến 0
00:18:47 - 00:18:51, Thì chúng ta thấy là nó đã có sự dịch chuyển
00:18:51 - 00:18:56, Băng đầu chúng ta sẽ có x1 là hình số 1 như thế này
00:18:57 - 00:19:02, Và trong cái không gian lây tân z thì ở đây chúng ta sẽ tìm cách trực quan
00:19:10 - 00:19:17, Rồi, thì cái số 1 này thì ở bên trong cái không gian lây tân nó sẽ nằm ở đây
00:19:17 - 00:19:23, Số 0 này trong không gian lây tân thì nó nằm ở đây
00:19:24 - 00:19:34, Và cái ý nghĩa của cái hàm interpolate này, nội suy này là từ 2 cái điểm này chúng ta sẽ lấy mẫu đều
00:19:34 - 00:19:42, Và với mỗi điểm này chúng ta sẽ vẽ lên
00:19:42 - 00:19:48, Giống như trên
00:19:48 - 00:19:53, Rồi, thì chúng ta thấy là cái đường đi của nó cũng khá là mượt
00:19:53 - 00:20:03, Rồi, và chúng ta có thể sử dụng cái code interpolate.gif để mà
00:20:03 - 00:20:07, Thay vì chúng ta vẽ thành một giải như thế này thì nó sẽ tạo ra thành một Ấn thôi
00:20:07 - 00:20:11, Và khi chúng ta chạy cái này thì nó sẽ tạo ra một cái file.gif
00:20:11 - 00:20:17, Thì chúng ta có thể tải cái file.gif này về và nhìn thấy được cái sự dịch chuyển của nó
00:20:18 - 00:20:22, Từ số 1 chuyển sang số 0 theo dạng là hoạt hoạt
00:20:22 - 00:20:31, Và bài tập thực hành tiếp theo cho chúng ta, đó chính là chúng ta thử nghiệm điều gì xảy ra nếu như cái
00:20:31 - 00:20:37, Layton Dimension của mình là bằng 1, tức là cái không gian của mình nó kéo về rất là chật hẹp
00:20:37 - 00:20:42, Ở đây chúng ta mới chọn một cái không gian 2 chiều để mà chúng ta có thể trực quan được thôi
00:20:43 - 00:20:50, Trên cái mặt phẳng đó thì nó sẽ có một cái điểm yếu đó là nó bị chật quá và dẫn đến kéo các con số lại gần nhau như thế này
00:20:50 - 00:21:00, Nhưng mà nhìn chung thì các cái phân bố này là đều dễ hiểu và nó có giống như cái prior distribution đó là dạng Gaussian
00:21:02 - 00:21:09, Rồi và nếu chúng ta đưa về cái không gian 1 chiều thì điều gì sẽ xảy ra thì đó chính là cái bài tập thêm cho chúng ta
