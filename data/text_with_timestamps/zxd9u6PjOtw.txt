00:00:00 - 00:00:18, Chúng ta sẽ xem cái ma trận W này có cái giá trị là bao nhiêu
00:00:18 - 00:00:28, Chúng ta sẽ thấy là cái W này sẽ là một cái Array, là một cái List bao gồm hai phần tử
00:00:28 - 00:00:35, Thì phần tử đầu tiên chính là cái số trọng số, số filter của phép biến đổi Cognition đầu tiên
00:00:35 - 00:00:39, Và thành phần thứ hai chính là cái BIAS, tại vì chúng ta có sử dụng BIAS
00:00:39 - 00:00:44, W0 chính là cái trọng số của mình
00:00:44 - 00:00:48, Rồi để xem coi cái trọng số này có kích thước bao nhiêu
00:00:50 - 00:00:56, Chúng ta là chấm chef, trong đó 3, 3, 1, 6, 3, 3 chính là cái kích thước của cái channel
00:00:56 - 00:01:02, Và 1 chính là cái input, dimension của input của đầu vào của mình, nó chỉ có một channel thôi
00:01:02 - 00:01:07, Nó sẽ là một, và output của mình sẽ là 6, 6 cái filter
00:01:08 - 00:01:16, Vậy thì để trực quan, chúng ta sẽ có số filter là 6, rồi chúng ta sẽ diệt qua Y từ 0 cho đến 5 để truyền vô đây
00:01:16 - 00:01:26, Rồi đây là W0, W0.chef chính là 3, 3, 1, 6, thì chúng ta sẽ lấy cái chỉ số Y chạy ở đây trước
00:01:26 - 00:01:31, Rồi sau đó lấy chỉ số Z chạy ở đây, thì ở đây một cách tổng quát trong cái lớp Cognition số 2
00:01:31 - 00:01:39, Rồi sau đó thì số 1 này sẽ chuyển thành là số 16, do đó thì ở đây chúng ta sẽ để là Y chạy cho một cái rank
00:01:39 - 00:01:43, Rank này thì ở đây chúng ta để là 1, nhưng mà sắp tới có thể để là 16
00:01:47 - 00:01:54, Đây chính là 6 cái filter ở cái lớp đầu tiên, thì chúng ta có thể hiểu cái ý nghĩa của cái filter này
00:01:54 - 00:02:04, Đây chính là chúng ta lấy cái size số, cái sự chênh lệch của cái vùng phía bên phải, phía dưới so với lại cái vùng ở phía trái bên trên
00:02:04 - 00:02:11, Ý nghĩa của filter này đó là lấy cái sự chênh lệch giữa cái hàng ở giữa so với lại hai cái hàng ở phía trên và phía dưới
00:02:11 - 00:02:15, Thì mỗi một cái filter này nó sẽ thể hiện một cái đặc trưng nào đó
00:02:15 - 00:02:23, Rồi tiếp theo là chúng ta sẽ tiến hành thử nghiệm với một số cái biến thể khác nhau
00:02:23 - 00:02:27, Trước khi qua thử nghiệm một số biến thể khác nhau thì chúng ta sẽ thử cái hàng Redic
00:02:27 - 00:02:32, Cái hàng Redic thì CNN.Redic
00:02:32 - 00:02:43, Rồi, thì chúng ta sẽ truyền vào cái istat và mẫu dữ liệu thứ, ví dụ như là mẫu dữ liệu thứ 300
00:02:43 - 00:02:45, Rồi
00:02:45 - 00:02:54, Ok, ở đây thì hàng Redic, chúng ta sẽ xem lại cái hàng Redic của mình
00:02:54 - 00:03:10, Truyền vào cell.modo.istat, ok, bây giờ chúng ta sẽ xem tiếp cái istat của mình đã được load rồi và đã được chuẩn hóa rồi, đúng không?
00:03:10 - 00:03:12, Rồi
00:03:12 - 00:03:24, Ok, bây giờ chúng ta sẽ thử truyền vào như thế này
00:03:24 - 00:03:30, Rồi, chúng ta sẽ xem cái istat của mình
00:03:30 - 00:03:52, Rồi, ah, istat của mình là cái mạng kích thước là 28 x 28, do đó chúng ta phải reset, chúng ta phải reset nó về cái dạng là 28 x 1
00:03:52 - 00:04:01, Rồi sau đó chúng ta mới đưa vào để cho cái mô hình của mình có thể Redic được, CNN.Redic
00:04:01 - 00:04:03, Rồi
00:04:05 - 00:04:07, Cũng chưa được ha
00:04:10 - 00:04:21, Rồi, ah, ở đây cái số này sẽ phải để lên trước, đúng không? Cái này nó sẽ phải để lên trước, dạ là 1,28
00:04:21 - 00:04:36, Ok, được rồi, tức là nó sẽ phải để cái chỉ số của cái thứ tự lên trước, nó sẽ hơi ngược, hơi ngược
00:04:36 - 00:04:41, Rồi, bây giờ chúng ta sẽ thử xem cái nhãn này nó sẽ ra cái giá trị là bao nhiêu?
00:04:41 - 00:04:53, Tại vì ở đây nó chỉ trả ra 1 cái vector 1 hot, chúng ta sẽ phải có thêm 1 cái hàm nữa đó là argumentMax là np.argumentMax
00:04:58 - 00:05:00, Rồi, nó sẽ là 4
00:05:00 - 00:05:09, Và bây giờ chúng ta sẽ xem coi cái mổ thứ 300 này, x, y, test của mình, thứ 300 nó là bằng bao nhiêu?
00:05:09 - 00:05:11, Nó là 4
00:05:12 - 00:05:20, Rồi, bây giờ chúng ta sẽ thử những cái mổ khác ha, chúng ta sẽ thử những cái mổ khác, ở đây chúng ta sẽ để là Redic
00:05:21 - 00:05:23, Rồi
00:05:23 - 00:05:34, Rồi, ở đây sẽ là nhạng dự đoán là Red
00:05:36 - 00:05:41, Rồi, còn ở đây sẽ là nhãn thực tế
00:05:41 - 00:05:56, Và ở đây, cái chỉ số này chúng ta sẽ tham số hóa nó là idx là bằng 100, và chúng ta sẽ để đây là idx
00:05:59 - 00:06:07, Rồi, thì đại đa số chúng ta thấy là cái độ chính xác rất là cao, chúng ta thử rất nhiều những cái nhãn khác nhau ha
00:06:08 - 00:06:12, Thì nó đều ra là dự đoán và thực tế khớp nhau
00:06:12 - 00:06:17, Bây giờ, trong cái mạng CNN thì chúng ta thấy nó có rất nhiều những cái môn đuôn khác nhau
00:06:17 - 00:06:23, Và tại thời điểm hiện tại thì chúng ta sẽ chưa hiểu rõ cái vai trò của từng môn đuôn này
00:06:23 - 00:06:29, Do đó thì chúng ta sẽ làm một cái thiết nghiệm, nó gọi là Application Study với các cái biến thể khác nhau
00:06:29 - 00:06:36, Bằng cách, đó là chúng ta sẽ lần lượt thay đổi một số cái cấu hình của cái chương trình của mình
00:06:36 - 00:06:38, Chúng ta sẽ thay đổi một số cái cấu hình
00:06:38 - 00:06:48, Thì cái biến thể đầu tiên, đó là chúng ta sẽ bỏ đi cái thay cái hàm sigmoid bằng relu
00:06:48 - 00:06:53, Chúng ta sẽ thay cái sigmoid bằng relu, như vậy thì chúng ta sẽ copy cái code ở đây đem xuống
00:06:53 - 00:07:03, Rồi, chúng ta sẽ đem lên cái hàm này thay cái sigmoid bằng relu
00:07:03 - 00:07:11, Như vậy thì bản chất là cái biến thể này chúng ta không cần phải cài đặt lại
00:07:11 - 00:07:15, Mà chúng ta chỉ sửa cái tham số của mình thôi
00:07:15 - 00:07:21, Chúng ta chỉ sửa cái tham số khi gọi cái hàm build thôi
00:07:23 - 00:07:27, Rồi, và ở đây là relu
00:07:27 - 00:07:41, Sau đó thì chúng ta sẽ tiến hành là cnn.trend, xtrend, ytrend, oh
00:07:41 - 00:07:47, Và lưu ý ở đây chúng ta sẽ để cái history là history số 2
00:07:47 - 00:07:52, Rồi, bây giờ chúng ta sẽ tiến hành build cái này
00:07:52 - 00:07:57, Và tranh thủ trong thời gian chờ đợi thì chúng ta sẽ thử
00:07:57 - 00:08:05, Biết code trước cho cái phần là vẽ cái giá trị loss
00:08:05 - 00:08:10, Chúng ta sẽ thêm một cái đường nữa đó là history số 2
00:08:10 - 00:08:16, Rồi, và ở đây sẽ là trend loss v1
00:08:17 - 00:08:24, Đây sẽ là trend loss v2, trong đó v2 đó là dùng relu
00:08:30 - 00:08:33, Rồi, tương tự như vậy, bây giờ chúng ta sẽ chờ đợi
00:08:33 - 00:08:41, Chúng ta sẽ viết trước cái code cho các biến thể tiếp theo
00:08:41 - 00:08:45, Biến thể bỏ hết các lớp pooling thì chúng ta làm cũng rất là nhanh
00:08:45 - 00:08:50, Pooling đúng không? Thì chúng ta sẽ bỏ, xóa đi
00:08:50 - 00:08:55, Và lưu ý đó là phải để gối đầu các cái biến
00:08:55 - 00:08:59, Ví dụ như ở đây c1 thì sẽ được truyền trực tiếp sang đây
00:08:59 - 00:09:02, Rồi c3 thì sẽ truyền trực tiếp sang đây
00:09:02 - 00:09:05, Vì vậy là chúng ta đã xong cái biến thể số 3
00:09:05 - 00:09:07, Chúng ta sẽ để là cnn v3
00:09:07 - 00:09:13, Rồi, bây giờ chúng ta sẽ cài cho cái biến thể cuối cùng
00:09:14 - 00:09:20, Ok, ở đây khi bỏ cái relu thì chúng ta thấy là nó đã chạy xong rồi
00:09:20 - 00:09:22, Và chúng ta sẽ quan sát thử
00:09:25 - 00:09:27, Ok, chúng ta sẽ vẽ
00:09:28 - 00:09:30, Thì nhìn vào cái sơ đồ này
00:09:31 - 00:09:34, Ok, ở đây chúng ta sẽ phải gom nó lại
00:09:35 - 00:09:37, Gom 2 cái legion này lại
00:09:39 - 00:09:41, Rồi, vẽ lại
00:09:44 - 00:09:49, Rồi, chúng ta sẽ thấy là cái relu phiên bản số 2
00:09:49 - 00:09:53, Nó giảm rất là nhanh, đúng không? Nó giảm rất là nhanh
00:09:53 - 00:09:56, Nó nằm bên dưới cái đường màu xanh
00:09:56 - 00:09:58, Thì điều đó có nghĩa là gì?
00:09:58 - 00:10:02, Điều đó là, ví dụ tại cái epoch số 5
00:10:02 - 00:10:06, Thì cái phương pháp v2, tức là khi sử dụng relu
00:10:06 - 00:10:10, Nó cho cái loss thấp hơn so với cái phiên bản số 1, tức là dùng sigmoid
00:10:10 - 00:10:13, Tức là nó đã giúp cho mình hội tụ nhanh hơn
00:10:13 - 00:10:18, Nhưng mà đương nhiên, khi số epoch càng lớn thì cả 2 thằng cũng sẽ tiện cận về
00:10:18 - 00:10:20, Nhưng mà nó sẽ tốn thời gian hơn
00:10:20 - 00:10:25, Thì tập MNIST là tập rất tuyến tính, rất là dễ, rất là đơn giản
00:10:25 - 00:10:31, Nó sẽ không thể nào thể hiện được cái sự khuất đại
00:10:31 - 00:10:35, Cái tốc độ mà trend của relu nhanh hơn so với sigmoid như thế nào
00:10:36 - 00:10:43, Khi chúng ta trend với tập dữ liệu lớn như MNIST, thì chúng ta sẽ thấy rõ relu hữu quả hơn rất là nhiều
00:10:43 - 00:10:49, Rất là nó sẽ giảm xuống, chúng ta sẽ thấy sự sụp giảm về loss của nó rất là nhanh
00:10:49 - 00:10:53, Thì đó chính là ý nghĩa của biến thể đầu tiên
00:10:53 - 00:10:59, Đó là bỏ sigmoid và thay thế là bằng relu, thì tốc độ hội tụ của nó sẽ nhanh hơn
00:10:59 - 00:11:04, Còn về đường chính xác, theo thời gian dài, đâu đó nó vẫn sẽ sắp sĩ với sigmoid
00:11:04 - 00:11:12, Nhưng mà với thời gian mà mình có thể chờ đợi được để có thể huyện, thì việc dùng sigmoid sẽ chậm hơn rất là nhiều
00:11:12 - 00:11:16, Tiếp theo, đó là chúng ta sẽ bỏ hết các lớp pooling
00:11:16 - 00:11:19, Rồi, chúng ta đã cài đặt rồi
00:11:19 - 00:11:26, Và bây giờ chúng ta sẽ sử dụng nó
00:11:27 - 00:11:34, Rồi, ở đây chúng ta sẽ để là CNN v3 và History ở đây sẽ là History số 3
00:11:34 - 00:11:41, Rồi, ở đây chúng ta sẽ khô phục ngược trở lại, chúng ta sẽ khô phục ngược trở lại là sigmoid
00:11:41 - 00:11:44, Rồi, chạy
00:11:44 - 00:11:58, Rồi, bây giờ chúng ta sẽ vẽ hàm loss khi có đồng thời cả 3 cái History 123
00:12:03 - 00:12:11, Rồi v3 thì ở đây sẽ là width down pooling
00:12:14 - 00:12:18, Width down pooling
00:12:18 - 00:12:22, Rồi, chúng ta có thể thu gọn lại một chút xíu
00:12:29 - 00:12:35, Rồi, tranh thủ trong khi chờ đợi thì chúng ta sẽ cài luôn cái phiên bản thứ 4
00:12:35 - 00:12:41, Cái phiên bản này, đó chính là chúng ta bỏ hết các lớp comps
00:12:41 - 00:12:51, Một điều rất là thú vị đó là chúng ta đặt sự nghi ngờ rằng là cái mạng comps thì cái vai trò của comps rõ ràng rất là lớn
00:12:51 - 00:12:56, Nhưng bây giờ chúng ta sẽ làm một thí nghiệm đó là bỏ hết cái comps thì xem điều gì sẽ xảy ra
00:12:56 - 00:13:01, Thì đó chính là chỉ ý nghĩa của cái phiên bản số 4
00:13:01 - 00:13:08, Rồi, bây giờ may quá cái phiên bản số 3 nó đã chạy xong và chúng ta sẽ xem thử
00:13:11 - 00:13:20, Rồi, chúng ta thấy là nếu như không có cái pooling thì cái loss của mình gần như không giảm, nó cứ giữ nguyên
00:13:20 - 00:13:23, Ah, loss gần như không giảm, nó cứ giữ nguyên
00:13:25 - 00:13:30, Rõ ràng là cái vai trò của pooling cũng rất là quan trọng
00:13:30 - 00:13:37, Nếu không có pooling thì cái loss của mình gần như là đi ngang, nó không giúp cho mình giảm xuống
00:13:38 - 00:13:40, Rồi
00:13:40 - 00:13:47, Ok, bây giờ chúng ta sẽ quen cái phiên bản tiếp theo, đó là không có cái lớp comps
00:13:47 - 00:13:55, Ở đây chúng ta phải sử dụng cái biến thể đầu tiên để mình code, chứ nếu không là sẽ nhầm lỡ
00:13:55 - 00:14:00, Rồi, không có comps chúng ta sẽ bỏ đi lớp này, bỏ đi lớp này
00:14:00 - 00:14:05, Rồi, và ở đây chúng ta sẽ truyền vào là input, S2 sẽ truyền vào đây
00:14:05 - 00:14:09, Tức là chúng ta sẽ giảm cái thước liên tiếp 2 lần
00:14:09 - 00:14:14, Rồi, ok, ở đây sẽ là CNN v4
00:14:19 - 00:14:30, Rồi, bây giờ chúng ta sẽ gọi cái hàm này khởi tạo để là v4, history là 4
00:14:30 - 00:14:33, Ah, run
00:14:33 - 00:14:37, Và tương tự như vậy chúng ta sẽ vẽ cái shuttle ở đây
00:14:38 - 00:14:42, Rồi, chúng ta sẽ có history là 4
00:14:47 - 00:14:54, Tray and loss ở đây sẽ là v4, without convolution
00:15:07 - 00:15:21, Rồi, chúng ta thấy là cái loss của mình cũng có giảm, tuy nhiên cái tốc độ giảm của nó khá là chậm
00:15:21 - 00:15:29, Tốc độ giảm khá chậm, thì điều này cũng minh chứng là cái vết là cái convolution của mình đã giúp cho cái việc huấn luyện nhanh hơn
00:15:29 - 00:15:34, Mặc dù accuracy thì nó cũng có suy hướng là nó càng lúc càng tăng, đúng không?
00:15:34 - 00:15:41, Nếu không có convolution, tốc độ nó sẽ chậm hơn rất nhiều
00:15:41 - 00:15:49, Rồi, cái đường màu đỏ là v4, thì chúng ta thấy là nó nằm ở phía trên, nếu không có convolution thì nó sẽ nằm phía trên
00:15:49 - 00:15:58, Vì vậy, cái phiên bản mà hoàn thiện nhất của chúng ta chính là cái phiên bản màu cam ở đây, là đường nằm ở dưới cùng
00:15:58 - 00:16:06, Tương ứng phiên bản số 2 là thay cái sigmoid bằng relu, trong đó vẫn phải giữ vừa có pooling và vừa có convolution
00:16:06 - 00:16:18, Như vậy, đây chính là cái bài tập tutorial để giúp chúng ta hiểu được vai trò của từng phép biến đổi ở bên trong mạng CNN
00:16:19 - 00:16:31, Hãy subscribe cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video hấp dẫn
