00:00:00 - 00:00:23, Chúng ta sẽ lựa chọn một phân bố sát xuất prior tức là tiền nghiệm, là Normal Gaussian
00:00:23 - 00:00:31, Tức là chúng ta có rất nhiều những dạng phân bố khác nhau, tuy nhiên một trong những phân bố rất phổ biến đó là phân bố chuẩn là Normal Gaussian
00:00:31 - 00:00:43, Chúng ta mong muốn phân bố không gian ẩn của mình sẽ là một phân bố giống với phân bố Gauss
00:00:43 - 00:00:53, Và cụ thể luôn là Normal Gaussian, tức là với muy là bằng 0 và sigma bình phương là bằng 1, thì đây là phân bố ẩn tiền nghiệm
00:00:53 - 00:01:09, Và mình sẽ huấn luyện mô hình để cho quý của z cho trước x với tham số phi là tuân theo phân bố Gauss
00:01:09 - 00:01:29, Vì vậy, phân bố đồng đều sẽ xoay xung quanh tâm của không gian ẩn, ví dụ như chúng ta có một không gian ẩn thì mình sẽ tìm cách đưa nó về cùng một tâm với nhau
00:01:29 - 00:01:49, Vì vậy, phân bố của mình sẽ xoay xung quanh tâm không gian ẩn, tức là muy bằng 0, quyến khích cho đặc trưng phân bố đồng đều xoay xung quanh tâm không gian ẩn
00:01:49 - 00:02:09, Vì vậy, phân bố đều sẽ xoay xung quanh tâm không gian ẩn, tức là nếu như ở trong mô hình autoencoder không có hình thông thường, thì mình sẽ tìm cách đưa nó về cùng một tâm
00:02:09 - 00:02:24, Vì vậy, phân bố của mình sẽ xoay xung quanh tâm không gian ẩn, tức là nếu như ở trong mô hình autoencoder không có hình thông thường, thì mình sẽ tìm cách đưa nó về cùng một tâm
00:02:24 - 00:02:39, Vì vậy, phân bố của mình sẽ xoay xung quanh tâm không gian ẩn, tức là muy bằng 0, quyến khích cho đặc trưng phân bố đồng đều xoay xung quanh tâm không gian ẩn
00:02:39 - 00:02:58, Vì vậy, phân bố của mình sẽ tìm cách đưa nó về cùng một tâm
00:02:58 - 00:03:11, Cái công thức khoảng cách đổ lỗi giữa qi và p thì nó được tính như thế nào? Nó sẽ được tính dựa trên công thức KL diversion giữa hai phân phối
00:03:11 - 00:03:35, Và khi chúng ta triển khai với p là bằng phân bố normal gauss là muy bằng 0 và sigma bình bằng 1, thì chúng ta sẽ có công thức đó là bằng một phần tổng của g chạy từ 0 cho đến k trừ 1 của sigma z cộng cho muy z bình phương trừ 1
00:03:35 - 00:03:45, Trừ cho lốc của sigma z, trong đó k chính là số chiều của vector của mình
00:03:45 - 00:04:07, K chính là số chiều của không gian ẩn
00:04:07 - 00:04:14, Và khi đó nó cũng chính là số chiều của muy, nó cũng chính là số chiều của sigma luôn
00:04:14 - 00:04:30, Rồi, khi chúng ta huấn luyện một mô hình VAE, thành phần chính quy hóa này nó khuyến khích cái chuyện gì? Đó là khi chúng ta cho cái loss này càng tiến về 0, tức là càng giảm
00:04:30 - 00:04:40, Rõ ràng là với công thức này chúng ta sẽ thấy sigma z sẽ có xu hướng tiến về 0
00:04:40 - 00:04:50, Ngược lại, ở đây chúng ta thấy có cái dấu trừ lốc của sigma z, thì nó lại khuyến khích cái sigma này là không quá nhỏ
00:04:50 - 00:04:57, Nó sẽ khuyến khích
00:04:57 - 00:05:05, Cái sigma z không quá nhỏ
00:05:05 - 00:05:19, Còn cái muy tiến về 0, tức là cái phân bố của mình, nó sẽ kéo từ một cái khu vực rất là xa, nó sẽ kéo về tiến về cái góc tạ độ 00 này
00:05:19 - 00:05:26, Đây là một cái muy bằng đầu, nó sẽ kéo cái muy này về góc tạ độ
00:05:26 - 00:05:40, Rồi, đồng thời sigma z nếu như cái phân bố của mình nó quá lớn, thì nó sẽ kéo cho cái sigma này tiến về đủ nhỏ
00:05:40 - 00:05:49, Nhưng nhờ có cái lốc của sigma z này thì nó sẽ khiến cho cái sigma của mình không quá nhỏ
00:05:49 - 00:05:55, Chứ còn nếu mà nó nhỏ quá thì có phải là thay vì chúng ta đưa về một phân bố thì cuối cùng nó sẽ đưa về một điểm không?
00:05:55 - 00:06:01, Phân bố của mình nó sẽ tiến về một điểm, như vậy là nó tương tự như autoencoder rồi
00:06:01 - 00:06:13, Như vậy thì cái sigma nó sẽ kéo về, đừng có quá lớn nhưng mà cũng đừng có quá nhỏ để hy vọng rằng là cái phân bố của mình nó thực sự là một cái phân bố có yếu tố ngộ nhiên
00:06:13 - 00:06:22, Chứ còn nếu mà sigma mà tiến về bằng 0, tức là nó sẽ co về một điểm, tức là nó sẽ đưa về một cái mô hình deterministic, tức là một cái mô hình tất định
00:06:22 - 00:06:26, Chứ không có yếu tố sắc xúc như trong cái mô hình VAR
00:06:26 - 00:06:35, Thì với cái công thức này nó sẽ quyến khích hai cái chuyện đấy, một đó là cái phân bố quy này sẽ tiến về 0, tiến về cái gốc tà độ
00:06:35 - 00:06:49, Cái thứ hai đó là mi z, nó sẽ tiến về một cái phân bố mà có cái độ lạc vừa đủ, chứ nó không có quá nhỏ nhưng nó cũng không quá to
00:06:49 - 00:06:52, Thì đây là cái công thức chính quy hóa
00:06:52 - 00:07:00, Và chúng ta có những cái tính chất gì từ cái việc chính quy hóa này, nó sẽ có những tính chất gì
00:07:00 - 00:07:08, Đầu tiên đó là cái tính liên tục, tức là những cái điểm gần nhau trong không gian thì nội dung hoàn toàn tương tự nhau khi giải mã
00:07:08 - 00:07:09, Chúng ta đang nói đến cái ý này
00:07:09 - 00:07:26, Thì ở bên trái là một cái mô hình không có được chính quy, tức là chúng ta chỉ có cái sai số giữa x trừ cho x mũ thôi
00:07:26 - 00:07:31, Sai số tái tạo thôi, chỉ có sai số tái tạo
00:07:31 - 00:07:38, Thì nếu không có thành vật chính quy hóa thì nó sẽ không đảm bảo được
00:07:38 - 00:07:46, Thứ nhất đó là hai điểm gần nhau trong không gian ẩn, là cái điểm màu xanh lá mà và màu đỏ ở đây thì nó gần nhau trong không gian tiền ẩn
00:07:46 - 00:07:55, Nhưng khi mà giải mã thì nó không có tương tự nhau, ví dụ cái điểm màu xanh lá này, xanh lá mà này thì nó sẽ ra hình vuông
00:07:56 - 00:08:07, Trong khi cái điểm màu đỏ thì lại tạo ra một cái hình giống hình tam giác, thì hai cái hình này nó không có tương tự nhau, mặc dù hai cái điểm này nó gần nhau
00:08:10 - 00:08:19, Cái thứ hai đó là đối với cái việc mà không chính quy hóa, nó sẽ có thể khiến cho cái điểm của cái không gian ẩn được giải mã nhưng mà không có ý nghĩa
00:08:19 - 00:08:29, Ví dụ như ở đây chúng ta chỉ có hình tam giác, hình tròn, hình vuông, nhưng mà có một cái điểm ở đây là cái điểm mà nó không quá gần ba cái điểm này
00:08:29 - 00:08:33, thì khi chúng ta giải mã nó ra một cái hình gì đấy mà nó không có ý nghĩa
00:08:35 - 00:08:46, Ví dụ như trong cái chữ số viết tay thì khi chúng ta decode ra lẽ ra nó phải ra là số, chữ số thì không cho đến chính nhưng cuối cùng nó sẽ ra một cái gì đấy, nó không phải là con số
00:08:46 - 00:08:53, Tức là một cái dữ liệu không có ý nghĩa, thì nếu như không có chính quy hóa nó sẽ khiến cho chúng ta bị hai cái bến đề này
00:08:54 - 00:09:04, Ngoài ra thì nhờ có chính quy hóa nó sẽ giúp cho chúng ta giải quyết được cái bến đề đó, đó là cái tính liên tục của dữ liệu của cái điểm biểu diễn trong không gian ẩn
00:09:04 - 00:09:12, Thì hai cái vector z và z phải nằm trong không gian ẩn mà giống nhau, gần nhau thì khi decode ra nó cũng phải giống nhau
00:09:12 - 00:09:22, Khi tính chất thứ hai, đó là kính tính đầy đủ là lấy mẫu từ không gian ẩn thì cái nội dung của mình nó sẽ phải có ý nghĩa, thì đây là một cái ví dụ này không có ý nghĩa
00:09:22 - 00:09:43, Còn nếu như chúng ta sử dụng một cái mô hình chính quy hóa, tức là bên cạnh cái size số tái thạo nó có thêm cái thành phần chính quy hóa là được biểu diễn bởi cái công thức là d của kl, đó là kl diversion của qi và p
00:09:43 - 00:09:59, Cái này là vết tắc nha, thì các cái điểm gần nhau thì được dạy mã tương tự và có ý nghĩa, ví dụ chúng ta thấy cái điểm màu cam và cái điểm màu tím thì hai cái hình này khi chúng ta dạy mã thì chúng ta thấy cái dán giấc nó cũng giống nhau
00:09:59 - 00:10:20, Mặc dù cái điểm màu tím thì nó sẽ hơi bo ở đây một chút, hơi bo tròn, nhưng nếu xét về hình thù thì nó cũng gần giống với hình tam giác, do đó thì hai cái điểm này khi chúng ta dạy nó sẽ có cái tính tương tự nhau và nó hoàn toàn là có ý nghĩa của nó, nó có cái lý do của nó
00:10:21 - 00:10:33, Rồi, thì đây chính là cái sự khác biệt của việc có chính quy hóa và không có chính quy hóa khi chúng ta huấn luyện với cái mô hình VAE
00:10:34 - 00:10:49, Thế thì một cái biểu diễn khác là sau khi chúng ta đã huấn luyện xong mô hình VAE, thì cái phân bố chuẩn tiền nghiệm sẽ đảm bảo được cái ưu tố về tính liên tục và tính đầy đủ
00:10:49 - 00:10:59, Cái tính liên tục nó thể hiện ở chỗ đó là những cái điểm nào mà gần nhau thì khi decode nó sẽ giống nhau và cái tính đầy đủ đó là mọi điểm của mình
00:10:59 - 00:11:06, Trong cái không gian thì khi chúng ta decode ra nó đều có cái ý nghĩa của nó chứ không phải là một cái nội dung vô nghĩa
00:11:06 - 00:11:11, Thì sau đây chúng ta sẽ nói có một cái ví dụ rõ hơn về cái chuyện này
00:11:11 - 00:11:17, Chúng ta thấy ở đây có 3 cái phân bố màu đỏ màu xanh và màu vàng
00:11:17 - 00:11:23, Thì ở đây là cái tâm cụm màu đỏ thì khi chúng ta decode ra nó sẽ ra cái hình tan giác
00:11:23 - 00:11:27, Còn đây là tâm cụm của bồ màu xanh decode ra là ra hình tròn
00:11:28 - 00:11:38, Thế thì một cái điểm ở lưng chừng ngay chính giữa tròn và tan giác thì chúng ta thấy cái hình này nó đều có cái ý nghĩa khá là phù hợp
00:11:38 - 00:11:43, Đúng không? là cái hình này nó sẽ có cái bo tròn giống như cái hình tròn
00:11:47 - 00:11:51, Nhưng đồng thời nó sẽ có cái néc thẳng, 3 cái néc thẳng
00:11:53 - 00:11:54, Giống như tan giác
00:11:58 - 00:12:05, Đó, thì cái điểm trung điểm này nó sẽ giống giống, nó sẽ giúp chúng ta tạo ra cái tính gọi là cái tính liên tục
00:12:07 - 00:12:11, Và đồng thời nó cũng có ý nghĩa, nó cũng có ý nghĩa chứ không phải là không
00:12:11 - 00:12:17, Nếu mà cái ở giữa này mà nó ra một cái điểm nào đó mà chúng ta không thể giải thích được thì đó là không có ý nghĩa
00:12:17 - 00:12:24, Rồi khi chúng ta tiến càng gần hơn về cái tâm thì chúng ta thấy là cái tan giác nó bớt bo tròn đi
00:12:24 - 00:12:26, Đúng không? nó bớt bo tròn đi
00:12:27 - 00:12:35, Hay nói cách khác, đó là nó dần nó nhọn đi để giống cái tan giác này
00:12:35 - 00:12:43, Còn khi mà cái điểm này tiến về phía tâm của cầm thì chúng ta thấy nó sẽ càng lúc nó sẽ càng tròn trĩ hơn
00:12:43 - 00:12:47, Này, tròn, tròn hơn
00:12:47 - 00:12:54, Tương tự như vậy cho các cái điểm còn lại, ví dụ như đây là trung điểm giữa hình tròn và hình vuông
00:12:54 - 00:12:59, Thì chúng ta thấy cái hình này nó vừa có cái dán giấc của hình vuông nhưng đồng thời nó sẽ có cái bo tròn
00:12:59 - 00:13:02, Và nó sẽ có những cái bo tròn như thế này
