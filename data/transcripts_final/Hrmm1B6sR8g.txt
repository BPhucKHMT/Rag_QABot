0:00:14 - 0:00:23, Chúng ta sẽ cùng đến với lý thuyết về mô hình tạo sinh dựa trên không gian xác suất.
0:00:23 - 0:00:30, Đó là, chúng ta sẽ tìm ra một cái hàm để ánh xạ một cái phân bố gaussian.
0:00:30 - 0:00:35, Về một cái phân bố cho ảnh trong thực tế để sinh dữ liệu.
0:00:35 - 0:00:41, Thế thì, cái mục tiêu của chúng ta, đó chính là chúng ta làm sao xác định được cái hàm decoder này.
0:00:41 - 0:00:47, Hàm này gọi là hàm decoder và tham số của cái mô hình này sẽ là theta.
0:00:47 - 0:00:58, Và hàm này sẽ nhận cái đầu vào là một cái giá trị nhiễu z, được lấy mẫu ở trong cái không gian gaussian, trong cái phân bố gaussian.
0:00:58 - 0:01:04, Với cái nhiễu này, chúng ta sẽ tạo ra một cái nội dung của một cái tấm ảnh.
0:01:04 - 0:01:11, Và cái ảnh này thì cái phân bố của cái ảnh đầu ra là P theta x, được ký hiệu bởi P theta x.
0:01:11 - 0:01:15, Thì nó có cái dạng là màu cam giống như thế này.
0:01:15 - 0:01:21, Tuy nhiên cái phân bố ảnh trong thực tế thì nó sẽ phức tạp, nó sẽ rất là phức tạp.
0:01:21 - 0:01:29, Và chúng ta luôn mong muốn cái P theta này sẽ gần với cái phân bố ảnh trong thực tế nhất có thể.
0:01:29 - 0:01:39, Thế thì, để ký hiệu cho cái phân bố ảnh trong thực tế thì chúng ta sẽ ký hiệu đó là P của data x.
0:01:39 - 0:01:44, Thì đây chính là cái data distribution, tức là cái phân bố ảnh trong thực tế.
0:01:44 - 0:01:56, Và mục tiêu là làm sao cái hàm decoder của chúng ta, nó có thể tạo ra cái P theta x giống với lại P data x nhất có thể.
0:01:56 - 0:02:03, Tức là đem cái phân bố của P theta x về tiến về cái P của data.
0:02:03 - 0:02:08, Tuy nhiên chúng ta thì lại không biết cái phân bố của dữ liệu trong thực tế.
0:02:08 - 0:02:12, Chúng ta không thể biết trước cái phân bố dữ liệu trong thực tế.
0:02:12 - 0:02:21, Dẫn đến là chúng ta sẽ không thể một cách tường minh, chúng ta tìm ra được cái hàm P theta x sao cho cái P theta x xấp xỉ với P data.
0:02:21 - 0:02:28, Chứ còn nếu cái P của data này mà chúng ta đã biết trước cái công thức của nó thì cái hàm này sẽ rất là dễ.
0:02:28 - 0:02:38, Vậy thì chúng ta làm sao có thể tạo ra được cái hàm decoder này thì chúng ta sẽ phải thực hiện cái công việc đó gọi là lấy mẫu dữ liệu.
0:02:38 - 0:02:46, Tức là chúng ta có thể lấy mẫu dữ liệu 1, dữ liệu 2, dữ liệu 3, dữ liệu 4 thì đây là toàn bộ những cái ảnh trong ảnh thực tế.
0:02:48 - 0:02:54, Và làm sao từ cái ảnh thực tế này chúng ta có thể đi ước lượng được cái hàm decoder.
0:02:54 - 0:02:57, Tìm sao chúng ta ước lượng được hàm decoder.
0:02:57 - 0:03:02, Vậy thì chúng ta sẽ ước lượng bằng cách đó là dùng maximum likelihood.
0:03:02 - 0:03:07, Giả sử như cái dữ liệu của mình đó là x1, x2 cho nên xm được lấy mẫu.
00:03:07 - 0:03:10, Cái ký hiệu này là ký hiệu lấy mẫu ha.
0:03:10 - 0:03:15, Được lấy mẫu từ cái phân bố dữ liệu trong thực tế.
0:03:15 - 0:03:21, Và chúng ta sẽ tìm cách để xấp xỉ cái maximum likelihood bằng cái công thức này.
0:03:21 - 0:03:27, Đó là chúng ta sẽ đi tìm cái theta sao cho cái maximum likelihood.
0:03:27 - 0:03:35, Thứ nữa là cái tích của các xác suất P theta XA là cao nhất, là lớn nhất.
0:03:35 - 0:03:38, Vậy thì làm sao để có thể làm được việc này?
0:03:38 - 0:03:46, Thì chúng ta có thể đưa về cái phân bố, chúng ta ký hiệu bằng cái phân bố P theta X này.
00:03:46 - 0:03:53, Và thay vì tính argument max của cái tích này thì chúng ta sẽ thêm cái log vào.
0:03:53 - 0:03:57, Vậy thì cái việc thêm log vào nó sẽ giúp chúng ta đơn giản hóa cái công thức này.
0:03:57 - 0:03:59, Vì log của tích sẽ đưa về cái hàm tổng.
0:03:59 - 0:04:02, Khi chúng ta biến đổi nó sẽ dễ dàng hơn.
0:04:02 - 0:04:11, Và nó sẽ tương đương với việc chúng ta tìm theta sao cho log tổng của các cái log này là lớn nhất.
0:04:11 - 0:04:17, Log của tích là bằng tổng của các cái log thành phần.
0:04:18 - 0:04:24, Vậy thì cái công thức này thì nó sẽ tương đương với cái việc đó là chúng ta,
0:04:24 - 0:04:34, cái công thức tổng của log của P theta thì nó sẽ tương đương với cái việc là chúng ta đi tính cái kỳ vọng.
0:04:34 - 0:04:44, Chúng ta sẽ đi tính cái kỳ vọng khi lấy cái mẫu x xấp xỉ theo, lấy cái mẫu x, lấy mẫu x, lấy mẫu theo cái phân bố data.
0:04:44 - 0:04:49, Và log của P theta x này là lớn nhất.
0:04:49 - 0:05:06, Tức là trong công thức trước thì chúng ta lấy tổng của mẫu thứ nhất cho đến mẫu thứ m. Thì công thức này chúng ta sẽ viết gọn lại đó là cái kỳ vọng, kỳ vọng của x khi lấy mẫu trong cái không gian P theta.
0:05:06 - 0:05:11, Và cái công thức này thì nó lại được đưa về cái công thức đó là tích phân.
0:05:11 - 0:05:21, Tích phân của P data x nhân với lại log của P theta x dx.
0:05:21 - 0:05:25, Thì ở trên là công thức kỳ vọng.
0:05:25 - 0:05:31, Và ở dưới đó là cái công thức triển khai ra theo dạng là tích phân.
0:05:31 - 0:05:37, Thì đây là cái xác suất của dữ liệu nhân với lại log của P theta x.
0:05:37 - 0:05:41, Thì đây chính là cái công thức của kỳ vọng và dạng tích phân.
0:05:41 - 0:05:48, Và chúng ta thấy rằng là ở đây cái tham số chúng ta cần phải ước lượng, chúng ta cần phải tìm.
0:05:48 - 0:05:52, Đó là theta. Do đó chúng ta sẽ cùng trừng thêm một cái đại lượng.
0:05:52 - 0:05:56, Đó là tích phân của P data log P data x.
0:05:56 - 0:05:59, Thì đó là trong cái vế sau này nè.
0:05:59 - 0:06:10, Cái vế sau không có theta. Do đó cái việc tìm theta sao cho cái giá trị này max cũng tương đương với cái việc là tìm theta sao cho toàn bộ cái hiệu này là nhỏ nhất.
0:06:10 - 0:06:16, Tại vì trong con mắt của theta thì đây là hằng số.
0:06:16 - 0:06:19, Thế thì tại sao chúng ta lại thêm cái vế này vào?
0:06:19 - 0:06:27, Chúng ta thêm cái vế này vào để sau này chúng ta biến đổi. Nó sẽ đưa về cái hàm trừ, ờ xà hàm chia.
0:06:27 - 0:06:34, Chúng ta rút cái thừa số chung là P data và P data ở đây.
0:06:34 - 0:06:45, Rút thừa số chung sau đó sẽ là log của P theta x trừ cho log của P data.
0:06:46 - 0:06:57, Thì log của trừ, ờ trừ hai log thì chúng ta sẽ bằng log của P theta chia cho log của P data.
0:07:01 - 0:07:04, Thì đây là cái công thức toán cấp 3 của mình.
0:07:04 - 0:07:10, Và khi chúng ta đưa về cái công thức P của theta chia cho P của data rồi.
0:07:10 - 0:07:15, Thì chúng ta sẽ tìm giá trị max, tìm giá trị min của nó.
0:07:15 - 0:07:18, Ở đây thì là chúng ta đi tìm max.
0:07:18 - 0:07:20, Ở đây là đi tìm max.
0:07:20 - 0:07:27, Nhưng mà chúng ta cái tham số, cái biến số mà chúng ta muốn tìm chính là cái theta ở trên.
0:07:27 - 0:07:33, Vậy thì để tìm cái giá trị max này thì nó sẽ tương đương với cái việc chúng ta đi tìm min.
0:07:33 - 0:07:36, Nhưng mà chúng ta đảo cái phân số này lại.
0:07:36 - 0:07:42, Chúng ta đảo cái phân số, đưa cái data lên và đưa cái P của theta xuống.
0:07:42 - 0:07:46, Thì tại sao chúng ta lại đảo xuống? Nó có cái dụng ý phía sau.
0:07:46 - 0:07:52, Khi chúng ta đảo cái thứ tự nó xuống là tử thành mẫu mẫu thành tử.
0:07:52 - 0:07:58, Thì cái công thức này, nó chính là cái công thức của KL diversion.
0:07:58 - 0:08:10, Nó có thể hiểu là một cái khoảng cách giữa hai cái phân bố P data và P theta.
0:08:10 - 0:08:17, Rồi, chú ý là ở phía trên là hàm max, thì ở phía dưới chúng ta sẽ làm hàm min.
0:08:17 - 0:08:20, Khi đưa về min chúng ta đảo thứ tự cho nhau.
0:08:20 - 0:08:24, Và chúng ta đã đưa nó về cái công thức của KL diversion.
0:08:24 - 0:08:27, Thì đây là một cái công thức rất là nổi tiếng.
0:08:27 - 0:08:33, Rồi, cái ý nghĩa của cái công thức này đó là gì?
0:08:33 - 0:08:42, Đó là chúng ta sẽ tìm cách để cho cái P theta này tiến về với P data.
0:08:42 - 0:08:49, Hay là cái phân bố màu cam này nè. Nó sẽ tiến về cái phân bố màu xanh lá.
0:08:49 - 0:09:03, Thì cái việc mà xấp xỉ cái dữ liệu tương ứng giúp tối thiểu cái độ tương đồng giữa cái phân bố đầu ra của cái mô hình của mình với lại cái phân bố của dữ liệu trong thực tế.
0:09:03 - 0:09:09, Thì đây chính là cái ý nghĩa của cái công thức KL diversion. Nó rất là dễ hiểu.
0:09:09 - 0:09:16, Tức là mục tiêu, tóm lại đó là mục tiêu của chúng ta là huấn luyện cái mô hình decoder làm sao
0:09:16 - 0:09:26, Để cho khi chúng ta với một cái mẫu nhiễu ngẫu nhiên z truyền vào qua cái hàm decoder này thì nó sẽ ra được một cái điểm.
0:09:26 - 0:09:36, Và khi chúng ta lấy mẫu trên toàn bộ cái Gaussian distribution này đưa qua cái hàm decoder thì nó sẽ ra cái phân bố là P theta.
0:09:36 - 0:09:47, Và cái P theta này làm sao để cho xấp xỉ với lại cái P của data thì chúng ta sẽ dùng cái công thức KL diversion như thế này.
0:09:47 - 0:09:58, Và cái việc mà tìm theta sao cho hai cái phân bố này tiến về nhau, cái phân bố của P theta tiến về phân bố của data
0:09:58 - 0:10:08, Nó sẽ tương đương với việc chúng ta tìm cái hàm nhỏ nhất, tức là tìm cái giá trị nhỏ nhất, tìm theta sao cho cái KL diversion này là nhỏ nhất.
0:10:08 - 0:10:15, Thì đó chính là cái ý nghĩa về xác suất thống kê của cái mô hình tạo sinh hình ảnh.
0:10:15 - 0:10:24, Tuy nhiên, làm sao chúng ta có thể tính được cái xác suất tích phân của tất cả các cái giá trị ẩn.
0:10:24 - 0:10:31, Thế thì chúng ta thấy là trong cái công thức ở trên, chúng ta sẽ có cái log của P theta.
0:10:31 - 0:10:40, Trong công thức ở trên thì chúng ta sẽ có cái công thức là log của P theta.
0:10:41 - 0:10:56, Thì cái log của P, thì nó sẽ có thể được đưa về cái công thức đó là log của tích phân Pxz với z là một cái vector trong không gian Gaussian, phân bố Gaussian.
0:10:56 - 0:11:04, Chúng ta sẽ lấy trên toàn bộ cái z này, ở đây là dz, tức là chúng ta sẽ lấy trên toàn bộ cái không gian của mình.
0:11:04 - 0:11:12, Vậy thì làm sao chúng ta có thể tính tích phân được cho tất cả các cái giá trị ẩn trên cái không gian phân bố Gaussian.
0:11:12 - 0:11:24, Thì đây là công việc rất là khó. Thế thì thay vì làm cái công việc này, thì chúng ta có thể đưa về cái công thức đó là log của Px.
0:11:24 - 0:11:37, Ở đây chúng ta sẽ bỏ cái tham số theta ra để cho nó đơn giản. Còn ở đây ý nghĩa đó là chúng ta xác định cái phân bố Px khi theta của mình có tham gia vô.
0:11:37 - 0:11:50, Ở đây chúng ta bỏ theta ra để cho nó dễ nhìn. Thì log của P sẽ là bằng log của Pxz, tức là log của Pxz chia cho Pxz.
0:11:50 - 0:12:03, Vậy thì Pxz thì đây chính là cái công thức của Bayes. Đây là công thức của Bayes. Tức là xác suất của x và z.
0:12:03 - 0:12:11, Khi chúng ta chia cho xác suất của z cho trước x thì đó chính là xác suất của P.
0:12:11 - 0:12:23, Hay nói cách khác là khi chúng ta nhân lên thì Px nhân với lại Pz cho trước x thì đó chính là bằng Pxz.
0:12:23 - 0:12:36, Tức là xác suất của x và z cùng xuất hiện thì nó sẽ là bằng xác suất x nhân với lại xác suất của z cho trước x.
0:12:36 - 0:12:47, Thì đây là dựa trên địa lý của Bayes. Tuy nhiên dựa trên công thức này thì chúng ta lại có câu hỏi đó là làm sao chúng ta đã biết được cái này.
0:12:47 - 0:12:59, Tức là làm sao chúng ta có thể biết được cái P của z khi cho trước x. Thì để làm được cái chuyện đó thì chúng ta sẽ sử dụng một cái hàm, đó là encoder.
0:12:59 - 0:13:10, Tức là trong công thức này là cho trước x, cho trước x, làm sao chúng ta xác định được cái P của z cho trước x thì đây là một cái hàm phân bố.
0:13:10 - 0:13:22, Vậy làm sao biết được cái P này? Thì chúng ta sẽ thêm vô một cái module là encoder để từ cái dữ liệu x mà chúng ta sampling trong thế giới thật P data.
0:13:24 - 0:13:36, Qua cái hàm encoder này thì chúng ta sẽ xác định được cái Quy zx, tức là phân bố của z khi biết trước x.
0:13:36 - 0:13:46, Tức là cho trước mẫu dữ liệu đầu vào và chúng ta xác định cái phân bố của z, của cái vector trong không gian tìm ẩn.
0:13:46 - 0:14:03, Thế thì chúng ta có cái công thức đó là P của x là bằng log của Px. Thì nhân với lại cái tích phân của Quy z cho trước x nhân với dz thì cái công thức này nó sẽ có cái giá trị là bằng một.
0:14:03 - 0:14:20, Tại vì khi chúng ta cho z dựa trên toàn bộ cái không gian xác suất của nó thì Quy của z cho trước x thì tổng xác suất này là cái tích phân xác suất này là luôn là bằng một.
0:14:20 - 0:14:27, Tại vì dz nó đã chạy, cái dz này nó chạy trên hết trong cái không gian xác suất của nó.
0:14:27 - 0:14:33, Vậy thì chúng ta sẽ triển khai cái công thức này, đem cái tích phân ra ngoài.
0:14:33 - 0:14:43, Thì sẽ có là Quy của zx cho trước x nhân với lại log của Px dz. Chúng ta đưa cái tích phân ra ngoài.
0:14:43 - 0:14:57, Thì cái công thức này nó sẽ tương đương với cái việc là chúng ta tính cái kỳ vọng của z, lấy mẫu trong cái z cho trước x.
0:14:57 - 0:15:11, Rồi, như vậy thì chúng ta sẽ lấy hết cái phân bố này, sau đó chúng ta sẽ lấy cái xác suất đó, chúng ta nhân với lại log của Px.
0:15:11 - 0:15:16, Thì cái công thức này nó sẽ tương đương với lại cái kỳ vọng ở đây.
0:15:16 - 0:15:27, Và chúng ta sẽ cùng nhân sử dụng cái công thức Bayes thì Px, hồi nãy chúng ta đã ghi cái công thức này rồi.
0:15:27 - 0:15:40, Px là bằng Px và z chia cho Pzx dựa trên cái luật Bayes.
0:15:40 - 0:15:52, Và khi chúng ta áp dụng cái công thức này vào thì chúng ta cùng nhân cho tử và mẫu là Quy của z cho trước x.
0:15:52 - 0:16:02, Rồi, sau đó chúng ta đảo thứ tự lại, chúng ta đem cái Quy qua và đem cái P qua đây.
0:16:02 - 0:16:07, Thì khi đó chúng ta sẽ có là Px, nhân cho Quy của zx.
0:16:07 - 0:16:16, Rồi, và từ cái công thức ở trên thì chúng ta sẽ có là log của tích, ở đây chúng ta có cái phép là phép nhân.
0:16:16 - 0:16:22, Chúng ta có cái phép nhân, thì log của tích sẽ là bằng tổng 2 log.
0:16:22 - 0:16:31, Rồi, thì là Px chia cho Quy của zx, còn ở đây sẽ là Quy của zx chia cho Px.
0:16:31 - 0:16:41, Và chúng ta sẽ thấy đây chính là cái công thức để mà chúng ta có thể ước lượng được cái Px này.
0:16:41 - 0:16:54, Và làm sao cho cái Px này nó tiến về cái P của data.x thì chúng ta sẽ dựa trên cái công thức này để chúng ta xây dựng cái mô hình.
0:16:54 - 0:17:10, Và ở trong cái công thức này thì kỳ vọng của Quy zx, của cái công thức là log của Quy zx chia cho P của zx, thì đây chính là cái công thức của KL diversion.
0:17:10 - 0:17:13, Đây chính là cái công thức của KL diversion.
0:17:13 - 0:17:30, Ở trên đây là Quy zx, thì nó sẽ là KL diversion của Quy zx và Pzx, tức là cái khoảng cách giữa 2 cái phân bố này.
0:17:30 - 0:17:35, Mà khoảng cách của 2 cái phân bố này thì đó là con số lớn hơn không.
0:17:35 - 0:17:42, Tuy nhiên làm sao chúng ta có được cái này, làm sao chúng ta có được cái P của zx thì cái chuyện đó là rất là khó.
0:17:42 - 0:17:51, Rất khó để chúng ta có thể xác định được cái phân bố thực tế của z cho trước x.