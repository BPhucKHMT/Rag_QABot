0:00:14 - 0:00:20, Chúng ta sẽ cùng đến với phần cài đặt minh họa cho học tương phản Contrasted Mini
0:00:20 - 0:00:25, Đây là một kỹ thuật được sử dụng trong huấn luyện với Moon Clip
0:00:25 - 0:00:33, Với dụng minh họa này, hình này là lấy từ paper gốc của Moon Clip
0:00:33 - 0:00:38, Và trong sơ đồ này, chúng ta thấy là một văn bản của mình
0:00:38 - 0:00:45, Thì qua text encoder, chúng ta có thể sử dụng class retrain model
0:00:45 - 0:00:51, Nhưng mà để cho đơn giản, với cài đặt, chúng ta tập trung nhiều vào phần học tương phản
0:00:51 - 0:00:58, Nên ở đây chúng ta sẽ sử dụng dữ liệu mô phỏng, đó là một random vector
0:00:58 - 0:01:03, Và tương tự như vậy cho dữ liệu ảnh, chúng ta cũng sẽ sử dụng random vector
0:01:03 - 0:01:08, Thể hiện ở trong hai cái hàm pos.random
0:01:08 - 0:01:16, Và như vậy chúng ta sẽ tiến hành các bước thực hiện với contrasted learning này
0:01:16 - 0:01:26, Đó là chúng ta sẽ có một bước từ vector mô phỏng chúng ta sẽ chuẩn hóa để tạo ra các vector
0:01:26 - 0:01:31, T1, T2, T3 cho đến Tn này
0:01:31 - 0:01:41, Với mỗi t, nó sẽ là một embedding cho một văn bản
0:01:41 - 0:01:47, Còn với một ảnh y1, y2 v.v., nó sẽ là một vector biểu diễn cho một ảnh
0:01:47 - 0:01:53, Trong cái ví dụ này chúng ta thấy nó có tất cả là n ảnh
0:01:53 - 0:02:00, Thì ở đây chúng ta sẽ có n chính là số lượng cặp hình ảnh văn bản
0:02:00 - 0:02:03, Trong một batch dữ liệu mà chúng ta ném đi để huấn luyện
0:02:03 - 0:02:12, Và mỗi một vector này y1, t1, in, tn
0:02:12 - 0:02:16, Biểu diễn bằng một vector có kích thước là d
0:02:16 - 0:02:23, D là bằng 8, để tạo ra ma trận y
0:02:23 - 0:02:28, Vì tức là embedding của ảnh y1 cho đến in
0:02:28 - 0:02:33, Thì chúng ta sẽ tạo torch.random n như d
0:02:33 - 0:02:39, Trong đó n là số ảnh và d chính là số chiều của mỗi vector biểu diễn
0:02:39 - 0:02:46, Thì nếu đúng ra thì nó sẽ lấy từ một pretrained model
0:02:46 - 0:02:52, Và chúng ta feed qua n ảnh
0:02:52 - 0:02:57, Nhưng ở đây để cho đơn giản thì chúng ta tạo ra vector ngẫu nhiên
0:02:57 - 0:03:02, Sau đó thì tương tự như vậy thì te
0:03:02 - 0:03:08, Nếu đúng là te nó sẽ phải tương đồng với ire
0:03:08 - 0:03:12, Tức là embedding của ảnh nó sẽ tương đồng với ire
0:03:12 - 0:03:19, Tuy nhiên ở đây chúng ta mong muốn là không phải sử dụng các dữ liệu thật
0:03:19 - 0:03:23, Nếu chúng ta sử dụng chính te này là i luôn
0:03:23 - 0:03:30, Thì ma trận này sẽ tiến về một ma trận đơn vị
0:03:30 - 0:03:35, Nhưng mà vì chúng ta muốn nó có cái tương chất gọi là ngẫu nhiên
0:03:35 - 0:03:38, Và không có sự tương đồng một cách tuyệt đối
0:03:38 - 0:03:56, Nên ở đây chúng ta sẽ cộng thêm một cái đại lượng nhiễu để cho tạo ra sự sai khác giữa ảnh và văn bản đủ nhỏ
0:03:56 - 0:04:03, Sau đó thì chúng ta sẽ tiến hành chuẩn hóa hai vector này
0:04:03 - 0:04:07, Thì để chuẩn hóa thì chúng ta phải viết thêm một cái hàm
0:04:07 - 0:04:11, Đó là hàm norm L2
0:04:11 - 0:04:16, Thế thì ở đây chúng ta sẽ cài đặt to-do này
0:04:16 - 0:04:19, Đó là L2Norm
0:04:19 - 0:04:26, Rồi, thì ở đây chúng ta sẽ def và chúng ta sẽ truyền vào một cái vector V
0:04:26 - 0:04:30, Về đây chúng ta để lại vector đây
0:04:30 - 0:04:53, Sau đó Vector sẽ là bằng Vector chia cho norm của norm L2
0:04:53 - 0:05:09, Rồi, thì ở đây nếu chúng ta chuẩn hóa trên full toàn bộ với mỗi một vector như thế này thì mọi chuyện là đơn giản rồi
0:05:09 - 0:05:18, Nhưng mà ở đây vì chúng ta đưa vector vào nó không phải là một vector mà nó là một cái ma trận Te và E
0:05:18 - 0:05:25, Mà chúng ta đang muốn lấy theo một cái trục là theo trục của N
0:05:25 - 0:05:35, Tức là với mỗi một cái dữ liệu thì chúng ta sẽ đi chuẩn hóa trên cái trục của đặc trưng thôi
0:05:35 - 0:05:37, Tức là cái trục D này thôi
0:05:37 - 0:05:42, Do đó thì ở đây chúng ta sẽ để lại cái dimension là bằng trừ 1
0:05:42 - 0:05:47, Rồi, thì nó sẽ đi chuẩn hóa theo cái trục D này
0:05:47 - 0:05:59, Rồi, ngoài ra thì chúng ta sẽ không có thay đổi số chiều do đó ở đây chúng ta sẽ để keepDim là bằng True
0:05:59 - 0:06:10, Thì đây chính là cái hàm chuẩn hóa L2 và chúng ta sẽ đặt hàm này cho cái biến là Te và sẽ gán ngược trở lại là Te
0:06:10 - 0:06:17, Rồi tương tự như vậy là Ea cũng sẽ là gán ngược trở lại cho Ea tức là Image Embedding
0:06:17 - 0:06:23, Tiếp theo thì chúng ta sẽ đi tính cái Logit bằng cách đó là tích vô hướng
0:06:23 - 0:06:27, Tạm thời là chúng ta sẽ không có dùng cái Temperature
0:06:27 - 0:06:31, Chúng ta sẽ để Te.
0:06:31 - 0:06:42, Thì ở đây chúng ta có thể dùng hàm dot vào dot hoặc là chúng ta sử dụng hàm của torch đó là .matmul
0:06:42 - 0:06:48, Rồi, và chúng ta sẽ truyền là Ea và Te
0:06:53 - 0:07:05, Rồi, thì sau khi chúng ta xử lý xong thì chúng ta sẽ ra được một cái Logit
0:07:05 - 0:07:14, Tức là cái kết quả của cái phép nhân tích vô hướng giữa hai cái ma trận
0:07:14 - 0:07:16, Thực ra là ma trận là chúng ta đang xử lý hàng loạt
0:07:16 - 0:07:22, Còn đúng ra thì nó sẽ là xử lý cho từng cái cặp vector 1 với nhau
0:07:22 - 0:07:32, Và ở đây chúng ta chú ý là khi chúng ta để đảm bảo được cái việc nhân tích vô hướng thì cái Te này nó sẽ phải chuyển vị
0:07:32 - 0:07:37, Tại vì ban đầu cái Te và Ea đều cùng có kích thước là Nd
0:07:37 - 0:07:43, Muốn nhân được với nhau thì cái Te là Nd, thì phải nhân, cái Ea là Nd
0:07:43 - 0:07:51, Thì nhân với lại cái Te nó sẽ là Dn, tức là cái ma trận kích thước là Nd
0:07:51 - 0:07:59, Sẽ nhân với cái ma trận là Dn
0:07:59 - 0:08:02, Đó là lý do tại sao chúng ta phải chuyển vị
0:08:02 - 0:08:09, Và đầu ra của mình nó sẽ trả về là một cái ma trận kích thước là Nd
0:08:09 - 0:08:16, Thì đúng như trong cái sơ đồ này là kích thước của mình sẽ là N, nhân cho N
0:08:16 - 0:08:22, Rồi, và chút nữa chúng ta cũng sẽ thử nghiệm xem khi chúng ta thêm tô vô thì nó sẽ như thế nào
0:08:22 - 0:08:27, Rồi bây giờ chúng ta sẽ kết nối với lại cái máy
0:08:27 - 0:08:33, Thì thật ra ở đây chúng ta cũng không cần phải có overview do là chúng ta tính toán dữ liệu cũng không có nặng
0:08:33 - 0:08:43, Rồi, torch initialize thì ở đây chắc là chúng ta chưa chạy cái lệnh này
0:08:43 - 0:08:48, March, ở đây chắc là dư một cái dấu
0:08:48 - 0:09:04, Rồi, bây giờ chúng ta sẽ chạy lại cái code này
0:09:04 - 0:09:13, Và sau khi chúng ta trực quan hóa cái logic thì chúng ta thấy là vì nó có cái yếu tố nhiễu nên cái ma trận của mình
0:09:13 - 0:09:25, Nó có thể là phát sáng cái đường ở giữa nhưng mà nó sẽ có cái đại lượng nhiễu nên chúng ta sẽ thấy là nó sẽ lè ra và phát sáng ở một số khu vực như thế này
0:09:25 - 0:09:34, Rồi, và đương nhiên khi contrasting learning thì chúng ta sẽ cố gắng là để cho cái ma trận này càng tiến về cái ma trận đơn vị
0:09:34 - 0:09:41, Rồi, sau đó chúng ta sẽ tạo cái ma trận đơn vị đó trong những cái phần sau
0:09:41 - 0:09:47, Rồi, trước hết thì chúng ta sẽ visualize cái dòng số 3 ở đây
0:09:47 - 0:09:49, Nhìn nó như thế nào
0:09:49 - 0:09:56, Thì để visualize dòng số 3 chúng ta sẽ lấy logic 3, 2 chấm, tức là lấy nguyên một cái dòng
0:09:56 - 0:10:03, Và ở đây chúng ta sẽ truyền là none 3 chấm, tức là chúng ta sẽ lấy hết tất cả cái ô này để vẽ lên
0:10:03 - 0:10:10, Rồi, chúng ta thấy bản chất nó chính là cái ô này đem xuống thôi
0:10:10 - 0:10:17, Thì ở đây chúng ta đang trực quan hóa cái dòng số 3 lên
0:10:17 - 0:10:19, Và đây là cái logic
0:10:19 - 0:10:25, Sau đó thì chúng ta sẽ viết cái hàm để mà tạo cái label
0:10:25 - 0:10:29, Thì ở đây chúng ta sẽ tạo một cái biến đó là label
0:10:29 - 0:10:31, One Hot
0:10:31 - 0:10:36, Thì nó sẽ là bằng F.One Hot
0:10:38 - 0:10:41, Rồi, truyền cái label vào
0:10:41 - 0:10:44, Và số class của mình nó sẽ là bằng n
0:10:44 - 0:10:49, Và để cho cái mô hình này có thể huấn luyện hiệu quả được
0:10:49 - 0:10:52, Thì chúng ta sẽ có thể là đưa vào GPU
0:10:52 - 0:10:55, Tuy nhiên thì ở đây chúng ta thấy là cái kích thước ma trận quá nhỏ
0:10:55 - 0:11:00, Chúng ta không cần phải truyền vào GPU mà chúng ta có thể dùng trực tiếp CPU để có thể tính toán được
0:11:00 - 0:11:04, Nên ở đây thì chúng ta chỉ cần gọi cái hàm như thế này là được
0:11:04 - 0:11:09, Rồi sau đó thì chúng ta sẽ trực quan hóa cái ma trận này
0:11:09 - 0:11:17, Thì chúng ta thấy là nếu đúng thì cái ma trận ở trên là sẽ phải đưa về đúng với lại cái ma trận đơn vị như thế này
0:11:17 - 0:11:24, Rồi, bây giờ chúng ta sẽ trực quan cái hàng số 3 của cái ma trận đơn vị này lên
0:11:24 - 0:11:27, Bằng cách đó là chúng ta sẽ gọi cái hàm imshow
0:11:27 - 0:11:30, Chúng ta copy cái code phía trên xuống
0:11:30 - 0:11:34, plt.imshow
0:11:35 - 0:11:40, Chúng ta sẽ imshow cái label one-hot
0:11:47 - 0:11:50, Rồi, và cũng lấy cái dòng số 3
0:11:51 - 0:11:53, Chúng ta sẽ lấy dòng số 3 đúng không?
0:11:53 - 0:11:55, Rồi, lấy dòng số 3, 2 chấm
0:11:55 - 0:11:58, Rồi, nâng
0:12:01 - 0:12:12, Rồi, thì ở đây chúng ta thấy là lẽ ra chúng ta phải show cái logit và cái one hot
0:12:12 - 0:12:15, Do đó thì chúng ta sẽ show cái logit trước
0:12:19 - 0:12:24, Cái ví dụ hồi nãy thì chúng ta nhập sai lỗi chúng ta
0:12:24 - 0:12:30, Rồi, thì nó đã lấy cái dòng thứ 3 đem xuống đây
0:12:30 - 0:12:38, Sau đó chúng ta sẽ show cái one hot để cho chắc chúng ta sẽ copy xuống
0:12:38 - 0:12:44, Rồi, ở đây chúng ta sẽ phải tạo một cái figure mới
0:12:54 - 0:13:05, Rồi, thì đây chính là cái dữ liệu của cái logit, tức là cái mà chúng ta nhân tích vô hướng giữa hai cái embedding
0:13:05 - 0:13:10, Còn cái bên hàng dưới cùng sẽ là cái ground truth mà lẽ ra chúng ta hướng về
0:13:10 - 0:13:17, Vì là cái ti, tức là cái embedding của ảnh và embedding của văn bản bắt đầu được lấy giống nhau, chỉ là cộng thêm nhiễu
0:13:17 - 0:13:20, Nên chúng ta thấy là tại vị trí này nó đã bật lên là 1 rồi
0:13:20 - 0:13:28, Nếu đúng thì giá trị bắt đầu với môn khởi tạo của mình sẽ lộn xộn chứ không phải là phát sáng như thế này
0:13:28 - 0:13:36, Vì đây chúng ta đang mô phỏng, cái việc mà contrastive learning nên chúng ta sẽ dùng cái dữ liệu dạng random như vậy
0:13:36 - 0:13:40, Rồi, thì cái hàm mất mát ở đây chúng ta tiếp theo sẽ sử dụng chính là
0:13:40 - 0:13:44, Sử dụng cái hàm cross entropy giữa hai cái vector này
0:13:45 - 0:13:58, Và khi đó thì chúng ta sẽ tính cái T3, tức là cái text embedding với lại y0, T3 với lại y1, T3 với y2 v.v.
0:13:58 - 0:14:03, Thì chúng ta sẽ tính theo hàng rồi sau đó chúng ta sẽ tính theo cột
0:14:03 - 0:14:07, Thì đầu tiên là chúng ta sẽ lấy cột trước thay vì hàng
0:14:07 - 0:14:11, Tóm lại đó là chúng ta sẽ tính theo từng hàng và từng cột
0:14:12 - 0:14:18, Trong cái sơ đồ này, với cái dòng số 3 là chúng ta sẽ tính theo hàng
0:14:18 - 0:14:25, Tức là với ảnh số 3 chúng ta sẽ đi so với tất cả các cái văn bản để xem coi là cái sai số của mình là bao nhiêu
0:14:25 - 0:14:34, Và đối xứng lại thì chúng ta cũng sẽ có cái khía cạnh là cột số 3 là cái văn bản đúng ra phải trả về
0:14:34 - 0:14:39, Thì nó sẽ đi so với lại những cái ảnh khác
0:14:39 - 0:14:43, Thì chúng ta sẽ tính cái loss theo tổng hàng và cột
0:14:43 - 0:14:50, Thế thì nếu mà chúng ta tính theo từng hàng và cột như vậy thì chúng ta sẽ phải viết một cái vòng for
0:14:50 - 0:14:55, Nhưng mà để có thể thực hiện trọn vẹn thì chúng ta cũng tính rất là dễ
0:14:55 - 0:15:01, Đó là chúng ta chỉ việc lấy lossA nè là bằng chính cái logic.
0:15:01 - 0:15:05, Chúng ta sẽ nhân với lại cái...
0:15:05 - 0:15:07, Chúng ta sẽ lấy để...
0:15:07 - 0:15:13, Xin lỗi ở đây chúng ta không phải là nhân vô hướng và chúng ta sẽ dùng cái hàm cross entropy
0:15:13 - 0:15:21, Chúng ta sẽ dùng cái hàm cross entropy để mà tính như vậy thì f.cross entropy
0:15:21 - 0:15:30, Rồi chúng ta sẽ truyền vào cái logic và cái one-hot vector tức là cái label của mình
0:15:30 - 0:15:40, Thì ở trong trường hợp này cái label của mình chính là cái nhãn mà chúng ta đã setup ở phía trên
0:15:40 - 0:15:43, Đây, cái label này
0:15:43 - 0:15:54, Thì được gán từ một, tức là ảnh thứ nhất, nhãn là một, ảnh thứ hai, nhãn là hai, và cái cặp ảnh thứ n, nhãn là n
0:15:54 - 0:15:58, Rồi, chúng ta sẽ show...
0:15:58 - 0:16:03, Chúng ta sẽ đi tính cái loss này bằng cách lấy logic nhân với lại cái label
0:16:03 - 0:16:11, Rồi, bây giờ chúng ta sẽ tính cái hàm loss này là logic và label
0:16:11 - 0:16:16, Rồi, sau đó chúng ta sẽ đi print nó ra
0:16:16 - 0:16:23, Rồi, chúng ta sẽ tính tương tự như vậy cho cái loss của t, tức là theo text
0:16:23 - 0:16:25, Với text tức là gì?
0:16:28 - 0:16:34, Chúng ta sẽ đi cố định cái text, ví dụ text là t3 và chúng ta sẽ cho y chạy từ trên xuống
0:16:34 - 0:16:46, Thế thì bản chất cái cách tính của t3 với trên theo hàng y3 và t3 thì chúng ta chỉ cần lật cái ma trận lại là xong
0:16:47 - 0:16:50, Rồi, sau đó ở đây chúng ta sẽ sửa lại cái code
0:16:53 - 0:16:58, Đó là chấm, chúng ta thêm một cái thành phần chuyển vị vào đây ha
0:16:58 - 0:17:02, Thì chúng ta sẽ lấy logic này, chuyển vị, cross entropy
0:17:02 - 0:17:09, Và loss tổng hợp thì sẽ là bằng trung bình cộng của hai loss này
0:17:09 - 0:17:15, Đó là bằng loss của y cộng cho loss của t
0:17:17 - 0:17:20, Rồi, thì bây giờ chúng ta sẽ lần lượt chạy cái code
0:17:20 - 0:17:25, Đây là theo cột ha, đây là trực quan hóa theo cột
0:17:25 - 0:17:31, Chúng ta lấy ra, thì nếu đúng cái cột này, nó sẽ phải hướng về cái vector này
0:17:31 - 0:17:34, Và nó sẽ bật sáng lên tại cái hàng thứ 3
0:17:34 - 0:17:37, Rồi, bây giờ chúng ta sẽ tính thử
0:17:37 - 0:17:42, Thì cái loss của mình đó là loss theo trực y, nó ra là 2,410
0:17:42 - 0:17:47, Và theo cái trực t, rất là văn bản, thì là 2,2406
0:17:47 - 0:17:50, Và trung bình cộng là 248