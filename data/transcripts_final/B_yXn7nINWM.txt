0:00:14 - 0:00:19, Chúng ta sẽ cùng đến với một cái biến thể tiếp theo, đó chính là Adam, Adaptive Moment Optimization.
0:00:19 - 0:00:28, Trong phần trước, chúng ta đã cùng tìm hiểu về biến thể Root Mean Square Propagation trên kỹ momentum
0:00:28 - 0:00:43, Và công thức cập nhật của momentum của mình sẽ là bằng alpha chia cho căn của epsilon cộng cho r, tất cả nhân cho g
0:00:43 - 0:00:49, G này sẽ được tính là bằng đạo hàm của hàm loss theo theta
0:00:49 - 0:00:54, Rõ ràng chúng ta thấy nó chưa có sử dụng momentum
0:00:58 - 0:01:06, Chúng ta chỉ mới sử dụng momentum trong công thức của r này mà thôi
0:01:06 - 0:01:08, Cụ thể là như thế nào?
0:01:08 - 0:01:14, Trong công thức này, r sẽ bằng beta nhân với r
0:01:14 - 0:01:20, R trong đó beta là một hệ số để kết hợp của r quá khứ
0:01:20 - 0:01:23, Cụ thể đây beta là bằng 0.9
0:01:23 - 0:01:29, R là bằng 0.9r, tức là quá khứ của mình là 90% quá khứ của mình
0:01:29 - 0:01:35, Cộng cho 1 trừ beta tất cả nhân cho g
0:01:35 - 0:01:38, G bình phương, g nhân g
0:01:38 - 0:01:52, Trong công thức này chúng ta thấy với beta mà bằng 0.9, tức là chúng ta lấy 90% thông tin của quá khứ kết hợp với 1 trừ beta tức là 10% thông tin của quá khứ
0:01:52 - 0:01:58, Thì thành phần này quá ít dẫn đến là tại những vòng lặp đầu tiên
0:01:58 - 0:02:02, Ví dụ đây là hàm loss của mình
0:02:02 - 0:02:11, Tại những vòng lặp đầu tiên, r sẽ là bằng 90% của số 0
0:02:11 - 0:02:14, Do đó thành phần này sẽ là bằng 0
0:02:14 - 0:02:21, Cộng cho 10% của g nhân g
0:02:21 - 0:02:29, Thế thì thành phần này quá bé tại những vòng lặp đầu tiên
0:02:29 - 0:02:38, Mặc dù thế năng của nó tại vị trí này chúng ta thấy rất là tốt, ở vị trí rất là cao do đó đạo hàm của mình rất dốc
0:02:38 - 0:02:41, Nhưng nó chỉ được lấy có 10% thôi
0:02:41 - 0:02:47, Như vậy thì ở những bước đầu tiên của Root Mean Square Propagation, nó sẽ đi rất chậm
0:02:47 - 0:02:52, Thì Adam Adaptive Moment Optimization đã cải tiến ở 2 chỗ
0:02:52 - 0:02:56, 1 là sẽ đưa momentum vào trong thành phần này
0:02:56 - 0:03:01, 2 là r thì chúng ta cũng sẽ chuẩn hóa
0:03:01 - 0:03:08, Chúng ta sẽ chuẩn hóa để tại những vị trí đầu tiên nó sẽ không bị bias
0:03:08 - 0:03:13, Nó sẽ không bị phụ thuộc vào r khởi tạo ban đầu là một con số khá là bé
0:03:13 - 0:03:18, Và cái thuật toán Adaptive Moment Optimization Adam
0:03:18 - 0:03:24, Thì thường là ổn định với siêu tham số hyperparameter mặc định nghĩa là sao
0:03:24 - 0:03:29, Nếu như các phương pháp trước thì siêu tham số chúng ta phải tune khá là nhiều để mà có thể hiệu quả
0:03:29 - 0:03:33, Thì Adam với siêu tham số mặc định của mình
0:03:33 - 0:03:40, Chúng ta cũng có thể chạy ra được một cái thuật toán mà nó ổn định
0:03:40 - 0:03:45, Và đó chúng ta không cần phải chú ý để mà tune quá nhiều với cái siêu tham số này
0:03:45 - 0:03:50, Thì chi tiết cái thuật toán Adam nó như thế nào thì chúng ta sẽ tìm hiểu trong những slide tiếp theo
0:03:50 - 0:03:56, Và chi tiết của cái thuật toán Adam Adaptive Moment Optimization là nằm ở đây
0:03:56 - 0:04:00, Chúng ta đầu tiên cũng sẽ khởi tạo là alpha là bằng 0.1
0:04:00 - 0:04:06, Đối với cái Decay Rate thì bình thường trong cái Root Mean Square Propagation chúng ta chỉ có duy nhất một cái beta
0:04:06 - 0:04:11, Thì ở đây chúng ta sẽ có hai cái beta là beta 1 và beta 2
0:04:11 - 0:04:22, Trong đó beta 1 là cái hệ số momentum là cái Decay Rate cho cái momentum của gradient
0:04:22 - 0:04:26, Cho cái việc cập nhật cái momentum của gradient
0:04:26 - 0:04:34, Còn cái beta 2 sẽ là cho cái việc cập nhật cái hệ số chuẩn hóa, cái thành phần chuẩn hóa
0:04:34 - 0:04:42, Chuẩn hóa cái Learning Rate
0:04:42 - 0:04:54, Và chúng ta sẽ yr thì chúng ta sẽ có thêm s, s chính là cái thành phần momentum cho gradient
0:04:54 - 0:04:59, Thành phần momentum cho gradient
0:04:59 - 0:05:03, Thì đây chính là cái momentum cho cái beta gradient của mình
0:05:03 - 0:05:11, Và công thức của mình là bình thường là trong cái phần Root Mean Square Propagation thì s của mình chính là chỉ bằng g thôi
0:05:11 - 0:05:15, Còn bây giờ s của mình nó sẽ là bằng cái thành phần quá khứ
0:05:15 - 0:05:21, Nhân với lại, cộng với lại cái thành phần gradient hiện tại
0:05:21 - 0:05:25, Đây là hiện tại
0:05:25 - 0:05:27, Còn đây là cái thành phần quá khứ
0:05:31 - 0:05:36, Và nó sẽ là bằng 90% của quá khứ cộng cho 10% của hiện tại
0:05:36 - 0:05:41, Và như hồi nãy chúng ta đã lập luận thì cái việc mà lấy quá nhiều cho cái quá khứ
00:05:41 - 0:05:44, Nó sẽ khiến cho những cái bước cập nhật đầu tiên rất là chậm
00:05:44 - 0:05:48, Thì chúng ta sẽ có cái thành phần chuẩn hóa ở phía sau, chúng ta sẽ giải thích sau
00:05:48 - 0:05:54, Đây, s mũ, đó chính là cái thành phần chuẩn hóa cho cái s ở phía trên
00:05:54 - 0:06:00, Thế thì tại sao cái việc chuẩn hóa với cái công thức này thì những cái vòng lặp đầu tiên của mình
00:06:00 - 0:06:03, Nó sẽ có cái giá trị không quá bé
00:06:03 - 0:06:09, Thì bây giờ chúng ta giả sử, s ban đầu của mình là một cái con số rất là bé
00:06:09 - 0:06:15, Như đã giải thích, s sẽ là bằng quá khứ là bằng 90% của cái s ban đầu là bằng 0
00:06:15 - 0:06:17, Tức là cái thành phần này là bằng 0
00:06:17 - 0:06:21, Tức là ở những cái vòng lặp đầu tiên là ban đầu
00:06:21 - 0:06:24, Thì s sẽ là bằng quá khứ là bằng 0
00:06:24 - 0:06:31, Cộng cho 10% của g thì cái thành phần này rất là bé
00:06:31 - 0:06:40, Nhưng khi chúng ta chia cho căn của 1 trừ beta mũ t với t là số thứ tự t là cái bước lặp của mình
00:06:40 - 0:06:43, Thì ở cái vòng lặp đầu tiên, tức là t bằng 1
00:06:43 - 0:06:45, Vòng lặp đầu tiên t bằng 1
00:06:45 - 0:06:53, Thì khi đó s sẽ là bằng 1 trừ cho beta mũ của mình là 0.9 mũ 1
00:06:53 - 0:06:57, 0.9 mũ 1 tức là 0.9
00:06:57 - 0:07:00, Thì 1-0.9 tức là 0.1
00:07:00 - 0:07:05, Thì s mà chia cho 0.1 tương đương với s chúng ta sẽ nhân lên 10 lần
0:07:05 - 0:07:08, Thì ban đầu s của mình rất là thấp
00:07:08 - 0:07:11, Nó chỉ bằng khoảng 0.9 cái đạo hàm gốc của mình thôi
00:07:11 - 0:07:13, Nhưng mà chúng ta chia cho 0.1 tức là nhân 10 lên
00:07:13 - 0:07:19, Thì có phải là s của mình tương đương với đạo hàm tại thời điểm ban đầu
00:07:19 - 0:07:23, Thì nó đã được boost lên 10 lần
0:07:23 - 0:07:29, Thì công thức này sẽ giúp chúng ta boost tại những thời điểm đầu tiên
0:07:29 - 0:07:34, Thế thì khi t mà càng lớn, đương nhiên không thể nào mà t tiến đến vô cùng được
00:07:34 - 0:07:40, T chỉ là khoảng 10-20, ví dụ vậy, cỡ 10 cho đến 20 đi
00:07:40 - 0:07:45, Thì khi đó là beta 1 mũ t
00:07:45 - 0:07:49, Beta 1 là một con số bé hơn 1, lớn hơn 0
00:07:49 - 0:07:52, Nên nó sẽ tiến đến, nó sẽ tiến về 0
00:07:52 - 0:07:58, Do đó 1 trừ beta 1 mũ t, nó sẽ tiến về 1
0:07:58 - 0:08:00, 1 trừ 0 tức là bằng 1
00:08:00 - 0:08:05, Tức là khi đó s mũ của chúng ta sẽ xấp xỉ bằng s chia cho 1
0:08:05 - 0:08:08, Tức là nó sẽ bằng nguyên bản của momentum ban đầu của mình
00:08:08 - 0:08:12, Thì khi t mà càng lớn thì gần như không cần boost lên nữa
0:08:12 - 0:08:16, Còn khi t của mình nhỏ, khoảng 1-2 thì nó sẽ boost lên rất là nhiều lần
0:08:16 - 0:08:20, Đó là ý nghĩa của công thức chuẩn hóa này
0:08:20 - 0:08:29, Tương tự như vậy, cho thành phần để cập nhật chuẩn hóa của learning rate
0:08:29 - 0:08:34, Chúng ta cũng sẽ dùng công thức này để giúp cho việc mà tại những thời điểm đầu tiên
00:08:34 - 0:08:39, Nó không quá bé, nó sẽ xấp xỉ bằng với lại đạo hàm của mình luôn
0:08:39 - 0:08:46, Và ý nghĩa của công thức này, tương tự như trong root mean square propagation
0:08:46 - 0:08:51, Mục tiêu của nó là để tách ra, tìm ra là 1 cái vector
00:08:51 - 0:08:57, Nên nó sẽ tách ra thành những learning rate riêng khi chúng ta cập nhật vô thành phần đạo hàm
0:08:57 - 0:09:02, Và nó làm theo nguyên tắc, thành phần đạo hàm nào ở bên đây
00:09:02 - 0:09:08, Của g mà càng nhỏ thì learning rate sẽ càng lớn
00:09:08 - 0:09:13, Thành phần của g này lớn thì learning rate sẽ nhỏ
00:09:13 - 0:09:19, Như vậy là Adam đã có những cải tiến chính
0:09:19 - 0:09:22, Đó là nó có thêm momentum cho vector gradient
0:09:22 - 0:09:28, Nó có thêm thành phần chuẩn hóa để những vòng lặp đầu tiên không quá nhỏ
0:09:28 - 0:09:34, Thành phần đạo hàm của mình không quá bé, hoặc là cái phần chuẩn hóa đạo hàm cũng không quá bé
0:09:34 - 0:09:39, Nó bị sai lệch so với lại đạo hàm tại thời điểm đó
0:09:41 - 0:09:48, Trong sơ đồ này thì chúng ta sẽ có trực quan hóa để cho thấy tốc độ hội tụ của từng thuật toán
0:09:48 - 0:09:53, Ở đây Adam là đường màu vàng của mình
0:09:53 - 0:09:58, Đường màu vàng này
0:09:58 - 0:10:04, Đường màu vàng sẽ rớt xuống rất nhanh, hội tụ rất nhanh
0:10:04 - 0:10:11, Khi đến khu vực saddle point và valley thung lũng
0:10:11 - 0:10:19, Thung lũng có 2 cái thành, giảm từ dốc, đi ngang, xong rồi lại đi lên, đó gọi là valley
0:10:19 - 0:10:24, Adam rớt xuống nhanh nhất, rất nhanh
0:10:24 - 0:10:29, Còn thuật toán root mean square propagation là đường màu đen
0:10:29 - 0:10:34, Chúng ta thấy nó sẽ rớt chậm hơn
0:10:34 - 0:10:40, Còn stochastic gradient descent, đối với stochastic gradient descent là cái chấm màu đỏ
0:10:40 - 0:10:44, Chúng ta thấy nó bị dao động qua lại và nó đứng yên luôn
0:10:44 - 0:10:46, Nó không thoát ra được chỗ này luôn
0:10:46 - 0:10:51, Momentum thì khá hơn một chút xíu là cái điểm màu xanh lá
0:10:51 - 0:10:55, Chúng ta thấy là khi momentum rớt xuống nó cũng sẽ dao động qua lại
0:10:55 - 0:11:00, Nhưng mà vì có một số kiểu tối nhiễu nên nó sẽ dần dần thoát ra được
0:11:00 - 0:11:02, Và nó đến được cái rảnh này nó di chuyển
0:11:02 - 0:11:09, Trong khi các phương pháp cải tiến khác thì nó cũng bị hiện tượng dao động qua lại rất nhiều
0:11:09 - 0:11:11, Nó bị hiện tượng dao động qua lại
0:11:11 - 0:11:13, Bật qua bật lại
0:11:13 - 0:11:17, Còn Adam là cái đường màu vàng thì nó sẽ rớt thẳng xuống luôn
0:11:17 - 0:11:20, Nó sẽ đi theo cái đường cập nhật hoàn hảo
0:11:20 - 0:11:22, Cái đường cập nhật mà tối ưu ở đây
0:11:22 - 0:11:29, Bên phải thì đó là cái sơ đồ về giá trị của hàm loss khi chúng ta sử dụng các thuật toán khác nhau
0:11:29 - 0:11:31, Thì Adam là cái đường màu tím
0:11:31 - 0:11:35, Nó cho cái giá trị hàm loss hội tụ nhanh hơn và nó thấp nhất
0:11:35 - 0:11:37, Loss càng thấp càng tốt
0:11:37 - 0:11:40, Thì cái training loss của mình càng thấp càng tốt
0:11:40 - 0:11:44, Thì chúng ta thấy là nó hội tụ nhanh hơn nhiều so với lại các thuật toán
0:11:44 - 0:11:52, Như là root mean square, AdaDelta, AdaGrad, v.v.
0:11:54 - 0:11:59, Thì kết luận đó là một số phương pháp tối ưu mô hình học sâu bằng cách
0:11:59 - 0:12:05, Trong cái phần này chúng ta đã được thảo luận qua những cách để tùy chỉnh learning rate
0:12:05 - 0:12:07, Cho từng cái tham số
0:12:07 - 0:12:11, Và thuật toán, câu hỏi là thuật toán nào sẽ được chọn khi huấn luyện
0:12:11 - 0:12:13, Thì câu trả lời đó là không chắc chắn
0:12:13 - 0:12:16, Nó sẽ tùy thuộc vào cái dữ liệu của các bạn như thế nào
0:12:16 - 0:12:19, Nó phụ thuộc vào cái mô hình của mình nó có phức tạp hay không
0:12:19 - 0:12:23, Ví dụ đối với những cái mô hình phức tạp mà nhiều tham số
0:12:23 - 0:12:29, Thì khi đó chúng ta sẽ phải dùng các cái thuật toán ví dụ như là
0:12:29 - 0:12:34, Root Mean Square, Prop, Propagation hoặc là Adam
0:12:34 - 0:12:37, Nhưng đối với những cái mô hình mà ít tham số
0:12:37 - 0:12:40, Thì khi đó Adam và Root Mean Square là không cần thiết
0:12:40 - 0:12:44, Mà chúng ta chỉ cần Stochastic Gradient Descent là đủ
0:12:44 - 0:12:47, Rồi nếu mà dữ liệu của mình không quá phức tạp
0:12:47 - 0:12:50, Thì chúng ta có thể dùng Stochastic Gradient Descent
0:12:50 - 0:12:54, Nhưng nếu mà phức tạp thì chúng ta sẽ dùng 2 cái thuật toán bên đây
0:12:54 - 0:12:59, Thì đa số các cái thuật toán đều có cái sự phổ biến nhất định của mình
0:12:59 - 0:13:03, Và được lựa chọn tùy theo cái sự quen thuộc của người dùng
0:13:03 - 0:13:09, Như vậy thì đến đây chúng ta đã tìm hiểu qua 2 cái biến thể rất là nổi tiếng của Adaptive Learning Rate
0:13:09 - 0:13:12, Đó là Root Mean Square, Propagation và Adam
0:13:12 - 0:13:19, Thì cái Root Mean Square, Propagation là một cái tiền đề để cho Adam có thể cải tiến
0:13:19 - 0:13:22, Và Adam nó có một cái cải tiến khá là quan trọng
0:13:22 - 0:13:26, Đó là chuẩn hóa để giúp cho những cái bước cập nhật đầu tiên của mình nó không quá chậm
0:13:26 - 0:13:32, Và đây chính là những cái thuật toán Optimizer được sử dụng trong rất nhiều những cái mô hình học sâu
0:13:32 - 0:13:35, Những cái mô hình mà dựa trên Gradient về sau
0:13:35 - 0:13:38, Và nó sẽ là tiền đề cho chúng ta đi tiếp