0:00:15 - 0:00:25, Trước khi đến với những mô hình học sâu, thì chúng ta sẽ có một trong những nền tảng đầu tiên của Deep Learning, đó chính là mạng Neural Network.
0:00:26 - 0:00:30, Đây có thể nói là một trong những mạng học sâu đời đầu.
0:00:30 - 0:00:41, Và ý tưởng của mạng Neural Network là một biến thể cho rất nhiều kiến trúc về sau, cụ thể là CNN, RNN và thậm chí là Transformer.
0:00:45 - 0:00:47, Bài báo là Attention Ion Unit
0:00:54 - 0:00:57, Nhưng thực tế thì trong kiến trúc này
0:00:57 - 0:01:01, vai trò của mạng Neural Network
0:01:01 - 0:01:03, hay là thành phần MLP
0:01:03 - 0:01:05, trong bài báo đó họ dùng là MLP
0:01:05 - 0:01:07, thì vai trò này cực kỳ quan trọng
0:01:07 - 0:01:10, trong việc phi tuyến hóa bài toán của mình
0:01:10 - 0:01:13, để giúp giải quyết được bài toán phức tạp
0:01:15 - 0:01:22, Nó là một mô hình bước đầu để đánh dấu vào việc giải quyết các bài toán phi tuyến tính.
0:01:23 - 0:01:28, Trong các mô hình như CNN và ANN, chúng ta cũng có thể thấy có chữ NN ở đây.
0:01:28 - 0:01:30, Nó chính là chữ Neural Network.
0:01:30 - 0:01:35, Vậy thì chúng ta có thể thấy là vai trò của mạng Neural Network rất là quan trọng.
0:01:35 - 0:01:41, Sự khác biệt của mạng Neural Network so với những kiến trúc mạng thuộc nhóm tuyến tính
0:01:46 - 0:01:54, Nếu chúng ta bỏ 2 lớp ẩn này đi và bỏ dấu 3 chấm này đi thì nó chính là Softmax Regression
0:01:56 - 0:01:59, Đây chính là một mô hình hồi quy Softmax
0:02:03 - 0:02:09, Nhưng khi chúng ta chèn thêm các lớp ẩn, ở đây nó gọi là lớp ẩn
0:02:09 - 0:02:11, Hoặc thuật ngữ tiếng Anh gọi là hidden layer
0:02:15 - 0:02:17, khi chúng ta thêm các hidden layer này vào
0:02:18 - 0:02:22, thì nó sẽ giúp chúng ta giải quyết được các bài toán phi tuyến tính
0:02:24 - 0:02:29, và chúng ta cũng có một quy tắc đó là ngay sau lớp biến đổi tuyến tính là sigma
0:02:29 - 0:02:32, thì chúng ta phải có một cái hàm kích hoạt
0:02:32 - 0:02:35, hàm này bắt buộc phải là một cái hàm phi tuyến
0:02:35 - 0:02:39, tại vì nếu không thì hai lớp biến đổi tuyến tính liên tiếp
0:02:39 - 0:02:42, thì nó sẽ tạo ra thành một tổ hợp tuyến tính
0:02:45 - 0:02:51, các bài toán phi tuyến do đó chúng ta phải có những thành phần phi tuyến tính như thế này
0:02:52 - 0:02:59, Và tham số của mô hình của chúng ta sẽ được đánh số từ theta1, theta2, cho đến thetaL
0:02:59 - 0:03:02, tức là ở đây chúng ta có L layer
0:03:03 - 0:03:08, Và ở cái module cuối cùng đó là hàm Loss Function, hàm độ lỗi
0:03:08 - 0:03:11, thì chúng ta cũng sẽ sử dụng công thức Cross entropy
0:03:15 - 0:03:17, chúng ta không thay đổi cái chỗ này
0:03:18 - 0:03:20, và ở đây thì chúng ta sẽ minh họa xem
0:03:21 - 0:03:27, ý nghĩa hình học của mạng Neural Network như thế nào
0:03:27 - 0:03:32, thì ở bên trái chúng ta có 1 cái ví dụ là 2 tập điểm hình tròn và tam giác
0:03:32 - 0:03:34, là có mối quan hệ phi tuyến tính
0:03:34 - 0:03:41, tại vì chúng ta không thể nào dùng 1 cái đường thẳng để chia 2 tập điểm này ra làm 2 phần
0:03:45 - 0:03:49, và dùng đường cong như thế này, thì mới có thể chia được.
0:03:49 - 0:03:52, Vậy thì làm sao mạng Neural Network này
0:03:52 - 0:03:58, có thể phân chia 2 tập tròn và tam giác này ra làm 2 phần?
0:03:59 - 0:04:02, Thì chúng ta đã biết trong mạng Logistic Regression
0:04:02 - 0:04:09, thì mỗi một cái node này tương ứng là một đường phân lớp tuyến tính.
0:04:10 - 0:04:13, Và cụ thể là các trọng số mà nối đến Neural Network này
0:04:15 - 0:04:18, pham tham số của trường thẳng của mình. Như vậy, cái Neural đầu tiên
0:04:18 - 0:04:22, nó sẽ giúp cho chúng ta tạo ra được 1 cái
0:04:22 - 0:04:24, đường phân lớp, tách ra làm 2 phần
0:04:24 - 0:04:28, và đây là 1 cái bộ phân loại yếu. Nhưng nhiều cái bộ phân loại yếu
0:04:28 - 0:04:31, ráp lại với nhau. Chúng ta có nhiều cái bộ phân loại yếu
0:04:32 - 0:04:35, và ở những cái nodes tiếp theo, nó đã tổng hợp
0:04:35 - 0:04:37, có trọng số
0:04:37 - 0:04:41, các cái nodes ở đằng trước. Như vậy là nó đang tạo ra
0:04:41 - 0:04:42, các cái đặc trưng cấp cao
0:04:45 - 0:04:49, các đặc trưng cấp cao tổng hợp từ những đặc trưng trước đó
0:04:50 - 0:04:53, Mỗi đặc trưng ở phía trước sẽ là đường thẳng như thế này
0:04:53 - 0:04:57, Và khi chúng ta tính tổng trọng số lại
0:04:57 - 0:05:02, thì nó đã giúp chúng ta dần dần phân tách tập hình tròn
0:05:02 - 0:05:05, và hình tam giác ra làm hai phần tách biệt như thế này
0:05:05 - 0:05:10, Đó chính là ý nghĩa hình học của mạng Neural Network
0:05:10 - 0:05:14, Các layer đầu tiên sẽ tạo ra các đặc trưng cấp thấp
0:05:15 - 0:05:18, phân tách ra thành 2 phần như thế này
0:05:18 - 0:05:28, nhưng ở những đặc trưng phía sau sẽ tạo ra những đặc trưng cấp cao hơn, phi tuyến tính hơn và phức tạp hơn để giải quyết các bài toán phức tạp của chúng ta
0:05:28 - 0:05:33, Đó chính là sơ lược về kiến trúc mạng Neural Network
0:05:45 - 0:05:47, Hãy subscribe cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video hấp dẫn