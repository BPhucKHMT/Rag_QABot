0:00:14 - 0:00:20, Chúng ta sẽ cùng đến với vấn đề thứ 2 khi huấn luyện với một mô hình học sâu đó chính là Vanishing-Gradient.
0:00:20 - 0:00:28, Nếu như cái vấn đề về overfitting hay là quá khớp với dữ liệu thì nó rất tổng quát cho các mô hình máy học.
0:00:28 - 0:00:32, Mọi mô hình máy học đều sẽ gặp những vấn đề về overfitting.
0:00:32 - 0:00:38, Còn vấn đề về Vanishing-Gradient thì thường chỉ dành cho các mô hình học sâu.
0:00:38 - 0:00:44, Vậy thì cơ chế của cái hiện tượng Vanishing-Gradient là gì và làm sao để khắc phục được vấn đề này?
0:00:44 - 0:00:47, Chúng ta sẽ cùng đến trong những phần tiếp theo.
0:00:48 - 0:00:54, Đầu tiên, như đã nói đây là một mô hình chỉ dành cho các deep learning model.
0:00:54 - 0:01:00, Khi chúng ta làm với các deep model thì chúng ta mới cần xem xét đến vấn đề này.
0:01:00 - 0:01:10, Và hiện tượng thể hiện của nó đó chính là mô hình của mình sẽ hội tụ chậm hoặc là thậm chí không hội tụ sau vài vòng lặp.
0:01:11 - 0:01:15, Tức là có thể ở những vòng lặp đầu tiên thì nó sẽ hội tụ rất tốt.
0:01:15 - 0:01:22, Nhưng mà sau vài vòng lặp thì nó sẽ không còn hội tụ được nữa hoặc là tốc độ hội tụ của mình rất chậm.
0:01:23 - 0:01:30, Vậy thì bây giờ chúng ta sẽ cùng lý giải xem cái bản chất bên trong của hiện tượng Vanishing-Gradient là gì.
0:01:30 - 0:01:35, Và chúng ta sẽ sử dụng các mô hình toán học để giải thích cho nó.
0:01:35 - 0:01:42, Đầu tiên chúng ta giả sử hàm lỗi của mô hình đó chính là hàm J theta.
0:01:43 - 0:01:51, Và như chúng ta biết là các mô hình học sâu thì nó sẽ rất phức tạp và có nhiều layer biến đổi liên tiếp nhau.
0:01:51 - 0:01:55, Do đó thì cái hàm lỗi của mình nó sẽ là một cái dạng hàm hợp,
0:01:55 - 0:01:59, nó là một cái hàm hợp gồm nhiều hàm thành phần.
0:02:01 - 0:02:11, Ví dụ như ở đây chúng ta có hàm J theta là bằng Fn trừ 1 cho đến F2, F1 và theta.
0:02:11 - 0:02:17, Và gradient theo tham số của mình nó sẽ được tính theo cái công thức này.
0:02:17 - 0:02:26, Lưu ý là cái theta của mình nó sẽ là vector hoặc là một tập hợp các cái tham số thành phần.
0:02:26 - 0:02:30, Ví dụ như chúng ta xét trên cái tham số thành phần là theta i.
0:02:32 - 0:02:40, Thì khi đó đạo hàm của J theo theta i nó sẽ là bằng đạo hàm của Fn theo Fn trừ 1,
0:02:40 - 0:02:51, nhân với đạo hàm của Fn trừ 1 theo Fn trừ 2, nhân cho đạo hàm F2 theo F1 và nhân đạo hàm của F1 theo theta i.
0:02:53 - 0:03:03, Thì đây là một cái dạng quy tắc gọi là nối tiếp, quy tắc chuỗi trong cái việc là khai triển đạo hàm hay gọi là chain rule.
0:03:03 - 0:03:16, Rồi, thế thì cái hiện tượng vanishing gradient sẽ được giải thích dựa trên cái công thức chain rule này.
0:03:16 - 0:03:24, Đó là trong cái mô hình học sâu thì chúng ta mặc dù là nó có rất nhiều những cái layer biến đổi,
0:03:24 - 0:03:31, nhưng mà đâu đó những cái hàm ở đây sẽ là những cái hàm được thực hiện lặp đi lặp lại nhiều lần.
0:03:31 - 0:03:34, Thì đó là có những cái hàm xuất hiện nhiều lần.
0:03:34 - 0:03:42, Ví dụ trong mô hình học sâu thì activation function hàm kích hoạt là một trong những hàm mà xuất hiện nhiều lần.
0:03:42 - 0:03:55, Tại vì chúng ta có một cái nguyên tắc đó là ngay sau cái phép biến đổi tuyến tính là một cái hàm kích hoạt.
0:03:55 - 0:04:00, Và đây là hàm phi tuyến.
0:04:00 - 0:04:07, Thế thì tại sao chúng ta lại phải có một cái hàm kích hoạt?
0:04:07 - 0:04:12, Tại vì nếu không có hàm kích hoạt thì chúng ta chỉ có phi tuyến tính.
0:04:12 - 0:04:18, Thế thì sau cái phép biến đổi tuyến tính mà là một phép biến đổi tuyến tính thì bản chất đó là một tổ hợp tuyến tính.
0:04:18 - 0:04:22, Do đó thì nó cũng chỉ có thể giải quyết được các cái bài toán đơn giản.
0:04:22 - 0:04:30, Khi chúng ta chèn vào một cái hàm phi tuyến ở giữa thì nó sẽ giúp chúng ta giải quyết được các cái bài toán phức tạp hơn.
0:04:30 - 0:04:38, Và giá trị đạo hàm, lưu ý thứ nhất đó là nó sẽ có một số loại hàm xuất hiện nhiều lần.
0:04:38 - 0:04:44, Và đồng thời đó là cái giá trị đạo hàm của nó nó nhỏ hơn một.
0:04:44 - 0:04:49, Thì tại sao cái giá trị nhỏ hơn một thì nó sẽ gây ra hiện tượng vanishing gradient?
0:04:49 - 0:04:59, Thì đó là do nếu như ở trong số các cái hàm ở đây mà có cái đạo hàm bé hơn một thì chúng ta hồi xưa cấp ba chúng ta đã học một cái hàm
0:04:59 - 0:05:06, đó là chúng ta học một cái công thức đó lim của a mũ n khi n tiến đến vô cùng.
0:05:06 - 0:05:15, Đó là bằng 0 nếu a lớn hơn 0 và nhỏ hơn một. Tức là một cái con số nhỏ hơn một.
0:05:15 - 0:05:23, Rồi thì hay nói cách khác đó là những con số mà nhỏ hơn một khi nhân với nhau thì thay vì nó tăng lên thì nó giảm xuống.
0:05:23 - 0:05:37, Lấy ví dụ như là 0.9 chúng ta nhân với 0.9 thì nó sẽ ra là 0.81 thì 0.81 nó nhỏ hơn 0.9 tức là càng nhân nó sẽ càng nhỏ.
0:05:37 - 0:05:50, Rồi cứ như vậy 0.9 mà mũ 3 thì sẽ là bằng 0.81 nhân 0.9 thì đâu đó nó xấp xỉ là 0.72 tức là nó còn nhỏ hơn cả con số 0.81 nữa.
0:05:50 - 0:06:02, Và cứ như vậy thì cái con số này khi n của mình mà tiến đến vô cùng thì cái con số này nó sẽ tiến về 0.
0:06:02 - 0:06:13, Thì khi n đủ lớn thì trong mô hình học sâu của mình sẽ có rất nhiều những cái hàm hợp và có nhiều lớp biến đổi thực hiện đi thực hiện lại nhiều lần.
0:06:13 - 0:06:24, Các mô hình của mình đã được chứng minh là càng sâu thì nó sẽ cho khả năng phi tuyến tính hóa càng cao. Do đó n sẽ có xu hướng lớn.
0:06:24 - 0:06:38, Thế thì nếu như trong công thức đạo hàm này có những cái hàm lặp đi lặp lại và cái đạo hàm đó ví dụ như cái đạo hàm này là có cái giá trị tuyệt đối nó bé hơn 1.
0:06:38 - 0:06:49, Thì khi đó các cái hàm, các cái giá trị nhỏ hơn 1 nhân với nhau thì nó sẽ tiến về 0. Nó sẽ kéo toàn bộ cái giá trị đạo hàm này về 0.
0:06:50 - 0:06:57, Thì đó là cái vấn đề về Vanishing Gradient, tức là đạo hàm của mình càng lúc càng bị tiến về 0.
0:06:57 - 0:07:05, Và đặc biệt đó là các cái tham số mà càng xa cái bước tính Loss thì cái đạo hàm của mình càng dễ bị tiêu biến.
0:07:05 - 0:07:12, Thế thì thế nào là các cái tham số mà càng xa các cái bước tính Loss thì nó càng dễ tiêu biến.
0:07:12 - 0:07:15, Ví dụ như chúng ta có một cái mạng Neural như thế này.
0:07:19 - 0:07:36, Rồi, thì cái quy trình tính toán của chúng ta đó là những cái tham số ở cái layer số 1, layer số 2 và layer số 3.
0:07:36 - 0:07:43, Rồi sau đó chúng ta sẽ đi tính, ở đây chúng ta sẽ đi so sánh một cái Loss.
0:07:43 - 0:07:47, Chúng ta sẽ đi tính một cái Loss giữa giá trị dự đoán và giá trị thực tế.
0:07:47 - 0:07:58, Thì cái định nghĩa xa gần đó là trong cái ví dụ của mạng Neural Network thì cái bộ tham số theta 3 là gần với cái bước tính Loss.
0:08:00 - 0:08:06, Nhưng mà các cái tham số theta 1 và theta 2 thì nó sẽ xa hơn.
0:08:06 - 0:08:13, Và cụ thể ở đây xa nhất chính là theta 1 thì đây là cái khoảng cách gọi là xa.
0:08:14 - 0:08:25, Thì các cái tham số mà càng xa cái bước tính Loss, tức là các cái tham số càng về cái lớp đầu tiên của cái mô hình học sâu, cụ thể đó là theta 1 hoặc là theta 2.
0:08:26 - 0:08:31, Thì cái giá trị đạo hàm sẽ càng dễ bị tiêu biến.
0:08:31 - 0:08:37, Còn những cái giá trị mà ở đầu thì nó sẽ ít bị tiêu biến hơn.
0:08:37 - 0:08:44, Nguyên nhân đó là khi chúng ta chạy thuật toán lan truyền ngược về thuật toán Back Propagation lan truyền ngược,
0:08:44 - 0:08:53, thì khi chúng ta tính đến những cái tham số đạo hàm của những cái tham số theta 3 thì nó sẽ ít cái số phép biến đổi hơn.
0:08:53 - 0:08:56, Số cái phép biến đổi là phép nhân hơn.
0:08:56 - 0:09:06, Trong khi đó, khi chúng ta lan truyền ngược cái loss về cái tham số ở càng xa, tức là ở những lớp đầu tiên,
0:09:06 - 0:09:11, thì chúng ta sẽ thực hiện cái thuật toán Back Propagation, chúng ta thực hiện cái phép nhân càng lúc càng nhiều.
0:09:11 - 0:09:20, Mà cái việc phép nhân càng nhiều mà gặp những cái con số, đặc biệt là cái đạo hàm của các activation function mà nhỏ, nhỏ hơn 1,
0:09:20 - 0:09:25, thì khi đó các con số nhỏ hơn 1 nó nhân nhiều lần với nhau, nó sẽ tiến về về 0.
0:09:25 - 0:09:35, Còn những cái trọng số ở những lớp cuối, thì số lần chúng ta thực hiện phép nhân đạo hàm của chain rule này,
0:09:35 - 0:09:46, nó ít, kể cả khi cái giá trị đạo hàm của các hàm kích hoạt này mà nhỏ, thì số lần nhân của nó nó ít hơn.
0:09:46 - 0:09:54, Do đó thì các cái tham số càng xa thì nó càng dễ bị tiêu biến.
0:09:55 - 0:10:04, Và thế thì nếu mà cái đạo hàm của G theo thetaG mà nó tiêu biến, tức là nó sẽ tiến về 0, thì vấn đề gì xảy ra?
0:10:04 - 0:10:13, Chúng ta phải quay trở lại cái công thức cập nhật tham số của thuật toán Gradient Descent, đó là thetaG,
0:10:13 - 0:10:18, thì sẽ là bằng thetaG trừ cho alpha nhân cho đạo hàm của G theo thetaG.
0:10:19 - 0:10:27, Vì cái đại lượng mà đạo hàm ở đây xấp xỉ 0 nên alpha nhân với nó sẽ xấp xỉ 0,
0:10:27 - 0:10:34, alpha là một con số còn nhỏ hơn 1 nữa, nó có thể là 0.001 nữa, và càng nhỏ hơn.
0:10:34 - 0:10:40, Thế thì cái con số này sẽ càng tiến về 0, thế thì thetaI bằng thetaI trừ 0,
0:10:40 - 0:10:49, tức là xấp xỉ với lại con số thetaI hay nói cái khác thetaI không cập nhật, tức là gần như không cập nhật.
0:10:50 - 0:10:59, Thì nó giải thích cho cái hiện tượng đó là hội tụ chậm, tại vì nó không có cập nhật tham số thì lấy đâu mà nó hội tụ?
0:10:59 - 0:11:05, Hoặc thậm chí đó là không hội tụ luôn, hội tụ chậm hoặc thậm chí là không hội tụ.
0:11:06 - 0:11:14, Rồi, như vậy thì ở đây chúng ta đã lý giải về mặt nguyên lý toán học của hiện tượng vanishing gradient.
0:11:14 - 0:11:24, Cái nguyên lý số 1 đó là trong mô hình học sâu có rất nhiều những cái hàm lặp đi lặp lại nhiều lần, cụ thể đó là cái hàm kích hoạt.
0:11:25 - 0:11:32, Thì các cái hàm kích hoạt này đương nhiên là không chỉ hàm kích hoạt, nó có thể có những hàm khác tùy vào thiết kế của mô hình của mình.
0:11:33 - 0:11:42, Các cái hàm kích hoạt này nếu chúng ta chọn lựa mà không tốt thì có khả năng cái đạo hàm của nó là một con số nhỏ hơn 1.
0:11:42 - 0:11:54, Khi chúng ta thực hiện cái chain rule này mà những cái đạo hàm nào thực hiện đi thực hiện lại nhiều lần, khi nhân nhiều lần với những con số nhỏ hơn 1 thì nó sẽ tiến về 0.
0:11:54 - 0:12:06, Và đặc biệt cái hiện tượng này càng dễ thấy đối với những cái tham số mà ở những cái lớp đầu tiên của cái mạng học sâu, tức là nó xa cái bước tính loss.