0:00:14 - 0:00:22, Chúng ta sẽ cùng đến với một mô hình tạo sinh đầu tiên, đó là mô hình Autoencoder hay còn gọi là bộ tự mã hóa.
0:00:22 - 0:00:31, Nền tảng của mô hình này là dựa trên hướng tiếp cận là học, không giám sát.
0:00:31 - 0:00:39, Mục tiêu đó là làm sao chúng ta sẽ biểu diễn được đặc trưng của dữ liệu gốc ban đầu.
0:00:39 - 0:00:45, Ví dụ như đầu vào của chúng ta là ảnh của một con số viết tay.
0:00:45 - 0:00:55, Chúng ta sẽ tìm cách biểu diễn ảnh này thành một vector đặc trưng, trong đó nó có chiều thấp hơn.
0:00:55 - 0:01:04, Giả sử như ảnh của chữ số viết tay có kích thước 28 x 28, đây là kích thước chuẩn trong tập dữ liệu MNIST.
0:01:04 - 0:01:14, 28 x 28 mà khi chúng ta flatten, tạo thành một vector, nó sẽ có kích thước 784 chiều.
0:01:14 - 0:01:23, Như vậy thì autoencoder là bước đầu tiên là làm sao để học ra được một cách biểu diễn vector x có số chiều lớn này,
0:01:23 - 0:01:29, và một vector z có số chiều nhỏ hơn, tức là z này phải có số chiều nhỏ hơn 784 chiều này.
0:01:29 - 0:01:36, Và cái dữ liệu mà chúng ta học là dữ liệu không có gán nhãn, tức là chúng ta chỉ có con số này thôi,
0:01:36 - 0:01:42, chứ chúng ta không có cái nhãn, đây là số 3. Thì dữ liệu huấn luyện của chúng ta là dữ liệu không có gán nhãn,
0:01:42 - 0:01:46, vì chúng ta đang tiếp cận theo cái hướng là học không giám sát.
0:01:46 - 0:02:01, Thì cái encoder này nó sẽ học một ánh xạ từ cái dữ liệu gốc ban đầu là nhiều chiều sang một cái dữ liệu z là trong một cái không gian ẩn, thấp chiều hơn.
0:02:01 - 0:02:09, Thì nếu như chúng ta vẽ trực quan hóa thì giả sử như đây là cái không gian flatten, tức là một cái không gian tiềm ẩn của mình.
0:02:09 - 0:02:18, Và đầu vào của mình sẽ có một cái hình là x, cái dữ liệu này là dữ liệu x, có số chiều lớn, chúng ta sẽ map nó vào một cái không gian z.
0:02:18 - 0:02:26, Và z này có số chiều nhỏ hơn, số chiều của x. Thì đây là cái bước đầu tiên là encode.
0:02:26 - 0:02:29, Nhưng mà làm sao để có thể học được cái không gian ẩn này?
0:02:29 - 0:02:36, Tại vì nếu như chúng ta chỉ đơn giản đó là làm giảm cái số chiều xuống thì chúng ta có thể dùng một cái trực quan rất là ngây thơ.
0:02:36 - 0:02:42, Đó chính là chúng ta sẽ random chọn một cái vector z có số chiều nhỏ hơn số chiều của x.
0:02:42 - 0:02:51, Là sao? Thế thì nó thiếu một cái sợi dây để liên kết về mặt ý nghĩa giữa z và x. Nó thiếu một cái sợi dây liên kết đó.
0:02:51 - 0:02:58, Do đó thì chúng ta sẽ tìm cách để huấn luyện mô hình, sao cho nó có thể sử dụng được cái đặc trưng z này.
0:02:58 - 0:03:02, Sử dụng được cái đặc trưng z để từ đó nó có thể tái tạo ngược trở lại được cái x.
0:03:02 - 0:03:05, Chứ còn nếu không thì chúng ta random một cái vector là sao?
0:03:05 - 0:03:14, Thì như vậy, cái mục tiêu là tái tạo lại được cái dữ liệu gốc x. Nó sẽ là cái sợi dây vô hình để liên kết cái đặc trưng z,
0:03:14 - 0:03:16, cái đặc trưng biểu diễn z với lại cái ảnh gốc ban đầu.
0:03:16 - 0:03:23, Và để làm được việc này thì chúng ta sẽ cần có một cái mô hình, đó là một cái decoder.
0:03:23 - 0:03:31, Với cái decoder này nó sẽ giúp cho chúng ta tái tạo ra được một cái x mũ.
0:03:31 - 0:03:39, Và cái x mũ này thì mình muốn là nó phải có cái nội dung giống với lại cái ảnh đầu vào x.
0:03:39 - 0:03:43, X và x mũ phải có nội dung giống với lại cái ảnh đầu vào x.
0:03:43 - 0:03:52, Như vậy thì decoder nó sẽ tìm cách để học một cái ánh xạ, học một cái ánh xạ từ cái không gian ẩn z về để xây dựng lại cái x mũ này.
0:03:52 - 0:03:59, Thế thì nếu chúng ta trực quan hóa trong cái không gian tiềm ẩn, latent, latent space,
0:03:59 - 0:04:12, từ cái ảnh ban đầu là x, đây là x, thì chúng ta qua cái encoder chúng ta sẽ ánh xạ nó sang một cái không gian latent,
0:04:12 - 0:04:16, để tạo ra thành một cái vector có số chiều ít hơn là z.
0:04:16 - 0:04:23, Và để mà có cái sợi dây liên kết giữa z với lại x thì chúng ta phải tái tạo ngược trở lại được.
0:04:23 - 0:04:32, Thì để cho cái việc này tạo ra được mối quan hệ về mặt ý nghĩa giữa vector biểu diễn z và x thì chúng ta phải có thêm cái thành phần tái tạo.
0:04:32 - 0:04:36, Về mặt công thức, làm sao chúng ta có thể tái tạo lại được?
0:04:36 - 0:04:42, Thì đó chúng ta sẽ phải có một cái hàm đó gọi là hàm loss.
0:04:42 - 0:04:48, Và cái hàm loss này là một cái hàm mean squared error, là lấy sai số giữa cái x và x mũ.
0:04:48 - 0:04:54, Chúng ta lấy x trừ x mũ rồi bình phương lên, thì cái hàm lỗi này là nó không cần nhãn.
0:04:54 - 0:04:58, Tại vì sao? Nhãn của nó thật ra cũng chính là cái x.
0:04:58 - 0:05:03, Nhãn của nó cũng chính là x. Chúng ta lấy x trừ cho x mũ rồi bình phương.
0:05:03 - 0:05:06, Thì đây là mean squared error.
0:05:06 - 0:05:14, Và với cái biểu diễn gọn gàng hơn, tức là thay vì chúng ta sẽ phải có nhiều lớp biến đổi như thế này,
0:05:14 - 0:05:17, rồi nhiều lớp biến đổi như thế này.
0:05:17 - 0:05:22, Thì từ nay về sau chúng ta chỉ cần dùng một cái ký hiệu hình thang.
0:05:22 - 0:05:30, Hai ký hiệu hình thang đó là từ x chúng ta đưa về z và chúng ta dùng cái hình thang là phía bên trái là lớn,
0:05:30 - 0:05:37, và phía bên phải đó là cái cạnh nhỏ, tức là ám chỉ từ một không gian nhiều chiều về không gian ít chiều.
0:05:37 - 0:05:41, Sau đó decoder, đây là encoder.
0:05:41 - 0:05:49, Còn decoder là chúng ta sẽ làm một cái hình thang ngược lại, là từ không gian ít chiều hơn về không gian nhiều chiều hơn.
0:05:49 - 0:05:53, Nó sẽ tạo ra cái thằng x mũ.
0:05:53 - 0:05:58, Và sai số cái hàm loss của mình lúc này vẫn dùng là một cái hàm, không gán nhãn.
0:05:58 - 0:06:02, Đó là chúng ta dùng chính cái x sẽ là cái nhãn luôn.
0:06:02 - 0:06:07, Thì từ nay về sau chúng ta sẽ ký hiệu như thế này cho nó gọn.
0:06:08 - 0:06:14, Và số chiều trong không gian ẩn thì nó sẽ tương ứng với lại cái chất lượng của cái việc tái tạo.
0:06:14 - 0:06:20, Nếu như chúng ta dùng cái vector biểu diễn z mà chỉ có hai chiều,
0:06:20 - 0:06:25, thì khi chúng ta tái tạo lại thì cái chất lượng nó không có được tốt, nó sẽ bị như thế này,
0:06:25 - 0:06:27, nó mờ, rồi không rõ nét.
0:06:27 - 0:06:32, Nhưng nếu mà chúng ta tăng cái số chiều lên là 5 chiều, thì chúng ta thấy nó đã rõ ràng hơn.
0:06:32 - 0:06:35, Còn cái round trip thì chúng ta thấy nó là rất là rõ.
0:06:35 - 0:06:38, Rõ, thì khi chúng ta tăng cái số chiều lên,
0:06:38 - 0:06:43, nó sẽ giúp cho chúng ta tạo ra cái dữ liệu mà gần với lại cái round trip hơn.
0:06:43 - 0:06:46, Thì không gian ẩn nhỏ hơn,
0:06:46 - 0:06:48, cái không gian ẩn nhỏ hơn,
0:06:48 - 0:06:52, nó sẽ là một cái nút thắt, là một cái bottleneck cho huấn luyện rất là lớn.
0:06:52 - 0:06:56, Tại vì từ một không gian nhiều chiều, giảm xuống một không gian ít chiều hơn,
0:06:56 - 0:06:57, nó đã mất thông tin rồi.
0:06:57 - 0:07:00, Nhưng mà nếu mà mất những thông tin mà không quan trọng thì không sao.
0:07:00 - 0:07:03, Nhưng mà đoạn này nếu nó quá nhỏ,
0:07:03 - 0:07:05, nó quá nhỏ, ví dụ như 2 chiều,
0:07:05 - 0:07:10, thì nó sẽ không đủ thông tin để có thể tái tạo được về cái ảnh gốc ban đầu.
0:07:11 - 0:07:13, Nó sẽ tạo ra cái bottleneck.
0:07:15 - 0:07:21, Vậy thì cái autoencoder là nó sẽ cho chúng ta học cái biểu diễn đặc trưng,
0:07:21 - 0:07:23, là để buộc cái mạng,
0:07:23 - 0:07:28, nó phải học được cái cách biểu diễn trong cái không gian ẩn thấp chiều hơn.
0:07:28 - 0:07:30, Đó là cái bottleneck hidden layer,
0:07:30 - 0:07:34, là từ một cái vector x về một cái vector z.
0:07:34 - 0:07:38, Là nó buộc cái mạng, nó sẽ phải học về cái không gian ít chiều hơn.
0:07:38 - 0:07:40, Đồng thời với cái reconstruction loss,
0:07:40 - 0:07:47, từ cái này, chúng ta tái tạo ngược trở lại để tạo ra cái x-mũ,
0:07:47 - 0:07:53, thì nó sẽ buộc không gian ẩn phải lưu giữ lại cái đặc trưng.
0:07:53 - 0:07:56, Tức là cái z này nó không phải là một cái vector ngẫu nhiên nữa,
0:07:56 - 0:07:58, mà nó phải lưu giữ được.
0:07:58 - 0:08:01, Nó phải lưu giữ được, hoặc là mã hóa được,
0:08:01 - 0:08:04, càng nhiều thông tin về cái dữ liệu gốc ban đầu, càng tốt.
0:08:05 - 0:08:11, Thì cái này là từ x-mũ mà chúng ta tái tạo ngược trở lại về x,
0:08:11 - 0:08:14, thì nó gọi là tự mã hóa là autoencoding.
0:08:14 - 0:08:20, Và autoencoding thì viết tắt của chữ là automatically encoding data,
0:08:20 - 0:08:22, hay còn gọi là self encoding.
0:08:22 - 0:08:28, Rồi thì đây chính là cái kiến trúc của autoencoder.