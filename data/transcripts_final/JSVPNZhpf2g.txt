0:00:14 - 0:00:20, Tiếp theo chúng ta sẽ cùng tìm hiểu về một số biến thể của thuật toán Gradient Descent
0:00:20 - 0:00:26, Đầu tiên đó là batch Gradient Descent và viết tắt của chữ BGD
0:00:26 - 0:00:34, Đó là chúng ta sẽ truyền toàn bộ dữ liệu huấn luyện vào mô hình của mình
0:00:34 - 0:00:41, Theta là bằng theta trừ alpha nhân cho đạo hàm của...
0:00:41 - 0:00:44, ở đây chúng ta sẽ có chữ kỳ vọng
0:00:44 - 0:00:48, tức là chúng ta đang tính trên toàn bộ dữ liệu của mình
0:00:48 - 0:00:52, kỳ vọng của hàm loss, hàm lỗi
0:00:52 - 0:00:59, Và ví dụ như ở đây chúng ta có một hình ảnh minh họa là một đoạn dữ liệu
0:00:59 - 0:01:10, thì expectation tức là E sẽ được tính trên full toàn bộ dữ liệu của mình
0:01:10 - 0:01:17, Và trong đoạn code mã giả chúng ta thấy là chúng ta sẽ lặp Y chạy trên số lượng Epoch
0:01:17 - 0:01:23, Mỗi một Epoch là một lượng chúng ta huấn luyện trên toàn bộ dữ liệu của mình
0:01:23 - 0:01:29, Chúng ta sẽ duyệt qua Y với toàn bộ số Epoch
0:01:29 - 0:01:38, Và với mỗi Epoch chúng ta sẽ tính Gradient trên full toàn bộ dữ liệu
0:01:38 - 0:01:41, Vậy là chúng ta đưa toàn bộ data của chúng ta vào
0:01:41 - 0:01:47, Và chúng ta sẽ có được Gradient cho từng tham số
0:01:47 - 0:01:51, Và ở đây thì tham số sẽ là bằng tham số trừ cho Learning Rate Alpha
0:01:51 - 0:01:54, Nhân cho Vector Gradient của mình
0:01:54 - 0:01:58, Thì ở đây chính là cái mã giả của Batch Gradient Descent
0:02:00 - 0:02:04, Phiên bản thứ hai, biến thể thứ hai đó là Stochastic Gradient Descent
0:02:04 - 0:02:06, Viết tắt của chữ SGD
0:02:06 - 0:02:09, Thì ở đây chúng ta sẽ truyền trên từng mẫu 1
0:02:09 - 0:02:14, Chúng ta sẽ truyền từng mẫu 1 để huấn luyện cho cái mô hình của mình
0:02:14 - 0:02:20, Thì công thức của chúng ta sẽ là theta là bằng theta trừ cho Alpha
0:02:20 - 0:02:23, Nhân cho nabla của theta
0:02:23 - 0:02:26, Và ở đây chúng ta sẽ tính trên 1 mẫu dữ liệu Y
0:02:30 - 0:02:34, Và 1 cái mẫu dữ liệu Y này thì sẽ được lấy ngẫu nhiên
0:02:38 - 0:02:43, Thế thì nếu chúng ta minh họa nguyên 1 cái đoạn dữ liệu của chúng ta như thế này
0:02:43 - 0:02:50, Thì phương pháp Stochastic Gradient Descent là chúng ta bốc ra ngẫu nhiên 1 cái mẫu dữ liệu
0:02:51 - 0:02:54, Để chúng ta đưa vào và chúng ta huấn luyện
0:02:54 - 0:02:58, Thì cái mẫu dữ liệu này là một cặp XE và EA
0:02:58 - 0:03:01, Và trong cái mã giả này thì chúng ta sẽ
0:03:01 - 0:03:06, Có là Y cũng sẽ lặp qua tất cả số epoch của mình
0:03:06 - 0:03:09, Với mỗi epoch thì chúng ta sẽ shuffle
0:03:10 - 0:03:12, Cái data này sẽ xáo
0:03:14 - 0:03:19, Tức là nó sẽ được sắp xếp một cách ngẫu nhiên
0:03:19 - 0:03:21, Thay đổi cái thứ tự ngẫu nhiên
0:03:21 - 0:03:24, Và với mỗi 1 cái example in data
0:03:24 - 0:03:29, Thì cái example của chúng ta chính là mẫu dữ liệu này
0:03:30 - 0:03:32, Chính là cái mẫu dữ liệu này
0:03:32 - 0:03:35, Thì khi đó chúng ta sẽ đưa cái mẫu dữ liệu này
0:03:35 - 0:03:39, Vào bên trong cái hàm EVALUATE GRADIENT để tính đạo hàm
0:03:39 - 0:03:43, Như vậy thì params rad là cái gradient
0:03:43 - 0:03:46, Chỉ được tính trên 1 mẫu dữ liệu ngẫu nhiên ở đây
0:03:46 - 0:03:49, Và công thức cập nhật thì cũng tương tự như là
0:03:49 - 0:03:51, Thuật toán gradient descent nguyên bản
0:03:51 - 0:03:53, Đó là params bằng params trừ cho alpha
0:03:53 - 0:03:56, Nhân cho cái thành phần nabla này
0:03:58 - 0:04:00, Và đến với biến thể
0:04:00 - 0:04:02, Có cái sự lai ghép
0:04:02 - 0:04:04, Nó có sự ngẫu nhiên
0:04:04 - 0:04:07, Của cái stochastic gradient descent
0:04:07 - 0:04:11, Nhưng đồng thời là nó sẽ không có tính trên full toàn bộ dữ liệu
0:04:11 - 0:04:13, Thì đó chính là mini-batch
0:04:13 - 0:04:17, Gradient Descent viết tắt của chữ MGD
0:04:17 - 0:04:20, Thì chúng ta sẽ truyền vào 1 khối
0:04:20 - 0:04:22, Chúng ta sẽ truyền vào 1 khối dữ liệu
0:04:22 - 0:04:23, Để huấn luyện
0:04:23 - 0:04:25, Và ở đây chúng ta sẽ có cái khái niệm
0:04:25 - 0:04:28, Nó gọi là batch size tức là kích thước của 1 khối
0:04:28 - 0:04:31, Thì kích thước của 1 khối
0:04:31 - 0:04:33, Thì thường là lũy thừa của 2
0:04:33 - 0:04:35, Ví dụ như là 1, 2, 4, 8
0:04:35 - 0:04:38, Thì tại sao người ta hay chọn cái lũy thừa của 2
0:04:38 - 0:04:40, Là vì trong máy tính của chúng ta
0:04:40 - 0:04:42, Kiến trúc của mình là kiến trúc nhị phân
0:04:42 - 0:04:46, Nên thường là các cái bộ chẳng của số 2
0:04:46 - 0:04:49, Thì nó sẽ vừa fit với lại cái dữ liệu của mình
0:04:49 - 0:04:51, Nên tính toán nó sẽ nhanh hơn
0:04:51 - 0:04:54, Thế thì cái công thức của mini-batch gradient descent
0:04:54 - 0:04:58, Theta sẽ là bằng theta trừ cho alpha nhân cho nabla
0:04:58 - 0:05:01, Thế thì ở đây chúng ta thấy là nó sẽ đi lấy trên mẫu dữ liệu
0:05:01 - 0:05:04, Từ y cho đến y cộng n
0:05:04 - 0:05:06, Thì đây là 1 batch
0:05:06 - 0:05:10, Tương tự như vậy cho y chúng ta sẽ lấy trên 1 batch
0:05:10 - 0:05:12, Để chúng ta huấn luyện
0:05:12 - 0:05:17, Thế thì nếu chúng ta vẽ cái khối này là toàn bộ dữ liệu
0:05:17 - 0:05:20, Thì chúng ta sẽ chia cái dữ liệu của mình ra
0:05:20 - 0:05:23, Thành từng batch mỗi cái này là 1 batch
0:05:23 - 0:05:28, Rồi thì cái mã giả của chúng ta là như sau
0:05:28 - 0:05:33, Với mỗi y chạy từ nằm trong cái range của số lượng epoch
0:05:33 - 0:05:36, Tức là chúng ta cũng duyệt qua tất cả các cái số epoch của mình
0:05:36 - 0:05:41, Và tại 1 cái thời điểm này thì chúng ta sẽ xáo ngẫu nhiên
0:05:41 - 0:05:44, Tức là chúng ta sẽ shuffle cái mẫu dữ liệu của chúng ta
0:05:44 - 0:05:46, Để cho nó có cái tính ngẫu nhiên trong đó
0:05:46 - 0:05:49, Và tại 1 thời điểm thì chúng ta sẽ lấy ra 1 batch
0:05:49 - 0:05:51, Ví dụ như ở đây là 1 batch
0:05:51 - 0:05:56, Chúng ta sẽ duyệt qua tất cả cái batch của data của mình
0:05:56 - 0:06:00, Và chúng ta sẽ truyền nó vào cái hàm evaluate gradient
0:06:01 - 0:06:04, Thì ở đây chúng ta sẽ tính được cái gradient
0:06:04 - 0:06:09, Chúng ta cũng sẽ cập nhật tương tự như thuật toán stochastic gradient descent
0:06:12 - 0:06:18, Như vậy thì chúng ta sẽ có 1 cái bảng so sánh về các cái biến thể này
0:06:18 - 0:06:22, Đầu tiên đó là cái batch gradient descent
0:06:22 - 0:06:29, Thì chúng ta thấy là vì nó được tính trên full toàn bộ cái dữ liệu
0:06:29 - 0:06:32, Nên rõ ràng là cái chi phí tính toán lớn
0:06:32 - 0:06:39, Và đồng thời là cái bộ nhớ của chúng ta có thể sẽ là sẽ phải cần rất là nhiều
0:06:39 - 0:06:43, Trong khi đó stochastic gradient descent thì ngược lại
0:06:43 - 0:06:46, Là tại 1 thời điểm chúng ta chỉ tính trong duy nhất 1 mẫu dữ liệu thôi
0:06:46 - 0:06:53, Do đó thì cái bộ nhớ, cái chi phí về bộ nhớ của chúng ta
0:06:55 - 0:06:58, Thì sẽ tiết giảm rất là đáng kể
0:06:58 - 0:07:03, Nhưng mà bù lại thì có thể cái thời gian tính toán của chúng ta sẽ lâu
0:07:03 - 0:07:09, Ở trên là cái chi phí tính toán bao gồm là cái tài nguyên tính toán là CPU
0:07:09 - 0:07:11, Rồi RAM là tốn
0:07:11 - 0:07:14, Nhưng ở phía dưới thì chúng ta lại liên quan điểm tối tính toán
0:07:14 - 0:07:20, Thay vì chúng ta tính hết 1 khối lớn thì chúng ta lại tính từng mẫu từng mẫu
0:07:20 - 0:07:23, Do đó cái thời gian của chúng ta sẽ lâu hơn
0:07:23 - 0:07:26, Và mini-batch thì đâu đó nó sẽ là nằm ở giữa
0:07:26 - 0:07:32, Tức là chúng ta sẽ tính trên 1 khối lớn là n mẫu dữ liệu mà thôi
0:07:32 - 0:07:37, Thì ở đây nó sẽ là cân bằng hơn giữa 2 phương pháp ở trên
0:07:37 - 0:07:44, Và cái landscape tức là cái đồ họa của hàm lỗi của chúng ta cho từng biến thể này
0:07:44 - 0:07:49, Đối với Batch Gradient Descent thì chúng ta sẽ thấy cái đường đi của nó rất là mượt
0:07:49 - 0:07:52, Nó rất là mượt
0:07:52 - 0:07:55, Là vì nó được tính trên tổng thể toàn bộ dữ liệu của mình
0:07:55 - 0:07:57, Nên cái đường đi của nó sẽ mượt
0:07:57 - 0:08:04, Còn Stochastic Gradient Descent thì chúng ta sẽ thấy nó rất là zíc zắc, rất là nhấp nhô
0:08:04 - 0:08:06, Thì cái này đó là do có cái yếu tố ngẫu nhiên
0:08:09 - 0:08:13, Sẽ có lúc chúng ta gặp những mẫu dễ thì cái loss của mình nó thấp
0:08:13 - 0:08:15, Nhưng gặp những mẫu cao thì loss nó cao
0:08:15 - 0:08:19, Nhưng mà nhìn chung sau 1 số lần lặp nhiều đủ lớn
0:08:20 - 0:08:26, Thì nó sẽ hội tụ và tiến về cái giá trị hàm lỗi của mình nó sẽ càng lúc càng thấp
0:08:26 - 0:08:32, Và trong nhiều lý thuyết người ta đã chứng minh đó là Stochastic Gradient Descent
0:08:32 - 0:08:36, Nó sẽ giúp cho cái mô hình của mình học bền vững và ổn định hơn
0:08:36 - 0:08:38, Nhờ có những cái bước nhảy như thế này
0:08:38 - 0:08:41, Những cái bước nhảy dập lên dập xuống như thế này
0:08:41 - 0:08:46, Nó sẽ giúp cho mô hình của chúng ta thoát ra những cái điểm cực tiểu cục bộ
0:08:50 - 0:08:57, Để hy vọng nó có thể tiến đến được những cái điểm tối ưu, những điểm cực tiểu toàn cục
0:08:57 - 0:09:00, hoặc là những điểm cực tiểu tối ưu hơn
0:09:00 - 0:09:02, Thì nhờ có những cái dập như thế này
0:09:02 - 0:09:10, Và Mini-batch Gradient Descent thì nó sẽ ở mức lưng chừng ở giữa nó cũng tương đối smooth
0:09:10 - 0:09:12, nhưng mà nó cũng sẽ có những cái zíc zắc nhất định
0:09:12 - 0:09:15, Thì đây là Mini-batch Gradient Descent
0:09:15 - 0:09:19, Và thông thường thì người ta hay sử dụng Mini-batch Gradient Descent
0:09:19 - 0:09:23, vì nó cân bằng được yếu tố về thời gian và chi phí tính toán