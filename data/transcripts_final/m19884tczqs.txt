0:00:14 - 0:00:18, Chúng ta sẽ cùng đến một mô hình, cơ bản tiếp theo là mô hình Softmax.
0:00:18 - 0:00:24, Cái đường đi của mô hình này là nó nằm trong nhóm tuyến tính và thuộc bài toán là phân loại.
0:00:24 - 0:00:32, Nhưng trong trường hợp này, số lớp phân loại của chúng ta là nhiều hơn 2, tức là ca lớn hơn 2.
0:00:32 - 0:00:40, Mô hình dưới dạng đồ thị của Softmax Regression là như sau.
0:00:40 - 0:00:52, Input đầu vào của chúng ta cũng có các đặc trưng, đó là Input Feature và đồng thời chúng ta cũng có thành phần Bias.
0:00:53 - 0:01:06, Nếu như trong mô hình Logistic Regression, mỗi một đặc trưng, mỗi một Neuron chúng ta đưa vào để xử lý,
0:01:06 - 0:01:14, thì nó đều có một phần tổng và một hàm kích hoạt, tức là một phép biến đổi tuyến tính.
0:01:14 - 0:01:21, Ngay sau phép biến đổi tuyến tính, chúng ta sẽ có một hàm kích hoạt phi tuyến.
0:01:22 - 0:01:30, Và để giải quyết được bài toán phân lớp đa lớp, chúng ta có thể sử dụng nhiều hơn một Neuron này.
0:01:35 - 0:01:46, Tuy nhiên, việc chúng ta sử dụng nhiều Neuron như vậy, nó có khả năng gây ra hiện tượng gọi là nhập nhằng.
0:01:46 - 0:01:54, Cụ thể là trong mô hình của chúng ta, nếu như chúng ta có các tập điểm dạy như thế này,
0:01:56 - 0:02:06, thì nếu chúng ta dùng ba cái node này độc lập nhau, thì nó sẽ tách ra làm ba cái phần mặt phẳng mỗi một node tương ứng với lại một cái đường thẳng.
0:02:06 - 0:02:11, Thì nó sẽ có những cái khu vực mà chúng ta sẽ không biết là nó sẽ xếp về lớp nào.
0:02:11 - 0:02:19, Ví dụ nếu theo cái đường thẳng số 1 này, thì chúng ta sẽ nói cái điểm chấm hỏi này là hình tròn.
0:02:19 - 0:02:24, Nhưng nếu chiếu theo cái đường phân lớp số 2 này, thì chúng ta nói đó là hình cộng.
0:02:24 - 0:02:27, Thì do đó nó tạo ra sự nhập nhằng.
0:02:27 - 0:02:36, Và mô hình Softmax Regression là sẽ loại bỏ đi cái thành phần sigmoid độc lập này để đưa chung vào một cái hàm Softmax.
0:02:37 - 0:02:43, Thì qua cái hàm Softmax này chúng ta sẽ tính ra các cái giá trị y ngã 1, y ngã 2 và y ngã k.
0:02:43 - 0:02:50, Y ngã 1 nếu mà bình thường nó tính thì nó chỉ dựa trên một giá trị z1 thôi.
0:02:50 - 0:02:58, Nhưng qua cái hàm Softmax thì nó sẽ phải có cái sự tổng hợp của tất cả những cái giá trị z1, z2 cho đến zk.
0:02:59 - 0:03:08, Thì cái công thức Softmax của mình cụ thể nó sẽ là Softmax của z.
0:03:08 - 0:03:10, Ví dụ như ở đây là zi.
0:03:12 - 0:03:19, Nhưng nó sẽ có sự tham gia của các cái z còn lại là từ z1 cho đến zk.
0:03:20 - 0:03:33, Và nó sẽ là bằng e-mũ của zi chia cho tổng của e-mũ zk với k là chạy từ 1 cho đến k lớn.
0:03:33 - 0:03:35, k lớn là tổng số lớp của mình.
0:03:35 - 0:03:39, Thì cái công thức Softmax này nó sẽ có rất nhiều những cái điểm lợi.
0:03:39 - 0:03:42, Thứ nhất đó là tính đạo hàm của nó cũng dễ.
0:03:42 - 0:03:55, Nhưng đồng thời là nó sẽ đưa về một cái không gian xác suất trong đó các cái thành phần y này, y ngã y này của chúng ta thì đều là lớn hơn 0 và bé hơn 1.
0:03:55 - 0:04:00, Các cái giá phân bố xác suất của mình thì nó là từ 0 cho đến 1.
0:04:00 - 0:04:09, Và đồng thời tổng của các cái y ngã y này thì đều là bằng 1 với y chạy từ 1 cho đến k.
0:04:10 - 0:04:14, Thì đây chính là đưa về một cái không gian xác suất khá là đẹp.
0:04:14 - 0:04:22, Và chúng ta sẽ dựa trên cái y ngã nào mà cho cái xác suất cao nhất thì chúng ta sẽ kết luận nó thuộc về cái phần lớp đó.
0:04:22 - 0:04:27, Còn đối với cái hàm lỗi thì chúng ta sẽ sử dụng cái hàm Cross entropy.
0:04:27 - 0:04:32, Thế thì Cross entropy nếu như cái thành phần Softmax này chúng ta ký hiệu là y ngã.
0:04:33 - 0:04:45, Thì khi đó hàm lỗi J theta của Xi thì sẽ được ghi là bằng Cross entropy của y ngã y.
0:04:45 - 0:04:59, Thì cái công thức của mình nó sẽ là bằng log y, log y ngã trừ và tổng với y chạy từ 1 cho đến k.
0:04:59 - 0:05:01, Và ở đây là chỉ số y.
0:05:01 - 0:05:04, Thì đây là công thức của Cross entropy.
0:05:04 - 0:05:15, Tương tự như là binary Cross entropy thì Cross entropy nó sẽ tạo ra cái sự mô hình trừng phạt nặng hơn khi chúng ta dự đoán sai.
0:05:15 - 0:05:18, Và sẽ giúp cho cái mô hình của mình huấn luyện nhanh hơn.
0:05:18 - 0:05:26, Thì sau đây chúng ta sẽ xem cái ý nghĩa về mặt hình học của các cái tham số của mình.
0:05:26 - 0:05:32, Đầu tiên đó là chúng ta sẽ thấy cái mô hình của mình nó sẽ có một cái tham số là theta.
0:05:32 - 0:05:40, Và qua cái tham số cái mô hình của mình nó sẽ chia cái không gian của mình ra làm nhiều phần.
0:05:40 - 0:05:47, Nếu như trong cái phần trước chúng ta thấy là chúng ta dùng mô hình logistic theo kiểu là màu vàng và không phải màu vàng.
0:05:47 - 0:05:52, Một xanh lá và không phải xanh lá thì nó sẽ chia ra thành các cái không gian như thế này.
0:05:52 - 0:05:56, Và nó sẽ có những cái khu vực là bị nhập nhằng như thế này.
0:05:56 - 0:05:59, Mình sẽ không biết được là nó thuộc về lớp nào.
0:05:59 - 0:06:02, Nhưng qua cái mô hình softmax regression
0:06:05 - 0:06:11, Mô hình hồi quy softmax thì cái đầu ra của mình là nó sẽ cho một cái phân bố xác suất.
0:06:11 - 0:06:19, Và với cái phân bố xác suất này thì chúng ta chỉ có thể từ một cái điểm chúng ta chỉ có thể kết luận nó thuộc về một lớp duy nhất mà thôi.
0:06:20 - 0:06:27, Nhờ cái cách mà chuẩn hóa theo kiểu softmax này nếu ở đây thì chúng ta sẽ gán nó về lớp màu vàng.
0:06:27 - 0:06:29, Nếu ở đây thì có thể gán về lớp màu đỏ.
0:06:29 - 0:06:33, Do đó nó sẽ chia cái không gian của mình ra.
0:06:33 - 0:06:35, Chia cái không gian của mình ra.
0:06:41 - 0:06:48, Ví dụ vậy thành 4 phần và với mỗi một cái điểm thì nó sẽ thuộc vào một phần chứ nó không có thuộc vào hai cái phần.
0:06:48 - 0:06:52, Gây ra cái sự nhập nhằng giống như trong cái mô hình logistic regression.