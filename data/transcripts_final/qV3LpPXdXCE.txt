0:00:14 - 0:00:21, Chúng ta sẽ cùng đến với mô hình tiếp theo đó là clip Routed Language Image Retraining
0:00:21 - 0:00:35, Routed Language Image Retraining là clip phát triển từ clip, clip cho thấy tiềm năng trong việc khai thác thông tin dựa trên một cặp dữ liệu huấn luyện, đó là hình ảnh và văn bản
0:00:35 - 0:00:39, và khai thác dữ liệu này để cho bài toán, đó là Image Classification
0:00:39 - 0:01:03, Tuy nhiên, điều gì xảy ra nếu chúng ta khai thác nó cho những bài toán, ví dụ như là phân đoạn ngữ nghĩa, bài toán Pose Estimation xác định tư thế của người, dáng người, bài toán phát hiện đối tượng
0:01:03 - 0:01:15, Chúng ta thấy trong thị giác máy tính có rất nhiều bài toán khác nhau và clip đã thể hiện được tiềm năng trong bài toán đầu tiên, đó là bài toán Classification
0:01:15 - 0:01:33, Vậy thì, làm sao chúng ta có thể khai thác nó cho các bài toán khác? Đó chính là câu hỏi chúng ta đặt ra là làm sao áp dụng được ý tưởng tương tự với clip, nhưng mà cho bài toán phân biệt, phát hiện đối tượng, phân đoạn ngữ nghĩa đối tượng và xác định tư thế Pose Estimation
0:01:33 - 0:01:50, Như vậy thì, nó đòi hỏi chúng ta phải nắm bắt được thông tin thị giác, nhưng mà nó không phải ở mức độ toàn cục, tại vì trong mô hình clip, chúng ta truyền vào nguyên một tấm ảnh và chúng ta sẽ ra được vector biểu diễn thông qua image encoder, tắc chứ ic
0:01:50 - 0:02:16, Rồi câu mô tả của chúng ta, thì qua text image encoder là ic, text encoder là te, nó sẽ tạo ra một cái văn bản, sau đó chúng ta sẽ so hai cái này lại với nhau, thế thì nó chưa có cơ chế để mà chúng ta khai thác trên từng vùng của đối tượng
0:02:16 - 0:02:26, Vậy thì làm sao chúng ta có thể nắm bắt được thông tin thị giác ở mức độ chi tiết hơn và vùng cục bộ, thay vì là vùng toàn bộ, tấm ảnh
0:02:26 - 0:02:39, Thì cái kết quả chính mà clip đã đạt được đó là clip G-clip có thể học những cái không cần dữ liệu, tức là Zero-Shot Transfer cho bài toán phát hiện đối tượng
0:02:39 - 0:02:52, Trên hình đây chúng ta thấy là khi chúng ta đưa vào một cái code prompt là raccoon, thì nó đã chỉ ra được cái vị trí của cái đối tượng đó ở đâu, kèm theo cái score tương ứng của mình là bao nhiêu phần trăm
0:02:52 - 0:03:07, Khi chúng ta gõ cái prompt mà đa đối tượng, ở đây là một đối tượng, còn đây là một đối tượng, còn nếu chúng ta đưa vào nhiều hơn một đối tượng, ví dụ như là person, bicycle, car, motorbike, motorcycle
0:03:07 - 0:03:28, Và giữa các đối tượng này thì chúng ta để cách nhau bởi một cái dấu chấm để phân biệt, thì nó cũng khoanh vùng được Apple, rồi prompt là tương tự như vậy cho các ví dụ khác, thì chúng ta thấy là nó có thể khoanh vùng được cái bánh xe, có thể khoanh vùng được nguyên chiếc xe
0:03:28 - 0:03:43, Rồi vân vân, thì đây chính là thế mạnh của clip sau với lệch clip, đó là nó có thể học không cần dữ liệu Zero-Shot Transfer cho cái bài toán phát hiện đối tượng
0:03:43 - 0:03:59, Và clip thì chế tên bài báo của mình đó là Routed Language Image Retraining, thì chúng ta cũng sẽ có bắt gặp những cụm từ mới, thì cái cụm từ mà chúng ta đang cần quan tâm ở đây đó chính là Routed Language Image
0:03:59 - 0:04:12, Thì cái Routed Language Image đó là chúng ta sẽ tìm cách để liên kết giữa các cái từ hoặc là cụm từ trong câu với một vật thể hoặc là một vùng của tấm ảnh, ví dụ chúng ta có nguyên một cái tấm ảnh như thế này
0:04:12 - 0:04:32, Và chúng ta có một cái code prompt đó là ví dụ như một cái bus.person, thì giả sử như trong hình này thì nó có một chiếc xe bus, nó có một chiếc xe bus
0:04:32 - 0:04:51, Và có một người ở đây thì nó sẽ tìm cách liên kết cái từ bus này với lại cái phần, cái khu vực mà tương ứng có cái nội dung hình ảnh đó, rồi nó liên kết cái person với lại cái khu vực mà tương ứng có cái đối tượng trong hình ảnh đó, đó là cái vùng màu đen ở đây
0:04:51 - 0:05:12, Và như vậy thì nó đã liên kết được giữa văn bản và hình ảnh, thì Routed Language Image đó là một cái phương pháp hoặc là một cái bài toán mà giúp cho chúng ta khoanh vùng được cái đối tượng ảnh mà tương đương với lại cái nội dung ở bên trong tấm hình của mình
0:05:12 - 0:05:25, Thì cái ý tưởng chính đó là đưa bài toán phát hiện đối tượng Object Detection trở về thành một cái bài toán đó là phân lập hoặc là từ hoặc là cụm từ trong câu hay là phrase, routing
0:05:25 - 0:05:33, Và chúng ta sẽ phát hiện vật thể và phân lập cái cụm từ, hai đối tượng nó có cái sự tương đồng cao
0:05:33 - 0:05:44, Thì ở đây là một cái ví dụ, chúng ta thấy đó là khi chúng ta đưa vô một cái câu mô tả thì trong cái câu mô tả tương ứng với một tấm ảnh, đây là một cái cặp ảnh và câu mô tả
0:05:44 - 0:05:57, Thì nó sẽ có cái cụm từ là đi kèm với lại cái khu vực mà cái ảnh nó có nội dung đó, small, vial, op, vaccine
0:05:57 - 0:06:04, Thì đây là một cái hình ảnh tương ứng với cái mô tả của cái hình ảnh tương ứng với lại cái câu mô tả đó
0:06:04 - 0:06:13, Thế thì clip, clip có khả năng nhận diện được các đối tượng ít gặp và trừu tượng
0:06:13 - 0:06:19, Tức là bên cạnh những đối tượng rất phổ biến như là car, dog, bus, person, v.v.
0:06:19 - 0:06:29, Thì ở đây chúng ta thấy là những cụm từ này là những cái từ ít gặp, ống tiêm, vaccine, v.v. đó là những cái từ ít gặp
0:06:29 - 0:06:37, Thì thậm chí là những cái từ mà có tính trừu tượng cao ví dụ như là the view, là khoanh vùng nguyên cái này là the view
0:06:37 - 0:06:46, Rồi beautiful caribbean, caribe sea, thì ở đây chúng ta thấy là nó biết đây là một cái biển ở caribbean để mà nó khoanh vùng
0:06:46 - 0:06:56, Thì đây chính là những cái khái niệm rất là trừu tượng và ít gặp trong thực tế mà clip nó vẫn có thể phát hiện bằng định vị được
0:06:56 - 0:07:02, Vậy thì cách thức hoạt động và các cái module chính của clip là thực hiện như thế nào
0:07:02 - 0:07:17, Thì đầu tiên đó là chúng ta sẽ tạo ra một cái code prompt tổng quát là tên của 80 vật thể trong tập dữ liệu coco được phân cách bởi dấu chấm
0:07:17 - 0:07:26, Tức là mỗi một cái vật thể thì sẽ cách nhau với các vật thể khác, ví dụ chữ person, cách chữ bicycle là dấu chấm như thế này
0:07:26 - 0:07:39, Và kèm theo là một cái câu mô tả, một cái cặp câu mô tả giữa hình ảnh và chúng ta sẽ đưa các cái từ này, cái code prompt
0:07:39 - 0:07:48, Và cái câu mô tả này qua textencoder thì chúng ta sẽ tạo ra được là P0, P1, P2 v.v.
0:07:48 - 0:08:01, Và tương tự như vậy thì cái ảnh chúng ta cũng sẽ khoanh vùng các cái đối tượng này và qua cái visualencoder để tạo ra cái O0, O1, O2
0:08:01 - 0:08:10, Lưu ý đó là trong cái mô hình clip thì chúng ta sẽ kết hợp hai cái loại dữ liệu là hình ảnh và văn bản ở bước trung gian
0:08:10 - 0:08:22, G-clip thì là ở bước trung gian, trong khi đó với clip thì chúng ta chỉ kết hợp nó ở cái bước cuối cùng thôi
0:08:22 - 0:08:25, Còn ở đây chúng ta sẽ thực hiện ở bước trung gian
0:08:25 - 0:08:41, Thì tại sao lại có những cái việc như vậy? Đó là khi chúng ta tổng hợp ra cái đặc trưng thì ở phía trên là chỉ thuần nội dung cái đặc trưng về văn bản
0:08:41 - 0:08:54, Còn ở bên dưới là các cái đặc trưng mà chúng ta thuần về hình ảnh và qua cái Fusion này thì chúng ta sẽ kết hợp được với nhau
0:08:54 - 0:09:09, Đó là lấy cái đặc trưng của hình ảnh để bỏ vào văn bản và lấy nội dung của văn bản đưa vào hình ảnh để cho nó có thể tương tác để giúp cho hình ảnh nó có thể thực sự được thấy ở trong văn bản này
0:09:09 - 0:09:21, Và cái sự khác biệt lớn nhất đó chính là cái module thứ 2, đó là chúng ta sẽ đo cái độ tương đồng giữa mỗi vùng và từ mô tả thay thế cho cái module phân lớp truyền thống
0:09:21 - 0:09:31, Và ở trên đây chúng ta sẽ có m từ khóa là P1, P2, m token là P1, P2 và Pm
0:09:31 - 0:09:44, Thì chúng ta có một cái lưu ý ở đây là có những cái từ mà dài và có hai, gọi là giống như tiếng Việt của mình là từ F là hair dryer thì nó sẽ tách ra làm hai, làm hai token
0:09:44 - 0:09:59, hoặc có những cái từ hoặc cụm từ nó sẽ đi một cái combo với nhau, ví dụ như là blue dot, nó là một cái cụm từ thì nó sẽ tách ra làm hai token
00:09:59 - 0:10:15, Thì khi đó chúng ta thấy là tính cái độ tương đồng nó sẽ là hai cái phần tử trên cái ma trận mà có cái giá trị lớn, ví dụ như đây là chữ tính từ và đây là danh từ thì chẳng hạn
0:10:15 - 0:10:37, Hoặc trong cái ví dụ trên thì chúng ta thấy là hair dryer thì đây chính là hair và đây là dryer, hair và dryer này thì nó sẽ đi chung với nhau một cái combo khi chúng ta so sánh cái độ tương đồng với cái feature của cái hình ảnh là của cái máy sấy tóc
0:10:37 - 0:11:03, Thì cái ý tưởng của nó cũng tương tự như clip nhưng mà clip thì sẽ tách nó ra thành các cái khu vực, các cái mảnh hình ảnh nhỏ rồi sau đó nó sẽ đi tính độ tương đồng với những cái từ ở bên trong cái chuỗi prompt của mình
0:11:03 - 0:11:05, Cái token được tạo ra bởi cái prompt của mình
0:11:06 - 0:11:15, Sau khi chúng ta đã tính được cái ma trận này rồi thì chúng ta sẽ đi tính cái alignment loss giữa cái ground truth, đây là cái ma trận ground truth
0:11:17 - 0:11:31, Và ở trên đó là cái ma trận mà chúng ta được tính từ cái độ tương đồng giữa cái vùng ảnh, cục bộ với lại các cái token ở bên trong cái prompt của mình
0:11:31 - 0:11:45, Và alignment loss này thì mục đích là để xác định xem cái đối tượng đó là gì, bên cạnh đó thì chúng ta sẽ có cái region feature để tính cái localization loss
0:11:46 - 0:11:53, Mục tiêu đó là để cho chúng ta xác định xem cái vị trí, cái sai số khi dự đoán vị trí
00:11:53 - 0:12:02, Thế thì quá trình huấn luyện của clip thì nó sẽ bao gồm đầu tiên đó là cái phrase routing
0:12:03 - 0:12:14, Với mỗi một cái tấm ảnh ở đây, O1, O2, ON chúng ta sẽ truyền vào, truyền vào đây và chúng ta sẽ ra được O
0:12:15 - 0:12:21, O chính là cái feature hoặc là cái embedding của cái vùng ảnh
0:12:23 - 0:12:35, Và sau đó thì chúng ta sẽ lấy cái prompt, prompt của mình thì nó sẽ được hình thành từ cái câu là detect person bicycle car third brush v.v.
0:12:37 - 0:12:50, Rồi, và cái ở đây thêm một cái ý nữa đó là cái feature mà biểu diễn cho cái đặc trưng hình ảnh thì đó là một cái đặc trưng có kích thước đó là D chiều
0:12:50 - 0:12:56, Còn cái ảnh của mình thì nó sẽ có N cái vùng, N vùng
0:12:57 - 0:13:09, Rồi tương tự như vậy thì prompt thì đặc trưng của mỗi từ hoặc là token thì chúng ta sẽ biểu diễn bởi các cái vector và đại diện nó là ma trận p
0:13:09 - 0:13:22, Ma trận p này thì cũng tương tự đó là nó sẽ có chiều của cái đặc trưng văn bản sẽ là D chiều, giống như bên đây để mà sau này chúng ta có thể nhân tích vô hướng được
0:13:23 - 0:13:33, Và số token của chúng ta thì sẽ có m token, và m này thì thường lớn hơn N rất là nhiều
00:13:33 - 0:13:42, Rồi sau khi chúng ta lấy cái O và P chúng ta nhân với nhau thì chúng ta sẽ ra được cái S routing
0:13:43 - 0:13:49, Thì cái S routing này nó sẽ có kích thước đó là N nhân m, trong đó N là số vật thể hoặc số vùng, m là số token
0:13:50 - 0:14:10, Và cuối cùng là chúng ta sẽ dựa trên cái S routing này để chúng ta đi tính cái loss của mình, trong đó loss của mình thì nó sẽ có cái T phải là được mở rộng ra từ T bao gồm là N đối tượng
0:14:10 - 0:14:22, Rồi, thế thì clip đó là cho phép chúng ta có thể tạo ra một cái câu prompt thủ công
0:14:23 - 0:14:31, Ở trên ví dụ trên thì chúng ta có thể thấy những cái đối tượng hiếm và lạ thì ta có thể mô tả đối tượng đó bằng một cái nguồn tự nhiên
0:14:31 - 0:14:41, Bình thường thì chúng ta mô tả đó là Car, Bus, Car, Dog, đây là những cái đối tượng tương đối là phổ biến
00:14:41 - 0:14:56, Thì chúng ta sẽ không cần phải mô tả nhiều hơn, nhưng đối với cái từ lạ, ví dụ như là cá đuối, là stingray, thì nếu chúng ta đưa vào cái mô hình clip để mà phát hiện đối tượng
00:14:56 - 0:15:05, thì chúng ta thấy là cái khả năng phát hiện của nó khá là thấp, nó chỉ phát hiện được một con và với cái Confidence là khoảng 0.21
0:15:06 - 0:15:19, Nhưng khi chúng ta mô tả nó dài dòng hơn, nhiều thông tin hơn, ví dụ như là stingray with a flat and round, thì ở đây chúng ta sẽ, mô hình của mình nó sẽ hiểu
00:15:19 - 0:15:32, và nó sẽ phát hiện ra cái đối tượng nhiều hơn và dày đặc hơn. Thì cái việc mà bổ sung thêm cái câu mô tả này nó gọi là manual prompt tuning
00:15:33 - 0:15:42, và cái việc này thì cũng hoàn toàn có thể làm tự động nếu như chúng ta có cái dữ liệu có gắn nhãn thì có thể tự động làm cái quá trình là prompting này
00:15:42 - 0:15:50, bằng cách đó là chúng ta sẽ huấn luyện một cái module nhỏ là một cái phép biến đổi tuyến tính thôi, là một cái linear mapping thôi để mà trực tiếp học ra cái prompt
00:15:51 - 0:16:07, cái prompt này sẽ được học và tạo tự động. Nếu như chúng ta có cái dữ liệu thì chúng ta có thể cho nó học để mà bổ sung thêm cái câu mô tả này
00:16:07 - 0:16:22, Với cái slide này, với cái bài về G-Clip thì chúng ta thấy là cái tiềm năng của mô hình ngôn ngữ hình thị giác nó có thể cho chúng ta giải quyết được những cái bài toán phức tạp hơn
0:16:23 - 0:16:31, so với cái bài toán phân đoạn so với bài toán phân loại đối tượng đó là các cái bài toán như là phát hiện đối tượng
0:16:37 - 0:16:42, Cảm ơn các bạn đã xem video hấp dẫn