0:00:14 - 0:00:22, Chào các bạn, chúng ta sẽ cùng đến với chủ đề đầu tiên trong mô máy học nâng cao, đó chính là mô hình dựa trên Gradient.
0:00:23 - 0:00:26, Tại sao chúng ta cần phải tìm hiểu mô hình dựa trên Gradient?
0:00:26 - 0:00:33, Thì hiện nay các mô hình hiện đại và có rất nhiều những ứng dụng trong thực tế
0:00:33 - 0:00:40, Ví dụ như các mô hình ngôn ngữ lớn, ChatGPT, các mô hình sinh ảnh như diffusion model
0:00:40 - 0:00:47, Đều được huấn luyện dựa trên một phương pháp dựa trên Gradient, tức là chúng ta tính đạo hàm
0:00:47 - 0:00:52, Và cái phương pháp này là nguyên lý của nó là gì?
0:00:52 - 0:01:03, Và tại sao nó có những ưu điểm để các mô hình hiện đại đều tập trung để sử dụng Gradient làm nền tảng để huấn luyện?
0:01:03 - 0:01:06, Chúng ta sẽ cùng tìm hiểu trong bài học ngày hôm nay
0:01:07 - 0:01:14, Đầu tiên chúng ta sẽ cùng tìm hiểu về ý tưởng của mô hình dựa trên Gradient
0:01:14 - 0:01:19, Ý tưởng của mô hình dựa trên Gradient thì chúng ta sẽ nhận dữ liệu đầu vào là x
0:01:19 - 0:01:23, Thì x này có thể là bất cứ loại dữ liệu nào
0:01:23 - 0:01:31, Ví dụ như nó có thể là dữ liệu dạng vector, nó cũng có thể là dữ liệu dạng ma trận hoặc là tensor
0:01:35 - 0:01:43, Với hai loại là vector, ma trận hoặc là tensor, chúng ta có thể biểu diễn rất nhiều những loại dữ liệu khác nhau
0:01:43 - 0:01:49, Ví dụ như có thể biểu diễn dữ liệu văn bản, dữ liệu dạng hình ảnh, video
0:01:49 - 0:01:58, Và khi chúng ta đưa qua mô hình thì mô hình của mình sẽ được ký hiệu bằng một hàm, đó là hàm fx
0:01:58 - 0:02:04, Nhưng chúng ta lưu ý là đối với hàm fx ở đây nó sẽ có một tham số, đó là tham số theta
0:02:04 - 0:02:09, Cũng tương tự như trong toán phổ thông của chúng ta
0:02:09 - 0:02:25, Ví dụ như chúng ta giải tìm m, tìm tham số m sao cho phương trình bậc 2
0:02:25 - 0:02:46, Ví dụ như x bình trừ m x cộng 3 m bình trừ 1 bằng 0 có 2 nghiệm
0:02:46 - 0:02:54, Ví dụ vậy, đây là bài toán mà hồi phổ thông chúng ta được làm qua
0:02:54 - 0:03:02, Tham số chính là m của mình và dạng thức của hàm của mình là hàm bậc 2
0:03:02 - 0:03:13, Chúng ta có thể ký hiệu f của m x bằng 0, trong đó fmx là công thức ở vế bên tay trái
0:03:13 - 0:03:21, Chúng ta xác định được dạng thức của hàm f rồi
0:03:21 - 0:03:32, Tuy nhiên trong đó chúng ta sẽ có nhiều tham số để quyết định việc dự đoán m chính xác đến mức độ nào
0:03:32 - 0:03:39, Chúng ta phải tìm tham số sao cho hàm fx, nó thỏa mãn việc đó là dự đoán chính xác
0:03:39 - 0:03:50, Đầu ra của hàm fx là y ngã và đây là giá trị mà chúng ta được dự đoán từ mô hình
0:03:50 - 0:03:58, Đương nhiên là việc dự đoán một giá trị nào đó chúng ta luôn mong muốn là nó sẽ xấp xỉ với giá trị thực tế
0:03:58 - 0:04:03, Giá trị thực tế chúng ta sẽ ký hiệu là bằng y
0:04:03 - 0:04:12, Để cho giá trị dự đoán xấp xỉ được với giá trị thực tế thì chúng ta phải có một hàm gọi là hàm lỗi
0:04:12 - 0:04:14, Ký hiệu là chữ j
0:04:14 - 0:04:22, Hàm lỗi này sẽ có biến số, lúc này nó không phải là x nữa mà biến số của mình lúc này nó sẽ là theta
0:04:22 - 0:04:28, Biến số của nó sẽ là theta và x,y của mình nó sẽ đóng vai trò giống như là tham số
0:04:28 - 0:04:39, Nó sẽ khác so với lại hồi xưa, khi mà chúng ta đặt cái tên biến mà là x,y thì nó sẽ ngầm hiểu đó là biến số
0:04:39 - 0:04:46, Còn trong trường hợp này thì cái hàm lỗi của mình x,y sẽ là tham số và nó chính là cái dữ liệu huấn luyện
00:04:46 - 0:04:50, Đó chính là cái dữ liệu huấn luyện
00:04:50 - 0:05:01, Và mình sẽ phải tìm cái biến số theta làm sao cho cái việc dự đoán này là chính xác nhất
00:05:01 - 0:05:10, Và cái việc tìm cái giá trị theta cho cái này chính xác nhất thì nó sẽ tương đương với cái việc là chúng ta đi tìm một cái hàm min
00:05:10 - 0:05:15, Tìm theta sao cho hàm lỗi là đạt giá trị nhỏ nhất
00:05:15 - 0:05:21, Thế thì ba cái công việc chúng ta cần phải làm khi xây dựng một cái mô hình dựa trên Gradient
00:05:21 - 0:05:28, Một, đó là chúng ta sẽ xác định xem cái hàm fx của mình nó sẽ có cái dạng thức như thế nào
00:05:28 - 0:05:38, Thứ hai, đó là chúng ta sẽ thiết kế cái hàm lỗi là g theta x sao cho cái việc mà dự đoán càng chính xác thì cái lỗi của mình thấp
00:05:38 - 0:05:50, Nhưng đó chưa phải là một cái tiêu chí để thiết kế một cái hàm lỗi càng chính xác thì càng nhỏ
00:05:50 - 0:05:56, Nhưng cái đó chưa phải là một cái tiêu chí duy nhất mà chúng ta sẽ còn rất nhiều những cái tiêu chí khác
00:05:56 - 0:06:06, Mình có thể kể một vài cái tiêu chí ví dụ như là nó có thể hoạt động tốt khi chúng ta làm việc hoạt động tốt
00:06:07 - 0:06:16, khi huấn luyện với dữ liệu mà mất cân bằng
00:06:22 - 0:06:31, Tức là cái y này của mình nó sẽ có nhiều class ví dụ vậy và có những class thì xuất hiện rất nhiều nhưng có những cái class rất ít
00:06:31 - 0:06:45, Thì cái hàm lỗi này nó phải làm sao để cho hướng cái mô hình đến cái việc là kể cả những mẫu dữ liệu mà ít thì vẫn có thể được cho cái vai trò ngang bằng với lại những cái mẫu dữ liệu nhiều
00:06:45 - 0:06:54, Rồi ngoài ra thì chúng ta sẽ có những cái tiêu chí nữa ví dụ như hàm lỗi như thế nào để cho cái việc huấn luyện nhanh, hội tụ, huấn luyện nhanh
00:06:54 - 0:06:59, Tức là nó sẽ mau chóng để mà tìm ra được cái tham số tốt nhất
00:06:59 - 0:07:08, Rồi cái việc thiết kế hàm mô hình cũng vậy nó cũng sẽ dựa trên cái tính chất của y, cái giá trị thực tế với x để mà chúng ta sẽ thiết kế
0:07:08 - 0:07:14, Ví dụ như nếu y mà phụ thuộc một cách tuyến tính với x thì chúng ta sẽ có các cái hàm tuyến tính
0:07:14 - 0:07:21, Nhưng nếu mà y phụ thuộc một cách phi tuyến với là x thì chúng ta sẽ có các cái hàm là phi tuyến tính
0:07:21 - 0:07:34, Rồi sau này là tùy thuộc với x của mình, nó là dữ liệu dạng vector, dạng ma trận hay là dữ liệu như thế nào đó mà chúng ta cũng sẽ có những kiểu thiết kế khác nhau
0:07:34 - 0:07:45, Và cái công việc cuối cùng khi chúng ta làm với một cái mô hình mà dựa trên Gradient, đó chính là chúng ta sẽ tìm một cái tham số tối ưu của hàm mô hình
0:07:45 - 0:07:56, Tức là chúng ta sẽ đi tìm cái theta, theta sao? Sao cho cái lỗi ở đây là nhỏ nhất? Thì lỗi mà nhỏ nhất là cái dự đoán càng chính xác
0:07:56 - 0:08:08, Tiếp theo thì chúng ta sẽ cùng so sánh với các cái mô hình khác, thì một số mô hình mà không có dựa trên Gradient, ví dụ như chúng ta có K-Nearest Neighbor
0:08:08 - 0:08:19, Mặc dù đây là một cái thuật toán máy học nhưng mà nó không thực sự là huấn luyện và bản chất của nó chỉ là truy vấn để tìm ra k cái láng giềng gần nhất
0:08:19 - 0:08:35, Và sau đó sẽ dựa trên nhãn của k cái láng giềng đó để từ đó nó dùng cái cơ chế đó gọi là voting để mà lấy ra những cái tập nhãn mà có xuất hiện nhiều nhất để từ đó gắn cái nhãn nhiều nhất đó vào cho cái đặc trưng của mình
0:08:35 - 0:08:48, Thì đây chính là cái ý tưởng của K-Nearest Neighbor. Hướng tiếp cận thứ 2 đó chính là Naive Bayes, thì đây là dựa trên các cái mô hình xác suất mà cụ thể đó là chúng ta dựa trên công thức xác suất có điều kiện như là công thức Bayes
0:08:48 - 0:08:54, Thì ước lượng cái phương pháp này thì nó sẽ ước lượng các cái tham số một cách tường minh
0:08:54 - 0:09:07, Cách tiếp cận thứ 3 đó chính là Decision Tree với các thuật toán ví dụ như là CART, ID3, C4.5 thì nó dựa trên luật để phân chia thành các cái nhánh quyết định
0:09:07 - 0:09:18, Ví dụ như ở đây chúng ta sẽ có một cái node, nó sẽ chia ra làm 2, 2 nhánh ví dụ vậy rồi sau đó chúng ta lại tiếp tục có những cái luật, nó sẽ có những cái luật lại tiếp tục chia xuống
0:09:18 - 0:09:32, Ví dụ như ở trên đây, cái luật của mình sẽ là trời có mây hay không, thì nếu có thì nó sẽ tiếp tục hỏi là cái độ ẩm của mình là cao hay thấp
0:09:32 - 0:09:41, Nếu mà độ ẩm cao thì nó sẽ kết luận là mưa, ví dụ vậy, thì đây là một cái mô hình khá là hiệu quả và dễ hiểu
00:09:41 - 0:09:50, Và mở rộng cho cái mô hình Decision Tree đó chính là Random Forest thì như cái tên gọi của mình, Random Forest nó sẽ kết hợp nhiều cái cây thành phần
00:09:50 - 00:10:00, Ví dụ như ở đây chúng ta có một cây thì Random Forest có thể kết hợp thêm nhiều cái cây khác để có thể tạo ra thành một cái khu rừng
00:10:00 - 00:10:13, Và Random Forest là một trong những thuật toán, một cái mô hình mà có thể chống được cái Overfitting và có cái tính tổng quát khá là cao để chúng ta chọn được cái tham số phù hợp
00:10:13 - 00:10:19, Thế thì cả 4 cái mô hình này đều là nằm trong cái nhóm đó là học có giám sát
00:10:19 - 00:10:29, Và thuật toán không giám sát thì chúng ta sẽ có các thuật toán liên quan đến cái Clustering
00:10:29 - 00:10:45, Ví dụ như là có K-Means, DBscan, Hierarchical Clustering, thì ý tưởng của các thuật toán này cũng là những thuật toán lặp việc cập nhật tâm cụm
00:10:45 - 00:10:48, Tức là chúng ta sẽ lặp đi lặp lại việc cập nhật tâm cụm
00:10:48 - 00:10:56, Và khác biệt so với các mô hình dựa trên Gradient đó là chúng ta không tính cái Vector đạo hàm, không dựa trên Gradient
00:10:56 - 00:11:02, Và trong cái bảng sau thì chúng ta sẽ so sánh các mô hình trên các khía cạnh khác nhau
000:11:02 - 0:11:06, Khía cạnh đầu tiên đó là cái cơ chế để tối ưu hóa mô hình của mình
0:11:06 - 0:11:12, Thì các thuật toán dựa trên Gradient thì đều dựa trên thuật toán Gradient Ascent
0:11:12 - 0:11:20, Và các biến thể của nó, ví dụ như Stochastic Gradient Ascent, Adam, Root Mean Square Propagation
0:11:20 - 0:11:23, thì đây đều là những cái thuật toán tối ưu hóa
0:11:23 - 0:11:28, Và các mô hình dựa trên Gradient thì đều dựa trên các thuật toán này
0:11:28 - 0:11:35, Trong khi đó các mô hình không dựa trên Gradient, thì cơ chế để tối ưu hóa dựa trên một công thức tường minh
0:11:35 - 0:11:43, hoặc dựa trên các chiến thuật tham lam, heuristic, ví dụ như là Naive Bayes, Decision Tree, K-Nearest Neighbor
0:11:43 - 0:11:47, Xét trên khía cạnh về khả năng diễn giải mô hình
0:11:47 - 0:11:51, thì các mô hình dựa trên Gradient thì thường có tính diễn giải khá là thấp
0:11:51 - 0:11:55, hay một cái cách gọi khác đó là thường có dạng Black Box, hộp đen
0:11:55 - 0:12:00, thì khả năng diễn giải của mình là sẽ thấp
0:12:01 - 0:12:10, Tuy nhiên các nghiên cứu gần đây thì họ cũng đã tìm cách trực quan hóa mô hình dựa trên Gradient
0:12:10 - 0:12:21, nó vận hành như thế nào, rồi giải thích cơ chế của nó để làm sao cho mô hình có thể tối ưu hóa được việc mà dự đoán
0:12:21 - 0:12:24, thì các nghiên cứu đó gần đây cũng được chú tâm rất nhiều
0:12:24 - 0:12:31, Trong khi đó thì mô hình không dựa trên Gradient thì nó có tính giải thích dễ dàng hơn
0:12:31 - 0:12:35, và đặc biệt là những mô hình như là Decision Tree, Random Forest
0:12:35 - 0:12:40, chúng ta nhìn vô cái cấu trúc cây thôi là chúng ta có thể hiểu được mô hình vận hành như thế nào
0:12:40 - 0:12:48, Xét trên cái hiệu quả của các tác vụ phức tạp thì thuật toán các mô hình dựa trên Gradient
0:12:48 - 0:12:55, thì nó sẽ cho kết quả rất tốt trên những lĩnh vực như là thị giác máy tính hoặc là xử lý ngôn ngữ tự nhiên
00:12:55 - 0:13:00, trong đó là trên những dữ liệu là Unstructured Data
0:13:04 - 0:13:11, ví dụ như là dữ liệu hình ảnh, dữ liệu văn bản, rồi dữ liệu âm thanh
0:13:11 - 0:13:18, và thì đây là những dữ liệu mô hình dựa trên Gradient làm việc rất tốt
0:13:18 - 0:13:23, trong khi đó mô hình không dựa trên Gradient thì thường tốt trên dữ liệu bảng
0:13:23 - 0:13:31, và có quy mô nhỏ ví dụ như là dữ liệu bảng thì nó bao gồm các cột ABC
0:13:31 - 0:13:39, và từng cái cột này thì nó sẽ có kiểu dữ liệu cố định và ý nghĩa của nó là cố định
0:13:39 - 0:13:45, các mô hình không dựa trên Gradient thì làm việc rất tốt trên dữ liệu này
0:13:45 - 0:13:48, đó là những dữ liệu Structured Data
0:13:53 - 0:13:57, và cuối cùng đó là xét trên khía cạnh chi phí huấn luyện
0:13:57 - 0:14:01, các mô hình dựa trên Gradient thì thường có chi phí huấn luyện rất cao
0:14:01 - 0:14:07, do nó cần rất nhiều tài nguyên tính toán, bộ nhớ, GPU, TPU
0:14:07 - 0:14:12, do các mô hình dựa trên Gradient có số lượng tham số rất lớn
0:14:12 - 0:14:19, ngược lại thì chi phí huấn luyện của các mô hình không dựa trên Gradient thì ít tốn kém hơn
0:14:19 - 0:14:26, như vậy chúng ta đã lượt qua những khía cạnh và chúng ta thấy mô hình dựa trên Gradient
0:14:26 - 0:14:33, nó có những điểm yếu cố hữu ví dụ như là mô hình của mình khả năng diễn giải sẽ thấp hơn
0:14:33 - 0:14:38, so với những mô hình như là Decision Tree và chi phí huấn luyện của nó sẽ cao hơn
0:14:38 - 0:14:43, tuy nhiên gần đây thì tại sao mô hình dựa trên Gradient lại càng trở nên phổ biến
0:14:43 - 0:14:49, nó có nhiều lý do, lý do đầu tiên đó là dưới sự phát triển của các mạng xã hội
0:14:49 - 0:14:55, thì các dữ liệu của mình sẽ ngày càng phong phú hơn và được lưu trữ công khai
0:14:55 - 0:15:04, thì ở giai đoạn đầu, ví dụ như là vào những năm 2010, thì cái quy mô dữ liệu của chúng ta đâu đó chỉ khoảng là 1-2 triệu ảnh
0:15:04 - 0:15:14, nhưng mà sau đó đến vào những năm 2020, cụ thể đó là công ty OpenAI được đầu tư thì họ xây dựng những mô hình
0:15:14 - 0:15:19, ví dụ như mô hình clip được huấn luyện trên tập dữ liệu rất lớn lên đến hàng trăm triệu mẫu dữ liệu
0:15:19 - 0:15:29, và gần đây hơn nữa thì vào những năm 2020, thì có tập dữ liệu LAION lên đến 5 tỷ ảnh
0:15:29 - 0:15:38, thì có thể nói là để huấn luyện được trên những quy mô dữ liệu lên hàng tỷ ảnh thì chỉ có thể có tập đoàn công nghệ lớn họ mới có thể làm được mà thôi
0:15:38 - 0:15:47, và một trong những lý do nữa để khiến mô hình dựa trên Gradient càng trở nên phổ biến đó là tài nguyên của mình
00:15:47 - 0:15:52, tài nguyên cụ thể là tài nguyên tính toán, nó ngày càng mạnh và đồng thời nó sẽ ngày càng rẻ
0:15:52 - 0:15:55, thì nói về phần cứng thì nó sẽ càng càng rẻ
0:15:55 - 0:16:02, và nhờ có tài nguyên tính toán sắp sau này, nó sẽ giúp chúng ta huấn luyện các mô hình nhanh chóng hơn
0:16:02 - 0:16:06, cuối cùng đó là sự hoàn thiện của các mô hình
0:16:06 - 0:16:10, chúng ta thấy đó là trước đây thì chúng ta có cái mạng Neural Network
0:16:10 - 0:16:16, thì nó không có hiệu quả trong việc huấn luyện với các dữ liệu lớn và các bài toán phức tạp
0:16:16 - 0:16:25, nhưng mà gần đây thì chúng ta thấy là có cái kiến trúc như là CNN, RNN, Convolutional Neural Network, Recurrent Neural Network
0:16:25 - 0:16:30, và gần đây nhất chính là Transformer, đó là những cái kiến trúc đã hoàn thiện hơn
0:16:30 - 0:16:38, giúp chúng ta có thể hấp thụ được lượng dữ liệu tốt hơn và có thể khai thác được hiệu quả tài nguyên của GPU
0:16:38 - 0:16:43, do đó thì các mô hình dựa trên Gradient đang ngày càng trở nên phổ biến
0:16:43 - 0:16:49, và không chỉ như vậy mà các thành tựu mới nhất của trí tuệ nhân tạo gần đây
0:16:49 - 0:16:57, ví dụ như là ChatGPT, Gemini, chúng ta thấy đó là đều có kiến trúc dựa trên Transformer
0:16:58 - 0:17:07, và cái kiến trúc dựa trên Transformer này, đó là dựa trên, đã đều được huấn luyện dựa trên thuật toán, đó là Gradient Descent
0:17:07 - 0:17:17, thì đây chính là một cái ví dụ minh chứng cho cái thành tựu của các mô hình dựa trên Gradient
0:17:27 - 0:17:32, Cảm ơn các bạn đã xem video hấp dẫn