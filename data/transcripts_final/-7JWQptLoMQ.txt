0:00:14 - 0:00:25, Chúng ta sẽ cùng tìm hiểu về sự tiến hóa của các mô hình dạng chuỗi, tức là cụ thể cho các mô hình kiểu như văn bản, có yếu tố thứ tự.
0:00:26 - 0:00:39, Trong hồi trước, chúng ta tìm hiểu về mô hình của mạng CNN liên quan đến loại dữ liệu đó là có yếu tố không gian, là không gian của ảnh bao gồm là chiều cao và chiều ngang.
0:00:39 - 0:00:54, Đối với mô hình dạng chuỗi, nó sẽ có một số đặc thù, đó là dữ liệu phụ thuộc theo 1 chiều, đối với dữ liệu về hình ảnh thì nó phụ thuộc 2 chiều.
0:00:54 - 0:01:12, Vậy thì chúng ta sẽ bắt đầu với mạng Neural Network. Vấn đề đối với mạng Neural Network khi chúng ta áp dụng vào dữ liệu văn bản đó là vì dữ liệu văn bản là một loại dữ liệu đặc biệt mà nó có độ dài là không cố định.
0:01:12 - 0:01:22, Trong khi đó chúng ta biết là trong một cái mạng Neural Network thì cái đầu vào của chúng ta là có số lượng Neural là cố định.
0:01:23 - 0:01:37, Cái số lượng Neural này là cố định. Ví dụ chúng ta có một cái 2 cái tình huống đó là tuyệt quá là chúng ta chỉ có 2 giá trị, 2 từ trong khi cái đầu vào của chúng ta có đến 4 Neural.
0:01:37 - 0:01:55, Rồi bầu trời xanh và nắng vàng óng ánh, ví dụ vậy thì nó sẽ có nhiều hơn 4 từ. Thì cái việc này nó khiến cho cái việc chúng ta sử dụng cái mạng Neural Network để phục vụ cho các bài toán bên xử lý ngôn ngữ tự nhiên là trở nên không khả thi.
0:01:55 - 0:02:04, Thế thì đâu đó cũng sẽ có một số giải pháp để giải quyết cái tình huống này, đó là chúng ta có thể sử dụng cái vector là Bag-of-Words.
0:02:04 - 0:02:17, Tức là Bag-of-Words nó sẽ biến mọi câu, mọi văn bản thành một cái vector cố định về số chiều. Tức là cái số chiều của mình là cố định.
0:02:17 - 0:02:31, Thì khi đó chúng ta map cái số chiều này vào cái input này thì nó sẽ giúp cho chúng ta không có bị vấn đề là sự chênh lệch về kích thước của dữ liệu đầu vào.
0:02:32 - 0:02:53, Tuy nhiên nếu chúng ta dùng cái phương pháp Bag-of-Words thì nó lại không quan tâm đến cái yếu tố thứ tự của từ. Tức là từ Do you, cái câu là Do you understand? Với lại you do understand?
0:02:53 - 0:03:12, Thì nếu biểu diễn bằng cái phương pháp Bag-of-Words thì hai cái câu này có cái vector giống nhau. Cái từ Do nó bật lên, từ you bật lên, đây là từ Do, đây là từ understanding.
0:03:12 - 0:03:24, Tại vì cái thứ tự của từ điển của mình mình sẽ không biết trước ha. Có thể là sắp theo thứ tự alpha, rồi từ you.
0:03:24 - 0:03:38, Thế thì cả hai cái câu Do you understand và you do understand thì đều có chung một cái vector biểu diễn, đó là những cái vị trí của từ Do từ you và từ understand sẽ bật lên là 1, còn những vị trí còn lại sẽ để là 0.
0:03:38 - 0:03:51, Nhưng mà cách làm này rõ ràng là không phù hợp vì cái câu Do you understand? Đây là bản chất là một cái câu hỏi. Còn you do understand? Đó là bản chất là một câu trả lời.
0:03:51 - 0:04:03, Hoặc là cái câu khẳng định. Đây là cái câu khẳng định. Thì dẫn đến đó là khác nhau hoàn toàn về mặt ngữ nghĩa.
0:04:03 - 0:04:14, Thì ở đây chúng ta sẽ thấy là nếu từ Do đặt trước từ you thì đó sẽ là câu hỏi. Nhưng mà từ Do đặt sau từ you thì nó lại là câu khẳng định.
0:04:14 - 0:04:29, Vậy thì mạng Recurrent Neural Network nếu chúng ta dùng một cách cơ bản như thế này thì hoàn toàn không thể giúp chúng ta mã hóa được và biểu diễn được đầy đủ cái ngôn ngữ của mình.
0:04:29 - 0:04:42, Do đó thì chúng ta sẽ có kiến trúc ANN. ANN thì nó sẽ encode cái thứ tự của các từ thông qua cái dấu mũi tên từ trái sang phải như thế này.
0:04:43 - 0:04:55, Ý nghĩa của cái việc ANN mã hóa thứ tự này đó là gì? Tại cái thời điểm t, chúng ta đưa vào một cái từ thứ t thì đến cái node hidden ở giữa đây, nó sẽ nhận thông tin của quá khứ.
0:04:55 - 0:05:16, ST chính là cái thông tin của quá khứ. Nó chứa cái thông tin của cái từ ST trừ 1 trước đó. Như vậy thì với cái dấu mũi tên này nó sẽ xử lý, nó sẽ nhận đầu vào là có cả thông tin của quá khứ lẫn thông tin của hiện tại.
0:05:16 - 0:05:31, Nhưng cái thông tin của cái từ hiện tại sẽ được có cái tín hiệu nó nhiều hơn, nó sẽ thể hiện được là nó đến sau. Còn cái ST trừ 1 nó sẽ chứa cái thông tin của từ ST trừ 1.
0:05:31 - 0:05:43, Nhưng mà vì nó đến sau nên cái số thao tác biến đổi trên ST trừ 1 của mình nó rất là nhiều. Thì cái lượng thông tin của nó sẽ bị mất mát.
0:05:43 - 0:05:53, Vậy thì đây chính là cái cách mà ANN đã encode cái thứ tự thông qua cái trình tự thực hiện các phép biến đổi.
0:05:53 - 0:06:06, Thế thì chi tiết cái nội dung tính toán của mình đó là gì? Thì tại cái thời điểm thứ t thì chúng ta sẽ tính bước số 1. Đó là chúng ta sẽ lấy thông tin của quá khứ.
0:06:06 - 0:06:21, Đây là thông tin của quá khứ. Kết hợp với lại cái thông tin của hiện tại để chúng ta có một cái thông tin đầy đủ vừa có quá khứ vừa có hiện tại.
0:06:21 - 0:06:38, Tức là chúng ta đang tính cái ST ở đây. Và hai ma trận U và W, mục đích của nó đó là đưa cái vector ST và ST trừ 1 là ở hai cái không gian khác nhau về cùng một không gian.
0:06:38 - 0:06:52, Vậy W và U sẽ giúp ST và cái thông tin quá khứ ST trừ 1 về cùng một không gian để mà chúng ta tính toán.
0:06:52 - 0:07:09, Và khi ST đã được trộn cái thông tin của cả quá khứ và hiện tại thì chúng ta sẽ tiến đến cái bước số 2. Đó là chúng ta sẽ bắt đầu decode.
0:07:09 - 0:07:23, Chúng ta sẽ đi tính cái giá trị dự đoán. Đó là y ngã t. Y ngã t được tính trực tiếp từ ST. Thì công thức của mình cũng rất đơn giản. Đó là softmax của V nhân ST.
0:07:23 - 0:07:52, Bản chất của V này đó là ánh xạ hoặc là chuyển từ cái không gian của cái lớp hidden này về cái không gian của cái output.
0:07:53 - 0:08:09, Thì ở đây chúng ta sẽ có 3 cái tham số đó là U, W và V. Thì 3 cái tham số này chúng ta để ý là nó xuất hiện trên tất cả các cái giá trị thời gian.
0:08:09 - 0:08:33, Tức là 3 cái giá trị U, V, W này là chia sẻ trọng số. V, W là share parameter. Vậy là cho dù cái văn bản của chúng ta có dài như thế nào đi chăng nữa thì nó vẫn dùng cùng một cái bộ tham số U, V, W.
0:08:33 - 0:08:51, Thì cái ANN đó là một cái mô hình mà nó có đa tác vụ cho lĩnh vực về NLP. Tại sao gọi là đa tác vụ? Tại vì với một cái kiến trúc của ANN chúng ta có thể giải quyết rất nhiều những cái bài toán khác nhau trong lĩnh vực xử lý ngôn ngữ tự nhiên.
00:08:51 - 0:09:10, Ví dụ, 1 to 1, tức là từ một cái từ đầu vào chúng ta sẽ tạo ra một cái từ đầu ra. Và đây có thể dùng trong cái lĩnh vực ví dụ như là dịch từ, rồi 1 to many. Ví dụ như chúng ta có cho trước một cái từ này là cái chủ đề.
0:09:10 - 0:09:29, Cái chủ đề của một cái bài thơ chẳng hạn và output của chúng ta sẽ là bài thơ. Many to 1 thì một cái ứng dụng khá là phổ biến và kinh điển. Đó chính là sentiment analysis, tức là phân loại cảm xúc văn bản. Đầu vào của chúng ta sẽ là một cái văn bản.
0:09:29 - 0:09:46, Và đầu ra sẽ cho biết là cái sentiment của nó là gì? Là positive hay là negative hay là neutral? Và cái tình huống phía sau đó là many to many thì nó sẽ có hai dạng dạng 1 và dạng 2.
0:09:46 - 0:09:58, Cả hai dạng này thì ứng dụng trong thực tế cũng rất là nhiều. Ví dụ như chúng ta nhận cái dữ liệu đầu vào sau đó chúng ta sẽ trả lời. Thì đây là cái ứng dụng trong lĩnh vực về chatbot.
0:09:58 - 0:10:14, Hoặc là lĩnh vực về dịch máy. Hoặc là về lĩnh vực, ví dụ như tóm tắt văn bản. Thì cái many to many dạng 1 có rất nhiều những cái ứng dụng hiện đại.
0:10:14 - 0:10:23, Many to many dạng 2 tức là nó khác biệt so với dạng 1 đó là đối với dạng 1 chúng ta phải xem hết toàn bộ input xong đó chúng ta mới đi trả lời.
0:10:23 - 0:10:39, Còn many to many dạng 2 thì chúng ta chỉ... chúng ta nhận được đến đâu thì chúng ta sẽ ra cái output trên đó. Nhận đến đâu và ra output trên đó thì có thể dùng trong cái bài toán là part-of-speech hoặc là gán nhãn từ loại.
0:10:39 - 0:10:51, Vì cái slide này chúng ta có thể thấy đó là ANN nó có tính đa tác vụ rất là cao. Nó có thể áp dụng cho rất nhiều những cái thể loại bài toán khác nhau của lĩnh vực ANN.
0:10:52 - 0:11:05, Bây giờ chúng ta sẽ đến cái kiến trúc của các biến thể của mạng ANN thì ở bên trái đó là một cái dạng biểu diễn dạng node như thế này là một cái ô tròn.
0:11:05 - 0:11:22, Nhưng mà để hình dung nó ở góc độ là vector thì chúng ta sẽ dùng cái dạng biểu diễn bên tay phải. Ví dụ cái từ đ thì nó sẽ có cái vector embedding tương ứng của nó và chúng ta sẽ encode nó thành một cái từ s, s1.
0:11:22 - 0:11:39, Thì cái s1 của mình nó sẽ là một cái vector biểu diễn tương tự như vậy. Movie, thông qua cái vector embedding của từ Movie chúng ta sẽ tính toán để tạo ra cái vector s2, cứ như vậy cho đến s5.
0:11:39 - 0:11:56, Cái cách biểu diễn này nó sẽ hình dung được, giúp chúng ta hình dung được cái dạng vector biểu diễn của các trạng thái ẩn trong mạng ANN. Còn cách bên phải ở đây thì nó sẽ cho chúng ta thấy được cái quy trình đi từ đâu đến đâu.
0:11:56 - 0:12:05, Nhưng mà nó sẽ khiến chúng ta nhầm lẫn rằng là đây là một cái giá trị scalar. Thực tế không phải, nó sẽ phải là một cái vector.
0:12:06 - 0:12:17, Thế thì chúng ta sẽ cùng đến với cái biến thể đầu tiên, đó là Bi-directional ANN. Thì cái vấn đề của các cái mạng ANN trước đây đó là gì? Đó là ANN chỉ đọc dữ liệu một chiều.
0:12:17 - 0:12:29, Ví dụ chúng ta có The Movie World terribly, thì nếu như chúng ta đọc đến cái chữ terribly, thì cái từ này nó sẽ mang cái ý nghĩa đó là negative.
0:12:33 - 0:12:42, Nó sẽ mang cái ý nghĩa là negative. Và nó mang cái nghĩa negative này thì nó sẽ truyền ra cái output, thì ở đây nó sẽ mang cái giá trị là negative.
0:12:43 - 0:12:54, Nhưng để mà biết được cái ý nghĩa thực sự của nó là positive hay negative thì chúng ta phải nhìn thấy được cái từ phía sau.
0:12:54 - 0:13:04, Đó là terribly exciting, tức là cái bộ phim này thì rất là hào hứng một cách khủng khiếp. Hoặc là chúng ta hay nói là cái bộ phim này là hay khủng khiếp.
0:13:04 - 0:13:13, Thì cái từ khủng khiếp nếu bắt đầu chúng ta nghe thì nó sẽ là tiêu cực nhưng mà có thêm cái từ hay thì nó sẽ là tích cực. Ở đây cũng hoàn toàn tương tự.
0:13:13 - 0:13:27, Cái từ exciting này là một cái nghĩa tích cực. Nhưng nó lại xuất hiện sau cái từ terribly dẫn đến là chúng ta không thấy được cái từ này để đưa ra cái nhận định là terribly này là tích cực hay tiêu cực.
0:13:27 - 0:13:43, Do đó thì khi chúng ta đi từ trái sang phải thì cái từ terribly này tự thân nó được hiểu là nghĩa tiêu cực. Do đó thì cái việc nhìn một chiều này sẽ khiến chúng ta đưa ra cái output nó không có toàn diện.
0:13:43 - 0:13:58, Do đó thì cần có cái ngữ cảnh để đọc từ bên phải. Tức là chúng ta cần phải có cái thông tin ngữ cảnh của các từ bên phải để bổ trợ thêm cho cái từ ở bên trái.
0:13:58 - 0:14:12, Do đó thì chúng ta sẽ có một cái bước nữa, nó sẽ có một cái module nữa để duyệt theo chiều ngược lại. Vậy thì cái biến thể Bi-directional ANN thì cái phần màu xanh là đi theo chiều thuận.
0:14:12 - 0:14:32, Còn cái B St là từ trái sang phải. Còn cái phần màu cam thì ở đây có thể là do chúng ta dùng màu sắc nó bị đảo ngược. Màu xanh sẽ là từ trái sang phải, màu cam sẽ là từ phải sang trái.
0:14:32 - 0:14:45, Vậy thì hai cái màu này đang đảo ngược. Chúng ta chỉ cần để ý vào cái sơ đồ ở đây. Khi chúng ta có được cái thông tin tại một cái từ nhưng mà đọc từ trái sang phải và từ phải sang trái.
0:14:45 - 0:15:02, Ví dụ cái từ worst này đi, cái từ terribly này đi. Thì nó đã có được đầy đủ thông tin. Nó sẽ có cái thông tin khi chúng ta đọc từ trái sang và đồng thời và cái vector màu cam này nó sẽ có được thông tin khi chúng ta đọc từ phải sang.
0:15:02 - 0:15:20, Thì cái từ terribly này sẽ thấy được cái từ exciting ở phía trước. Và khi chúng ta concatenate hai cái vector này lại với nhau thì cái vector này nó sẽ có cái tính đầy đủ toàn diện hơn. Và khi đó thì nó sẽ tạo ra cái output của mình nó sẽ chuẩn xác hơn.
0:15:20 - 0:15:41, Thì đây chính là cái biến thể Bi-directional ANN. Chúng ta sẽ cùng đến với cái biến thể tiếp theo đó là DeepStack ANN. Vấn đề ở đây đó là gì? Để mà có cái DeepStack ANN. Vấn đề đó là ANN. Cho đến bây giờ thì bản chất nó chỉ là một, nó chưa phải là một cái mô hình thật sự là sâu.
0:15:41 - 0:15:55, Cái sâu ở đây là nó mới chỉ sâu theo cái trục thời gian là đi theo cái trục này. Khi cái văn bản của mình ngắn thì nó không sâu nhưng mà khi văn bản của mình nó dài, những cái đoạn văn bản dài thì đó là một cái mô hình sâu theo trục thời gian.
0:15:55 - 0:16:03, Nhưng nếu xét về yếu tố đặc trưng thì cái đặc trưng này vẫn là một cái đặc trưng thấp, đặc trưng thấp thấp.
0:16:03 - 0:16:17, Hay nó cách khác đó là đặc trưng đơn giản. Nó không thể giúp chúng ta giải quyết được các bài toán phức tạp. Do đó chúng ta cần có một cái nhu cầu, đó là tăng cái độ sâu theo cái trục đứng như thế này.
0:16:18 - 0:16:27, Thì để làm chuyện đó chúng ta sẽ dùng cái kiến trúc đó là DeepStack chúng ta sẽ trồng nhiều lớp lên với nhau. Ở đây là layer 1, layer 2 và layer 3, nó được trồng lên nhau.
0:16:27 - 0:16:44, Và với mỗi cái layer thì chúng ta sẽ tổng hợp được cái thông tin đặc trưng ở một cái cấp độ. Ví dụ như ở đây sẽ là low level feature, đây là mid level feature, đây là low và đây sẽ là high level feature.
0:16:44 - 0:17:01, Thì khi đến cái đặc trưng ở trên tầng số 3 thì nó cũng giống như cái mạng CNN của một xử lý ảnh. Đó là đặc trưng của mình đã có cái tính chất gọi là đặc trưng phức tạp hơn, phi tuyến tính hơn.
0:17:01 - 0:17:13, Thì hy vọng là nó giúp chúng ta giải quyết được cái bài toán khó. Thì như vậy khi chúng ta trồng các layer lên thì chúng ta sẽ có cái trạng thái ẩn từ lớp thứ y.
0:17:13 - 0:17:29, Thì SI sẽ là đầu vào cho cái layer thứ y cộng 1, như vậy cái trạng thái ẩn của layer thứ y chính là cái SI. Nó sẽ là cái input để đi tính cái SI cộng 1.
0:17:29 - 0:17:37, Thì layer số 1 nó sẽ truyền lên layer số 2, rồi lên layer số 3. Thì đó là cái ý nghĩa.
0:17:38 - 0:17:57, Và chúng ta sẽ có cái công thức ở layer số 1 là S1 là ở đây. Và ở đây, ví dụ như là S1 tại vị trí T, thì nó sẽ được tính toán giống như một cái ANN bình thường.
0:17:57 - 0:18:06, Nó cũng sẽ nhận vào cái thông tin của quá khứ tại cái tầng thứ 1. Nó nhận thông tin từ đây sang.
0:18:09 - 0:18:18, Rồi kết hợp với thông tin tại cái thời điểm XT hiện tại để tổng hợp ra cái S1T. Sau đó chúng ta sẽ đi tính S2T.
0:18:19 - 0:18:32, Nó sẽ tổng hợp thông tin từ 2 phía. Thứ nhất là từ quá khứ. Nhưng mà quá khứ tại cái tầng thứ 2, tức là ST-1-2.
0:18:33 - 0:18:39, Sau đó nó sẽ tổng hợp, nó sẽ nhận cái thông tin từ ST1, S1T từ dưới lên.
0:18:39 - 0:18:48, Tức là layer số 2 sẽ được tính toán từ layer thứ 1. Đây chính là chúng ta đang tạo ra một đặc trưng mới.
0:18:48 - 0:19:09, Một đặc trưng tổng hợp mới từ cái lớp phía trước đó. Tương tự như vậy, từ S2T, chúng ta sẽ đi tính S3T.
0:19:09 - 0:19:17, Và đồng thời nó sẽ còn có kết hợp thông tin của S3T-1 ở phía trước là thông tin của quá khứ, ở tầng thứ 3.
0:19:18 - 0:19:23, Như vậy thì đây chính là công thức và kiến trúc của DeepStack ANN.
0:19:24 - 0:19:31, Trong một số tài liệu người ta luôn khuyến nghị, đó là chúng ta nên sử dụng DeepStack nhưng mà không nên dùng quá nhiều.
0:19:31 - 0:19:36, Ví dụ như là chúng ta dùng từ 2 cho đến khoảng 4 lớp là vừa đủ.
0:19:37 - 0:19:43, Còn chưa có nhiều tài liệu nói là khi chúng ta dùng thêm nhiều hơn nữa, ví dụ như là hàng chục hoặc hàng trăm lớp.
0:19:44 - 0:19:54, Thường là từ 2 cho đến 4 lớp. Riêng trong Transformer thì có thể là với dữ liệu phức tạp hơn, bài toán thách thức hơn, thì có thể là số lớp của mình sẽ cao hơn.
0:19:55 - 0:20:04, Thế thì chúng ta đã học về Bi-directional và DeepStack, do đó chúng ta cũng sẽ có một hỗn hợp của hai biến thể này.
0:20:05 - 0:20:16, Đó là vừa Bi-directional, tức là đi theo chiều từ trái sang phải, nhưng đồng thời cũng sẽ có chiều đi từ phải sang trái, thông qua dấu mũi tên nét đứt này.
0:20:16 - 0:20:24, Và nó sẽ stack lên nhiều tầng, thì đây sẽ là layer 1, đây là layer 2 và đây là layer 3.
0:20:25 - 0:20:36, Nó sẽ kết hợp vừa 2 chiều và vừa DeepStack để tận dụng được thế mạnh của từng cái biến thể của mình.
0:20:37 - 0:20:44, Đó là thứ nhất, đọc được nhiều chiều, cái thứ 2, đặc trưng của mình sẽ phân cấp tốt hơn, có nhiều thông tin hơn.
0:20:46 - 0:20:56, Hãy subscribe cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video hấp dẫn.