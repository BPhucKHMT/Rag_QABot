0:00:14 - 0:00:17, Trong phần trước chúng ta đã được tìm hiểu về momentum
0:00:17 - 0:00:25, Thì đây có thể nói là một trong những lý thuyết rất là quan trọng để làm tiền đề cho tất những cái biến thể biến thể về sau
0:00:25 - 0:00:28, Của các cái Optimizer, các cái thuật toán về tối ưu hóa
0:00:28 - 0:00:38, ý tưởng chính của Momentum đó là nó sẽ tận dụng được động năng của quá khứ để giúp cho chúng ta thoát ra khỏi điểm cực tiểu cục bộ.
0:00:38 - 0:00:44, Tức là tại vị trí này thì thế năng, tức là đạo hàm của mình nó mất, nhưng mà bù lại nó sẽ tận dụng được những thành phần
0:00:44 - 0:00:53, và động năng ở phía trước, nó sẽ giúp cho chúng ta thoát ra và hy vọng là đến được những khu vực có cực tiểu tốt hơn.
0:00:53 - 0:01:03, Và vấn đề của momentum đó là gì? Đối với những khu vực có độ dốc bất thường thì ví dụ như chúng ta thấy trong sơ đồ này
0:01:03 - 0:01:17, Cái phác thảo của HempLoss, chúng ta thấy là cái độ dốc của mình tăng lên, nó dốc xuống rất là cao, sau đó lập tức nó đi ngang, rồi sau đó nó lại lập tức đi lên
0:01:17 - 0:01:27, Tức là nó thay đổi trạng thái, đi xuống, đi ngang và sau đó là đi lên một cách rất là ngắn như vậy, thì đó chính là cái độ dốc bất thường
0:01:27 - 0:01:35, và nó sẽ khiến cho nó có rất nhiều những cái dao động mà bật qua bật lại, bật qua bật lại giữa hai cái thành dốc này.
0:01:35 - 0:01:39, Cụ thể hơn, đó là chúng ta xét tại một cái điểm ở đây.
0:01:39 - 0:01:45, Thì nếu như cái thành phần mà theo cái theta 1 của mình, mà đạo hàm của mình nó lớn,
0:01:45 - 0:01:50, tức là cái giá trị độ lớn này, nó lớn hơn so với lại cái thành phần theo theta 2,
0:01:50 - 0:01:59, khi chúng ta tổng hợp lại là cái vector tổng hợp thì nó sẽ bị thiên lệch về phía có thành phần radian lớn
0:01:59 - 0:02:01, tức là cụ thể đây là theta 1
0:02:01 - 0:02:06, khi đó là nó sẽ không có cân bằng mà nó sẽ bật qua bên tay phải
0:02:06 - 0:02:13, sau đó tại vị trí này chúng ta lại tiếp tục tính cái radian và chúng ta lại thấy cái hiện tượng mất cân bằng này lập lại
0:02:13 - 0:02:22, thì khi chúng ta cập nhật thì nó cũng sẽ bị thiên lệch về cái phía mà có cái thành phần lớn hơn, đó chính là thành phần theta 1
0:02:22 - 0:02:30, tức là một khi cái đạo hàm của mình theo thành phần, một cái thành phần nào đó mà lớn thì nó sẽ gây ra cái hiện tượng là bật qua bật lại
0:02:30 - 0:02:39, trong khi đó cái đường tối ưu, ideal path thì lẽ ra nó phải là đường đường màu xanh ở đây, nó sẽ phải đi theo cái trục này
0:02:39 - 0:02:44, Nó phải đi theo cái trục này, hay là đi theo cái trục của theo cái hướng của theta, theta2
0:02:44 - 0:02:47, Thì ở đây nó lại cứ bật qua trái, bật qua phải
0:02:47 - 0:02:51, Thì bây giờ chúng ta sẽ cải tiến cái momentum bằng cách nào
0:02:51 - 0:02:56, Thì muốn cải tiến thì chúng ta sẽ phải xem cái nguyên nhân của nó là gì
0:02:56 - 0:03:02, Nguyên nhân đó là vì khi chúng ta có hai cái thành phần radian theo theta1 và theta2
0:03:02 - 0:03:09, Nếu thành phần nào đó lớn thì lẽ ra chúng ta sẽ phải giảm learning rate của nó xuống
0:03:09 - 0:03:21, Nguyên nhân đó là do chúng ta dùng chung learning rate alpha
0:03:21 - 0:03:35, dùng cho g, g là đạo hàm của J theo theta 1 và đạo hàm của J theo theta 2
0:03:35 - 0:03:40, một cách tổng quát thì nó có thể là có theta 3 theta n
0:03:40 - 0:03:48, thì alpha a đang dùng chung cho radian theo theta 1 và radian theo theta 2
0:03:48 - 0:03:56, Bây giờ chúng ta mong muốn mỗi thành phần này sẽ có 1 cái alpha riêng
0:03:56 - 0:04:03, Và nó sẽ giúp chúng ta cân bằng lại, đó chính là ý tưởng của cải tiến adaptive learning rate
0:04:03 - 0:04:10, Giảm dao động dựa trên độ lớn của radian cập nhật gần đây, chúng ta giảm sự dao động đó
0:04:10 - 0:04:19, Ý tưởng chính đó là, giả sử chúng ta có vector g là bằng hai thành phần, là g1 và g2
0:04:19 - 0:04:27, Nếu như chúng ta lấy alpha nhân với g, thì alpha mà dùng chung
0:04:27 - 0:04:34, Thì không, bây giờ chúng ta sẽ dùng riêng, mỗi cái thành phần này sẽ có một cái alpha riêng
0:04:34 - 0:04:40, trong đó, ở đây là ý tưởng đầu tiên là tách riêng ha, tách learning rate cho từng tham số
0:04:40 - 0:04:45, và cái ý tưởng tiếp theo, đó là cái radian mà lớn thì learning rate nó sẽ nhỏ
0:04:45 - 0:04:51, tức là nếu cái thành phần theo G1 mà lớn, G2 mà nhỏ, ví dụ vậy
0:04:51 - 0:04:57, thì chúng ta sẽ có cái hệ số alpha 1 cân bằng ngược trở lại
0:04:57 - 0:05:00, nó cân bằng ngược trở lại
0:05:00 - 0:05:11, và thành phần G2 mà nhỏ thì chúng ta sẽ có cái Alpha 2
0:05:11 - 0:05:20, và khi đó thì cái vector tổng hợp của mình nó sẽ đều hơn thay vì là nó bị thiên lệch như thế này
0:05:20 - 0:05:24, Đường màu đỏ là đường mà nó bị thiên lệch.
0:05:28 - 0:05:32, Nó bị lệch về phía theta, phía theta 1.
0:05:32 - 0:05:37, Rồi, bằng ngược lại thì thành phần theta 2 nhờ có cái alpha 2 lớn,
0:05:37 - 0:05:41, nó sẽ kéo ra để cho nó cân bằng cả hai hướng.
0:05:42 - 0:05:46, Và chi tiết thuật toán Root Mean Square Propagation,
0:05:46 - 0:05:56, đó là chúng ta sẽ khởi tạo với cái alpha là bằng 0.01 và beta thì đây là cái hệ số decay rate
0:05:59 - 0:06:03, tức là cái hệ số mà để cập nhật cho cái momentum
0:06:03 - 0:06:09, thì chúng ta nhìn một cái công thức của cái thuật toán root mean square là
0:06:09 - 0:06:15, V tức là cái vận tốc cập nhật tham số của mình là cái thành phần ở phía trên mà chúng ta khoanh vùng ở đây
0:06:15 - 0:06:25, Cách làm cũ của alpha là theta trừ alpha nhân cho nabla J
0:06:25 - 0:06:32, Alpha là đây và đạo hàm radian là G
0:06:32 - 0:06:43, Cách làm cũ là đạo hàm quá lớn, thành phần radian lớn sẽ lớn hơn những thành phần còn lại
0:06:43 - 0:06:57, R là momentum để phục vụ chuẩn hóa learning rate
0:06:57 - 0:07:04, Chuẩn hóa Learning Rate, tức là thành phần nào của radian này mà lớn thì cái alpha của nó sẽ nhỏ
0:07:04 - 0:07:09, và thành phần nào mà của radian này mà nhỏ thì cái alpha của mình nó sẽ lớn
0:07:09 - 0:07:13, Đó, thì cái cách làm của chúng ta là như vậy
0:07:13 - 0:07:19, R này nó là một cái vector, chúng ta thấy là R được in đậm, nó là một cái vector
0:07:19 - 0:07:23, thì khi alpha chia cho căn của một vector thì nó sẽ biến thành một vector
0:07:23 - 0:07:27, thì chút nữa chúng ta sẽ ghi rõ hơn cái công thức của nó
0:07:27 - 0:07:33, Đây là hệ số tỷ lệ tùy chỉnh, tức là Adaptive Scaling Factor
0:07:33 - 0:07:38, Với mỗi thành phần của G, chúng ta sẽ có một cái alpha riêng
0:07:38 - 0:07:43, Tại sao nó là như vậy? Tại sao alpha chỉ có một cái giá trị scalar là một giá trị?
0:07:43 - 0:07:47, Tại sao khi chia cho căn R nó lại biến thành hệ số tùy chỉnh?
0:07:47 - 0:07:50, Chút nữa chúng ta sẽ chứng minh chi tiết hơn
0:07:50 - 0:07:54, Mỗi tham số sẽ có một cái hệ số tỷ lệ khác nhau
0:07:54 - 0:08:01, và ở đây chúng ta sẽ thấy là nó có một cái thành phần hơi lạ đó là epsilon là bằng một cái con số rất là bé
0:08:01 - 0:08:07, thì ở đây đó là để chống cho cái việc là có một cái thành phần r nào đó mà bằng 0
0:08:07 - 0:08:14, có một cái thành phần trong r nào đó mà bằng 0 thì khi đó chúng ta sẽ bị cái lỗi là division by zero chia cho xuống 0
0:08:14 - 0:08:16, thì chúng ta sẽ cộng thêm epsilon để chống cái hiện tượng đó
0:08:16 - 0:08:24, Và phép toán mà r chia cho căn của epsilon cộng r được thực hiện trên từng phần tử
0:08:24 - 0:08:27, Bây giờ chúng ta sẽ cùng xem xét
0:08:27 - 0:08:34, Giả sử như g là bằng 2 thành phần là g1 và g2
0:08:34 - 0:08:41, Rồi, khi đó g-r là phép tích Hadamard trên từng phần tử
0:08:41 - 0:08:51, d-r-g là g1 bình phương, g2 bình phương
0:08:51 - 0:09:02, sau đó nó sẽ được cộng với thành phần chuẩn hóa ở phía quá khứ
0:09:02 - 0:09:09, đây là beta của quá khứ, tức là 90% thành phần r chuẩn hóa
0:09:09 - 0:09:22, R là thành phần để chúng ta chuẩn hóa learning rate theo kiểu đạo hàm thành phần nào mà càng nhỏ thì learning rate sẽ càng nhỏ
0:09:22 - 0:09:30, Trong công thức thành phần chuẩn hóa này chúng ta sẽ chia cho cái căn, thế thì tại sao nó lại có cái căn
0:09:30 - 0:09:35, Nếu chúng ta lại bỏ đi thành phần epsilon đây để cho nó dễ hình dung
0:09:35 - 0:09:40, epsilon này chỉ mang tính chất đó là chống cái việc chia cho 0 thôi
0:09:40 - 0:09:44, còn nó sẽ không có nhiều ý nghĩa lắm trong việc chuẩn hóa
0:09:44 - 0:09:53, thì khi chúng ta lấy alpha mà chia cho căn của epsilon cộng cho r
0:09:53 - 0:09:55, thì nó sẽ xấp xỉ
0:09:55 - 0:09:58, tại vì có cái epsilon này nên mình mới để cái dấu xấp xỉ
0:09:58 - 0:10:06, đó là alpha chia cho căn của vector g1 bình g2 bình
0:10:06 - 0:10:08, cộng cho thành phần quá khứ
0:10:08 - 0:10:11, nhưng tại thời điểm ban đầu chúng ta thấy là r bằng 0
0:10:11 - 0:10:18, nên xem như chúng ta tạm bỏ qua thành phần này để chúng ta dễ hình dung khái niệm của việc chuẩn hóa này là gì
0:10:18 - 0:10:24, thì là g1 bình phương và g2 bình phương
0:10:24 - 0:10:29, Thì ở đây là chúng ta thực hiện phép căn là phép căn trên ElementWise
0:10:29 - 0:10:32, do đó nó sẽ là căn của các thành phần bên trong
0:10:32 - 0:10:35, như vậy thì nó sẽ là trị tuyệt đối của G1
0:10:35 - 0:10:38, và trị tuyệt đối của G2
0:10:38 - 0:10:45, Rồi, và vì đây là cái phép chia trên phép áp dụng trên từng phần tử
0:10:45 - 0:10:48, do đó thì cái này nó sẽ là bằng một cái vector
0:10:48 - 0:10:52, trong đó alpha chia cho trị tuyệt đối của G1
0:10:52 - 0:10:56, và α chia cho trị tuyệt đối của G2
0:10:56 - 0:11:02, như vậy thì chúng ta nhìn lại nếu như G1 của chúng ta mà lớn
0:11:02 - 0:11:10, nếu G1 mà lớn thì khi đó α chia cho G1
0:11:10 - 0:11:14, đó chính là tương ứng cái α1 của mình đề cập trong slide trước
0:11:14 - 0:11:18, thì α1 là bằng α chia cho trị tuyệt đối G1
0:11:18 - 0:11:26, G1 lớn thì trị tuyệt đối của G1 sẽ lớn và 1 phần trị tuyệt đối của G1 sẽ nhỏ do đó thằng này sẽ là nhỏ
0:11:26 - 0:11:36, Ngược lại, nếu như G2 mà nhỏ thì khi đó là 1 phần trị tuyệt đối của G2 sẽ lớn do đó thành phần này sẽ lớn
0:11:36 - 0:11:39, như vậy là nó đáp ứng được yêu cầu mà chúng ta đã thiết kế ban đầu
0:11:39 - 0:11:44, thành phần gradient nào mà lớn thì learning rate sẽ nhỏ và ngược lại
0:11:44 - 0:11:51, Thì khi đó, đây chính là thành phần chấp chơi cho chúng ta chuẩn hóa
0:11:51 - 0:11:57, Thì khi đó cái công thức V này của mình sẽ biến thành là V là bằng
0:11:57 - 0:12:06, alpha 1, tức là alpha chia cho trị tuyệt đối của G1
0:12:06 - 0:12:08, nhân với lại G1
0:12:08 - 0:12:16, Thành phần thứ 2 sẽ là alpha chia cho trị tuyệt đối của G2
0:12:16 - 0:12:27, Ngoài việc chia chuẩn hóa này sẽ khiến cho 2 thành phần gradient cân bằng hơn
0:12:27 - 0:12:35, Đây là ý tưởng để giúp chúng ta thoát ra khỏi vấn đề thiên lệch
0:12:35 - 0:12:38, thì chúng ta xét trong hình ở bên tay phải
0:12:39 - 0:12:45, ở đây chúng ta thấy là tại vị trí A thì nó bị thiên lệch về hướng theta 1
0:12:45 - 0:12:53, nhưng mà nhờ có cái thành phần alpha 1 và alpha 2 được chuẩn hóa ở đây
0:12:53 - 0:12:55, đây là alpha 1, đây là alpha 2
0:12:57 - 0:12:58, thì khi đó
0:12:58 - 0:13:08, Khi đó, vector tổng hợp của mình thay vì nó bị lệch về phía này thì nó sẽ đi đều hơn.
0:13:08 - 0:13:15, Và khi đó thì chúng ta sẽ vẽ lại. Thay vì nó bật qua phải, rồi lại bật qua trái theo một cái góc rất là gắt như thế này,
0:13:15 - 0:13:19, thì nó sẽ bật qua một cái góc là 4,5 độ.
0:13:19 - 0:13:23, Rồi, qua đây nó sẽ bật một cái góc 4,5 độ.
0:13:23 - 0:13:28, Rồi, qua đây nó sẽ bật một cái góc 4,5 độ.
0:13:28 - 0:13:34, Thì chúng ta thấy là chỉ số bước mà nó di chuyển ít hơn nhiều so với đường màu đỏ này.
0:13:34 - 0:13:40, Thì cái đường màu xanh lá đã giúp chúng ta giảm bớt những cái hiện tượng bật qua bật lại.
0:13:53 - 0:14:03, Hãy subscribe cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video hấp dẫn