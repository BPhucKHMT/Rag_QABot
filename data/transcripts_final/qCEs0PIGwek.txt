0:00:14 - 0:00:29, Chúng ta sẽ cùng tìm hiểu về mô hình khuếch tán diffusion model và sự khác biệt giữa diffusion model so với mô hình variational autoencoder ra sao, quá trình khuếch tán được thực hiện như thế nào.
0:00:29 - 0:00:40, Chúng ta sẽ nhắc lại công thức của mô hình xác suất đằng trước, đó là log P của x sẽ là bằng tổng của hai kỳ vọng này.
0:00:40 - 0:00:52, Thì thành phần số 2, bản chất của nó chính là công thức của KL divergence của phân bố V, z cho trước x và P của z cho trước x.
0:00:52 - 0:01:07, Đây là kỳ vọng của một con số lớn hơn bằng 0, do đó chúng ta sẽ có cái chặn dưới của mình, đó chính là số hạng bên tay trái.
0:01:07 - 0:01:16, Cái số hạng bên tay trái, kỳ vọng này gọi là evidence lower bound, tức là cái chặn dưới của log Px.
0:01:16 - 0:01:31, Khi chúng ta huấn luyện, chúng ta sẽ tìm cách để vì chúng ta muốn cực đại hóa log Px, chúng ta sẽ đem cái chặn dưới, chúng ta cũng cực đại hóa nó luôn.
0:01:31 - 0:01:36, Khi chúng ta nâng cái chặn dưới lên thì đồng thời nó cũng sẽ nâng cái Px lên.
0:01:36 - 0:01:42, Và công thức này sau khi chúng ta biến đổi thì nó sẽ ra cái biểu thức bên tay phải.
0:01:42 - 0:01:59, Thì ở đây cụ thể là gì? Kỳ vọng của Qz cho trước x, log P của x và z sẽ là bằng P của x cho trước z, nhân với lại P của z.
0:01:59 - 0:02:03, Chia cho Qz của x.
0:02:03 - 0:02:05, Thì chúng ta sẽ tách cái thành phần này ra.
0:02:05 - 0:02:23, Chúng ta sẽ tách ra thì nó sẽ là bằng kỳ vọng của Qz cho trước x và log của Px cho trước z.
0:02:23 - 0:02:35, Sau đó chúng ta sẽ cộng cho cái log của tích của thành phần này và cái thành phần bên tay phải.
0:02:35 - 0:02:39, Thì chúng ta tách nó ra, log của tích sẽ là bằng tổng của 2 log.
0:02:39 - 0:02:52, Do đó nó sẽ là bằng cộng cho Qz cho trước x và log của Pz chia cho Qz cho trước x.
0:02:52 - 0:02:57, X sẽ là bằng log của Pz.
0:02:57 - 0:03:11, Sau đó là trừ cho kỳ vọng của Qz.
0:03:11 - 0:03:21, Rồi, kỳ vọng của log của Qz cho trước x.
0:03:21 - 0:03:31, Rồi, thế thì ở đây chúng ta có thể đảo thứ tự của nó lại.
0:03:31 - 0:03:38, Chúng ta đảo thứ tự lại bằng cách đó là cái vế này thì tương đương với lại cái biểu thức ở bên tay trái.
0:03:38 - 0:03:41, Còn cái vế này thì chúng ta sẽ đảo thứ tự lại.
0:03:41 - 0:03:49, Nó sẽ là bằng trừ của kỳ vọng của Qz cho trước x,
0:03:49 - 0:04:10, trừ cho kỳ vọng của Qz cho trước x,
0:04:10 - 0:04:18, nhân cho Pz.
0:04:18 - 0:04:28, Rồi, thì cái công thức này thì nó sẽ là bằng trừ của kỳ vọng của Qz cho trước x,
0:04:28 - 0:04:37, nhân cho log của Qz cho trước x, chia cho Pz.
0:04:37 - 0:04:45, Thì bản chất của cái công thức này, Q của x, log của Q của z cho trước x, chia cho x,
0:04:45 - 0:04:51, thì đây chính là KL divergence này.
0:04:51 - 0:04:58, Là KL divergence của Qz cho trước x.
0:04:58 - 0:05:05, Rồi, với cái Pz, vế bên trái là giống và vế bên phải cũng giống luôn.
0:05:05 - 0:05:06, Chúng ta có cái dấu trừ ở đằng trước.
0:05:06 - 0:05:12, Như vậy thì chúng ta đã chứng minh được cái công thức của cái elbow này.
0:05:12 - 0:05:16, Thế thì bây giờ, cái ý nghĩa của cái công thức này đó là gì?
0:05:16 - 0:05:20, Thì chúng ta sẽ cùng tìm hiểu trong cái phần tiếp theo.
0:05:29 - 0:05:33, Rồi, thì ở đây chính là chúng ta sẽ có cái mô hình đầu tiên,
0:05:33 - 0:05:36, đó là mô hình Variational Autoencoder.
0:05:36 - 0:05:43, Với cái công thức này, thì chúng ta sẽ thấy là 2 vế và ý nghĩa của 2 vế đó là gì?
0:05:52 - 0:05:59, Rồi, thì đối với cái vế bên trái, ý nghĩa của nó đó là khi chúng ta từ cái z,
0:05:59 - 0:06:00, tức là cho trước z nè.
0:06:00 - 0:06:03, Z là một cái vector trong không gian ẩn của mình.
0:06:03 - 0:06:06, Nó là một cái random variable.
0:06:06 - 0:06:11, Nhưng nó sẽ được tính ra cái P.
0:06:11 - 0:06:16, Thì bản chất của cái này đó chính là cái xác suất của x.
0:06:16 - 0:06:18, Là xác suất của x cho trước z.
0:06:18 - 0:06:25, Thì cái ý nghĩa của nó đó là cái giá trị mà mình tạo ra x từ z.
0:06:25 - 0:06:28, Cái giá trị của x tạo ra từ z.
0:06:28 - 0:06:34, Nó sẽ giống với lại cái ảnh ban đầu, giống với lại cái ảnh góc.
0:06:34 - 0:06:44, Thì cái mô hình Variational Autoencoder, nó đã đưa cái việc đó là chúng ta cực đại hóa cái công thức này.
0:06:44 - 0:06:50, Thì khi cực đại hóa, thì tức là cái thành phần này sẽ là giá trị lớn nhất.
0:06:50 - 0:06:55, Mà cái xác suất để cho cái x lớn nhất khi cho trước z,
0:06:55 - 0:07:00, nó chính là chúng ta tạo ra một cái tấm hình giống với ảnh ban đầu.
0:07:00 - 0:07:05, Từ z, chúng ta tạo ra x.
0:07:05 - 0:07:09, Thì làm sao cho cái x này nó giống với lại ảnh ban đầu nhất.
0:07:09 - 0:07:13, Thì đó chính là cái ý nghĩa của cái công thức log của Px cho trước z.
0:07:13 - 0:07:15, Ở đây chúng ta đừng quan tâm cái log nữa,
0:07:15 - 0:07:20, tại vì khi chúng ta cực đại hóa cái hiệu này,
0:07:20 - 0:07:28, thì chúng ta sẽ cực đại hóa cái thành phần này và cực tiểu hóa cái thành phần KL Divergence này.
0:07:28 - 0:07:32, Tại vì ở đây có dấu trừ nên chúng ta sẽ đi cực tiểu hóa.
0:07:32 - 0:07:38, Còn cực tiểu hóa của một cái hàm log thì nó sẽ tương đương với việc là mình đi cực tiểu hóa cái Px.
0:07:38 - 0:07:47, Tại vì log là một cái hàm đồng biến, nên chúng ta đi tìm giá trị log của một cái biểu thức fx.
0:07:47 - 0:07:53, Thì nó sẽ tương đương, chúng ta đi tìm cái việc chúng ta tìm max của cái log của fx,
0:07:53 - 0:07:58, thì nó sẽ tương đương với việc chúng ta đi tìm max của fx.
0:07:58 - 0:08:07, Do đó thì cái việc mà đi cực đại hóa cái này tương đương với cái việc là chúng ta đi cực đại hóa P của x cho trước z.
0:08:07 - 0:08:14, Và chúng ta cho trước cái z của mình là đi nằm trong cái phân bố của Qzx.
0:08:14 - 0:08:23, Qzx của mình chính là cái kết quả của quá trình encode.
0:08:23 - 0:08:29, Tức là chúng ta biến từ cái ảnh góc ban đầu về cái vector z trong không gian latent.
0:08:29 - 0:08:34, Và cái z này thì nó sẽ tuân theo cái phân bố này.
0:08:34 - 0:08:38, Cái z này nó sẽ là phân bố Q.
0:08:38 - 0:08:45, Thế thì khi chúng ta lấy mẫu hết tất cả các z, lấy mẫu hết z theo cái phân bố Q này,
0:08:45 - 0:08:48, chúng ta đi decode để tạo ra cái x mũ này.
0:08:48 - 0:08:54, Thì cái x mũ này nó phải giống, nó giống thật nhất.
0:08:54 - 0:09:01, Vâng, thì cái Px cho trước z chính là làm sao cho cái x mũ này là giống thật nhất, giống ảnh góc.
0:09:01 - 0:09:07, Còn cái thành phần thứ hai thì thay vì chúng ta đi cực đại hóa cái dấu trừ này,
0:09:07 - 0:09:17, thay vì chúng ta đi cực đại hóa cái vế bên tay phải thì tương đương với việc là chúng ta sẽ đi cực tiểu hóa cái thành phần KL divergence thôi.
0:09:17 - 0:09:23, Thì đây chính là cái thành phần chính quy trong cái mô hình Variational Autoencoder.
0:09:23 - 0:09:31, Và cái Pz này, cái Pz này là một cái phân bố tiên nghiệm.
0:09:31 - 0:09:43, Và chúng ta mong muốn cái phân bố tiên nghiệm này, nó sẽ là một cái normal Gaussian.
0:09:43 - 0:09:51, Thì là một cái phân bố chuẩn Gaussian N01.
0:09:51 - 0:09:58, Rồi, thì đây chính là cái mô hình Variational Autoencoder.
0:09:58 - 0:10:10, Và ý nghĩa của nó đó là từ x, chúng ta sẽ qua cái encode, chúng ta sẽ tạo ra được một cái phân bố, ra một cái phân bố xác suất là qi, phi, zx.
0:10:10 - 0:10:13, Thì phi ở đây là cái tham số của encoder.
0:10:13 - 0:10:21, Và từ cái qi này chúng ta sẽ sample, chúng ta sẽ sample, thì khi chúng ta sample thì chúng ta sẽ có được hết cái kỳ vọng này, tính được cái kỳ vọng này.
0:10:21 - 0:10:25, Chúng ta sample một cái z và chúng ta decode để tạo ra cái x mũ.
0:10:25 - 0:10:34, Thì chúng ta luôn mong muốn là cái x mũ này giống thật, nó thể hiện qua cái việc là cực đại hóa cái P của x cho trước z.
0:10:34 - 0:10:46, Rồi, thì so sánh cái mô hình Variational Autoencoder với mô hình diffusion, thì Variational Autoencoder là một pha, là từ cái ảnh góc x,
0:10:46 - 0:10:55, nó encode thành một cái vector z, một cái không gian nhiễu, một cái phân bố nhiễu.
0:10:55 - 0:11:00, Và cái phân bố nhiễu này chúng ta sẽ sample ra để có được cái vector z này.
0:11:00 - 0:11:06, Thì đây là cái hình ảnh minh họa của cái vector z. Đó là một cái nhiễu.
0:11:06 - 0:11:14, Qua cái hàm decode thì chúng ta sẽ tạo ra được một cái phân bố xác suất là P theta x cho trước z.
0:11:14 - 0:11:17, Và chúng ta lấy mẫu để có được cái x mũ này.
0:11:17 - 0:11:23, Thì trong cái mô hình Variational Autoencoder là decode nó sẽ tạo ra cái x mũ.
0:11:23 - 0:11:31, Và chúng ta sẽ mong muốn cho cái x mũ này xấp xỉ với lại cái ảnh góc ban đầu, đó là cái vế thứ nhất.
0:11:31 - 0:11:41, Cái vế thứ hai đó là cái thành phần chính quy hóa là cái nhiễu này, thì nó sẽ tuân theo cái phân bố 0.1.
0:11:42 - 0:11:46, Còn đối với cái mô hình diffusion ở dưới đây thì thay vì một bước,
0:11:46 - 0:11:52, Thứ nhất, cái sự khác biệt thứ nhất đó là thay vì một bước thì ở đây chúng ta sẽ có nhiều bước encoding,
0:11:52 - 0:11:55, nhiều bước nhỏ encoding.
0:11:55 - 0:12:03, Và cuối cùng chúng ta cũng đến được cái random noise như thế này theo cái phân bố normal distribution.
0:12:03 - 0:12:13, Nhưng cái sự khác biệt thứ hai đó là trong mô hình VAE thì nó sẽ có tham số, có cái tham số phi.
0:12:13 - 0:12:17, Còn ở đây là không có tham số, các encoding này là không có tham số.
0:12:22 - 0:12:30, Thì đó chính là cái sự khác biệt lớn nhất mà chúng ta thấy được giữa hai cái mô hình VAE và AE.
0:12:30 - 0:12:37, VAE thì sẽ có tham số và chỉ có một bước, một bước nhảy là đến được cái random noise.
0:12:37 - 0:12:45, Thế thì chúng ta thấy là cái việc mà chúng ta ánh xạ trực tiếp từ ảnh thế giới thực sang cái random noise này mà chỉ qua một bước,
0:12:45 - 0:12:49, thì đây là một cái bài toán khó.
0:12:49 - 0:12:56, Và chưa kể khi chúng ta decode ngược trở lại từ cái random noise này thì đây cũng là một cái bài toán khó.
0:12:56 - 0:13:03, Tuy nhiên khi thay vì chúng ta giải một cái bài toán khó với một cái encoder thì chúng ta sẽ chia nhỏ nó ra thành nhiều bước.
0:13:03 - 0:13:09, Thì những cái bước encoding này nó sẽ là một cái mô hình dễ hơn, dễ hơn rất là nhiều.
0:13:09 - 0:13:16, Chúng ta phun một ít nhiễu lên trên cái X0 để được cái X1, rồi sau đó phun một ít nhiễu lên để thành X2.
0:13:16 - 0:13:25, Và cái ảnh của mình dần dần nó đã bị mất hết cái đối tượng góc ban đầu.
0:13:25 - 0:13:32, Và khi chúng ta phun đến cái bước thứ T thì ở đây là nó xấp xỉ với một cái normal distribution.
0:13:32 - 0:13:41, Thì chút nữa chúng ta cũng sẽ có cái phần chứng minh cái ảnh này nó sẽ tiến về cái normal distribution.
0:13:41 - 0:13:47, Sau đó chúng ta giải mã thì cái việc chúng ta giải mã decoding từ những cái bước nhỏ này,
0:13:47 - 0:13:52, thì cái mô hình của mình nó sẽ dễ học hơn, dễ học để tìm ra được cái theta.
0:13:52 - 0:13:57, Tức là thay vì chúng ta giải quyết một bài toán khó, mô hình phức tạp để có thể giải mã được,
0:13:57 - 0:14:03, thì những cái bài toán dễ như thế này thì chúng ta sẽ cần cái số lượng tham số nó ít hơn,
0:14:03 - 0:14:08, cái tính khả thi của nó nó cao hơn và chất lượng hình ảnh của nó cũng sẽ cao hơn.
0:14:08 - 0:14:13, Thì đó chính là cái sự khác biệt giữa VAE và AE.
0:14:22 - 0:14:27, Hãy subscribe cho kênh Ghiền Mì Gõ để không bỏ lỡ những video hấp dẫn.