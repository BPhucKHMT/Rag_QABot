0:00:14 - 0:00:19, Trên đây là mã giả cài đặt thuật toán clip, rất đơn giản
0:00:19 - 0:00:24, Đầu tiên là image encoder này, chúng ta có thể lấy pre-trained model
0:00:24 - 0:00:28, ví dụ như ResNet hoặc Vision Transformer là VIT
0:00:28 - 0:00:34, Còn ResNet thì chúng ta có thể sử dụng các pre-trained model ví dụ như ResNet 101
0:00:34 - 0:00:42, Text Encoder thì chúng ta có thể sử dụng mô hình Continuous Belt Work là Work2Vac
0:00:46 - 0:00:52, Hoặc là mô hình Text Transformer sử dụng BERT
0:00:52 - 0:00:58, Đây là large pre-trained model sử dụng Transformer
0:00:58 - 0:01:02, và được sử dụng rất nhiều trong lĩnh vực xử lý ngôn ngữ tự nhiên
0:01:02 - 0:01:10, Một ảnh y khi đưa qua image encoder thì chúng ta sẽ ra được một feature
0:01:10 - 0:01:14, là if, viết tắt của chữ feature representation
0:01:14 - 0:01:18, và nó sẽ có cái kích thước đó là n, nhân di
0:01:18 - 0:01:22, N là số ảnh của mình
0:01:22 - 0:01:34, và di là số chiều của vector biểu diễn
0:01:34 - 0:01:42, Mỗi text encoder và image encoder sẽ có số chiều khác nhau
0:01:42 - 0:01:46, Do đó di tức là viết tắt của chữ image là dimension của image
0:01:46 - 0:02:08, text encoder sẽ trả về biến tf là n, và mỗi mô tả sẽ được biểu diễn bởi vector dt
0:02:08 - 0:02:12, t là viết tắt của chữ text, và t là số chiều của vector biểu diễn văn bản
0:02:12 - 0:02:20, Và chúng ta sẽ có một chung multimodal embedding
0:02:20 - 0:02:28, tức là chúng ta sẽ đưa cái di và dt này về cùng một cái không gian
0:02:28 - 0:02:32, tức là đưa cùng về cùng một cái vector có cùng một kích thước
0:02:32 - 0:02:40, Thế thì một cái vector di chiều chúng ta sẽ nhân dot product với lại một cái ma trận là w,i
0:02:40 - 0:02:46, để chuyển từ một vector di chiều sang vector de chiều
0:02:46 - 0:02:50, Thì hai cái thao tác này mục đích của mình đó là chuyển
0:02:50 - 0:03:10, Cái không gian đặc trưng của văn bản và hình ảnh về cùng một không gian
0:03:10 - 0:03:18, Thế thì không gian ở đây cụ thể là không gian gồm có de chiều
0:03:18 - 0:03:24, Và đương nhiên là chúng ta sẽ normalize nó lại để cho cái việc huấn luyện của mình dễ hơn
0:03:24 - 0:03:26, và đỡ bị hiện tượng overfitting hơn
0:03:26 - 0:03:32, Sau đó thì chúng ta sẽ đi tính scale dot product, chúng ta sẽ đi tính tích vô hướng
0:03:32 - 0:03:38, Rồi có thêm một cái nhân thêm cái exp, tức là cái hàm e mũ t
0:03:38 - 0:03:44, Thì mục tiêu của cái này đó là nằm trong cái công thức của contrastive loss
0:03:44 - 0:03:50, Để mà có thêm cái tham số là t
0:03:50 - 0:03:54, t ở đây chính là nó viết tắt của chữ là temperature
0:03:54 - 0:04:04, Thì khi t của mình mà nó càng lớn thì nó sẽ khiến cho cái mô hình của mình nó sẽ nhọn hơn
0:04:04 - 0:04:10, và nó sẽ phân biệt rất là rõ đối tượng này với đối tượng kia
0:04:10 - 0:04:18, Nhưng mà thực tế thì chúng ta sẽ thấy có nhiều cái hình ảnh mà nó có sự giao thoa với nhau
0:04:18 - 0:04:22, Ví dụ như trong cái ảnh đó nó sẽ có hai con vật vừa chó vừa mèo
0:04:22 - 0:04:26, thì nó phải không nên quá gắt để mà đưa về một cái phân lớp nào đó
0:04:26 - 0:04:30, hoặc là bản chất là thậm chí trong nếu mà chỉ có một con chó thôi
0:04:30 - 0:04:34, thì chúng ta thấy là cái con chó nó cũng sẽ có cái hình thù đâu đó cũng sẽ giống con mèo
0:04:34 - 0:04:43, do đó thì chúng ta không nên để cái t này tiến về một, mà chúng ta cho nó ở mức giữa đoạn đó là khoảng 0.2
0:04:43 - 0:04:45, ví dụ 0.1, 0.2, ví dụ vậy
0:04:45 - 0:04:53, Còn nếu mà tiến về 0, thì tiến về 1, 1 thì nó sẽ quá gắt
0:04:53 - 0:04:57, Còn nếu mà tiến về 0 thì nó lại quá mềm, nó lại quá mềm
0:04:57 - 0:05:01, và nó sẽ không có sự phân biệt rõ ràng giữa đối tượng này với đối tượng kia
0:05:01 - 0:05:05, do đó t, chúng ta sẽ cho nó khoảng là ở mức giữa giữa
0:05:05 - 0:05:13, và khi chúng ta đã tính được cái logit này rồi, tức là cái độ tương đồng giữa hai cái ảnh và văn bản này rồi
0:05:13 - 0:05:21, Vì vậy, chúng ta sẽ tiến hành đi tính loss và ở đây sẽ có hai cái loại loss đối xứng với nhau
0:05:21 - 0:05:23, Loss Y và Loss T
0:05:23 - 0:05:30, Trong đó Y, tức là chúng ta sẽ lấy một cái ảnh, ví dụ trong cái ma trận của chúng ta ở đằng trước
0:05:30 - 0:05:34, Trong cái ma trận của chúng ta, ví dụ chúng ta xét ở hàng này đi
0:05:35 - 0:05:39, Chúng ta xét ở cái ma trận này và tại cái O này
0:05:39 - 0:05:45, thì chúng ta sẽ có Y, có nghĩa là chúng ta sẽ lấy theo hàng
0:05:45 - 0:05:50, tức là các cái giá trị loss theo hàng
0:05:50 - 0:05:52, và
0:05:52 - 0:05:59, Loss T, tức là chúng ta sẽ lấy theo hình ảnh
0:05:59 - 0:06:03, Loss T chúng ta sẽ lấy theo cột như thế này
0:06:03 - 0:06:08, Thì cái việc mà chúng ta tính cả loss theo hàng và theo cột
0:06:08 - 0:06:11, thì để giúp cho cái mô hình của mình nó có cái tính đối xứng
0:06:11 - 0:06:15, khi mà so sánh giữa hình ảnh với một loạt các cái văn bản
0:06:15 - 0:06:17, so sánh giữa hình ảnh nè
0:06:17 - 0:06:22, với một loạt các cái văn bản là T1, T2, T3, Tn
0:06:22 - 0:06:29, nhưng đồng thời nó cũng sẽ có sự so sánh giữa một văn bản với một loạt các hình ảnh
0:06:29 - 0:06:32, một văn bản với một loạt các hình ảnh
0:06:32 - 0:06:34, nó thể hiện ở cái cột này
0:06:34 - 0:06:36, thì đó là cái sự tổng hợp
0:06:36 - 0:06:40, Còn cái label ở đây thì nó sẽ có label range
0:06:40 - 0:06:45, tức là 12012... cho đến n trừ 1
0:06:45 - 0:06:49, thì ý của nó là cái nhãn của các đối tượng của mình
0:06:49 - 0:06:52, sẽ là đối với ảnh thứ nhất
0:06:52 - 0:06:56, thì cái nhãn của mình sẽ là đối tượng đầu tiên
0:06:56 - 0:06:59, ảnh thứ hai thì cái nhãn của mình sẽ là thứ hai
0:06:59 - 0:07:03, thì ý của nó là nó đang muốn làm theo chạy đường chéo này của mình
0:07:03 - 0:07:09, và khi chúng ta đã có cái loss này rồi thì chúng ta sẽ dùng thuật toán gradient descent
0:07:09 - 0:07:13, các thuật toán optimizer dựa trên gradient descent để huấn luyện
0:07:13 - 0:07:19, và vấn đề đó là làm sao chúng ta có được cái bộ dữ liệu để huấn luyện
0:07:19 - 0:07:24, đó là một cái ảnh và một cái văn bản mô tả
0:07:24 - 0:07:28, thế thì chúng ta sẽ lấy từ trên internet
0:07:28 - 0:07:33, trên internet thì ví dụ như Instagram là có khoảng hơn 3 tỷ ảnh
0:07:33 - 0:07:38, và chúng ta có một số meta data ứng với cái mỗi ảnh đó
0:07:38 - 0:07:42, thì cái tên file ảnh có thể chứa cái thông tin của nội dung ảnh
0:07:42 - 0:07:45, chứa là đâu đó trên mạng xã hội của chúng ta
0:07:45 - 0:07:50, cái nguồn dữ liệu mô tả này thì đã được người dùng người ta mô tả rồi
0:07:50 - 0:07:54, và chúng ta chỉ việc là khai thác cái lượng dữ liệu này
0:07:54 - 0:07:59, và OpenAI thì xây dựng một cái bộ dữ liệu là WIT
0:07:59 - 0:08:04, là Web Image Text thì bao gồm là 400 triệu cặp dữ liệu ảnh và văn bản
0:08:04 - 0:08:09, rồi 500 ngàn truy vấn dựa trên danh sách các từ xuất hiện trên Wikipedia
0:08:09 - 0:08:13, tức là các ảnh này có cái nội dung về chủ đề gì
0:08:13 - 0:08:18, thì cũng sẽ đa dạng hóa bằng cái việc đó là
0:08:18 - 0:08:23, chúng ta sẽ truy vấn bằng 500 ngàn câu truy vấn
0:08:23 - 0:08:30, được lấy từ trên nguồn bách khoa toàn thư để tăng tính đa dạng của data set
0:08:30 - 0:08:40, và cái kết quả của clip đó là gì
0:08:40 - 0:08:45, clip cho phép chúng ta có thể Zero-shot Transfer rất là hiệu quả
0:08:45 - 0:08:50, và sau khi clip đã ra đời thì có rất nhiều mô hình khai thác clip
0:08:50 - 0:08:55, để làm các mô hình như là phân loại hình ảnh phát hiện đối tượng
0:08:55 - 0:09:00, phân đoạn ngữ nghĩa đối tượng, bài toán captioning, bài toán mô tả hình ảnh
0:09:00 - 0:09:05, rất là nhiều
0:09:05 - 0:09:10, và cái sơ đồ trên đây là việc sử dụng contrastive loss
0:09:10 - 0:09:15, cho sự hiệu quả gấp hơn rất nhiều lần so với các phương pháp huấn luyện khác
0:09:15 - 0:09:20, cụ thể đó là để cho được độ chính xác trên Zero-shot
0:09:20 - 0:09:25, đâu đó khoảng 15%
0:09:25 - 0:09:34, thì tập dữ liệu của chúng ta chỉ cần là khoảng 33 triệu ảnh
0:09:34 - 0:09:39, trong khi đó nếu như chúng ta sử dụng phương pháp dạng prediction
0:09:39 - 0:09:43, hoặc sử dụng một mô hình ngôn ngữ transformer để huấn luyện
0:09:43 - 0:09:47, thì nó sẽ phải tốn lên đến 134 triệu
0:09:47 - 0:09:52, và ở đây sẽ là 400 triệu
0:09:52 - 0:09:57, như vậy thì cái ý của cái biểu đồ này cho biết là phương pháp huấn luyện của contrastive loss
0:09:57 - 0:10:02, của clip cho sự hiệu quả rất là cao, gấp 4 lần
0:10:02 - 0:10:06, ít nhất là gấp 4 lần
0:10:06 - 0:10:11, tuy nhiên cái hạn chế của nó là nó sẽ kém trừu tượng
0:10:11 - 0:10:14, nó sẽ kém hiệu quả trên các bài toán có tính trừu tượng cao
0:10:14 - 0:10:18, trong đó có các đối tượng tương tác với nhau chi tiết
0:10:18 - 0:10:25, thì nó sẽ không có hiệu quả hoặc là những bài toán như là đếm vật thể
0:10:25 - 0:10:30, những bài toán đếm, đó cũng là một bài toán nằm trong nhóm trừu tượng cao
0:10:30 - 0:10:34, và cái kết quả kém trên dữ liệu có phân phối khác
0:10:34 - 0:10:39, ví dụ như chúng ta huấn luyện trên tập dữ liệu trên mạng internet
0:10:39 - 0:10:44, nhưng khi chúng ta sử dụng nó trên tập dữ liệu MNIST
0:10:44 - 0:10:48, thì có thể kết quả nó lại không có tốt bằng
0:10:48 - 0:10:53, và do đó chúng ta cần phải cẩn thận trong việc lựa chọn tập dữ liệu
0:10:53 - 0:10:58, khi chúng ta tiến hành đánh giá, đó chính là những điểm hạn chế của clip
0:11:09 - 0:11:14, Cảm ơn các bạn đã xem video hấp dẫn