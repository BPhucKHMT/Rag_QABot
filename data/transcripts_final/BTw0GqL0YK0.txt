0:00:14 - 0:00:25, Chúng ta sẽ cài đặt thuật toán RadarLesson và cài đặt biến thể nâng cao của RadarLesson, đó là Momentum
0:00:26 - 0:00:37, Trong thuật toán RadarLesson, chúng ta dùng 1 hàm chỉ có 1 điểm cực tiểu thôi, do đó nó sẽ không thể minh họa được cho tình huống Momentum
0:00:37 - 0:00:44, Chúng ta sẽ dùng 1 hàm có nhiều hơn 1 điểm cực tiểu, và cụ thể đó là hàm có 2 điểm cực tiểu
0:00:44 - 0:00:55, Thì cái hàm này chúng ta có thể chọn là 1 công thức là 1 hàm bậc 4, là bằng theta mũ 4 cộng cho theta mũ 3 trừ cho 5 theta bình cộng 3
0:00:56 - 0:01:03, Thì ở đây nó sẽ có 2 điểm cực tiểu, và cái giá trị mình khởi tạo ở đây thì nó là 1 cái giá trị đủ nhỏ thôi
0:01:03 - 0:01:09, Tại vì nếu vì cái hàm này là hàm bậc 4, nếu chúng ta để con số là 12 thì con số nó sẽ rất là lớn
0:01:09 - 0:01:15, Do đó chúng ta sẽ chọn 1 con số đủ nhỏ, theta khởi tạo ban đầu có thể là bằng 4 thôi
0:01:16 - 0:01:25, Và alpha cũng tương tự như vậy, alpha là 1 cái con số rất là lớn, trong trường hợp này là 0.5 là 1 con số rất là lớn
0:01:25 - 0:01:36, Chúng ta có thể sử dụng 1 cái alpha khoảng là 0.01 để cho nó đủ nhỏ
0:01:36 - 0:01:43, Tại vì khi chúng ta làm với cái hàm lớn như thế này thì dùng con số alpha lớn nó sẽ khiến cho mình bị phân kỳ
0:01:44 - 0:01:51, Tiếp theo là chúng ta sẽ cài đặt các công thức ở đây
0:01:57 - 0:02:10, Thì đầu tiên chúng ta sẽ sửa lại cái hàm của mình, hàm j là theta mũ 4 cộng cho theta mũ 3 trừ cho 5 nhân theta bình phương cộng 3
0:02:11 - 0:02:27, Và sẽ sửa lại cái hàm đạo hàm, 4 nhân theta mũ 3 cộng cho 3 nhân theta bình phương trừ cho 10 nhân theta
0:02:28 - 0:02:41, Sau đó chúng ta sẽ tiến hành cài đặt, theta ban đầu là 12, nó sẽ rất là lớn, ở đây chúng ta sẽ sửa lại nó phải bằng 4 thôi
0:02:42 - 0:02:58, Rồi alpha là 0.1 và v ban đầu sẽ được gán bằng 0, rồi delta là 0.1 và beta là 0.9
0:02:58 - 0:03:09, Trong cái công thức cập nhật thì chúng ta vẫn sẽ tính đạo hàm nhưng mà chúng ta sẽ có thêm alpha là bằng alpha nhân với delta
0:03:09 - 0:03:19, Tức là cứ mỗi một lần lặp là nó sẽ giảm xuống 10 lần, tuy nhiên thì con số này chúng ta sẽ tìm cách chúng ta tune nó để cho nó trực quan được
0:03:19 - 0:03:28, Thứ 2 là v sẽ là bằng beta nhân với lại v quá khứ, tức là v trước đó, cộng cho alpha nhân với lại đạo hàm
0:03:28 - 0:03:34, Thì đạo hàm ở đây chúng ta đã được tính sẵn trong cái biến đó là derivative, dẫn đến là chúng ta không cần phải tính lại nữa
0:03:34 - 0:03:37, Và ở đây theta sẽ là bằng theta trừ cho v
0:03:37 - 0:03:40, Rồi bây giờ chúng ta sẽ trực quan hóa nó
0:03:44 - 0:03:48, Rồi để trực quan hóa thì chúng ta sẽ run ở đây
0:03:48 - 0:03:58, Vì cái khoảng giá trị của mình là rất là lớn nên nếu chúng ta để cái biên trái và biên phải là trừ 10 và 12 như ở đây
0:03:58 - 0:04:07, Vì vậy chúng ta sẽ phải sửa lại khoảng giá trị là từ trừ 4 cho đến 4 thôi
0:04:12 - 0:04:19, Khi chạy chúng ta thấy là cái đồ thị của mình nhìn nó có vẻ hơi không có được mượt mà nó bị gấp khúc
0:04:19 - 0:04:27, Thì để giảm cái hiện tượng này chúng ta sẽ cho cái khoảng lấy mẫu nhỏ xuống đó là khoảng 0.1
0:04:28 - 0:04:30, Rồi sau đó chúng ta sẽ chạy lại
0:04:30 - 0:04:34, Chúng ta thấy là cái hàm của mình đã cái đường thẳng của mình nhìn nó mượt hơn
0:04:35 - 0:04:37, Và khi chạy thì chúng ta thấy là
0:04:39 - 0:04:41, Đó đến đây thì nó lại rất là chậm
0:04:42 - 0:04:43, Nó là rất là chậm
0:04:43 - 0:04:44, Thì nguyên nhân đó là do đâu?
0:04:46 - 0:04:50, Nguyên nhân đó là do cái decay rate của alpha
0:04:52 - 0:04:55, Alpha tại một thời điểm cứ mỗi một vòng lặp
0:04:55 - 0:04:59, Thì alpha là bằng alpha nhân với lại delta
0:05:01 - 0:05:04, Tức là alpha là bằng alpha nhân 0.1
0:05:04 - 0:05:06, Hay nói cách khác đó là alpha chia 10 đi
0:05:08 - 0:05:11, Alpha ban đầu của mình đã đủ nhỏ rồi mà mình còn chia 10 nữa
0:05:11 - 0:05:12, Đúng không?
0:05:12 - 0:05:18, Thì dẫn đến đó là đến đây nó gần như không có sự tham gia của đạo hàm hiện tại
0:05:18 - 0:05:20, Dẫn đến là tại những vị trí này
0:05:20 - 0:05:22, Đạo hàm hiện tại nó rất là bé
0:05:23 - 0:05:26, Do đó thì mình sẽ sửa lại cái hệ số này
0:05:26 - 0:05:27, Nâng nó lên
0:05:27 - 0:05:30, Delta là bằng 0.2
0:05:32 - 0:05:33, Tức là giảm vừa thôi
0:05:34 - 0:05:36, Rồi chúng ta lưu và chạy
0:05:38 - 0:05:41, Thì chúng ta thấy là nó nhảy xa hơn
0:05:41 - 0:05:46, Lý do đó là vì nó lấy được cái đạo hàm thực sự tại cái vị trí này
0:05:46 - 0:05:49, Còn nếu mà chúng ta chia cho 10 thì cái phần đạo hàm
0:05:50 - 0:05:52, Mà nó cập nhật vô cái bước nhảy của mình rất là ít
0:05:52 - 0:05:57, Ban đầu đã là 0.01 là tương đối nhỏ là vừa
0:05:57 - 0:05:59, Nhưng mà lần sau lại chia 10
0:05:59 - 0:06:01, Rồi lần tiếp theo lại chia 10 nữa
0:06:01 - 0:06:04, Thì cái phần đạo hàm tại vị trí này nó gần như không tham gia vô
0:06:04 - 0:06:08, Vì do đó chúng ta giảm nó bớt bớt thôi là khoảng 0.2
0:06:08 - 0:06:10, Và khi chúng ta giảm xuống
0:06:10 - 0:06:14, Thì chúng ta thấy là tại cái vị trí này
0:06:14 - 0:06:17, Nó đã thoát ra được khỏi cái điểm cực tiểu cục bộ
0:06:17 - 0:06:19, Nó đã thoát khỏi được cái điểm cực tiểu cục bộ ở đây
0:06:19 - 0:06:23, Và nó trượt xuống đây để mà nó chạm được đến cái điểm cực tiểu
0:06:23 - 0:06:30, Thứ 2 thì đây chính là cái ý nghĩa của thuật toán Momentum
0:06:30 - 0:06:34, Bây giờ nếu chúng ta sửa lại thêm một chút xíu
0:06:34 - 0:06:40, Là ví dụ như ở đây chúng ta để là cho Delta này lên 0.3
0:06:40 - 0:06:41, Thì điều gì sẽ xảy ra
0:06:42 - 0:06:45, Nếu cho Delta lên 0.3
0:06:45 - 0:06:47, À Delta lên 0.3
0:06:52 - 0:06:54, Thì nó cũng sẽ thoát ra được
0:06:54 - 0:06:57, Nhưng đến đây là nó vẫn còn quá mạnh
0:06:57 - 0:07:02, Dẫn đến là nó sẽ cứ thế mà đi lên tiếp luôn
0:07:02 - 0:07:07, Do là cái thành phần Delta của mình
0:07:07 - 0:07:09, Là nó nhận được cái alpha
0:07:09 - 0:07:14, Cho cái thành phần alpha ban đầu của mình là một con số chưa có đủ nhỏ
0:07:14 - 0:07:18, Do đó muốn để cho nó là 0.3 ở đây
0:07:18 - 0:07:20, Thì chúng ta phải giảm tiếp alpha nữa
0:07:20 - 0:07:25, Tức là một khi chúng ta tăng Delta lên thì chúng ta phải giảm alpha để cho nó cân bằng
0:07:25 - 0:07:29, Ví dụ ở đây sẽ là 0.01
0:07:29 - 0:07:33, Rồi, chúng ta sẽ chạy lại
0:07:38 - 0:07:40, Nếu chúng ta giảm nhỏ quá
0:07:40 - 0:07:42, Alpha mà bé quá
0:07:42 - 0:07:47, Thì đến đây là cái thành phần đạo hàm nó không còn đóng góp gì nhiều vô nữa
0:07:47 - 0:07:49, Nó không còn đóng góp gì nhiều vô nữa
0:07:49 - 0:07:51, Và đến đây là nó bị dừng luôn
0:07:51 - 0:07:57, Với những cái ví dụ này chúng ta thấy việc tune các siêu tham số rất là quan trọng
0:07:57 - 0:08:00, Nếu như chúng ta tune mà không đúng
0:08:00 - 0:08:03, Một số quá nhỏ, một số quá lớn
0:08:03 - 0:08:05, Thì dẫn đến đó là hai tình huống
0:08:05 - 0:08:08, Tình huống đầu tiên là như chúng ta thấy ở đây
0:08:08 - 0:08:10, Khi alpha quá nhỏ
0:08:10 - 0:08:12, Thì cái thành phần đạo hàm ở đây
0:08:12 - 0:08:16, Nó tham gia vô cái việc mà cập nhật tham số cũng rất là ít
0:08:16 - 0:08:18, Và nó mau chóng nó bị tiêu biến đi
0:08:18 - 0:08:20, Dẫn đến là nó sẽ bị đứng ở đây luôn
0:08:22 - 0:08:29, Rồi, do đó chúng ta phải chọn alpha vừa đủ để có thể bắt đầu được
0:08:31 - 0:08:35, Rồi, khi chúng ta chọn alpha vừa đủ rồi
0:08:35 - 0:08:38, Nhưng mà cái delta của mình nó lại quá lớn
0:08:38 - 0:08:42, Thì dẫn đến đó là nó sẽ lấy cái giá trị đạo hàm
0:08:42 - 0:08:45, Nó không có giảm đủ nhanh
0:08:45 - 0:08:47, Nó không có giảm alpha đủ nhanh
0:08:47 - 0:08:51, Dẫn đến là nó còn kế thừa cái thành phần đạo hàm rất là lớn ở trên đây
0:08:51 - 0:08:55, Để mà nó kéo qua đây và cập nhật tiếp
0:08:58 - 0:09:01, Do đó thì ở đây chúng ta phải giảm xuống là khoảng 0.2
0:09:01 - 0:09:04, Giảm xuống là khoảng 0.2
0:09:04 - 0:09:06, Và chúng ta sẽ chạy lại
0:09:06 - 0:09:13, Giảm xuống 0.2 thì có vẻ như là vừa đủ để nó vừa thoát ra khỏi điểm cực tiểu
0:09:13 - 0:09:16, Chúng ta vừa thoát ra khỏi điểm cực tiểu cục bộ
0:09:16 - 0:09:19, Và tiến đến một điểm cực tiểu tốt hơn
0:09:19 - 0:09:24, Thì là khi chúng ta chọn alpha và delta vừa đủ
0:09:24 - 0:09:28, Còn nếu chúng ta chọn delta mà nhỏ quá
0:09:28 - 0:09:31, Thì cái việc giảm alpha này quá nhanh
0:09:31 - 0:09:35, Cái việc giảm alpha quá nhanh dẫn đến đó là những cái vòng lặp sau
0:09:35 - 0:09:40, Nó sẽ bị dừng trước lúc thoát ra khỏi điểm cực tiểu
0:09:40 - 0:09:46, Đến đây là bắt đầu nó sẽ không thoát ra khỏi được điểm cực tiểu cục bộ ở đây rồi
0:09:50 - 0:09:53, Thì đó chính là cái điểm yếu của mình
0:09:53 - 0:09:57, Khi chúng ta chọn tham số quá nhiều
0:09:57 - 0:09:59, Khi một thuật toán của mình mà nó có quá nhiều tham số
0:09:59 - 0:10:02, Thì cái thuật toán Adam là một cái thuật toán mà
0:10:02 - 0:10:06, Nó khá là bền vững và ổn định với các cái siêu tham số
0:10:06 - 0:10:10, Khi chúng ta chọn những cái alpha và theta mặc định ban đầu
0:10:10 - 0:10:12, Thì nó có thể là ra một cái thuật toán tốt hơn
0:10:12 - 0:10:16, Tuy nhiên Adam nó sẽ phù hợp cho những cái mô hình
0:10:16 - 0:10:19, Mà có nhiều tham số
0:10:19 - 0:10:23, Còn Adam và root mean square propagation
0:10:23 - 0:10:28, Thì nó lại phù hợp cho những cái mô hình mà có nhiều hơn hai tham số
0:10:28 - 0:10:33, Rồi như vậy thì ở đây chúng ta sẽ trả lại cái tham số để mà có thể chạy được
0:10:33 - 0:10:38, Đó chính là 0.2 và alpha là bằng 0.1
0:10:48 - 0:10:53, Rồi thì chúng ta thấy là nó đã di chuyển rất là nhanh ở cái điểm cực tiểu cục bộ đầu tiên
0:10:53 - 0:10:56, Và sau đó thì nó sẽ vượt qua được
0:10:57 - 0:11:02, Thì đối với cái thuật toán này chúng ta sẽ có một cái chú ý
0:11:02 - 0:11:08, Đó là vì ở đây nó may mắn nó không chạm được đến cái điểm mà đủ nhỏ
0:11:08 - 0:11:10, Tức là cái điểm mà đạo hàm đủ nhỏ
0:11:10 - 0:11:13, Chứ nếu nó chạm vô được cái điểm đủ nhỏ này
0:11:13 - 0:11:18, Mà nó gặp cái lệnh if này thì nó sẽ thoát ra khỏi cái chương trình của mình luôn
0:11:18 - 0:11:22, Do đó đa số các cái thuật toán momentum
0:11:23 - 0:11:28, Và huấn luyện các cái optimizer không có dùng cái điều kiện này
0:11:28 - 0:11:30, Mà họ sẽ chạy với một số vòng lặp nhất định
0:11:30 - 0:11:34, Tại vì nếu vô tình chúng ta chạm được cái điểm cực tiểu cục bộ
0:11:34 - 0:11:37, Thì nó sẽ thoát ra khỏi cái chương trình của mình luôn
0:11:37 - 0:11:40, Do đó thì mình sẽ sửa lại một lần nữa
0:11:40 - 0:11:42, Thay vì chúng ta dùng cái điều kiện này
0:11:44 - 0:11:48, Thì chúng ta sẽ cho một cái vòng lặp ở đây là for
0:11:48 - 0:11:51, in range
0:11:51 - 0:11:54, Của chúng ta cho nó chạy khoảng 100 lần
0:11:56 - 0:11:57, Rồi chúng ta sẽ dừng
0:11:57 - 0:12:01, Và ở đây chúng ta sẽ bỏ qua cái biến epsilon
0:12:02 - 0:12:06, Thì đây chính là cái biến thể momentum
0:12:08 - 0:12:09, Và thuật toán nó vẫn chạy được
0:12:09 - 0:12:12, Cho đến khi nào mà nó gặp đủ 200 vòng lặp
0:12:12 - 0:12:17, Thì nó sẽ kết thúc cái thuật toán momentum của mình
0:12:19 - 0:12:24, Rồi, trong phần này chúng ta đã cùng cài đặt thuật toán momentum
0:12:24 - 0:12:27, Và cho thấy được cái hiệu quả của nó
0:12:27 - 0:12:29, Thoát ra khỏi điểm cực tiểu cục bộ
0:12:29 - 0:12:33, Nhưng mà nó sẽ có một vấn đề đó là cái siêu tham số của mình
0:12:33 - 0:12:38, Chúng ta sẽ phải chọn những cái siêu tham số cho phù hợp
0:12:38 - 0:12:43, Mặc định ban đầu đó là alpha của mình nên là con số đủ nhỏ
0:12:43 - 0:12:48, Đối với những hàm đa thức trên thế này là 0.01
0:12:48 - 0:12:51, Nhưng mà sau này khi chúng ta làm với các mô hình học sâu
0:12:51 - 0:12:53, Thì alpha nó có thể nhỏ hơn nữa
0:12:53 - 0:12:56, Ví dụ như là 0.0001 tức là 10 mũ trừ 4
0:12:56 - 0:13:01, Và các learning rate, các hệ số này
0:13:01 - 0:13:05, Thì chúng ta những biến thể sau nó cũng đã cải tiến
0:13:05 - 0:13:08, Để cho kể cả khi ban đầu V bằng 0
0:13:08 - 0:13:12, Nhưng mà cái thuật toán của mình nó vẫn có thể chạy nhanh ở những vòng lặp đầu tiên
0:13:12 - 0:13:16, Vì vậy thì chúng ta kết thúc cái bài thực hành momentum ở đây