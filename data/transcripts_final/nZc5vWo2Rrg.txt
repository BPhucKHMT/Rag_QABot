0:00:14 - 0:00:18, Tiếp theo chúng ta sẽ bàn về tốc độ của mô hình Diffusion
0:00:18 - 0:00:27, Đối với mô hình Diffusion thì vấn đề lớn nhất là nó phải sampling rất nhiều bước trung gian để có thể encode và decode
0:00:27 - 0:00:30, Như vậy thì làm sao để có thể sinh ra ảnh với tốc độ nhanh hơn
0:00:30 - 0:00:38, Nguyên nhân là trong quá trình thêm nhiễu vào, nhiễu sau sẽ phụ thuộc vào nhiễu trước
0:00:38 - 0:00:45, Tức là để denoise được xt-2 thì phải có xt-1
0:00:45 - 0:00:48, Mà muốn tính được xt-1 thì phải có xt
0:00:48 - 0:00:54, Chính sự tuần tự này sẽ khiến chúng ta chậm
0:00:54 - 0:00:59, Nguyên nhân của tuần tự này là nó giả định theo chuỗi Markov,
0:00:59 - 0:01:08, tức là phải có x thứ t trừ 1 xong chúng ta mới có thể tính được xt.
0:01:08 - 0:01:10, Đó chính là nguyên nhân.
0:01:10 - 0:01:12, Và ngược lại khi chúng ta denoise cũng như thế.
0:01:12 - 0:01:16, Như vậy thì thời gian chạy của diffusion sẽ là bằng t
0:01:16 - 0:01:18, nhân cho thời gian chạy của GAN và VAE.
0:01:18 - 0:01:22, Vậy thì chúng ta sẽ nhắc lại công thức tạo sinh của mô hình của mình.
0:01:22 - 0:01:34, Trong mô hình tạo sinh của mình thì công thức sử dụng theo cách thức số 2 của chúng ta đó là đoán xem cái nhiễu tại một thời điểm t
0:01:34 - 0:01:42, so với lại cái nhiễu đúng là bao nhiêu thì chúng ta sẽ có mi của qi xt phải x0 là bằng công thức này
0:01:42 - 0:01:47, và mi của theta xt t thì nó sẽ là bằng công thức này
0:01:47 - 0:01:54, trong công thức này thì chúng ta thấy nó có tính chất gọi là Markov
0:01:54 - 0:01:59, tức là phải tính xt cũ trước rồi mới tính xt
0:01:59 - 0:02:06, tuy nhiên có một bài báo khác đó là DDIM
0:02:06 - 0:02:11, tức là denoising diffusion implicit model
0:02:11 - 0:02:16, thì đã bỏ đi cái yếu tố gọi là chuỗi Markov
0:02:16 - 0:02:25, tức là chúng ta sẽ không có yêu cầu quý xt, xt trừ 1 phải là một chuỗi Markov, tức là phải tính được xt trừ 1
0:02:25 - 0:02:27, xong rồi chúng ta mới tính được cái xt này
0:02:27 - 0:02:32, thì ở đây chúng ta sẽ dùng cái sơ đồ này để dễ hình dung
0:02:32 - 0:02:36, đó là từ xt chúng ta có thể tính trực tiếp lên x1
0:02:36 - 0:02:39, xin lỗi từ x0 chúng ta có thể tính trực tiếp lên x1
0:02:39 - 0:02:43, từ x0 chúng ta có thể tính trực tiếp đến x2 mà không cần thông qua
0:02:43 - 0:02:45, không cần thông qua cái bước tính x1 này
0:02:45 - 0:02:48, thì công thức của mình sẽ là qi của xt
0:02:48 - 0:02:52, khi chúng ta biết trước xt trừ 1 x0
0:02:52 - 0:02:56, thì lúc này chúng ta sẽ tính trực tiếp từ x0 mà không cần qua xt trừ 1
0:02:56 - 0:03:01, vậy thì ở trên công thức này chúng ta thấy bản chất của các công thức
0:03:01 - 0:03:06, nó chỉ là một sự tính toán với các hệ số a, b và b
0:03:06 - 0:03:07, a và b
0:03:07 - 0:03:15, hàm mi của quý xt, xt0, xt0 là bằng a, b, axt, bε
0:03:15 - 0:03:23, tư tưởng như vậy mi theta của xt, t là bằng axt, bε, theta xt
0:03:23 - 0:03:29, Thế thì chúng ta chỉ cần tìm a và b sao cho miễn là cái xt nó thỏa mãn
0:03:29 - 0:03:35, xt là bằng căn của alpha t x0 cộng cho 1 trừ căn alpha
0:03:35 - 0:03:40, 1 trừ... cộng cho căn của 1 trừ alpha epsilon
0:03:40 - 0:03:42, thì như vậy là đã đúng được
0:03:42 - 0:03:45, Thì cái mô hình DDIM ý tưởng của nó đó là
0:03:45 - 0:03:51, thay vì chúng ta đi từng bước phụ thuộc bước thứ t
0:03:51 - 0:03:54, chúng ta tính xong thì chúng ta mới đến được bước thứ T cộng 1
0:03:54 - 0:04:00, thì nó sẽ dùng một cái công thức trực tiếp từ x0 cho đến cái vị trí thứ T luôn
0:04:00 - 0:04:02, và ngược lại cũng vậy
0:04:02 - 0:04:07, thế thì nó sẽ nhảy cóc, nói một cách nôm na đó là nó sẽ tính toán nhảy cóc
0:04:07 - 0:04:11, cái bước mà encoding và decoding
0:04:11 - 0:04:16, và như vậy thì cái tốc độ của DDIM có thể nhanh hơn
0:04:16 - 0:04:20, gấp 10 hoặc thậm chí là gấp 100 lần so với lại DDPM
0:04:20 - 0:04:22, DDPM
0:04:23 - 0:04:25, Đây là mô hình probabilistic
0:04:25 - 0:04:27, tức là mô hình có xác suất
0:04:27 - 0:04:28, Còn ở đây là implicit
0:04:30 - 0:04:32, Tức là một mô hình mà
0:04:32 - 0:04:34, nó có thể tính một cách đơn định
0:04:34 - 0:04:36, không có kiểu yếu tố nhiễu trong đó
0:04:36 - 0:04:37, không có yếu tố nhiễu
0:04:37 - 0:04:38, Thế thì
0:04:38 - 0:04:41, xét về tốc độ thì DDPM nó hơn
0:04:41 - 0:04:43, Còn xét về độ chính xác thì
0:04:43 - 0:04:45, nó gần như tương đương và thậm chí là
0:04:45 - 0:04:47, tốt hơn
0:04:47 - 0:04:49, ở một số tình huống ví dụ
0:04:49 - 0:05:05, với số Step 10, FID là 10, DDPM là 13, DDPM là 300
0:05:05 - 0:05:16, khi thực hiện với 1000 step, DDPM cho độ chính xác cho FID là tốt nhất
0:05:16 - 0:05:20, DDPM cũng gần như tương đương, 4.0
0:05:20 - 0:05:28, nhưng từ 100 trở về trước, ở đây là 4,0, còn ở đây là gần 10,4,0, rất tốt hơn 10 nhiều
0:05:28 - 0:05:40, Với DDIM, số step của mình mà nhỏ hơn 100, độ chính xác FID của mình tốt hơn hẳn so với DDPM.
0:05:40 - 0:05:46, Tương tự như vậy cho bộ CelebA-64, kết quả cũng hoàn toàn tương tự như vậy.
0:05:46 - 0:05:52, DDIM có thể nói là một trong những cải tiến đột phá
0:05:52 - 0:05:59, trong việc đó là cải tiến về tốc độ của một mô hình diffusion
0:05:59 - 0:06:03, chuyển từ dạng probabilistic sang dạng deterministic
0:06:03 - 0:06:05, để mà mình có thể lấy mẫu nhanh.
0:06:05 - 0:06:10, Và một kỹ thuật khác đó là progressive distillation
0:06:10 - 0:06:15, khi nói đến mô hình học máy thì chúng ta sẽ có kỹ thuật
0:06:15 - 0:06:23, tức là huấn luyện một mô hình teacher có một số tham số rất là lớn
0:06:23 - 0:06:27, và mô hình student có số lượng tham số ít hơn
0:06:27 - 0:06:32, trong trường hợp này thì chúng ta sẽ huấn luyện mô hình teacher và student phối hợp với nhau
0:06:32 - 0:06:36, để sao cho chúng ta, thay vì chúng ta phải đi từng bước như thế này
0:06:36 - 0:06:42, thì chúng ta có thể đi những cái đường tắt mà vẫn có thể đến được đích
0:06:42 - 0:06:46, progressive distillation
0:06:46 - 0:06:55, ở đây chúng ta sẽ huấn luyện teacher trước và sau đó chúng ta sẽ distill vào knowledge của student theo đường màu vàng này
0:06:55 - 0:07:05, thì student của mình là đi theo các đường tắt, tức là nó bỏ qua các bước trung gian ở đây
0:07:05 - 0:07:10, sau đó, nếu là progressive có nghĩa là gì?
0:07:10 - 0:07:14, nó lấy chính cái đường tắt này, tức là cái mô hình mà đi
0:07:14 - 0:07:18, denoise theo cái kiểu đường tắt này để làm teacher
0:07:18 - 0:07:21, để làm teacher, là cái đường màu vàng này là teacher
0:07:21 - 0:07:25, thì chúng ta sẽ đi một cái đường tắt hơn nữa, đó là student
0:07:25 - 0:07:28, chúng ta sẽ bỏ qua cái node này
0:07:28 - 0:07:32, bỏ qua cái node của teacher cũ
0:07:32 - 0:07:37, để tạo ra một student mới có bước nhảy cóc nhanh hơn
0:07:37 - 0:07:40, thì đây chính là Progressive Length Distillation
0:07:40 - 0:07:48, chúng ta từng bước giảm số bước của mình xuống để tăng tốc độ denoise
0:07:48 - 0:07:57, Mô hình Guided Distillation ý tưởng cũng là dùng Distillation nhưng mà kết hợp với Latent Diffusion
0:07:57 - 0:08:07, Đây là một mô hình cho chúng ta vừa đạt được tốc độ huấn luyện và tốc độ inference của mình.
0:08:07 - 0:08:15, Đây là mô hình có Condition là Y, cho phép chúng ta điều hướng mô hình của mình.
0:08:15 - 0:08:24, Vì vậy, ở đây là một mô hình Guided Distillation là giao thoa hoặc là kết hợp của Progressive
0:08:24 - 0:08:28, tức là chúng ta sẽ đi các đường đi tắt
0:08:28 - 0:08:32, thay vì chúng ta đi từng bước, từng bước, từng bước thì chúng ta sẽ đi tắt
0:08:32 - 0:08:36, hoặc thậm chí là tắt hơn, tức là chúng ta có thể đi trực tiếp từ đây sang đây
0:08:36 - 0:08:41, thông qua cái việc là chưng cất tuần tự
0:08:41 - 0:08:48, sau đó chúng ta kết hợp với mô hình Latent Diffusion, tức là chúng ta chỉ làm bước
0:08:48 - 0:08:56, encode và decode ở trên không gian latent thôi, tức là chúng ta không làm trong không gian ảnh mà làm trên không gian latent
0:08:56 - 0:09:05, và kết quả của Guided Distillation thì chúng ta thấy là rất là đẹp và
0:09:05 - 0:09:18, Các bước từ 2 bước, 4 bước và 8 bước thì kết quả gần như tương đương nhau, không có sự phân biệt gì nhiều
0:09:18 - 0:09:24, Với chỉ 2 bước mà kết quả của chúng ta rất là tốt
0:09:24 - 0:09:31, So với đương nhiên là 8 bước nhiều bước hơn thì nó sẽ đẹp hơn, chi tiết hơn nhưng mà 2 bước thì kết quả cũng rất là tốt
0:09:31 - 0:09:39, và khi chúng ta denoise mà chỉ có hai bước thì rõ ràng tốc độ mình nhanh hơn rất là nhiều so với lại denoise cả t bước
0:09:39 - 0:09:43, thì đây là cái kết quả vào năm 2023
0:09:43 - 0:09:47, và chúng ta sẽ có cái mô hình consistency model
0:09:47 - 0:09:56, tức là chúng ta sẽ kết hợp các cái loss lại với nhau là bằng min của EMA của XT và T
0:09:56 - 0:09:59, Rồi F của theta x t phải
0:09:59 - 0:10:03, thì đây là một cái target network hay còn gọi là mô hình của teacher
0:10:03 - 0:10:07, Còn online network sẽ là mô hình student
0:10:07 - 0:10:16, Tóm lại là đối với những cái giải pháp mà giảm cái tốc độ thì chúng ta sẽ dùng cái mô hình đó là teacher và student
0:10:16 - 0:10:22, Kết hợp hai cái network này lại để chúng ta có thể tạo ra cái mô hình mà
0:10:22 - 0:10:27, tốt hơn, đi tắt hơn và multi-step hơn
0:10:27 - 0:10:30, ví dụ như ở đây chúng ta thấy là cái đường này nè
0:10:30 - 0:10:32, là đi 1 phát 1 đến đích
0:10:32 - 0:10:35, thì nó đã tiết giảm cho chúng ta rất là nhiều các bước denoise
0:10:37 - 0:10:40, như vậy thì chúng ta có thể là single step generation
0:10:40 - 0:10:45, là một bước nhảy từ xt lớn về x0
0:10:45 - 0:10:56, Và kết quả của Latent Consistency Model thì cũng hoàn toàn tương tự như các mô hình trước
0:10:56 - 0:11:02, Với 4 Step Inference, 4 Step Denoise thì kết quả của mình vẫn cho chất lượng rất tốt
0:11:02 - 0:11:12, Ngoài ra thì chúng ta sẽ có các kỹ thuật liên quan đến vấn đề về Fine-tune lại mô hình
0:11:12 - 0:11:17, Fine-tune mô hình diffusion
0:11:20 - 0:11:25, Khi nói về mô hình diffusion thì tham số thường rất lớn
0:11:25 - 0:11:29, Lý do đó là nó phải encode cả văn bản
0:11:29 - 0:11:33, Cộng với lại encode cả thông tin về mặt hình ảnh
0:11:34 - 0:11:37, Do đó số lượng tham số của mình
0:11:37 - 0:11:42, của các mô hình diffusion thường rất là lớn
0:11:42 - 0:11:46, có những mô hình lên đến gần 1 tỷ tham số
0:11:46 - 0:11:50, có những mô hình hiện đại hơn thì có thể lên đến 3 tỷ, 4 tỷ tham số
0:11:50 - 0:11:58, để tạo được những kỹ ảnh chất lượng tốt và hiểu được văn bản, hiểu được yêu cầu đầu vào của mình
0:11:58 - 0:12:00, thì số tham số là lớn
0:12:00 - 0:12:05, và chúng ta muốn fine-tune mô hình diffusion này với data set của mình
0:12:05 - 0:12:13, Thì khi đó nó rất dễ bị hiện tượng overfitting.
0:12:13 - 0:12:16, Tại vì trong các cái bài trước chúng ta đã nói rồi,
0:12:16 - 0:12:20, hiện tượng overfitting xảy ra khi số lượng tham số lớn và khi dữ liệu của mình ít.
0:12:20 - 0:12:23, Và thông thường mình không phải là một cái doanh nghiệp lớn,
0:12:23 - 0:12:26, thì số data của mình sẽ ít hơn rất là nhiều.
0:12:26 - 0:12:31, Do đó thì nó sẽ bị hai yếu tố này, tham số lớn và dữ liệu thì ít.
0:12:31 - 0:12:33, Thì hiện tượng overfitting.
0:12:33 - 0:12:42, Như vậy thì để có thể fine-tune được thì chúng ta có thể sẽ sử dụng một kỹ thuật nó gọi là low-rank adaptation
0:12:42 - 0:12:50, Ý tưởng của mình rất là đơn giản đó là trong các thao tác tính toán của các mạng transformer
0:12:50 - 0:12:55, thì thao tác attention là một trong những thao tác mà được tính nhiều nhất
0:12:55 - 0:13:09, trong attention thì tham số wkv là những cái ma trận xin lỗi chính xác là wkwv
0:13:09 - 0:13:15, đây chính là những cái tham số để ánh xạ từ x về wkv
0:13:15 - 0:13:25, Đây là tham số mà chiếm nhiều dung lượng tham số nhất
0:13:29 - 0:13:36, Và giả sử chúng ta lấy 1 ma trận WQ ra, thì ma trận này có kích thước là N nhân M
0:13:36 - 0:13:49, Nếu chúng ta fine-tune trên toàn bộ cái ma trận này, thì số lượng tham số chúng ta là lớn
0:13:49 - 0:13:57, Do đó ý tưởng của Low-Rank Adaptation đó là chúng ta sẽ cộng cái ma trận WQ mà đã train trước đó
0:13:57 - 0:14:03, thì chúng ta sẽ đóng băng nó lại, hiểu đóng băng là chúng ta sẽ để cái dấu chấm này hoặc là dấu gạch này đi, là đóng băng lại
0:14:03 - 0:14:06, sau đó chúng ta sẽ cộng nó với 1 cái low-rank ma trận
0:14:06 - 0:14:08, 1 cái ma trận có cái hạng thấp
0:14:08 - 0:14:12, thì ví dụ như đây là 1 cái ma trận A
0:14:12 - 0:14:14, A thấp
0:00:14 - 0:14:17, thế thì 1 cái ma trận low-rank là 1 cái ma trận mà có thể
0:14:17 - 0:14:20, A của chúng ta có thể phân rã ra được
0:14:20 - 0:14:22, bằng 2 cái ma trận
0:14:22 - 0:14:24, có cái rank thấp
0:14:29 - 0:14:32, ví dụ ở đây là n nhân m
0:14:32 - 0:14:37, ở đây sẽ là n và m
0:14:37 - 0:14:43, nhưng mà phần ma trận bên trái thì kích thước của mình sẽ là d
0:14:43 - 0:14:49, thì a của mình là bằng tích của hai ma trận low rank
0:14:49 - 0:14:54, trong đó d sẽ rất bé so với n, d rất bé so với m
0:14:54 - 0:15:01, khi đó số lượng tham số của ma trận a sẽ rất ít so với n và m
0:15:01 - 0:15:06, Như vậy thì chúng ta đã giảm số lượng tham số của mô hình xuống
0:15:06 - 0:15:10, Thông qua việc kết hợp ma trận WQ đã được train trước đó
0:15:13 - 0:15:16, Với một ma trận low-rank là A
0:15:17 - 0:15:22, Nếu chúng ta fine-tune từ đầu thì nó sẽ là n nhân m tham số
0:15:22 - 0:15:27, Trong khi đó nếu chúng ta fine-tune mà kết hợp WQ đóng băng
0:15:27 - 0:15:28, Cộng với lại A
0:15:28 - 0:15:30, Cộng với ma trận A này
0:15:30 - 0:15:34, thì số tham số của mình là không tính, tại vì đã đóng băng rồi nên không tính
0:15:34 - 0:15:39, mà bên đây sẽ là n cộng m, tất cả nhân d
0:15:39 - 0:15:46, vì d rất bé hơn so với nm nên tỷ số này chắc chắn là số tham số nm lớn hơn
0:15:46 - 0:15:48, số tham số 3 rất là nhiều
0:15:48 - 0:15:55, thì đây là một kỹ thuật rất phổ biến và cho độ chính xác, cho tính hiệu quả rất là cao
0:15:55 - 0:16:08, Rồi, thì đây là những cái mô hình thay vì chúng ta fine-tune trên toàn bộ cái ma trận W
0:16:08 - 0:16:12, Giống như hồi nãy chúng ta nói thay vì chúng ta fine-tune trên toàn bộ cái ma trận W
0:16:12 - 0:16:13, Kích thước là D nhân K
0:16:13 - 0:16:21, Thì bây giờ chúng ta sẽ lấy cái ma trận này cộng với một cái ma trận kích thước là D nhân R, R nhân cho K
0:16:21 - 0:16:26, R là rank của ma trận mới
0:16:26 - 0:16:30, R này sẽ rất bé, chúng ta thấy nó rất nhỏ so với D
0:16:30 - 0:16:33, Đây chính là ý tưởng của Low-Rank
0:16:33 - 0:16:37, Low-Rank LoRA đã kết hợp với rất nhiều phương pháp
0:16:37 - 0:16:43, ví dụ như Latent Consistency Model kết hợp với các mô hình Distillation
0:16:43 - 0:16:49, để cho những ảnh có kết quả rất đẹp và chất lượng cao
0:16:49 - 0:16:54, như đây là Latent Consistency Model
0:16:54 - 0:16:58, thì LCM LoRA cho kết quả ở hàng trên
0:16:58 - 0:17:01, rồi với cái Backbone SD là 1.5
0:17:01 - 0:17:05, Backbone tốt hơn là SD XL
0:17:05 - 0:17:07, rồi SD 1 tỷ tham số
0:17:07 - 0:17:09, thì với cái mô hình số tham số càng nhiều
0:17:09 - 0:17:11, đương nhiên là cái chất lượng sẽ càng cao
0:17:11 - 0:17:15, nhưng mà cho dù gì đi chăng nữa thì
0:17:15 - 0:17:17, khi dùng LCM với lại LoRA
0:17:17 - 0:17:19, thì vừa đạt được tốc độ tốt
0:17:19 - 0:17:23, mà vừa cái dữ liệu của mình không có bị hiện tượng overfitting.