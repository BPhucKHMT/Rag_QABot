0:00:14 - 0:00:25, Như vậy, trong phần này chúng ta đã cùng nghiên cứu về vấn đề Overfitting.
0:00:25 - 0:00:32, Đây là một vấn đề kinh điển trong lĩnh vực học máy và có hai lý do.
0:00:32 - 0:00:39, Một là do mô hình quá phức tạp, tham số quá nhiều, trong khi đó dữ liệu của mình quá ít.
0:00:39 - 0:00:44, Và để giải quyết vấn đề Overfitting, chúng ta sẽ có một trong hai cách.
0:00:44 - 0:00:51, Đó là 1 là tăng data length và 2 là chúng ta giảm tham số của mô hình xuống.
0:00:51 - 0:01:01, Nhưng mà đương nhiên các công ty công nghệ muốn mô hình của mình có tính chất tổng quát cao và có khả năng nhớ được dữ liệu,
0:01:01 - 0:01:05, thì họ làm điều ngược lại ở khía cạnh số lượng tham số.
0:01:05 - 0:01:16, Các công ty công nghệ lớn tăng số lượng param lên, nhưng đồng thời data của họ tăng lên cũng rất nhiều để cho tương ứng.
0:01:17 - 0:01:24, Tại vì họ không có thiếu tài nguyên tính toán và dữ liệu nên cái việc này là khả thi đến với họ.
0:01:24 - 0:01:31, Rồi, vấn đề thứ 2 là chính là cái vấn đề về Vanishing Gradient.
0:01:31 - 0:01:40, Đây là một trong những vấn đề rất là đau đầu khi chúng ta làm với các thuật toán, các mô hình học sâu.
0:01:40 - 0:01:46, Tại vì với những mô hình học sâu thì chúng ta sẽ phải nhân đạo hàm rất là nhiều lần.
0:01:46 - 0:01:56, Và xu hướng khi mà cần tiến đến cái giá trị cực tiểu thì đạo hàm của mình sẽ càng giảm, dẫn đến đó là cái gradient của mình sẽ giảm, dẫn đến là cái mô hình của mình không hội tụ.
0:01:56 - 0:02:04, Sau đó thì đối với 2 cái vấn đề này, chúng ta đã có những cái cải tiến của các biến thể liên quan đến mạng CNN
0:02:04 - 0:02:14, và các mô hình dựa trên chuỗi, ví dụ như là RNN và Transformer.
0:02:14 - 0:02:27, Trong đó Transformer là mô hình sinh sau đẻ muộn. Nó đã tận dụng được rất nhiều thành tựu của các kiến trúc trước đó.
0:02:27 - 0:02:44, Ví dụ như là Skip Connection, Ví dụ như là LayerNorm, Ví dụ như là cái optimizer là AdamW, Ví dụ như là StackLayer.
0:02:44 - 0:03:01, Ví dụ như là
0:03:01 - 0:03:13, Ví dụ như là TransGPT, Ví dụ như là Transformer.
0:03:13 - 0:03:22, Và việc huấn luyện các mô hình mà Transformer hiện nay thì đã có rất nhiều những doanh nghiệp công ty,
0:03:22 - 0:03:31, hoặc là tổ chức nghiên cứu lớn mà thường là tổ chức nghiên cứu ở bên trong doanh nghiệp lớn để tạo ra các mô hình
0:03:31 - 0:03:37, cho cộng đồng có thể sử dụng, ví dụ như là DeepSix, Ví dụ như là Llama, v.v.
0:03:37 - 0:03:45, Các mô hình này đã góp phần cho việc nghiên cứu các mô hình học sâu hiện đại ngày càng trở nên thuận tiện hơn.
0:03:45 - 0:03:51, Trên đây là bài giảng về quá trình tiến hóa của các mô hình học sâu.