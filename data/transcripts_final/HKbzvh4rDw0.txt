0:00:14 - 0:00:27, Chúng ta sẽ cùng vận dụng những lý thuyết về balancing gradient và overfitting đã được tìm hiểu ở những phần trước để lý giải cho sự tiến hóa của các mạng CNN.
0:00:27 - 0:00:38, Các mạng CNN ở đây sẽ được khảo sát từ giai đoạn mới bắt đầu có ý tưởng của mạng CNN cho đến những mạng CNN hiện đại hơn.
0:00:38 - 0:00:46, Đầu tiên đó chính là mô hình Logistic Regression hay là một cái Perceptron đơn giản.
0:00:46 - 0:00:52, Ở đây chúng ta thấy mạng của chúng ta chỉ có duy nhất một node, duy nhất một neuron.
0:00:53 - 0:01:01, Và ý nghĩa của mô hình này là nó sẽ phân chia không gian đặc trưng ra, làm thành một mặt phẳng.
0:01:01 - 0:01:14, Cụ thể ở đây, các tham số theta 0, theta 1, theta 2 và theta m, thì đây chính là các tham số của phương trình đường thẳng.
0:01:14 - 0:01:24, Và chính xác hơn là của mặt phẳng tại vì chúng ta sẽ xét trong không gian nhiều chiều.
0:01:24 - 0:01:33, Và cái mặt phẳng này nó sẽ chia không gian ra làm hai phần, một cách tuyến tính.
0:01:33 - 0:01:44, Và ví dụ như chúng ta có tập hợp các cái điểm như thế này, thì nếu như các điểm mà có thể phân chia được bởi một cái siêu phẳng hay là một cái mặt phẳng,
0:01:44 - 0:01:50, thì khi đó chúng ta có thể sử dụng cái mạng Logistic Regression hoặc là Perceptron.
0:01:50 - 0:01:56, Tại vì với cái đường thẳng này thì nó sẽ tách ra làm hai, một cách dễ dàng đối với dữ liệu tuyến tính.
0:01:56 - 0:02:04, Vấn đề xảy ra đó là mỗi một cái Perceptron thì nó chỉ có thể giải quyết được một cái bài toán tuyến tính.
0:02:04 - 0:02:15, Trong khi đó sẽ có rất nhiều những cái bài toán phức tạp hơn và chúng ta không thể sử dụng một cái mạng Perceptron để có thể giải quyết được.
0:02:15 - 0:02:22, Chúng ta lấy một cái tình huống ví dụ đó là hai cái tập điểm nằm trong và nằm ngoài vòng tròn.
0:02:22 - 0:02:27, Đó là hình tròn và dấu cộng, các điểm tròn và cộng như thế này.
0:02:27 - 0:02:32, Thì chúng ta thấy là với một đường thẳng thì không thể nào tách ra làm hai phần được,
0:02:32 - 0:02:36, mà chúng ta cần phải có một cái tổ hợp các cái đường thẳng.
0:02:36 - 0:02:44, Cụ thể đó là chúng ta sẽ cần có cái tổ hợp này để mà chia ra làm hai.
0:02:44 - 0:02:54, Thế thì để mà có thể phối hợp và tổng hợp được các cái thông tin của một loạt các cái đặc trưng như thế này,
0:02:54 - 0:03:00, thì nó đòi hỏi chúng ta không phải chỉ tăng theo cái chiều dọc như thế này.
0:03:00 - 0:03:09, Tại vì với mỗi một cái neuron này thì chúng ta sẽ có một cái bộ phân loại yếu, đó là một cái mạng phẳng.
0:03:09 - 0:03:18, Nhưng các cái mạng phẳng này muốn mà có thể giải quyết được bài toán vi tuyến thì nó đòi hỏi phải có một cái thao tác để tổng hợp.
0:03:18 - 0:03:23, Và thao tác tổng hợp đó thì nó sẽ được đặt ở cái layer tiếp theo.
0:03:23 - 0:03:28, Còn nếu không có cái layer này thì các cái neuron nó sẽ là độc lập nhau.
0:03:28 - 0:03:34, Neuron số 1, neuron số 2 và neuron số 3 là độc lập.
0:03:34 - 0:03:39, Nó sẽ không thể phối hợp với nhau.
0:03:39 - 0:03:48, Nhưng nhờ có cái neuron số 4 ở cái layer tiếp theo, nó tổng hợp lại để phối hợp và tạo ra một cái đặc trưng mạnh hơn.
0:03:48 - 0:03:57, Đó là ý nghĩa của việc mà chúng ta tăng thêm một cái layer, nó sẽ giúp chúng ta tạo ra một cái đặc trưng mới.
0:03:57 - 0:04:05, Vì vậy, với một neuron thì đặc trưng của chúng ta quá đơn giản nên không giải quyết được các bài toán phức tạp hoặc là bài toán vi tuyến.
0:04:05 - 0:04:14, Vậy thì đến với cái mạng neural network, nhờ có các cái hidden layer, tức là các cái lớp ẩn.
0:04:14 - 0:04:22, Các cái lớp ẩn này thì nó sẽ tổng hợp, ví dụ như một cái node ở đây, chúng ta thấy là kết nối với các cái node ở phía trước.
0:04:22 - 0:04:26, Tức là nó đang tổng hợp đặc trưng.
0:04:30 - 0:04:35, Các cái đặc trưng ở lớp trước, đó là những đặc trưng đơn giản.
0:04:35 - 0:04:49, Nhưng mà qua cái quá trình tổng hợp ở đây thì nó sẽ tạo ra các cái đặc trưng mới phức tạp hơn và nó phi tuyến hơn.
0:04:50 - 0:04:57, Rõ ràng các cái đặc trưng phi tuyến tính, nó sẽ giúp cho chúng ta giải quyết được những cái bài toán phức tạp.
0:04:57 - 0:05:02, Các cái đặc trưng phức tạp sẽ giúp cho chúng ta giải quyết được các cái bài toán phi tuyến tính.
0:05:02 - 0:05:08, Và MLP, tức là cái mạng neural network của mình, một cái tên gọi khác của mạng neural network,
0:05:08 - 0:05:14, nó sẽ tổng hợp đặc trưng phức tạp hơn từ các cái đặc trưng đơn giản qua các cái lớp ẩn.
0:05:14 - 0:05:19, Với lớp ẩn này, tổng hợp đặc trưng đơn giản để tạo ra thành các đặc trưng mới.
0:05:21 - 0:05:29, Và khi chúng ta tổ hợp, chúng ta tạo ra càng thêm nhiều layer, thì các cái cấp độ đặc trưng của chúng ta cũng sẽ đa dạng hơn.
0:05:29 - 0:05:42, Từ đơn giản cho đến trung bình, rồi sau đó sẽ đến là các cái đặc trưng cấp cao, hoặc phức tạp.
0:05:44 - 0:05:51, Thì tổ hợp các cái đặc trưng này, nó sẽ phối hợp với nhau để có thể giải quyết được các bài toán khó.
0:05:51 - 0:06:00, Vậy thì vấn đề của mạng neural network đó là gì? Đó là khi chúng ta xử lý trên cái dữ liệu lớn, có kích thước lớn,
0:06:00 - 0:06:04, ví dụ như dữ liệu ảnh, thì số lượng tham số có thể bùng nổ.
0:06:04 - 0:06:12, Chúng ta lấy ví dụ như ở đây chúng ta sẽ không dùng vector, chúng ta không dùng những cái vector mà có số chiều nhỏ,
0:06:12 - 0:06:18, mà chúng ta sẽ dùng mạng neural network để xử lý trên cái ảnh có cái kích thước lớn.
0:06:18 - 0:06:24, Thì ở đây giả sử chúng ta xét một cái ảnh có kích thước rất là tương đối là khiêm tốn,
0:06:24 - 0:06:28, thực tế thì ảnh có thể lớn hơn, độ phân giải lớn hơn.
0:06:28 - 0:06:40, Thì 200 x 200 này, khi chúng ta flatten ra, thì nó sẽ tương đương với một cái vector mà có kích thước đó là 200 x 200, tức là 40 nghìn.
0:06:40 - 0:06:48, Và giả sử như cái mạng neural network của chúng ta có một cái hidden layer có số chiều đúng bằng số chiều input,
0:06:48 - 0:06:58, tức là đây là một cái kiến trúc mà tương đối là đơn giản, thực tế thì có thể là số lượng neuron của mình có thể nhiều hơn.
0:06:58 - 0:07:09, Thế thì ở đây sẽ là có 40 nghìn neuron, vì ở đây chúng ta kết nối đầy đủ nên mỗi một cái neuron output sẽ kết nối với tất cả những neuron input.
0:07:09 - 0:07:17, Như vậy thì chúng ta sẽ có tất cả là 40 nghìn nhưng 40 nghìn thì đâu đó là cỡ 1.6 tỷ tham số.
0:07:17 - 0:07:31, Và với cái mô hình mà có cái số lượng tham số bùng nổ như thế này, thì trong hồi trước chúng ta đã giới thiệu đó là nó gây ra cái hiện tượng overfitting,
0:07:31 - 0:07:40, do không thể nào mà thu thập được một cái lượng dữ liệu mà xấp xỉ lên đến hàng tỷ mẫu như thế này. Cái quy mô nó quá lớn.
0:07:40 - 0:07:52, Vậy thì từ đó nó sẽ cho ra đời là cái mạng CNN. Mạng CNN thì bản chất là nó có một cái phần rút trích đặc trưng riêng,
0:07:52 - 0:08:02, riêng đó là dùng cái phép biến đổi convolution. Thì các lớp convolution và relu hoặc là sigmoid, hoặc một hàm kích hoạt là sigmoid,
0:08:02 - 0:08:13, thì mục đích của nó đó là để rút trích ra đặc trưng ảnh. Và đặc trưng này nó sẽ được sắp xếp từ đơn giản cho đến phức tạp.
0:08:13 - 0:08:23, Và các lớp MLP phía sau thì mục đích của nó là để phân lớp đặc trưng.
0:08:23 - 0:08:33, Còn ở cái lớp phía trước mục tiêu của nó đó là chúng ta sẽ rút trích đặc trưng.
0:08:33 - 0:08:42, Và các đặc trưng này thì sẽ được đi từ thấp cho đến cao, từ đặc trưng level thấp cho đến đặc trưng level cao.
0:08:42 - 0:08:55, Vậy thì với một cái mạng CNN thì chúng ta sẽ tìm hiểu xem ý nghĩa của nó là gì. Ví dụ như trong phần trước,
0:08:55 - 0:09:04, chúng ta đã tìm hiểu ý nghĩa của một cái mạng Neural network. Đây là mỗi cái node này tổng hợp đặc trưng ở lớp trước đó.
0:09:04 - 0:09:16, Vậy thì CNN thì sao? Với mỗi một cái lát cắt ở đây, với ảnh dữ liệu đầu vào, chúng ta thấy ở đây sẽ có 3 chiều.
0:09:16 - 0:09:23, Trong đó chiều ngang và chiều dọc chính là chiều không gian là width và height. Đây là hai chiều không gian.
0:09:23 - 0:09:32, Còn chiều này sẽ là chiều độ sâu d. Đây chính là chiều của đặc trưng.
0:09:32 - 0:09:45, Rồi, tức là số lượng đặc trưng của mình. Ví dụ trong cái feature map ở đây, thì mỗi một cái lát cắt ở đây
0:09:45 - 0:09:55, nó sẽ là một cái đặc trưng của cái input đầu vào. Trên hình chúng ta thấy là một cái vùng màu đen, thì đây sẽ là một cái feature.
0:09:55 - 0:10:03, Và feature này là của một cái lát cắt đầu tiên. Chúng ta sẽ có rất nhiều những cái lát cắt.
0:10:03 - 0:10:11, Và với mỗi cái lát cắt này, nó sẽ là một cái feature, một cái đặc trưng.
0:10:11 - 0:10:19, Thì nhiều cái đặc trưng chúng ta tổng hợp lại, thì nó sẽ giúp cho chúng ta có cái góc nhìn đa chiều về đối tượng của mình.
0:10:19 - 0:10:25, Và sau đó thì chúng ta hoàn toàn có thể sử dụng các cái đặc trưng này để phục vụ cho cái tác vụ phía sau.
0:10:25 - 0:10:36, Như vậy thì cái xu hướng phát triển chung của cái mạng CNN, đó là đầu tiên, đó là tăng độ sâu.
0:10:36 - 0:10:44, Nếu như ở những cái layer đầu tiên thì cái độ sâu của mình khá là thấp. Ví dụ như ảnh đầu vào của mình thì có thể độ sâu là 3.
0:10:44 - 0:10:54, Rồi sau đó đến các cái đặc trưng cấp thấp thì cái độ sâu của mình có thể từ 64 cho đến khoảng 128.
0:10:54 - 0:11:04, Nhưng mà càng về sau, chúng ta sẽ càng có nhiều cái layer. Ví dụ như ở đây chúng ta có thêm các cái layer ở giữa.
0:11:04 - 0:11:08, Tương ứng là các cái đặc trưng cấp thấp, cấp vừa và cấp cao.
0:11:08 - 0:11:16, Và càng tiến đến về các cái đặc trưng cấp cao thì cái độ sâu của mình, cái độ sâu của đặc trưng cũng sẽ càng tăng.
0:11:20 - 0:11:26, Tăng về độ sâu của đặc trưng.
0:11:26 - 0:11:42, Hay nói cách khác, đó là số lượng đặc trưng cấp cao, đặc trưng mà cấp cao, 2 levels cũng sẽ tăng lên.
0:11:42 - 0:11:50, Và nhờ có nhiều cái đặc trưng cấp cao này thì giúp cho chúng ta phân loại được dễ dàng hơn cho các bước sau.
0:11:50 - 0:11:54, Thì ở đây chúng ta thấy là nó sẽ tăng dần về độ sâu.
0:11:54 - 0:12:02, Tại vì ở những cái đặc trưng lớp đầu là những cái đặc trưng cấp thấp thì nó rất là đơn giản và ít.
0:12:02 - 0:12:13, Ví dụ như có đặc trưng biên cạnh hay chiều dọc, đặc trưng biên cạnh hay chiều ngang, rồi đặc trưng về đường sọc, đường chéo, rồi đặc trưng về màu, màu xanh, màu đỏ, màu vàng v.v.
0:12:13 - 0:12:23, Thì cái số lượng đặc trưng đó rất là ít, nhưng mà khi lên các đặc trưng cấp cao, nó là tổ hợp, tổ hợp tổng hợp các đặc trưng ở lớp trước đó.
0:12:23 - 0:12:41, Ví dụ một cái lát cắt này, nó sẽ là tổ hợp của một loạt các lát cắt ở trước đó để có thể tổ hợp ra một cái lát cắt này.
0:12:41 - 0:12:51, Do đó thì ở phía sau nó sẽ là tổ hợp của những đặc trưng ở phía trước nên số lượng chắc chắn nó sẽ nhiều hơn, số lượng tổ hợp đặc trưng.
0:13:02 - 0:13:04, Tăng dần.
0:13:04 - 0:13:26, Và khi đến cái lớp cuối cùng thì cái đặc trưng của mình là đặc trưng cấp cao nhất rồi, thì nó sẽ hỗ trợ cho cái việc phân biệt và các cái đặc trưng này đã đủ để có thể tuyến tính và đủ thông tin để có thể giúp cho chúng ta giải quyết cái tác vụ của mình.
0:13:26 - 0:13:41, Và đi kèm cùng với sự tăng về độ sâu thì chúng ta sẽ thấy là các cái mạng CNN của mình nó sẽ lần lượt giải quyết các cái vấn đề mà chúng ta thường gặp phải khi chúng ta tăng cái độ sâu này.
0:13:41 - 0:13:55, Tức là khi tăng độ sâu thì có thể thấy ngay một cái vấn đề đầu tiên đó chính là vấn đề về overfitting là khi bùng nổ số lượng đặc trưng, số lượng tham số của mô hình thì dẫn đến là overfitting.
0:13:55 - 0:14:07, Đồng thời là các cái phép biến đổi này là hàm hợp lồng nhau rất là nhiều thì có những cái kiến trúc mạng mà lên đến là 20, 30, thậm chí là 100 layer.
0:14:07 - 0:14:17, Tức là số layer này lên đến hàng trăm thì nó sẽ gây ra cái hiện tượng là vanishing gradient như chúng ta đã giải thích trong phần trước.
0:14:17 - 0:14:33, Do đó thì các cái mạng CNN của mình bên cạnh cái việc tăng độ sâu để tạo ra các đặc trưng đa dạng từ đơn giản trung bình đến phức tạp thì chúng đồng thời là cũng tìm cách giải quyết những cái vấn đề khi mà tăng cái độ sâu này lên.
0:14:33 - 0:14:37, Thì đó chính là cái xu hướng phát triển chung của mạng CNN.
0:14:47 - 0:14:51, Hãy subscribe cho kênh Ghiền Mì Gõ để không bỏ lỡ những video hấp dẫn.