0:00:14 - 0:00:21, Tiếp theo, chúng ta sẽ cùng tìm hiểu cách thức mà một mạng tạo sinh đối kháng, mạng GAN được huấn luyện như thế nào.
0:00:22 - 0:00:29, Đầu tiên, chúng ta sẽ nhắc lại, trong một kiến trúc của mạng GAN, sẽ bao gồm hai mô đun.
0:00:29 - 0:00:37, Mô hình G mục tiêu là tạo ra dữ liệu giả để cố đánh lừa mô hình phân loại D.
0:00:37 - 0:00:47, Còn ngược lại, D là mô hình phân loại cố gắng thực hiện xác định xem dữ liệu nào là thật và dữ liệu nào giả do G tạo ra.
0:00:47 - 0:00:56, Khi huấn luyện, với mục tiêu đối nghịch cho D và G, chúng ta sẽ tạo ra một hàm loss đặc biệt.
0:00:56 - 0:01:03, Đối với điểm tối ưu toàn cục, G có khả năng tạo ra một phân phối là dữ liệu thật.
0:01:03 - 0:01:20, Tức là khi mô hình này huấn luyện thành công, G sẽ càng lúc cho cái độ chính xác, ảnh thực càng lúc càng giống thật.
0:01:20 - 0:01:26, Và hàm lỗi trong việc huấn luyện một mạng GAN sẽ được tính như thế nào?
0:01:27 - 0:01:31, Thế thì trước mắt chúng ta sẽ xét đối với mô hình phân loại D.
0:01:31 - 0:01:42, Đối với mô hình phân loại D, chúng ta sẽ tìm cách cực đại hóa kỳ vọng này.
0:01:42 - 0:01:53, Trong đó, đối với dữ liệu thật, chúng ta luôn mong muốn đó là xác suất để dự đoán điểm dữ liệu thật là cao nhất.
0:01:53 - 0:01:57, Tức là nó sẽ tiến về 1.
0:01:57 - 0:02:08, Ngược lại, đối với những điểm dữ liệu FAKE, tức là dữ liệu giả, thì người ta sẽ luôn mong muốn là cái giá trị này tiến về 0.
0:02:08 - 0:02:09, Tức là nó thấp.
0:02:09 - 0:02:22, Khi thằng này tiến về 0, tức là nó cho biết đó là dữ liệu giả, thì 1 trừ 0, nguyên cả cái vế này, nó sẽ càng cao, càng tốt.
0:02:22 - 0:02:24, Và nó sẽ tiến về 1.
0:02:24 - 0:02:27, Nó cũng hoàn toàn tương tự như cái vế bên đây.
0:02:27 - 0:02:36, Như vậy thì tổng hợp lại, đó là cả dữ liệu thật và dữ liệu giả, thì log của DX cộng cho log của 1 trừ D của GX,
0:02:36 - 0:02:41, thì nó đều phải được hướng đến sao cho đạt giá trị nhỏ nhất.
0:02:41 - 0:02:51, Và chúng ta chú ý là tại thời điểm này, chúng ta sẽ đi tìm cái discriminator, tức là cái bộ phân loại.
0:02:51 - 0:02:55, Còn G trong trường hợp này, đó là cố định.
0:02:55 - 0:02:58, G trong trường hợp này là cố định.
0:03:01 - 0:03:07, Rồi, và G của G chính là cái dữ liệu fake này nè.
0:03:07 - 0:03:10, Đây chính là G của G.
0:03:11 - 0:03:22, Thì qua cái D, nó sẽ cố gắng làm sao cho cái D của G, tức là cái xác suất thuộc về cái lớp dữ liệu thật là thấp nhất có thể, tức là tiến về 0.
0:03:22 - 0:03:30, Rồi, thì đây là cái phần hàm lỗi cho discriminator, tức là cái mô hình phân loại.
0:03:31 - 0:03:40, Chúng ta sẽ đến một cái hàm lỗi tiếp theo liên quan đến cái bộ tạo sinh, tức là cái mô hình tạo sinh G.
0:03:40 - 0:03:49, Thì nhắc lại, G mục tiêu đó là làm sao cho cái dữ liệu giả của mình, nó có thể đánh lừa được cái mô hình phân loại D.
0:03:49 - 0:03:59, Thế thì ở khía cạnh ngược lại, chúng ta thấy là đối chiếu với cái công thức loss của discriminator, chúng ta sẽ thấy nó có cái dạng thức giông giống nhau.
0:03:59 - 0:04:01, Nó sẽ có dạng thức giống nhau.
0:04:01 - 0:04:07, Cái khác ở đây đó là thay vì chúng ta tìm max, thì ở đây chúng ta lại tìm min.
0:04:07 - 0:04:11, Tức là chúng ta đang đi làm một cái công việc ngược lại.
0:04:11 - 0:04:28, Và tương tự như trong cái công thức hàm loss của discriminator, thì cái công thức của logDx, thì dx của mình, đó là cái xác suất thuộc về cái lớp dữ liệu giả.
0:04:28 - 0:04:31, Dữ liệu thật.
0:04:31 - 0:04:44, Thì nếu như thật sự đó là dữ liệu thật, thì chúng ta luôn đối với generator, chúng ta muốn kéo cái xác suất đó xuống thấp.
0:04:48 - 0:04:50, Càng thấp càng tốt.
0:04:50 - 0:05:00, Còn đối với cái dữ liệu giả thì sao? Đối với dữ liệu giả thì đây là cái ảnh tạo bởi dữ liệu giả.
0:05:00 - 0:05:10, Và dữ liệu dx, tức là cái xác suất mà để phân biệt đó là dữ liệu thật hay không, thì mình lại luôn mong muốn đó là càng cao càng tốt.
0:05:10 - 0:05:17, Tại vì đó là dữ liệu giả, thì mình muốn đánh lừa mà. Mình muốn đánh lừa nên mình phải đưa cái xác suất này nó lên cao.
0:05:17 - 0:05:29, Và nó càng tiến về 1 thì càng tốt. Do đó thì 1-D của dx thì nó sẽ tiến về 0, tức là càng thấp càng tốt. Nguyên cái vế này sẽ là càng thấp càng tốt.
0:05:35 - 0:05:44, Rồi như vậy tóm lại cả 2 số hạng đó là logD và log của 1-D của G(z) thì đều là càng thấp càng tốt.
0:05:44 - 0:05:57, Do đó thì chúng ta sẽ đi tìm D sao cho cái kỳ vọng này là nhỏ nhất. Lưu ý là cái công thức kỳ vọng ở đây là được tính trên những cái mẫu dữ liệu mà mình đã lấy mẫu.
0:05:57 - 0:06:09, Rồi và trong công thức này chúng ta đi tìm D để cho tối ưu do đó, D của mình sẽ là 1 cái hàm cố định.
0:06:09 - 0:06:23, Vậy thì khi chúng ta kết hợp cả huấn luyện cả D và G thì sao? Khi chúng ta huấn luyện cả D và G thì chúng ta sẽ có 1 cái hàm, nó gọi là hàm minimax.
0:06:23 - 0:06:37, Vì đây là 1 cái bài toán gọi là minimax. Trong đó cái vế bên trong là chúng ta sẽ phải đi tìm D sao cho cái kỳ vọng này là lớn nhất trước.
0:06:37 - 0:06:47, Tức là chúng ta sẽ đi huấn luyện D trước. Sau khi chúng ta huấn luyện D cho tốt xong thì chúng ta sẽ đi tiếp tục huấn luyện G.
0:06:47 - 0:06:55, Chúng ta sẽ tìm G sao cho nguyên cái vế này là nhỏ nhất. Thì khi đó là D của mình sẽ là cố định.
0:06:55 - 0:07:01, Tức là khi chúng ta đi tìm G để cho cái giá trị của mình nhỏ nhất thì D lúc này nó đã được cố định rồi.
0:07:01 - 0:07:09, Và cứ như vậy thì chúng ta sẽ, cái quá trình này chúng ta sẽ cập nhật sao cho đến khi nào mà nó hội tụ.
0:07:10 - 0:07:19, Thì khi mà nó hội tụ, tức là D nó trở nên là càng lúc càng tốt và G cũng càng lúc trở nên càng tốt.
0:07:19 - 0:07:25, Thì khi đó là cái mô hình của mình, cả 2 thằng giống như là 2 đôi bạn cùng tiến.
0:07:25 - 0:07:31, G thì sẽ càng lúc càng tạo ra những cái tấm ảnh chất lượng hơn, chân thật hơn.
0:07:32 - 0:07:37, Còn D thì cái khả năng phân biệt, cái ảnh thật và ảnh giả ngày càng cao.
0:07:37 - 0:07:44, Do là cái khả năng giả hình của G càng cao thì D của mình cũng sẽ càng có tính phân biệt cao hơn.
0:07:44 - 0:07:54, Do đó thì G và D giống như là 2 đôi bạn cùng tiến là sẽ là càng lúc cái khả năng của nó sẽ, cái hiệu năng của nó sẽ ngày càng cao.
0:07:54 - 0:08:02, Vậy thì khi mà cái mô hình, sau khi cái mô hình của mình đã được huấn luyện,
0:08:02 - 0:08:10, thì chúng ta sẽ sử dụng G như là một cái hàm tạo sinh để tạo ra cái dữ liệu mới mà mình chưa từng bắt gặp trước đó.
0:08:10 - 0:08:16, Cái cách tạo sinh của chúng ta cũng rất là dễ. Cái thành phần D thì chúng ta sẽ bỏ qua và không sử dụng.
0:08:16 - 0:08:27, Và chúng ta chỉ cần random một cái noise G, rồi sau đó từ cái noise G này chúng ta qua cái hàm G thì chúng ta sẽ tạo ra được một cái tấm ảnh mới.
0:08:27 - 0:08:35, Và cái việc biến đổi cái phân phối đối với G nó cũng khá là có tính là liên tục.
0:08:35 - 0:08:41, Ví dụ như ở đây chúng ta có một cái vector G tuân theo phân bố 0,1 nó nằm ở phía trên.
0:08:41 - 0:08:47, Thì khi chúng ta qua cái hàm generator nó sẽ tạo ra cái hình, cái con ngỗng màu đen như thế này.
0:08:47 - 0:08:54, Và tương đối với trong cái không gian của cái ảnh thật, nó nằm ở đây. Đây là phân phối dữ liệu mục tiêu.
0:08:54 - 0:09:03, Và khi chúng ta sampling một cái điểm khác ở phía dưới đây, thì qua cái hàm G chúng ta có hình một cái con chim như thế này.
0:09:03 - 0:09:05, Và nó nằm ở đây.
0:09:05 - 0:09:13, Thế thì khi chúng ta di chuyển từ cái điểm ở trên xuống dưới cái điểm ở dưới, cứ lần lượt với mỗi điểm chúng ta lấy mẫu,
0:09:13 - 0:09:19, thì chúng ta qua cái hàm G thì chúng ta cũng sẽ được các cái mẫu dữ liệu.
0:09:19 - 0:09:30, Và khi chúng ta vẽ các cái mẫu dữ liệu này lên thì chúng ta thấy là có một cái sự dịch chuyển mượt mà từ cái ảnh con ngỗng màu đen sang cái con chim màu cam này.
0:09:31 - 0:09:42, Thì chúng ta từ trái sang phải thì cũng giống như là chúng ta lấy mẫu từ trên xuống dưới với mỗi điểm trong cái không gian latent, chúng ta qua generator, chúng ta tạo ra một tấm ảnh.
0:09:42 - 0:09:48, Thì chúng ta thấy là dần dần cái con ngỗng màu đen này, cái đầu của mình nó hướng về phía bên đây,
0:09:48 - 0:09:55, thì nó dần dần là cái đầu nó sẽ không rõ hướng, rồi sau đó qua đây sẽ là hướng về bên tay trái.
0:09:56 - 0:10:08, Về màu sắc thì chúng ta thấy là nó đang màu đen, đến đây thì nó bắt đầu dần dần ngã màu chuyển sang cái cánh,
0:10:08 - 0:10:16, và sau đó là cái phần bụng chuyển sang màu cam, thì từ trái sang phải chúng ta thấy có cái sự thay đổi một cách mượt mà,
0:10:16 - 0:10:20, thì là biến đổi cái phân phối với cái mô hình gan.
0:10:25 - 0:10:35, Hãy subscribe cho kênh Ghiền Mì Gõ để không bỏ lỡ những video hấp dẫn.