0:00:14 - 0:00:26, Thế thì nãy giờ chúng ta đã nói về những mô hình tuyến tính, tức là cái giá trị đầu ra y nó sẽ phụ thuộc một cách tuyến tính với lại x.
0:00:26 - 0:00:38, Thế thì nếu trong trường hợp mà y nó phụ thuộc một cách không có tuyến tính, tức là y nó sẽ phụ thuộc phi tuyến với lại x, thì cái giá trị này của mình, cái hàm của mô hình của mình nó sẽ như thế nào?
0:00:38 - 0:00:53, Thì chúng ta sẽ sử dụng một cái mạng đó là mạng Neural Network. Cái sự khác biệt lớn nhất giữa mạng Neural Network so với lại cái mạng Softmax Regression, đó chính là nó có rất nhiều cái lớp hidden layer ở giữa.
0:00:54 - 0:00:59, Đây là các lớp hidden layer.
0:01:03 - 0:01:12, Và đến cái lớp cuối cùng thì nó sẽ tương đương với một cái hàm, tương đương với một cái mô hình đó là mô hình Softmax của mình.
0:01:12 - 0:01:20, Như vậy sự khác biệt của Neural Network và Softmax đó là chúng ta sẽ chèn thêm rất nhiều những cái lớp biến đổi trung gian ở giữa.
0:01:20 - 0:01:28, Với cái việc chèn nhiều lớp biến đổi trung gian ở giữa, nó sẽ giúp cho chúng ta tạo ra được các đặc trưng ở nhiều cấp độ khác nhau.
0:01:28 - 0:01:35, Ở những cái đặc trưng ở lớp đầu tiên thì đây là những cái đặc trưng cấp thấp.
0:01:35 - 0:01:47, Còn đối với những cái lớp tiếp theo thì nó sẽ tạo ra những cái đặc trưng và đặc trưng cấp giữa, tức là midlevel feature.
0:01:47 - 0:01:53, Còn những cái đặc trưng ở lớp cuối cùng, đó sẽ là highlevel feature, tức là những cái đặc trưng cấp cao.
0:01:53 - 0:02:04, Và với những cái đặc trưng cấp cao này thì nó sẽ giúp cho chúng ta phân biệt các mẫu dữ liệu đầu vào một cách rất là dễ dàng, do có tính trừu tượng của nó.
0:02:04 - 0:02:11, Với đây chúng ta sẽ có một cái mô hình trực quan để ví dụ cho dữ liệu của mình, đó là phụ thuộc một cách phi tuyến tính.
0:02:11 - 0:02:21, Với 2 tập điểm trong và ngoài vòng tròn này, chúng ta thấy rằng là không thể nào chúng ta dùng một đường thẳng để chia ra làm 2 phần được.
0:02:21 - 0:02:27, Không có cách nào để chia ra 2 tập mô hình tròn và hình tam giác bằng một đường thẳng.
0:02:27 - 0:02:34, Do đó để chia ra được thì chúng ta chỉ có thể dùng một cái đường phân lớp mà dạng phi tuyến tính như thế này.
0:02:34 - 0:02:36, Nó sẽ đi zigzag như thế này.
0:02:36 - 0:02:40, Và làm sao có thể làm được cái chuyện này?
0:02:40 - 0:02:53, Ý tưởng rất là đơn giản, với mỗi một cái node trong cái mạng Neural Network ở đây, tương ứng đó chính là một cái Logistic Regression.
0:02:57 - 0:03:04, Nó chính là một cái node tổng và sigmoid, tức là một cái lớp hồi quy tuyến tính.
0:03:04 - 0:03:12, Thì đặc trưng cái trọng số của mình đó, nó chính là cái hệ số của cái phương trình đường thẳng để phân tách ra làm 2.
0:03:12 - 0:03:18, Cái trọng số của một cái node Logistic, tức là một cái node hình tròn bao gồm tổng và sigmoid ở đây,
0:03:18 - 0:03:23, thì nó sẽ là một cái trọng số của một cái phương trình đường thẳng để phân tách ra làm 2.
0:03:23 - 0:03:35, Vậy thì chúng ta mượn cái ý tưởng đó để giải thích tại sao một cái mạng Neural Network có thể phân tách được ra khi mà cái mô hình của mình không có thể dùng được một đường thẳng.
0:03:35 - 0:03:40, Với cái node mà chúng ta đang tô đỏ ở đây, thì nó sẽ tạo ra được một cái đường biên.
0:03:40 - 0:03:45, Thì đây nó được gọi là một cái bộ phân lớp yếu.
0:03:46 - 0:03:55, Và một cái bộ phân lớp yếu này, nhiều cái bộ phân lớp yếu này, ví dụ như cái đường màu đen ở đây,
0:03:55 - 0:04:05, thì chúng ta sẽ có một cái đường khác, rồi cái đường màu xanh lá ở đây, thì chúng ta sẽ có một cái đường khác tương tự như vậy.
0:04:05 - 0:04:17, Và tổ hợp, ở cái lớp biến đổi Hidden layer tiếp theo, nó sẽ tổ hợp lại các cái đường phân lớp màu đỏ, màu đen, màu xanh lá và màu nâu
0:04:17 - 0:04:27, để tạo ra thành một cái đường bao khép kín. Và với cái đường bao khép kín này, thì nó đã giúp cho chúng ta phân tách ra được.
0:04:27 - 0:04:32, Làm 2 phần. Đó là cái vùng màu tam giác và hình tròn.
0:04:32 - 0:04:38, Thì đây chính là cái ý tưởng tại sao một cái mạng Neural Network có thể giải quyết được một cái bài toán phi tuyến tính.
0:04:38 - 0:04:48, Là nhờ các cái đặc trưng, các cái bộ phân lớp yếu ở các lớp đầu tiên tổ hợp lại với nhau để tạo ra các cái đặc trưng cấp giữa,
0:04:48 - 0:04:59, tức là đặc trưng cấp vừa. Các cái đặc trưng cấp vừa, nó sẽ tổ hợp lại để tạo ra các cái đặc trưng ở các lớp tiếp theo, đó là đặc trưng cấp cao.
0:04:59 - 0:05:11, Các cái đặc trưng cấp cao này, nó sẽ giúp cho chúng ta khoanh vùng được một cách chính xác và khi độ phức tạp của chúng ta sẽ ngày càng cao khi chúng ta tích hợp càng nhiều lớp phía dưới.
0:05:11 - 0:05:23, Thì đó chính là sơ lược qua những cái kiến trúc mạng dựa trên dạng đồ thị tính toán, chúng ta có thể tạo ra được các cái mô hình giải quyết được các cái bài toán phi tuyến tính và phi tuyến.
0:05:23 - 0:05:33, Và đây là những cái mô hình rất là cơ bản, thì đối với cái mạng Neural Network thì nó đã bắt đầu cho cái kỷ nguyên của Deep Learning, các cái mô hình học sâu.
0:05:33 - 0:05:45, Thì người ta đã có rất nhiều những cái cải tiến, CNN, ANN, Transformer đã có rất nhiều cái cải tiến từ cái mạng Neural Network này để cho mô hình chúng ta có thể huấn luyện một cách dễ dàng hơn
0:05:45 - 0:05:52, và có thể học được trên những cái dữ liệu phức tạp hơn và bài toán phức tạp hơn cũng như là tổng quát hơn.
0:05:52 - 0:06:02, Thì trên đây đó là giới thiệu về mạng Neural Network, chúng ta sẽ kết thúc cái phần đầu đó là lý thuyết về gradient, các cái mô hình dựa trên gradient.
0:06:02 - 0:06:15, Trong những phần tiếp theo thì chúng ta sẽ tìm hiểu về các cái mô hình hiện đại hơn, bao gồm các cái mạng học sâu như là CNN, ANN, Transformer, cũng như là các cái mô hình cho AI tạo sinh.