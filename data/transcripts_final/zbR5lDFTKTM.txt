0:00:14 - 0:00:22, Chúng ta sẽ cùng đến với một trong những mô hình rất quan trọng trong phần này, đó chính là mô hình GPT-4V
0:00:22 - 0:00:37, Chữ V là viết tắt của chữ Vision, đó có tham gia của một language model, là Generative Pre-trained Transformer
0:00:37 - 0:00:57, Mô hình ngôn ngữ lớn có sự phối hợp với dữ liệu về hình ảnh, tuy nhiên đây là một mô hình closed-source, nên chúng ta không bàn nhiều về kiến trúc của nó
0:00:57 - 0:01:03, Nhưng tư tưởng và cách thức vận hành của nó là vấn đề quan trọng mà chúng ta cần phải thảo luận
0:01:03 - 0:01:09, Sau này thì có những mô hình tương tự với GPT-4V
0:01:09 - 0:01:23, Ví dụ như là mô hình LLaVA, chúng ta sẽ tìm hiểu LLaVA trong những phần tiếp theo đã sử dụng ý tưởng của GPT-4V để phát triển
0:01:23 - 0:01:29, GPT-4V có cái gì đặc biệt so với mô hình trước đây
0:01:29 - 0:01:37, Đầu tiên chúng ta sẽ xem các tình huống sử dụng của GPT-4V, đó là Interleaved Image Text Pair
0:01:37 - 0:01:44, Với cái prompt đầu vào của mình, nó có thể mix với nhiều thể thức khác nhau
0:01:44 - 0:01:52, Ví dụ như ở đây là chúng ta không chỉ có một câu hỏi mà thậm chí chúng ta có đến hai câu hỏi là nó đã phức tạp hơn bình thường rồi
0:01:52 - 0:01:57, Cái tiếp theo đó là cái tấm ảnh, nó cũng sẽ có ba tấm ảnh
0:01:57 - 0:02:08, Và cái câu prompt dạng text ở đây nó sẽ phải đi tương tác với lại các dữ liệu ảnh này để mà nó đi tìm ra câu trả lời
0:02:08 - 0:02:15, Và thậm chí là trong cái ví dụ bên tay phải nó còn phải có cái sự tương tác giữa hai ảnh với nhau
0:02:15 - 0:02:24, Ví dụ với cái câu prompt này thì nó chỉ đơn giản là nó đi tìm cái con số nào để thể hiện số đó là thuế phải trả
0:02:24 - 0:02:31, Thì in the first receipt thì mình đã trả 3,75 tiền thuế
0:02:31 - 0:02:35, Nó nằm ở đâu? Nó nằm ở bottom, đại khái vậy
0:02:35 - 0:02:39, Nằm ở phía dưới của cái hóa đơn này
0:02:39 - 0:02:44, Rồi in the second receipt thì nó lại có tương tự như vậy, nó nằm ở đâu?
0:02:44 - 0:02:47, Mô tả cái chi tiết cái vị trí của nó
0:02:47 - 0:02:54, Thế thì ở đây là một cái ví dụ mà cái câu prompt của mình nó tương tác lần lượt với lại các cái ảnh
0:02:54 - 0:02:56, Thì cái này nó vẫn chưa đủ cái độ phức tạp
0:02:56 - 0:03:01, Cái ví dụ kế bên nó là nó có sự tương tác giữa hai ảnh với nhau
0:03:01 - 0:03:07, Còn ở trên là các cái ảnh là độc lập nhau, nó chỉ đi trả lời cái câu hỏi là bao nhiêu tax?
0:03:07 - 0:03:09, Thuế phải đóng thôi
0:03:09 - 0:03:17, Còn ở đây là how much should I pay for the beer on the table according to the price on the menu?
0:03:17 - 0:03:19, Thì ở đây nó có hai cái tấm hình
0:03:19 - 0:03:25, Tấm hình đầu tiên là cái chai bia mà họ uống, cái người chụp đang uống
0:03:25 - 0:03:31, Và tấm thứ hai đó là cái tấm hình của cái menu và chúng ta thấy cái tấm hình này cũng khá là xấu
0:03:31 - 0:03:34, Nó chụp nghiêng rồi nó tối
0:03:34 - 0:03:39, Thì GPT-4V nó đã trả lời đó là nó sẽ lục vào bên trong cái...
0:03:39 - 0:03:44, Đầu tiên là nó sẽ xem cái tấm hình đầu tiên và nó lấy ra được cái tên của cái loại beer
0:03:44 - 0:03:47, Đó là Magna
0:03:47 - 0:03:53, Rồi sau đó nó sẽ đi tra vào bên trong đây là cái beer Magna thì nó nằm ở đâu?
0:03:53 - 0:03:55, Để mà từ đó là nó...
0:03:55 - 0:03:59, Ví dụ đây, nó tìm ra được cái Magna nằm ở đây chẳng hạn
0:03:59 - 0:04:02, Thì nó sẽ trả lời là 6 đô la
0:04:02 - 0:04:06, Và sau đó nó còn lập luận và thực hiện cái thao tác đếm
0:04:06 - 0:04:12, Là trong cái đây có hai chai bia, do đó nó sẽ lấy 6 nhân cho 2 là bằng 12
0:04:12 - 0:04:17, Như vậy thì ở đây chúng ta thấy cái mô hình này nó còn có cái sự gọi là reasoning
0:04:19 - 0:04:21, Tức là cái sự suy luận
0:04:21 - 0:04:27, Chứ nó không chỉ đơn giản là information extraction, tức là rút trích thông tin ra từ tấm ảnh
0:04:27 - 0:04:29, Mà nó có cái sự reasoning ở đây
0:04:29 - 0:04:37, Thì để đạt được cái reasoning này thì nó sẽ phải nhờ đến cái kết quả hoặc là những thành tựu của GPT
0:04:37 - 0:04:50, Là cái mô hình mà pre-train dành cho cả decoder để phục vụ cho việc là xử lý cái văn bản
00:04:50 - 0:04:52, Hoặc là generate tạo sinh ra văn bản
00:04:52 - 0:05:01, Vậy thì GPT-4V là một cái mô hình cho phép xử lý đa dạng các dữ liệu đầu vào hay còn gọi là multimodality là đa thể thức
00:05:07 - 0:05:11, Và nó có thể xử lý dữ liệu đầu vào là dạng văn bản thông thường
00:05:11 - 0:05:13, Nó có thể gồm một hoặc là nhiều ảnh
00:05:13 - 0:05:18, Ví dụ như trong cái ví dụ này ta thấy là có thể lên đến vài ảnh, ba ảnh
00:05:18 - 0:05:23, Rồi văn bản trong ảnh, tức là trong tấm ảnh nó lại có văn bản
00:05:23 - 0:05:26, Bình thường là mình sẽ có văn bản riêng và ảnh riêng
00:05:26 - 0:05:29, Bây giờ trong ảnh nó lại có văn bản
00:05:29 - 0:05:34, Đó, thì đây là một cái ví dụ trong ảnh là có văn bản
00:05:34 - 0:05:41, Là trong tấm hình menu nó sẽ có các cái tên của các loại đồ uống và giá tiền
00:05:41 - 0:05:48, Rồi visual pointer tức là một cái dạng thức để cho chúng ta tương tác
00:05:48 - 0:05:53, Đối với là một cái dạng prompt, nó là một cái dạng prompt mới
00:05:53 - 0:05:56, Bình thường mình có prompt là dạng text và ảnh
00:05:56 - 0:06:00, Bây giờ cái prompt của mình có thể là dấu mũi tên giống như chúng ta đang vẽ ở đây
00:06:00 - 0:06:03, Cái mũi tên này nó cũng được gọi là một cái prompt
00:06:04 - 0:06:11, Nếu như cái mô hình này giả sử như nó giải không được thì chúng ta có thể chỉ vô đây
00:06:11 - 0:06:17, Giá của cái beer Magna nó là nằm ở đây, mình chỉ vô
00:06:17 - 0:06:22, Mô hình của mình sẽ hiểu được cái visual pointer này như là một cái loại prompt
00:06:22 - 0:06:29, Vậy thì GPT-4V nó có một vài đặc trưng chính, đó là gì
00:06:29 - 0:06:34, Đây là một cái bài toán có thể được thực hiện với GPT-4V
00:06:34 - 0:06:39, Đây là những cái bài toán mà GPT thực hiện được bao gồm những task rất là đơn giản
00:06:39 - 0:06:49, Ví dụ như là mô tả hình ảnh, image description, image captioning, rồi nhận dạng ảnh, recognition on different domains
00:06:49 - 0:06:54, Rồi kết hợp kiến thức đa thể thức, multimodal knowledge
00:06:54 - 0:07:02, Trong cái ví dụ ở trên chúng ta thấy là nó có sử dụng cái knowledge của văn bản là text
00:07:02 - 0:07:06, Đồng thời nó cũng có sử dụng cái knowledge của tấm ảnh
0:07:09 - 0:07:12, Rồi nó có sử dụng cái text trong ảnh
00:07:17 - 0:07:19, Thì đó là multimodality
00:07:19 - 0:07:23, Và có thể tương tác với các kiến thức tổng quát
00:07:23 - 0:07:27, Ví dụ như nó có thể hiểu về những người nổi tiếng như là David Beckham
00:07:27 - 0:07:29, Người nổi tiếng này
00:07:29 - 0:07:35, Rồi các cái địa danh như là Paris, Hà Nội v.v.
00:07:35 - 0:07:41, Địa danh nổi tiếng thì đó là những cái kiến thức tổng quát
00:07:41 - 0:07:50, Rồi và đồng thời nó có khả năng quan trọng là hiểu và suy luận
00:07:50 - 0:07:53, hay gọi là reasoning trên cái văn bản đơn thuần
00:07:53 - 0:07:58, hoặc là văn bản trong ảnh syntax understanding hoặc document reasoning
00:07:58 - 0:08:06, Thì đây chính là những cái khả năng nổi trội của GPT-4V so với những mô hình ngôn ngữ thị giác
00:08:06 - 0:08:08, mà chúng ta đã tìm hiểu ở phía trước
00:08:09 - 0:08:13, Tiếp theo thì chúng ta sẽ cùng tìm hiểu về khái niệm visual pointer
00:08:13 - 0:08:19, Ở bên tay phải là một hình ảnh ví dụ về visual pointer
00:08:19 - 0:08:25, Bên cạnh câu mô tả là display the pointed region in the image
00:08:25 - 0:08:27, Và chúng ta sẽ có một tấm ảnh
00:08:27 - 0:08:30, Trong tấm ảnh này, nó sẽ có một đường màu đỏ
00:08:30 - 0:08:34, Đây chính là một ví dụ của visual pointer
00:08:38 - 0:08:44, Với visual pointer sẽ hướng dẫn cho mô hình tập trung vào những phần quan trọng của tấm ảnh
00:08:44 - 0:08:48, Thay vì chúng ta nhìn vô, tấm ảnh này sẽ có cả rừng chữ và số
00:08:48 - 0:08:56, Với đường khoanh màu đỏ này, mô hình của mình biết là sẽ tập trung vào đây để phân tích số liệu của mình
00:08:56 - 0:09:02, Với đường màu đỏ này, mô hình GPT-4V đã nêu được
00:09:02 - 0:09:07, Và chúng ta đã đưa ra các phân tích tương ứng của tấm ảnh
00:09:07 - 0:09:12, Và chúng ta đã đưa ra các phân tích tương ứng của tấm ảnh
00:09:12 - 0:09:18, Vậy thì ngoài đường khoanh màu đỏ, nó sẽ còn những dạng visual pointer nào
00:09:18 - 0:09:23, Ví dụ như chúng ta có thể đưa vào tọa độ dạng số, hoặc là coordinate
00:09:23 - 0:09:31, Hoặc chúng ta có thể đưa vào blackbox, trong đó chúng ta loại bỏ hết tất cả những phần ảnh không liên quan
00:09:31 - 0:09:37, Và chỉ chừa cái vùng ảnh có liên quan đến việc suy luận hoặc là cái việc trả lời câu hỏi của chúng ta
00:09:37 - 0:09:44, Hoặc là nó có thể ở dạng là một cái mũi tên, chỉ vào những đối tượng mà chúng ta đang muốn quan tâm
00:09:44 - 0:09:53, Thì đây có thể là một trong những dạng khá là thú vị và gần với cách thức người tương tác khi mà trao đổi với nhau trên hình ảnh
00:09:54 - 0:10:04, Rồi cái dạng nữa đó là chúng ta có thể dùng một cái box, một cái khung kèm cái ảnh gốc thì nó sẽ có thêm một cái đường màu đỏ để khoanh vùng cái đối tượng chúng ta quan tâm
0:10:04 - 0:10:13, Và có những cái dạng mà freestyle hơn, ví dụ như là hình oval, hoặc là hand drawing, tức là một cái đường nét tự do
0:10:14 - 0:10:22, Với cái visual pointer, nó đã giúp cho cái việc tương tác giữa người và máy tính trở nên thuận tiện hơn
0:10:22 - 0:10:34, Và đây có lẽ là một trong những cái thể thức quan trọng đặc biệt mà GPT-4V nó khác biệt so với lại những cái mô hình vision language, cái mô hình thị giác ngôn ngữ trước đây
0:10:35 - 0:10:46, Vậy thì một vài cái ví dụ nữa để cho chúng ta thấy cái tính hiệu quả của GPT-4V liên quan đến cái việc là reasoning
0:10:46 - 0:10:56, Nếu như chúng ta đưa vào một cái câu prompt đó là count number of apples in the image thì nó sẽ đếm sai là có 12 quả táo
00:10:56 - 0:11:06, Nâng cấp hơn một chút xíu thì mình sẽ chỉ dẫn cho nó, đó là thêm một cái câu viết đằng sau đó là suy nghĩ step by step
00:11:06 - 0:11:18, Thì nó sẽ đưa ra là có 4 step, rồi step 1 là tính như thế nào, step 2 là làm gì, step 3 là làm thế nào, step 4 thậm chí đã kiểm tra lại
0:11:18 - 0:11:29, Nhưng cuối cùng nó vẫn ra sai và chỉ đến khi chúng ta đưa ra một cái chỉ dẫn đầy đủ và các cái bước đủ đơn giản
0:11:37 - 0:11:41, Thì nó mới có thể làm đúng ví dụ, câu đầu giống như nó khen
0:11:41 - 0:11:45, You are an expert in counting things in the image
00:11:45 - 0:11:57, Rồi, hãy đếm những số lượng apple trong tấm ảnh này row by row và đảm bảo rằng là nó ra được cái kết quả chính xác
00:11:57 - 0:12:07, Thế thì nó sẽ xét trong tấm hình này thì nó sẽ có 4 dòng và với mỗi dòng nó sẽ lần lượt liệt kê ra, nó sẽ đưa ra cái con số đếm
00:12:07 - 0:12:17, Ví dụ như là 4 apple, dòng số 2 là có 4 apple, dòng số 3 là có 3 apple và cuối cùng nó sẽ ra được con số đúng là 11 apple
0:12:17 - 0:12:31, Rồi, thì như vậy, GPT-4V ở đây là một cái ví dụ cho chúng ta thấy là nó có thể đưa ra, chúng ta có thể đưa vào các cái chỉ dẫn cộng với cái tấm ảnh
00:12:31 - 0:12:45, Và cái chỉ dẫn này thì biết nó giống như là một cái câu prompt mà chúng ta trò chuyện với chatbot, chỉ dẫn cho nó biết là phải làm như thế nào để mà suy luận
0:12:45 - 0:12:56, Thì đây chính là một cái ví dụ khi sử dụng GPT-4V giống như là chúng ta sử dụng với cái mô hình GPT-4o hoặc GPT-4 của nền tảng ChatGPT
00:12:57 - 0:13:04, Rồi, cái In-context Few-Shot Learning thì ở đây là một cái ví dụ Zero-Shot
00:13:04 - 0:13:19, Đại đa số mọi người khi mà làm việc thì đều hay sử dụng Zero-Shot, tại vì thứ nhất là họ nghĩ rằng cái mô hình của mình là tốt, cái mô hình của mình là xịn
00:13:19 - 0:13:25, Giờ đến đây là cái gì cũng biết, cái gì cũng biết
00:13:25 - 0:13:34, Cái thứ hai là bản thân mình là cái người sử dụng thì mình cũng lười, mình lười hướng dẫn cho nó nhiều
00:13:34 - 0:13:40, Thì đó là hai cái yếu tố khiến cho Zero-Shot là một trong những cái kỹ thuật được sử dụng rất là phổ biến
00:13:40 - 0:13:50, Ví dụ trong ví dụ này là chúng ta đưa vào cái prompt là In which year had the highest average gas price ở trong tháng 6
00:13:50 - 0:13:55, Thì ở đây là mô hình trả lời sai là 3,3 đô
00:13:55 - 0:14:04, Nếu mà chúng ta dùng là Zero-Shot nhưng mà chúng ta kêu nó là think step by step thì kết quả của mình cũng sai
0:14:04 - 0:14:12, Chỉ khi chúng ta đưa vào cái In-context Few-Shot, cụ thể ở đây là hai shot
0:14:12 - 0:14:17, Thì cái In-context Few-Shot này có nghĩa là gì? Chúng ta sẽ cho nó một cái cặp câu hỏi
0:14:17 - 0:14:27, Và ví dụ, câu hỏi ví dụ, cái đáp án thì nó sẽ bám theo cái cặp suy luận đó để mà nó trả lời cho những câu hỏi mới
0:14:27 - 0:14:37, Ví dụ như ở đây là chúng ta hỏi, ở đây chúng ta sẽ đưa cho nó là chỉ dẫn thêm là cái đồ thị này
00:14:37 - 0:14:49, Plot the National Gas Price, tức là plot giá gas ở trong nước là từ 2016 cho đến 2019
00:14:49 - 0:14:57, Rồi nó mô tả ra chi tiết là màu đỏ là gì, màu xanh là gì, màu xanh lá là gì, v.v.
00:14:57 - 0:15:10, Rồi sau đó thì nó sẽ đưa ra tính toán là năm mà có cái High Gas Price là vào tháng 6 năm 2018
00:15:10 - 0:15:19, Thì ở đây là nó đưa ra những cái Few-Shot, tức là cái kết quả
00:15:19 - 0:15:26, Vậy thì GPT nó đã dựa trên cái lập luận tương tự ở phía trên là với hai shot, đây là shot số 1
00:15:26 - 0:15:30, Đây là shot số 2 ở phía bên dưới, nó sẽ còn một cái câu nữa
00:15:30 - 0:15:39, Ví dụ thì khi chúng ta đưa vào một số liệu mới, ở trên là chúng ta cho ví dụ 2019, 2018
00:15:39 - 0:15:50, Và với số liệu mới này là 2023 thì nó sẽ tự động lập luận y chang như thế này để tìm ra được giá gas mà cao nhất là bao nhiêu
00:15:50 - 0:15:55, Thì nó bắt chước hai cái shot ở phía trên cho một cái loại dữ liệu mới
00:15:55 - 0:16:08, Và cái kết quả nó ra được đó là tháng 6 năm 2020, thì đó chính là cái hướng dẫn cho cái mô hình, cái cách thức để mà nó lập luận
00:16:08 - 0:16:13, Và với Few-Shot hay còn gọi là In-context learning
00:16:14 - 0:16:24, Như vậy tổng kết lại chúng ta đã cùng tìm hiểu qua các cái mô hình, mô hình ban đầu như là CLIP
00:16:24 - 0:16:32, Và cho đến bây giờ thì vẫn được dùng nhiều, CLIP thì dùng nhiều cho bài toán là Zero-Shot Image Classification
00:16:32 - 0:16:41, Sau đó chúng ta có phiên bản là GLIP, với cái sự cũng là Zero-Shot nhưng mà cho bài toán Detection
00:16:41 - 0:16:51, Rồi nâng lên là có mô hình CLIP, sau đó là sẽ có Visual Programming
00:16:54 - 0:16:59, Rồi cái mô hình mà chúng ta vừa mới tìm hiểu đó chính là GPT-4V
0:17:02 - 0:17:10, Vậy thì cái việc mà chúng ta sẽ có cái nhận định gì khi chúng ta đã tìm hiểu qua các cái mô hình này
0:17:10 - 0:17:19, Đó là cái việc mà chúng ta huấn luyện một cái mô hình từ đầu cho cái mô hình thị giác thì nó cần rất nhiều tài nguyên tính toán
0:17:19 - 0:17:22, Cũng như là cái dữ liệu của chúng ta rất là lớn
0:17:22 - 0:17:30, Nói vui đó là nhiều khi cái dữ liệu của mình nó lớn đến nỗi mà chúng ta không đủ dung lượng để chứa chứ đừng nói đến cái chuyện là chúng ta huấn luyện mô hình
0:17:30 - 0:17:34, Tại vì quy mô nó lên đến Internet Scale
0:17:37 - 0:17:39, Internet Scale Dataset
0:17:39 - 0:17:43, Và do đó thì thông thường chúng ta sẽ sử dụng cái mô hình đã huấn luyện sẵn
0:17:43 - 0:17:49, Nhưng mà quan trọng là chúng ta sẽ phải biết được cái công năng của từng mô hình, của từng phần trong mô hình là gì
0:17:49 - 0:18:00, Ví dụ như khi chúng ta nhìn vào cái mô hình CLIP thì chúng ta biết cái mô hình nào là mô hình chúng ta sẽ sử dụng để cho cái công việc gọi là Unimodal encoding
0:18:00 - 0:18:18, Và khi nào thì chúng ta sẽ sử dụng cái mô hình, mà cross-modal hay là image-text, image-text là text matching
00:18:18 - 0:18:31, Và khi nào thì chúng ta sẽ sử dụng cái language model ở phía sau để cho cái tác vụ là text generation
00:18:31 - 0:18:36, Thì chúng ta biết được cái công năng của từng mô hình và dùng mô hình nào là phù hợp cho cái bài toán của chúng ta
0:18:36 - 0:18:42, Cái thứ hai đó là phối hợp các cái thành phần huấn luyện sẵn đó cho cái bài toán của mình
0:18:42 - 0:18:48, Ví dụ như chúng ta khai thác cái module để mã hóa hình ảnh hoặc module mã hóa văn bản
0:18:48 - 0:19:06, Tại vì cái việc mà chúng ta cho hình ảnh và văn bản tương tác với nhau để học ra được encoding thì nó sẽ giúp cho chúng ta có cái tính tổng quát, có cái tính tương tác và phân biệt được ngữ nghĩa một cách rõ ràng hơn so với việc chúng ta chỉ học dựa trên văn bản không hoặc chỉ học dựa trên hình ảnh không
0:19:06 - 0:19:11, Và sử dụng các kỹ thuật Prompt Engineering một cách hiệu quả
00:19:11 - 0:19:17, Ví dụ như chúng ta sử dụng cái In-context learning với kỹ thuật Few-Shot Prompting
00:19:17 - 0:19:30, Chúng ta sẽ cho nó khoảng hai, ba ví dụ về cái instruction và cái instruction và cái kết quả của mình
00:19:30 - 0:19:41, Thì kết quả, cái Result thì nó sẽ bắt chước cái instruction và cái Result này
00:19:41 - 0:19:50, Khi chúng ta có một cái new instruction thì nó sẽ giúp cho chúng ta dự đoán ra được cái kết quả
00:19:50 - 0:19:54, Nó sẽ đưa ra một cái kết quả dự đoán
00:19:54 - 0:20:00, Thì đó là ý tưởng của In-context learning và đây là một trong những kỹ thuật dùng cũng rất là phổ biến
0:20:00 - 0:20:09, Rồi hướng dẫn mô hình suy luận một cách có hệ thống, chúng ta sẽ cho nó suy nghĩ theo kiểu step by step
00:20:09 - 0:20:15, Rồi có thể chỉ dẫn cho nó chi tiết hơn là chúng ta sẽ chia nó ra thành bước 1, bước 2, bước 3 như thế nào
0:20:15 - 0:20:22, Bước 1 chi tiết là sao? Càng đơn giản thì mô hình sẽ dễ thực hiện theo
0:20:22 - 0:20:26, Rồi hướng dẫn cho nó cách suy luận
0:20:26 - 0:20:35, Và cuối cùng đó là sử dụng Visual Pointer thì đây là một kỹ thuật để giúp cho chúng ta có thể hỗ trợ cho người dùng
0:20:35 - 0:20:38, Tạo ra một cái prompt một cách đơn giản và tự nhiên
0:20:38 - 0:20:48, Chúng ta có thể tạo ra một cái visual pointer là dạng mũi tên, nó có thể là một cái box hoặc là một cái scribble như thế này
0:20:48 - 0:20:50, Một cái đường mà zigzag
0:20:50 - 0:20:58, Vậy thì trên đây đó là một vài cái mô hình đầu tiên khi nói về mô hình ngôn ngữ thị giác
0:20:58 - 0:21:04, Trong những phần tiếp theo thì chúng ta sẽ nói về những cái mô hình hiện đại hơn, được train trên những dữ liệu lớn hơn
0:21:04 - 0:21:12, Và phục vụ cho các cái bài toán mà chúng ta đã đề cập trước đây, ví dụ như bài toán segmentation
0:21:12 - 0:21:19, Và bài toán sinh ngôn ngữ text generation
0:21:19 - 0:21:24, Cụ thể đó là cái mô hình là LLaVA
0:21:24 - 0:21:32, Còn đối với cái segmentation thì chúng ta có thể sử dụng hai cái mô hình, đó là SAM Grounding DINO
0:21:32 - 0:21:41, Đây là hai cái mô hình Zero-Shot Segmentation
0:21:41 - 0:21:49, Và mô hình SEEM là một cái mô hình tương tác đa thể thức
0:21:49 - 0:21:54, Trong những phần tiếp theo chúng ta sẽ tìm hiểu về các cái mô hình này