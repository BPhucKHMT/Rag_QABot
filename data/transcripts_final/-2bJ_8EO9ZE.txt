0:00:14 - 0:00:23, Chúng ta sẽ cùng đến với kỹ thuật tái tham số. Đây là một trong những kỹ thuật quan trọng trong mô hình VAE.
0:00:23 - 0:00:39, Một trong những rào cản chính là mô hình VAE không có đơn định mà có yếu tố ngẫu nhiên trong đó, dẫn đến khó khăn cho việc huấn luyện khi sử dụng giải thuật Gradient Descent.
0:00:39 - 0:00:53, Chúng ta sẽ xem đồ thị tính toán của kiến trúc VAE như thế nào. Chúng ta sẽ nhắc lại VAE đó sẽ bao gồm thành phần encoder và decoder.
0:00:55 - 0:01:05, Thì thành phần decoder này nó hoàn toàn tương tự như autoencoder.
0:01:10 - 0:01:18, Và nó có thể tính đạo hàm được và thực hiện thuật toán backpropagation khi chúng ta đã tính được cái sai số này rồi.
0:01:18 - 0:01:26, Cái hàm lỗi này rồi thì chúng ta có thể backpropagation được đến các cái bước, các cái trọng số theta của cái decoder này.
0:01:27 - 0:01:35, Nhưng đối với cái quá trình encode thì cái điểm nghẽn của mình nó sẽ nằm ở chỗ này. Điểm nghẽn của mình nó sẽ nằm ở chỗ này.
0:01:35 - 0:01:43, Đó là thao tác sampling và sampling thì đó là một cái thao tác mà trong giải tích nó không có và không thể tính đạo hàm được.
0:01:43 - 0:01:51, Nên nó sẽ khiến cho việc huấn luyện gặp khó khăn và ta không thể lan truyền ngược được.
0:01:51 - 0:02:00, Cái gradient qua các cái lớp lấy mẫu chúng ta sẽ lan truyền hàm lỗi về đây, về đây, về đây.
0:02:00 - 0:02:08, Nhưng mà đến đây thì chúng ta sẽ không lan truyền được. Vậy thì bây giờ chúng ta sẽ có những cái giải pháp gì?
0:02:08 - 0:02:18, Đó là chúng ta sẽ tái tham số lớp lấy mẫu. Thì cái lớp lấy mẫu của chúng ta chỉ là cái lớp này. Đây là cái lớp lấy mẫu.
0:02:22 - 0:02:24, Hay là sampling layer.
0:02:29 - 0:02:40, Rồi, nếu như chúng ta giữ nguyên cái công thức đó là z là xấp xỉ, z là sampling theo cái phân bố mi và sigma bình phương.
0:02:40 - 0:02:51, Rồi, thì rõ ràng chúng ta sẽ không có thể tính toán được. Rồi, chúng ta sẽ bỏ đi cái bước lấy mẫu này mà chúng ta sẽ đưa nó về một cái dạng thức khác.
0:02:51 - 0:02:53, Đó là cái dạng thức tái tham số.
0:02:53 - 0:03:10, Thì xét một cái mẫu ẩn vector z, thì trong đó mi và sigma là hai cái vector cố định được chia theo tỷ lệ là hằng số ngẫu nhiên được rút ra từ phân bố cho trước.
0:03:10 - 0:03:20, Thế thì khi đó, cái việc mà chúng ta lấy mẫu một cái vector z, nếu chúng ta lấy mẫu một cái vector z,
0:03:21 - 0:03:42, xoay xung quanh cái phân bố normal distribution mi và sigma bình, thì nó sẽ tương đương với cái việc chúng ta sẽ lấy mẫu một cái điểm trong cái phân bố là từ mi.
0:03:42 - 0:03:52, Rồi, không một, chúng ta sẽ lấy một cái epsilon.
0:03:52 - 0:04:01, Thì z, ở trong, lấy mẫu z theo cái phân bố Gaussian với tham số là cố định là mi và sigma bình phương,
0:04:02 - 0:04:09, thì nó sẽ tương đương với cái việc chúng ta lấy mẫu epsilon theo phân bố chuẩn là 0,1.
0:04:09 - 0:04:16, Và chúng ta sẽ nội suy từ epsilon này ra cái vector z bằng cái công thức như ở đây.
0:04:16 - 0:04:24, z sẽ là bằng mi cộng cho sigma nhân cho epsilon.
0:04:24 - 0:04:31, Trong đó epsilon là một cái biến nhiễu được lấy mẫu xoay xung quanh cái phân bố chuẩn 0,1 này.
0:04:31 - 0:04:34, Thì ở đây chúng ta dùng cái phép nó gọi là element y.
0:04:39 - 0:04:45, Rồi đó, tức là chúng ta sẽ tích từng phần tử.
0:04:45 - 0:04:47, Thì tại sao chúng ta lại dùng element y?
0:04:47 - 0:04:53, Tại vì mi và sigma đó là những cái vector, ví dụ như là vector d chiều.
0:04:55 - 0:04:58, Sigma cũng là một cái vector d chiều.
0:04:58 - 0:05:02, Thì epsilon của mình nó cũng sẽ thuộc một cái vector d chiều.
0:05:02 - 0:05:03, Đó là một cái vector nhiễu d chiều.
0:05:03 - 0:05:08, Trong đó từng cái chiều của sigma, xe gọi từng cái chiều của epsilon,
0:05:08 - 0:05:15, thì sẽ là một cái vector, là một cái giá trị được sampling theo phân bố chuẩn 0,1.
0:05:16 - 0:05:27, Thì khi chúng ta dùng cái công thức này là z bằng mi cộng cho sigma nhân tích từng phần tử với epsilon,
0:05:27 - 0:05:40, thì khi đó expectation của z của chúng ta nó đúng là bằng mi luôn.
0:05:40 - 0:05:47, Và variance của z cũng đúng là bằng sigma.
0:05:48 - 0:05:54, Thì đây chính là cái tính chất mà tái tham số hóa,
0:05:54 - 0:06:01, tức là thay vì chúng ta sử dụng một cái lớp sampling layer là lấy mẫu z theo cái biến,
0:06:01 - 0:06:05, theo cái phân bố Gaussian mi sigma bình,
0:06:05 - 0:06:08, thì chúng ta đi lấy mẫu, đi sampling cái này.
0:06:08 - 0:06:14, Chúng ta sẽ đi sampling epsilon.
0:06:15 - 0:06:23, Còn các giá trị mi và sigma này là cố định, là tất định, được tính toán từ encoder.
0:06:28 - 0:06:35, Thì khi đó là chúng ta sẽ không có bị cái hiện tượng gọi là lan truyền ngược, nó không có đi được.
0:06:35 - 0:06:37, Thì tại sao?
0:06:37 - 0:06:39, Ở đây chúng ta sẽ có một cái hình ví dụ minh họa,
0:06:40 - 0:06:49, đó là từ cái phi và x, thì chúng ta sẽ tính ra được hai cái giá trị mi và sigma.
0:06:49 - 0:06:51, Chúng ta sẽ tính ra được mi và sigma.
0:06:54 - 0:06:58, Với mi và sigma này, chúng ta sẽ sampling cái vector z.
0:06:58 - 0:07:02, Để từ vector z này chúng ta sẽ đi tiếp cái hàm decode.
0:07:02 - 0:07:06, Đây là cái hàm decode để tạo,
0:07:07 - 0:07:12, để tái tạo x1.
0:07:12 - 0:07:16, Vậy cái vector z này được thực hiện với phép sampling.
0:07:16 - 0:07:19, Đây là một phép không tính đạo hàm được.
0:07:25 - 0:07:29, Do đó khi chúng ta thực hiện thuật toán lan truyền ngược từ phía trên xuống,
0:07:29 - 0:07:35, đến cái bước này nó sẽ gặp một stochastic node, tức là một node ngẫu nhiên, có yếu tố ngẫu nhiên.
0:07:35 - 0:07:39, Thì nó sẽ không lan truyền về đây để cập nhật cái phi được.
0:07:39 - 0:07:41, Nó không huấn luyện được.
0:07:41 - 0:07:50, Trong khi đó, nếu như chúng ta tái tham số lại cái lớp lấy mẫu,
0:07:50 - 0:07:55, thì ở đây chúng ta sẽ sampling cái z epsilon tuân theo phân bố 0,1.
0:07:55 - 0:07:59, Thì cái node epsilon này là một stochastic node.
0:08:00 - 0:08:07, Nhưng nó là một cái giá trị và nó không phải đi cập nhật, nó sẽ không có đi cái bước lùi về sau nữa.
0:08:07 - 0:08:09, Đây là cái node cuối cùng rồi.
0:08:09 - 0:08:12, Nên nó sẽ không ảnh hưởng gì đến cái quá trình huấn luyện của mình.
0:08:12 - 0:08:18, Cái quá trình huấn luyện của mình đó là khi chúng ta tính được cái loss ở phía trên, chúng ta lan truyền xuống,
0:08:18 - 0:08:21, và đến đây thì chúng ta gặp cái z.
0:08:21 - 0:08:26, Và z này thì hoàn toàn là một cái node deterministic, tức là một cái node tất định,
0:08:26 - 0:08:28, tức là không có yếu tố ngẫu nhiên.
0:08:28 - 0:08:37, Nó sẽ được tính từ mi và sigma, được tính toán từ phi và x.
0:08:37 - 0:08:42, Qua cái bước encode, chúng ta sẽ có được một cái cặp mi và sigma cố định.
0:08:42 - 0:08:46, Còn epsilon ở đây là một cái giá trị chúng ta truyền vào z.
0:08:46 - 0:08:50, Thì có cái giá trị này, chúng ta nhân với mi, chúng ta nhân với sigma,
0:08:50 - 0:08:53, và sau đó cộng với mi thì nó sẽ ra một node tương đương.
0:08:53 - 0:08:57, Với cái việc là z của mình tuân theo một cái phân bố là Gaussian.
0:08:57 - 0:09:00, Tuân theo cái phân bố Gaussian.
0:09:00 - 0:09:06, Thì vì cái node này là tất định, là deterministic,
0:09:06 - 0:09:08, nên khi chúng ta lan truyền ngược về,
0:09:08 - 0:09:10, rồi chúng ta lại lan truyền ngược về đây,
0:09:10 - 0:09:14, để đi tính cái đạo hàm theo cái tham số phi,
0:09:14 - 0:09:16, thì khi đó chúng ta sẽ huấn luyện được.
0:09:16 - 0:09:24, Học là cập nhật lại được cái tham số của mô hình.
0:09:24 - 0:09:27, Còn cái bước này là chúng ta sẽ không có đi qua đây được,
0:09:27 - 0:09:30, là vì nó bị chặn bởi cái node stochastic này.
0:09:32 - 0:09:36, Như vậy thì chúng ta tóm tắt cái kiến trúc VAE,
0:09:36 - 0:09:39, là Variational Autoencoder.
0:09:39 - 0:09:44, VAE một cái nhiệm vụ của nó đó là biểu diễn dữ liệu thật.
0:09:44 - 0:09:46, Đây là dữ liệu thật.
0:09:46 - 0:09:50, Vào một cái không gian ẩn để huấn luyện,
0:09:50 - 0:09:52, tức là x sẽ được biểu diễn bởi một cái vector z.
0:09:52 - 0:09:58, Cái thứ 2, đó là chúng ta sẽ tái tạo lại cái dữ liệu này
0:09:58 - 0:10:00, thông qua học không giám sát,
0:10:00 - 0:10:02, tức là chúng ta không cần có cái nhãn,
0:10:02 - 0:10:04, thì là nhờ học không giám sát.
0:10:04 - 0:10:07, Thì tại sao chúng ta có thể tái tạo lại được?
0:10:07 - 0:10:11, Thì z qua cái decoder nó sẽ tạo ra cái x mũ,
0:10:11 - 0:10:13, và chúng ta sẽ lấy cái x mũ này,
0:10:13 - 0:10:15, chúng ta đi so sánh với lại x,
0:10:15 - 0:10:18, rồi sau đó chúng ta tính cái sai số,
0:10:18 - 0:10:22, và chúng ta sẽ tìm cách đi minimize cái này,
0:10:22 - 0:10:25, đi tìm min cái sai số này,
0:10:25 - 0:10:28, đó là tái tạo lại thông qua học không giám sát.
0:10:28 - 0:10:32, Cái thứ 3 đó là thủ thuật tham số hóa,
0:10:32 - 0:10:34, thủ thuật tái tham số hoặc là tham số hóa,
0:10:34 - 0:10:38, để huấn luyện cái mô hình dựa trên gradient.
0:10:38 - 0:10:40, Thì cái bước sampling ở đây,
0:10:40 - 0:10:46, cái sampling ở đây là một cái thao tác mà không có đạo hàm,
0:10:48 - 0:10:50, không tính đạo hàm được.
0:10:57 - 0:11:03, Nên chúng ta sẽ lấy mẫu một cái biến epsilon
0:11:03 - 0:11:05, theo phân bố là 0,1,
0:11:06 - 0:11:09, sau đó chúng ta sẽ tạo ra vector z bằng cách lấy epsilon,
0:11:09 - 0:11:13, đi nhân với lại sigma,
0:11:13 - 0:11:16, rồi sau đó cộng với mi để ra được cái vector z,
0:11:16 - 0:11:18, thì khi chúng ta huấn luyện,
0:11:18 - 0:11:20, chúng ta lan truyền cái sai số đến z này,
0:11:20 - 0:11:23, thì chúng ta lại tiếp tục lan truyền ngược vào đây,
0:11:23 - 0:11:26, trước chúng ta không có đi qua con đường này.
0:11:27 - 0:11:30, Tại vì epsilon là một cái
0:11:30 - 0:11:35, giá trị mà mình lấy mẫu ngẫu nhiên
0:11:35 - 0:11:37, và nó không có cái tham số gì để học ở đây,
0:11:37 - 0:11:39, nên chúng ta không có đi qua con đường này,
0:11:39 - 0:11:41, mà chúng ta sẽ đi qua cái đường encode.
0:11:43 - 0:11:44, Cái thứ tư,
0:11:44 - 0:11:48, đó là nó giải thích các cái biến tiềm ẩn bằng cách là sử dụng nhiễu,
0:11:48 - 0:11:51, tức là trong cái không gian tiềm ẩn của mình.
0:11:51 - 0:11:55, Một cái hình ảnh khi chúng ta
0:11:55 - 0:11:57, ánh xạ vào cái không gian latent,
0:11:57 - 0:12:01, thì nó sẽ ra một cái phân bố,
0:12:01 - 0:12:05, và cái phân bố này chúng ta hoàn toàn là ngẫu nhiên,
0:12:05 - 0:12:08, thì chúng ta sẽ random một cái vector z,
0:12:08 - 0:12:09, theo cái phân bố này,
0:12:09 - 0:12:12, từ cái vector z này chúng ta khôi phục ngược trở lại,
0:12:12 - 0:12:15, thì nó sẽ tạo ra cái mối quan hệ của chúng ta,
0:12:15 - 0:12:18, và chúng ta sẽ dùng nó để làm cái phân bố này,
0:12:18 - 0:12:19, để khôi phục ngược trở lại,
0:12:19 - 0:12:22, thì nó sẽ tạo ra cái mối quan hệ giữa
0:12:22 - 0:12:25, không gian tiềm ẩn và cái không gian ảnh thật,
0:12:25 - 0:12:29, nhưng đồng thời nó vẫn đảm bảo được cái yếu tố đó là
0:12:29 - 0:12:34, khi chúng ta có một cái vector z phải ở đây,
0:12:34 - 0:12:39, thì chúng ta gần với lại vector z thì decode ra nó cũng ra con số gần giống con số 3,
0:12:41 - 0:12:47, và nó sẽ là một cái công cụ để giúp cho chúng ta có thể tạo được cái mẫu dữ liệu mới,
0:12:47 - 0:12:48, nghĩa là sao?
0:12:48 - 0:12:51, khi chúng ta đã huấn luyện xong cái mô hình,
0:12:51 - 0:12:54, thì chúng ta sẽ không cần có cái bước encode nữa,
0:12:54 - 0:12:57, mà chúng ta sẽ sampling ngẫu nhiên một cái vector z,
0:12:57 - 0:13:00, và từ đó chúng ta gọi cái hàm decode,
0:13:01 - 0:13:03, để chúng ta tạo ra một cái tấm ảnh,
0:13:03 - 0:13:06, và cái ảnh này qua cái variational autoencoder,
0:13:06 - 0:13:12, thì nó sẽ tạo ra được một cái tấm hình mà nó có cái ý nghĩa,
0:13:12 - 0:13:15, nó sẽ không bị cái điểm yếu của autoencoder,
0:13:15 - 0:13:20, nó sẽ có những cái khoảng trống mà nó không tạo ra được một cái dữ liệu để có ý nghĩa,
0:13:20 - 0:13:26, thì cái phần này chúng ta sẽ làm trong cái phần bài thực hành,
0:13:26 - 0:13:31, để hiểu hơn là một cái vector z ngẫu nhiên trong không gian latent,
0:13:31 - 0:13:32, khi chúng ta decode,
0:13:32 - 0:13:36, nó ra một cái giá trị có nghĩa và một cái giá trị không có nghĩa như thế nào?
0:13:45 - 0:13:49, Hãy subscribe cho kênh Ghiền Mì Gõ để không bỏ lỡ những video hấp dẫn