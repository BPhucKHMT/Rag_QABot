0:00:14 - 0:00:23, Chúng ta sẽ lựa chọn một phân bố xác suất prior tức là tiền nghiệm, là Normal Gaussian
0:00:23 - 0:00:31, Tức là chúng ta có rất nhiều những dạng phân bố khác nhau, tuy nhiên một trong những phân bố rất phổ biến đó là phân bố chuẩn là Normal Gaussian
0:00:31 - 0:00:43, Chúng ta mong muốn phân bố không gian ẩn của mình sẽ là một phân bố giống với phân bố Gauss
0:00:43 - 0:00:53, Và cụ thể luôn là Normal Gaussian, tức là với mu là bằng 0 và sigma bình phương là bằng 1, thì đây là phân bố ẩn tiền nghiệm
0:00:53 - 0:01:09, Và mình sẽ huấn luyện mô hình để cho Q của z cho trước x với tham số phi là tuân theo phân bố Gauss
0:01:09 - 0:01:29, Vì vậy, phân bố đồng đều sẽ xoay xung quanh tâm của không gian ẩn, ví dụ như chúng ta có một không gian ẩn thì mình sẽ tìm cách đưa nó về cùng một tâm với nhau
0:01:29 - 0:01:49, Vì vậy, phân bố của mình sẽ xoay xung quanh tâm không gian ẩn, tức là mu bằng 0, khuyến khích cho đặc trưng phân bố đồng đều xoay xung quanh tâm không gian ẩn
0:01:49 - 0:02:09, Vì vậy, phân bố đều sẽ xoay xung quanh tâm không gian ẩn, tức là nếu như ở trong mô hình autoencoder không có hình thông thường, thì mình sẽ tìm cách đưa nó về cùng một tâm
0:02:09 - 0:02:24, Vì vậy, phân bố của mình sẽ xoay xung quanh tâm không gian ẩn, tức là nếu như ở trong mô hình autoencoder không có hình thông thường, thì mình sẽ tìm cách đưa nó về cùng một tâm
0:02:24 - 0:02:39, Vì vậy, phân bố của mình sẽ xoay xung quanh tâm không gian ẩn, tức là mu bằng 0, khuyến khích cho đặc trưng phân bố đồng đều xoay xung quanh tâm không gian ẩn
0:02:39 - 0:02:58, Vì vậy, phân bố của mình sẽ tìm cách đưa nó về cùng một tâm
0:02:58 - 0:03:11, Cái công thức khoảng cách độ lỗi giữa qi và p thì nó được tính như thế nào? Nó sẽ được tính dựa trên công thức KL divergence giữa hai phân phối
0:03:11 - 0:03:35, Và khi chúng ta triển khai với p là bằng phân bố Normal Gauss là mu bằng 0 và sigma bình bằng 1, thì chúng ta sẽ có công thức đó là bằng một phần tổng của g chạy từ 0 cho đến k trừ 1 của sigma z cộng cho mu z bình phương trừ 1
0:03:35 - 0:03:45, Trừ cho log của sigma z, trong đó k chính là số chiều của vector của mình
0:03:45 - 0:04:07, K chính là số chiều của không gian ẩn
0:04:07 - 0:04:14, Và khi đó nó cũng chính là số chiều của mu, nó cũng chính là số chiều của sigma luôn
0:04:14 - 0:04:30, Rồi, khi chúng ta huấn luyện một mô hình VAE, thành phần chính quy hóa này nó khuyến khích cái chuyện gì? Đó là khi chúng ta cho cái loss này càng tiến về 0, tức là càng giảm
0:04:30 - 0:04:40, Rõ ràng là với công thức này chúng ta sẽ thấy sigma z sẽ có xu hướng tiến về 0
0:04:40 - 0:04:50, Ngược lại, ở đây chúng ta thấy có cái dấu trừ log của sigma z, thì nó lại khuyến khích cái sigma này là không quá nhỏ
0:04:50 - 0:04:57, Nó sẽ khuyến khích
0:04:57 - 0:05:05, Cái sigma z không quá nhỏ
0:05:05 - 0:05:19, Còn cái mu tiến về 0, tức là cái phân bố của mình, nó sẽ kéo từ một cái khu vực rất là xa, nó sẽ kéo về tiến về cái góc tọa độ 00 này
0:05:19 - 0:05:26, Đây là một cái mu ban đầu, nó sẽ kéo cái mu này về góc tọa độ
0:05:26 - 0:05:40, Rồi, đồng thời sigma z nếu như cái phân bố của mình nó quá lớn, thì nó sẽ kéo cho cái sigma này tiến về đủ nhỏ
0:05:40 - 0:05:49, Nhưng nhờ có cái log của sigma z này thì nó sẽ khiến cho cái sigma của mình không quá nhỏ
0:05:49 - 0:05:55, Chứ còn nếu mà nó nhỏ quá thì có phải là thay vì chúng ta đưa về một phân bố thì cuối cùng nó sẽ đưa về một điểm không?
0:05:55 - 0:06:01, Phân bố của mình nó sẽ tiến về một điểm, như vậy là nó tương tự như autoencoder rồi
0:06:01 - 0:06:13, Như vậy thì cái sigma nó sẽ kéo về, đừng có quá lớn nhưng mà cũng đừng có quá nhỏ để hy vọng rằng là cái phân bố của mình nó thực sự là một cái phân bố có yếu tố ngẫu nhiên
0:06:13 - 0:06:22, Chứ còn nếu mà sigma mà tiến về bằng 0, tức là nó sẽ co về một điểm, tức là nó sẽ đưa về một cái mô hình deterministic, tức là một cái mô hình tất định
0:06:22 - 0:06:26, Chứ không có yếu tố xác suất như trong cái mô hình VAE
0:06:26 - 0:06:35, Thì với cái công thức này nó sẽ khuyến khích hai cái chuyện đấy, một đó là cái phân bố Q này sẽ tiến về 0, tiến về cái gốc tọa độ
0:06:35 - 0:06:49, Cái thứ hai đó là mu z, nó sẽ tiến về một cái phân bố mà có cái độ lệch vừa đủ, chứ nó không có quá nhỏ nhưng nó cũng không quá to
0:06:49 - 0:06:52, Thì đây là cái công thức chính quy hóa
0:06:52 - 0:07:00, Và chúng ta có những cái tính chất gì từ cái việc chính quy hóa này, nó sẽ có những tính chất gì
0:07:00 - 0:07:08, Đầu tiên đó là cái tính liên tục, tức là những cái điểm gần nhau trong không gian thì nội dung hoàn toàn tương tự nhau khi giải mã
0:07:08 - 0:07:09, Chúng ta đang nói đến cái ý này
0:07:09 - 0:07:26, Thì ở bên trái là một cái mô hình không có được chính quy, tức là chúng ta chỉ có cái sai số giữa x trừ cho x mũ thôi
0:07:26 - 0:07:31, Sai số tái tạo thôi, chỉ có sai số tái tạo
0:07:31 - 0:07:38, Thì nếu không có thành phần chính quy hóa thì nó sẽ không đảm bảo được
0:07:38 - 0:07:46, Thứ nhất đó là hai điểm gần nhau trong không gian ẩn, là cái điểm màu xanh lá mạ và màu đỏ ở đây thì nó gần nhau trong không gian tiền ẩn
0:07:46 - 00:07:55, Nhưng khi mà giải mã thì nó không có tương tự nhau, ví dụ cái điểm màu xanh lá này, xanh lá mạ này thì nó sẽ ra hình vuông
0:07:56 - 0:08:07, Trong khi cái điểm màu đỏ thì lại tạo ra một cái hình giống hình tam giác, thì hai cái hình này nó không có tương tự nhau, mặc dù hai cái điểm này nó gần nhau
0:08:10 - 0:08:19, Cái thứ hai đó là đối với cái việc mà không chính quy hóa, nó sẽ có thể khiến cho cái điểm của cái không gian ẩn được giải mã nhưng mà không có ý nghĩa
0:08:19 - 0:08:29, Ví dụ như ở đây chúng ta chỉ có hình tam giác, hình tròn, hình vuông, nhưng mà có một cái điểm ở đây là cái điểm mà nó không quá gần ba cái điểm này
0:08:29 - 0:08:33, thì khi chúng ta giải mã nó ra một cái hình gì đấy mà nó không có ý nghĩa
0:08:35 - 0:08:46, Ví dụ như trong cái chữ số viết tay thì khi chúng ta decode ra lẽ ra nó phải ra là số, chữ số thì 0 cho đến 9 nhưng cuối cùng nó sẽ ra một cái gì đấy, nó không phải là con số
0:08:46 - 0:08:53, Tức là một cái dữ liệu không có ý nghĩa, thì nếu như không có chính quy hóa nó sẽ khiến cho chúng ta bị hai cái vấn đề này
0:08:54 - 0:09:04, Ngoài ra thì nhờ có chính quy hóa nó sẽ giúp cho chúng ta giải quyết được cái vấn đề đó, đó là cái tính liên tục của dữ liệu của cái điểm biểu diễn trong không gian ẩn
00:09:04 - 0:09:12, Thì hai cái vector z và z phải nằm trong không gian ẩn mà giống nhau, gần nhau thì khi decode ra nó cũng phải giống nhau
0:09:12 - 0:09:22, Tính chất thứ hai, đó là tính đầy đủ là lấy mẫu từ không gian ẩn thì cái nội dung của mình nó sẽ phải có ý nghĩa, thì đây là một cái ví dụ này không có ý nghĩa
0:09:22 - 0:09:43, Còn nếu như chúng ta sử dụng một cái mô hình chính quy hóa, tức là bên cạnh cái sai số tái tạo nó có thêm cái thành phần chính quy hóa là được biểu diễn bởi cái công thức là d của kl, đó là KL divergence của qi và p
0:09:43 - 0:09:59, Cái này là viết tắt nha, thì các cái điểm gần nhau thì được giải mã tương tự và có ý nghĩa, ví dụ chúng ta thấy cái điểm màu cam và cái điểm màu tím thì hai cái hình này khi chúng ta giải mã thì chúng ta thấy cái dáng dấp nó cũng giống nhau
0:09:59 - 0:10:20, Mặc dù cái điểm màu tím thì nó sẽ hơi bo ở đây một chút, hơi bo tròn, nhưng nếu xét về hình thù thì nó cũng gần giống với hình tam giác, do đó thì hai cái điểm này khi chúng ta dạy nó sẽ có cái tính tương tự nhau và nó hoàn toàn là có ý nghĩa của nó, nó có cái lý do của nó
0:10:21 - 0:10:33, Rồi, thì đây chính là cái sự khác biệt của việc có chính quy hóa và không có chính quy hóa khi chúng ta huấn luyện với cái mô hình VAE
0:10:34 - 0:10:49, Thế thì một cái biểu diễn khác là sau khi chúng ta đã huấn luyện xong mô hình VAE, thì cái phân bố chuẩn tiền nghiệm sẽ đảm bảo được cái yếu tố về tính liên tục và tính đầy đủ
0:10:49 - 0:10:59, Cái tính liên tục nó thể hiện ở chỗ đó là những cái điểm nào mà gần nhau thì khi decode nó sẽ giống nhau và cái tính đầy đủ đó là mọi điểm của mình
0:10:59 - 0:11:06, Trong cái không gian thì khi chúng ta decode ra nó đều có cái ý nghĩa của nó chứ không phải là một cái nội dung vô nghĩa
0:11:06 - 0:11:11, Thì sau đây chúng ta sẽ nói có một cái ví dụ rõ hơn về cái chuyện này
00:11:11 - 0:11:17, Chúng ta thấy ở đây có 3 cái phân bố màu đỏ màu xanh và màu vàng
0:11:17 - 0:11:23, Thì ở đây là cái tâm cụm màu đỏ thì khi chúng ta decode ra nó sẽ ra cái hình tam giác
00:11:23 - 0:11:27, Còn đây là tâm cụm của bồ màu xanh decode ra là ra hình tròn
0:11:28 - 0:11:38, Thế thì một cái điểm ở lưng chừng ngay chính giữa tròn và tam giác thì chúng ta thấy cái hình này nó đều có cái ý nghĩa khá là phù hợp
0:11:38 - 0:11:43, Đúng không? là cái hình này nó sẽ có cái bo tròn giống như cái hình tròn
0:11:47 - 0:11:51, Nhưng đồng thời nó sẽ có cái nét thẳng, 3 cái nét thẳng
0:11:53 - 0:11:54, Giống như tam giác
0:11:58 - 0:12:05, Đó, thì cái điểm trung điểm này nó sẽ giống giống, nó sẽ giúp chúng ta tạo ra cái tính gọi là cái tính liên tục
0:12:07 - 0:12:11, Và đồng thời nó cũng có ý nghĩa, nó cũng có ý nghĩa chứ không phải là không
0:12:11 - 0:12:17, Nếu mà cái ở giữa này mà nó ra một cái điểm nào đó mà chúng ta không thể giải thích được thì đó là không có ý nghĩa
0:12:17 - 0:12:24, Rồi khi chúng ta tiến càng gần hơn về cái tâm thì chúng ta thấy là cái tam giác nó bớt bo tròn đi
0:12:24 - 0:12:26, Đúng không? nó bớt bo tròn đi
0:12:27 - 0:12:35, Hay nói cách khác, đó là nó dần nó nhọn đi để giống cái tam giác này
0:12:35 - 0:12:43, Còn khi mà cái điểm này tiến về phía tâm của cầm thì chúng ta thấy nó sẽ càng lúc nó sẽ càng tròn trịa hơn
0:12:43 - 0:12:47, Này, tròn, tròn hơn
0:12:47 - 0:12:54, Tương tự như vậy cho các cái điểm còn lại, ví dụ như đây là trung điểm giữa hình tròn và hình vuông
0:12:54 - 0:12:59, Thì chúng ta thấy cái hình này nó vừa có cái dáng dấp của hình vuông nhưng đồng thời nó sẽ có cái bo tròn
0:12:59 - 0:13:02, Và nó sẽ có những cái bo tròn như thế này