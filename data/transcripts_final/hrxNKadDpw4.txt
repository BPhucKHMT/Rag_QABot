0:00:14 - 0:00:27, Chào các bạn, chúng ta sẽ cùng đến với một trong những chủ đề rất là thú vị và có rất nhiều những ứng dụng trong thời gian gần đây, đó chính là Deep Generative Model hay là các mô hình tạo sinh học sâu.
0:00:27 - 0:00:39, Trong phần 1, chúng ta sẽ cùng tìm hiểu về hai mô hình nền tảng, đó là Variational Autoencoder và GAN, đó là Generative Adversarial Network.
0:00:39 - 0:00:48, Trong phần 2, chúng ta sẽ cùng tìm hiểu về kiến trúc của mô hình Diffusion Model, mô hình khuếch tán và khử nhiễu.
0:00:48 - 0:01:09, Trong phần 3, chúng ta sẽ mở đầu bằng một câu hỏi nhỏ như sau, đâu là gương mặt thật? Đáp án đó chính là tất cả những hình ở trong đây đều không phải là ảnh thật mà là ảnh được tạo ra bởi một mô hình tạo sinh, đó là mô hình dựa trên GAN.
0:01:09 - 0:01:21, Thì nguồn của ảnh này được lấy từ trang This Person Does Not Exist và khi chúng ta vào trang web này thì nó sẽ tạo ra ngẫu nhiên một tấm hình nào đó.
0:01:21 - 0:01:33, Thì cái này có rất nhiều ứng dụng trong thực tế, ví dụ như chúng ta cần minh họa về một cái người nào đó để chúng ta bàn về chủ đề nào đó để cần một cái người minh họa.
0:01:33 - 0:01:46, Với những luật hiện đại hiện nay thì đã hạn chế việc chúng ta lấy hình ảnh ở trên mạng internet để chúng ta minh họa, nó sẽ ảnh hưởng đến vấn đề về quyền riêng tư của một người nào đó.
0:01:46 - 0:01:56, Do đó việc chúng ta sử dụng một ảnh không có thật thì nó sẽ giúp chúng ta giải quyết được vấn đề này, đó là không vi phạm những vấn đề về quyền riêng tư.
0:01:56 - 0:02:10, Và chúng ta thấy là ba tấm hình này thì gần như là rất giống với ảnh thật và không có những cái nét nào mà khiến chúng ta cảm giác đó là ảnh không phải là người thật.
0:02:10 - 0:02:13, Nếu để ý kỹ thì chúng ta mới có thể thấy được.
0:02:14 - 0:02:28, Rồi, thế thì để mở đầu cho cái phần các cái mô hình liên quan đến tạo sinh thì chúng ta sẽ cùng nhắc lại về hai cái mô hình mà rất là phổ biến trong máy học.
0:02:28 - 0:02:31, Đó chính là học có giám sát và học không có giám sát.
0:02:31 - 0:02:33, Thế thì học có giám sát là gì?
0:02:33 - 0:02:45, Là chúng ta sẽ học từ cái dữ liệu và trong đó thì x là cái dữ liệu đầu vào và y là cái nhãn dữ liệu mà chúng ta cần phải huấn luyện để có thể dự đoán ra được.
0:02:45 - 0:02:47, Thì đây là cái nhãn cần dự đoán.
0:02:47 - 0:02:56, Và mục tiêu của cái học có giám sát đó chính là chúng ta sẽ phải đi ước lượng một cái hàm số để ánh xạ từ x sang y.
0:02:56 - 0:03:02, Để sau này khi chúng ta có một cái x mới, ví dụ như chúng ta ánh xạ, chúng ta tìm ra được một cái ánh xạ này đúng không?
0:03:02 - 0:03:12, Thì khi chúng ta có một cái x new thì qua cái hàm ánh xạ là f của x new thì chúng ta sẽ dự đoán được cái đầu ra là bao nhiêu?
0:03:12 - 0:03:15, Thì cái ứng dụng của học có giám sát rất là nhiều.
0:03:15 - 0:03:25, Chúng ta có thể dùng trong lĩnh vực về phân loại một cái đối tượng nào đó, bài toán hồi quy, dự đoán một cái giá trị có tính thứ tự nào đó.
0:03:25 - 0:03:29, Rồi phát hiện đối tượng, phân đoạn ngữ nghĩa đối tượng v.v.
0:03:29 - 0:03:36, thì học có giám sát nó đã đạt được những thành tựu hiện nay và có thể ứng dụng được rất là rộng rãi.
0:03:36 - 0:03:39, Thế thì còn một cái mảng nữa đó chính là học không có giám sát.
0:03:39 - 0:03:49, Thì học không có giám sát đó là dữ liệu đầu vào chúng ta sẽ không có y mà chúng ta chỉ có duy nhất một biến x đầu vào.
0:03:49 - 0:03:56, Chúng ta không có nhãn, thì đây chính là sự khác biệt lớn nhất giữa học không giám sát và học có giám sát.
0:03:56 - 0:04:06, Mục tiêu của học không giám sát đó là chúng ta sẽ học từ cái cấu trúc ẩn của dữ liệu, hay là chúng ta sẽ đi học cái phân bố của dữ liệu.
0:04:06 - 0:04:09, Phân bố của dữ liệu.
0:04:12 - 0:04:22, Rồi, thì mình có thể lấy một cái ví dụ như sau để chúng ta hiểu thế nào là chúng ta đi học một cái cấu trúc ẩn và học một cái phân bố dữ liệu.
0:04:22 - 0:04:25, Bằng cách đó là chúng ta sẽ trả lời cho cái câu hỏi sau.
0:04:25 - 0:04:34, Một bạn học sinh có điểm trung bình là ví dụ như là 8,8 điểm.
0:04:34 - 0:04:44, Thì theo các bạn đó là bạn này sẽ có cái học lực là giỏi, khá xuất sắc hay là dưới trung bình.
0:04:44 - 0:04:56, Thì đa số đó là chúng ta sẽ không biết được cái điểm này là cao hay thấp khi chúng ta không đặt nó ở trong cái phân bố của những cái bạn còn lại trong lớp.
0:04:56 - 0:05:06, Thì nếu như cái điểm của lớp mình mà có cái phân bố như sau, điểm 8,8 thì nằm ở cái khu vực này.
0:05:06 - 0:05:09, Ví dụ như điểm 8,8 là nằm ở khu vực này.
0:05:09 - 0:05:18, Thì bạn này là một bạn có học lực giỏi tại vì nhìn trong cái phân bố này bạn nằm ở cái top mà những người có điểm cao.
0:05:18 - 0:05:27, Nhưng ngược lại nếu trong một cái phân bố khác, điểm 8,8 của bạn nó lại nằm ở đây.
0:05:27 - 0:05:35, 8,8 thì khi đó là với cái phân bố này thì điểm của bạn là có học lực dưới trung bình.
0:05:35 - 0:05:42, Như vậy để kết luận được tính chất của dữ liệu x thì chúng ta phải học được cái phân bố.
0:05:42 - 0:05:49, Vì vậy, trong cái phân bố chúng ta sẽ xác định được cái phân bố của dữ liệu và từ đó chúng ta hình thành được cái cấu trúc ẩn của dữ liệu.
0:05:49 - 0:05:58, Ví dụ chúng ta chia cái không gian của mình ra làm 3 phần. Ví dụ vậy thì đây là những cái bạn mà có học lực kém.
0:05:58 - 0:06:02, Đây là những bạn có học lực trung bình và đây là những bạn có học lực giỏi.
0:06:02 - 0:06:06, Ví dụ vậy thì đây là chúng ta đang cấu trúc hóa cái không gian ẩn của mình.
0:06:06 - 0:06:14, Và những cái bài toán mà kinh điển liên quan đến cái học không giám sát đó chính là bài toán phân cụm, bài toán giảm chiều dữ liệu.
0:06:14 - 0:06:22, Thì đó là chúng ta đã cùng ôn lại một vài cái khái niệm về học có giám sát và học không có giám sát.
0:06:22 - 0:06:31, Và bây giờ chúng ta sẽ quay trở lại đến với cái chủ đề của chúng ta. Đó là mô hình tạo sinh hay còn gọi là generative model.
0:06:31 - 0:06:37, Thì mục tiêu của cái mô hình tạo sinh đó là chúng ta sẽ lấy mẫu dữ liệu huấn luyện.
0:06:37 - 0:06:42, Lấy mẫu dữ liệu huấn luyện đầu vào của một số cái phân bố, của một số cái phân phối.
0:06:42 - 0:06:51, Và từ đó chúng ta có thể tái tạo lại các cái phân phối đó. Tức là chúng ta sẽ đi lấy mẫu.
0:06:51 - 0:06:59, Đầu tiên đó là chúng ta lấy mẫu. Ví dụ chúng ta muốn tạo ra những cái ảnh mặt người giống với lại cái ảnh thật như vậy.
0:06:59 - 0:07:04, Trong cái ví dụ ở slide đầu tiên thì chúng ta sẽ đi lấy mẫu.
0:07:07 - 0:07:14, Và chúng ta sẽ lấy mẫu những cái người trong thế giới thực của mình. Đó là những cái ảnh thật.
0:07:16 - 0:07:21, Kìa như vậy. Rồi sau đó chúng ta sẽ đưa vào một cái hàm.
0:07:21 - 0:07:25, Và lưu ý là ở đây chúng ta không hề có cái nhãn dữ liệu. Chúng ta không có cái nhãn dữ liệu.
0:07:25 - 0:07:32, Rồi sau đó chúng ta sẽ đi học cái phân phối này. Để huấn luyện cái đầu vào của một cái phân phối.
0:07:32 - 0:07:39, Và sau khi chúng ta đã ra được một cái phân phối rồi, thì chúng ta sẽ đi tái tạo lại cái phân phối đó.
0:07:39 - 0:07:45, Ví dụ như từ các cái ảnh này chúng ta sẽ học ra được cái phân phối là như sau.
0:07:45 - 0:07:53, Thì lấy mẫu là cái ảnh này tương ứng là nằm trong cái phân phối này. Ở vị trí này.
0:07:53 - 0:07:57, Ảnh này thì tương ứng nằm ở vị trí này. Vân vân.
0:08:01 - 0:08:06, Và sau đó thì chúng ta sẽ đi học để tái tạo lại phân phối đó.
0:08:06 - 0:08:17, Từ đó có thể lấy mẫu một cái mẫu mới. Rồi sau đó chúng ta tạo ra một cái dữ liệu.
0:08:17 - 0:08:26, Và cái dữ liệu này vì chúng ta đã học ra được cái phân phối của các cái không gian mà có chứa cái ảnh gương mặt.
0:08:26 - 0:08:33, Nên khi chúng ta lấy mẫu trong cái phân phối này thì khi chúng ta khôi phục, chúng ta tái tạo lại thì nó cũng sẽ ra ảnh một cái gương mặt.
0:08:33 - 0:08:44, Và đó chính là cái ý nghĩa của cái mô hình tạo sinh. Tức là chúng ta sẽ đi lấy mẫu dữ liệu, huấn luyện nó vào để huấn luyện cho một cái phân phối.
0:08:44 - 0:08:50, Và sau đó để học cái cách để mà chúng ta học để mà tái tạo lại. Đây là tái tạo.
0:08:52 - 0:08:58, Chúng ta sẽ tái tạo lại cái tấm ảnh mới. Thì cái quá trình tái tạo này chính là cái quá trình tạo sinh ảnh.
0:08:58 - 0:09:08, Thế thì cái mô hình tạo sinh ở giai đoạn đầu nó được sử dụng để phục vụ cho cái công việc đó là tăng cường dữ liệu.
0:09:08 - 0:09:18, Tại vì các cái mô hình học máy chúng ta biết rằng là các cái mô hình học máy thì nó sẽ bị cái hiện tượng gọi là hiện tượng overfitting.
0:09:18 - 0:09:24, Mà hiện tượng overfitting thì có hai cách để giải quyết. Một đó là giảm cái độ phức tạp của mô hình.
0:09:24 - 0:09:33, Giảm cái số tham số hoặc là cái độ phức tạp của mô hình. Và hai là chúng ta đi tăng cường cái data lên.
0:09:33 - 0:09:42, Nếu chúng ta không giảm thì chúng ta sẽ đi tăng cái data lên. Và để tăng data này thì chúng ta sẽ phải đi thu thập dữ liệu rất là nhiều.
0:09:42 - 0:09:47, Thu thập dữ liệu rất là nhiều. Thế thì nhờ có các cái mô hình tạo sinh tạo ra các cái ảnh tự động.
0:09:47 - 0:09:53, Chúng ta chỉ cần đi lấy mẫu một cái điểm trong cái phân phối này. Xong rồi chúng ta tái tạo lại thì chúng ta đã có một cái ảnh mới.
0:09:53 - 0:10:02, Thì cái mô hình tạo sinh nó đã giúp cho chúng ta tự động tăng cường dữ liệu lên. Giúp cho cái việc huấn luyện mô hình gọi là đỡ bị hiện tượng overfitting hơn.
0:10:02 - 0:10:11, Thì đó chính là cái động cơ của mô hình tạo sinh. Và giả sử như chúng ta có một cái mẫu dữ liệu. Chúng ta lấy mẫu dữ liệu để huấn luyện.
0:10:11 - 0:10:18, Chúng ta lấy được cái mẫu này. Sau đó chúng ta xác định được cái phân phối của nó. Đây là ước lượng một cái hàm mật độ.
0:10:18 - 0:10:25, Thì chúng ta thấy cái khu vực này là xuất hiện dày đặc. Nhưng mà nó hơi ít hơn so với cái khu vực bên tay phải.
0:10:25 - 0:10:34, Khu vực này sẽ xuất hiện dày hơn. Còn ở cái khu vực ở giữa ở đây hoặc là ở ngoài rìa ở đây thì chúng ta thấy đó là khi lấy mẫu nó thưa hơn.
0:10:34 - 0:10:41, Do đó khi chúng ta ước lượng ra cái hàm mật độ thì nó sẽ có cái dạng như thế này. Đó là hai đỉnh.
0:10:41 - 0:10:53, Và khi chúng ta tạo sinh mẫu thì từ chúng ta sẽ tạo ra được những cái ảnh mới mà có cái tính chất giống như là cái tính chất của cái ảnh mà chúng ta đã lấy mẫu trước đó.
0:10:54 - 0:11:04, Thì đây là, trong cái ví dụ này thì đây chính là cái mẫu đầu vào để huấn luyện. Đó là cái ảnh trong thế giới thực. Lấy lại các cái điểm này.
0:11:06 - 0:11:16, Rồi. Và mẫu được tạo sinh thì chúng ta sẽ đi sampling liên tiếp trong cái không gian của mình. Những cái điểm màu xanh sẽ là những cái điểm chúng ta lấy mẫu trong thực tế.
0:11:16 - 0:11:28, Còn cái điểm màu đỏ, ví dụ ở đây. Thì đây chính là một cái điểm chúng ta random sampling trong cái không gian. Và sau đó chúng ta tái tạo lại. Thì nó sẽ tạo ra các cái điểm ở đây.
0:11:33 - 0:11:37, Nó sẽ tạo ra các cái mẫu mới. Thì đây là mẫu tạo sinh.
0:11:37 - 0:11:46, Thì đối với cái mẫu đầu vào thì chúng ta sẽ ký hiệu đó là dữ liệu huấn luyện. Chúng ta sẽ sampling trong cái Pdata.
0:11:46 - 0:11:58, Trong cái Pdata, tức là cái dữ liệu thế giới thực. Mẫu tạo sinh thì chúng ta sẽ đi sampling trong cái Pmodel. Tức là giả sử như chúng ta đã có cái model này rồi. Đây là cái model.
0:11:59 - 0:12:10, Rồi thì chúng ta sẽ sampling trong cái dữ liệu trong cái không gian của cái mô hình mà mình ước lượng được. Đây là cái mô hình ước lượng được. Rồi từ đó chúng ta tái tạo lại.
0:12:10 - 0:12:27, Và cái việc mà học một cái mô hình tạo sinh là chúng ta tìm cách nào đó để cho cái mô hình PmodelX nó sẽ có cái sự tương tự về mặt phân phối giống như là Pdata.
0:12:27 - 0:12:35, Thì đây chính là cái mục tiêu của các cái mô hình tạo sinh. Pmodel phải xấp xỉ, phải tương tự với lại cái P của data.
0:12:36 - 0:12:51, Vậy thì tại sao chúng ta cần phải có cái mô hình tạo sinh thì nó có khả năng là khám phá ra các đặc trưng cơ bản của dữ liệu. Ví dụ các đặc trưng về mặt màu da, các đặc trưng về tư thế đồng nhất.
0:12:51 - 0:13:00, Ví dụ như chúng ta huấn luyện với các ảnh gương mặt khác nhau thì chúng ta lấy rất nhiều những ảnh thuộc rất nhiều những chủng tộc.
0:13:00 - 0:13:11, Thì khi huấn luyện xong thì cái mô hình của mình nó vẫn có thể ý thức được cái vấn đề về màu da, màu da trắng, màu da đen, màu da vàng, ví dụ vậy.
0:13:11 - 0:13:20, Hoặc là tư thế thì có thể là nhìn trực diện hoặc là nhìn nghiêng về bên trái, nhìn nghiêng về bên phải hoặc là ngẩng lên trên. Đó thì là các cái tư thế của mỗi gương mặt.
0:13:20 - 0:13:26, Thì cái mô hình của mình nó ngầm, có thể học được cái đặc trưng cơ bản này của dữ liệu.
0:13:26 - 0:13:32, Và khi chúng ta áp dụng trong thực tế thì cái màu da, tư thế và ánh sáng nó cực kỳ đa dạng.
0:13:32 - 0:13:36, Rồi chúng ta thấy đây là những ảnh trong thực tế thì nó sẽ cực kỳ đa dạng.
0:13:36 - 0:13:43, Và chúng ta luôn mong muốn là cái mô hình của mình nó sẽ xấp xỉ hai cái không gian này.
0:13:43 - 0:13:49, Đây là mô hình P của Model và đây là P của Data.
0:13:50 - 0:13:57, Và mình luôn mong muốn là hai cái thằng này nó sẽ tương tự nhau, tìm cách để đưa cái P Model về gần với lại P Data.
0:13:57 - 0:14:03, Rồi, như vậy thì từ cái P Model này thì chúng ta làm sao có thể sử dụng được các cái thông tin này
0:14:03 - 0:14:08, để tạo ra các cái dữ liệu cân bằng và có cái tính đại diện hơn.
0:14:08 - 0:14:15, Thế thì chúng ta sẽ cùng thảo luận thế nào là cái sự cân bằng và có cái tính đại diện trong cái phần slide tiếp theo.
0:14:16 - 0:14:25, Đó là những cái điểm trong cái phân bố của mình thì nó sẽ có những cái điểm, nó gọi là điểm Outlier hay là những cái điểm lạ, điểm dữ liệu lạ.
0:14:25 - 0:14:32, Vấn đề đó là làm thế nào để khi gặp những cái đối tượng mới hoặc là hiếm trong dữ liệu.
0:14:32 - 0:14:40, Rồi chiến lược đó là chúng ta sẽ tận dụng được các cái mô hình tạo sinh để phát hiện ra các điểm dữ liệu lạ trong cái phân bố.
0:14:40 - 0:14:49, Rồi sau đó sử dụng các cái dữ liệu Outlier này để mà cho cái quá trình huấn luyện mô hình, nó sẽ cải thiện được cái mô hình tốt hơn.
0:14:49 - 0:14:54, Thì ở đây chúng ta sẽ dẫn nhập, giải thích cái ý này.
0:14:54 - 0:15:03, Các cái mô hình của mình khi được huấn luyện thì lấy dữ liệu ở trong thực tế mà dữ liệu chúng ta sampling trong thực tế thì 95%
0:15:03 - 0:15:06, Chúng ta lấy một cái ví dụ đó là dữ liệu mà lái xe trên đường.
0:15:06 - 0:15:11, 95% dữ liệu mà chúng ta lái xe, tức là nó nằm ở trong cái khu vực này.
0:15:11 - 0:15:14, Là trong cái khu vực này.
0:15:14 - 0:15:20, Thì đó là những cái tình huống rất là bình thường. Đây là những cái tình huống bình thường.
0:15:20 - 0:15:31, Tại vì chúng ta biết rồi là khi chúng ta đi lưu thông trên đường thì cái tình huống mà chúng ta gặp tai nạn rất là ít xảy ra.
0:15:31 - 0:15:34, Do đó thì chúng ta lấy những cái tình huống đó khá là thấp.
0:15:34 - 0:15:36, Mà đại đa số sẽ là những cái tình huống đẹp.
0:15:36 - 0:15:46, Ví dụ như là trời nắng, ví dụ như là đi đường cao tốc, ít xe cộ qua lại, rồi ví dụ đường là thẳng, không có bị gồ ghề, gập ghềnh.
0:15:46 - 0:15:50, Ví dụ vậy thì 95% dữ liệu là nó ở dạng bình thường.
0:15:50 - 0:15:57, Và cái việc phát hiện các cái điểm dữ liệu ngoại vi hay là những cái điểm outlier đó.
0:15:57 - 0:16:02, Trong cái quá trình mà huấn luyện để tránh được những cái tình huống không thể đoán được.
0:16:02 - 0:16:12, Tại vì khi chúng ta muốn xây dựng cái dữ liệu cho cái hệ thống xe tự lái, chúng ta phải lường trước những cái tình huống mà xe sẽ gặp những cái tình huống mà hiếm xuất hiện.
0:16:12 - 0:16:19, Ví dụ như có một người băng qua trước mặt, hoặc là đường thì gập ghềnh, đường bị quanh co, hoặc là thời tiết xấu.
0:16:20 - 0:16:28, Thế thì cái mà chúng ta mong muốn có cái dữ liệu để cho mô hình của mình, để cải thiện cái mô hình của mình,
0:16:28 - 0:16:34, đó là những cái tình huống là phân bố ở bên ngoại vi, tức là outlier.
0:16:34 - 0:16:40, Đây là cái outlier mà chúng ta mong muốn có dữ liệu để cho mô hình nó học.
0:16:40 - 0:16:46, Ví dụ đó là có cái giải phân cách, hoặc là có cái tình huống là máy bay lớn đi trên đầu,
0:16:46 - 0:16:58, hoặc là thời tiết xấu, thời tiết cực đoan, ví dụ như là mưa to, bão tuyết, hoặc là có tình huống người đi bộ băng qua đột ngột trước mặt mình, hoặc là đùa giỡn, v.v.
0:16:58 - 0:17:04, Thì đó chính là những cái tình huống ngoại vi mà chúng ta mong muốn có cái dữ liệu này để cho mô hình của mình nó học được.
0:17:04 - 0:17:11, Thì đó chính là cái lý do tại sao chúng ta cần có mô hình tạo sinh, để khi chúng ta sampling với những cái tình huống ngoại lệ này,
0:17:11 - 0:17:17, thì chúng ta sẽ có được những cái dữ liệu này, nó sẽ giúp cho cái bộ dữ liệu huấn luyện của chúng ta nó cân bằng hơn.
0:17:17 - 0:17:27, Nó giúp cân bằng hơn và dẫn đến là mô hình của mình nó sẽ không bị bias vào 95% những cái dữ liệu mà đẹp, dữ liệu bình thường ở đây.
0:17:27 - 0:17:38, Và một trong những cái hướng tiếp cận để mà tạo ra các cái mô hình tạo sinh, đó là chúng ta sẽ sử dụng mô hình là Latent Variable,
0:17:38 - 0:17:45, tức là mô hình dựa trên biến tiềm ẩn và có hai cái mô hình chúng ta sẽ cùng tìm hiểu trong cái buổi ngày hôm nay,
0:17:45 - 0:17:53, đó là mô hình về Autoencoder, đó là Autoencoder phiên bản gốc và Variational Autoencoder hay viết tắt là VAE
0:17:53 - 0:17:58, và Generative Adversarial Network, tức là mô hình tạo sinh đối kháng GAN.
0:17:58 - 0:18:04, Thì đây là hai cái mô hình dựa trên cái biến tiềm ẩn. Biến tiềm ẩn ở đây chính là cái VectorZ.
0:18:04 - 0:18:09, Và chúng ta sẽ cùng tìm hiểu chi tiết trong những cái phần tiếp theo.