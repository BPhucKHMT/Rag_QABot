0:00:14 - 0:00:23, Chúng ta sẽ mở rộng cài đặt của Autoencoder cho Variational Autoencoder
0:00:23 - 0:00:30, Trong phần trước, chúng ta đã cài hai cái module đó là Encode và Decode
0:00:30 - 0:00:41, Trong phần Variational Autoencoder, chúng ta nhận thấy cái kiến trúc này, phần Encode thì nó khác một chút ở cái bước cuối
0:00:41 - 0:00:45, Còn phần Decode thì cũng từ một cái vector z
0:00:45 - 0:00:52, Chúng ta giải nén ra và biến thành cái vector từ 2,5,12 lên 784
0:00:52 - 0:01:00, Hoàn toàn giống với cái kiến trúc từ 2,5,2,7,84 của Autoencoder
0:01:00 - 0:01:04, Do đó chúng ta sẽ tái sử dụng lại cái hàm này
0:01:04 - 0:01:07, Ở đây chúng ta sẽ chạy lại
0:01:07 - 0:01:09, Rồi
0:01:09 - 0:01:14, Và chúng ta sẽ tái sử dụng lại cái hàm Decoder này
0:01:15 - 0:01:28, Đối với phần Encode thì chúng ta sẽ thấy, thay vì chúng ta tạo ra một vector z gọi là Xác Định, tức là Cố Định
0:01:28 - 0:01:33, Thì ở đây chúng ta sẽ tạo ra hai cái giá trị đó là Mu và Sigma
0:01:33 - 0:01:40, Từ Mu và Sigma này thì chúng ta sẽ sampling để tạo ra một vector z theo phân bố
0:01:41 - 0:01:46, Đó là z là bằng phân bố Gaussian của Mu và Sigma
0:01:46 - 0:01:52, Và ở đây chúng ta cũng sẽ sử dụng một cái kỹ thuật đó là Reparameterization, tức là tái tham số
0:01:52 - 0:01:56, Thế thì trước tiên chúng ta sẽ cài cái Variational Encoder
0:01:56 - 0:02:03, Thì đối với cái Variational Autoencoder này thì chúng ta sẽ sử dụng tương tự như vậy
0:02:03 - 0:02:08, Chúng ta sẽ có cell.linear
0:02:09 - 0:02:16, Thì cái linear này là linear1 và sẽ là bằng Neural Network.linear
0:02:16 - 0:02:20, Đầu vào là 784 và đầu ra sẽ là 512
0:02:20 - 0:02:28, Rồi tương tự như vậy chúng ta sẽ có cái kỹ thuật đó là từ 512 về...
0:02:28 - 0:02:33, Ở đây là lớp linear số 2
0:02:33 - 0:02:39, Thì cái lớp linear số 2 này là từ 512 về cái phía vector Mu
0:02:39 - 0:02:42, Thì Mu của mình sẽ có kích thước là 2
0:02:42 - 0:02:47, Tuy nhiên ở đây chúng ta sẽ không có hardcode mà chúng ta sẽ để Latent Dim ở đây
0:02:47 - 0:02:52, Rồi tương tự như vậy chúng ta sẽ có cái linear số 3
0:02:52 - 0:02:58, Thì ở đây chúng ta sẽ tương tự nhưng có điều là chúng ta phải đặt tên lớp là lớp khác
0:02:58 - 0:03:01, Tại vì đây sẽ là dành cho Mu
0:03:01 - 0:03:05, Còn đây sẽ là dành cho Sigma
0:03:05 - 0:03:12, Rồi và đối với cái phần mà forward thì chúng ta cũng sẽ có các cái bước tương tự
0:03:12 - 0:03:14, Một đó là chúng ta sẽ flatten
0:03:14 - 0:03:23, Thì chúng ta sẽ để là x sẽ được gán ngược trở lại là bằng torch.flatten
0:03:23 - 0:03:31, X và dim là bằng một
0:03:31 - 0:03:37, Rồi sau đó chúng ta sẽ gọi đến cái bước đầu tiên
0:03:37 - 0:03:42, Bước biến đổi đầu tiên đó chính là chúng ta sẽ đưa qua cái lớp linear
0:03:42 - 0:03:45, Chúng ta sẽ đưa qua cái lớp linear và có một cái activation function
0:03:45 - 0:03:52, Thì ở đây activation function chúng ta có thể sử dụng đó là một cái hàm là relu
0:03:52 - 0:04:00, Thì đây là f.relu của cái self.linear1
0:04:00 - 0:04:06, Rồi và chúng ta sẽ truyền vào cái biến x và nó sẽ trả ra là x
0:04:06 - 0:04:10, Rồi tiếp theo thì chúng ta sẽ qua cái lớp thứ 2
0:04:10 - 0:04:13, Sau cái lớp 6 thì nó đã biến thành một cái vector 512
0:04:13 - 0:04:16, Bây giờ chúng ta sẽ chia nó ra làm hai nhánh
0:04:17 - 0:04:24, Thì Mu sẽ là bằng self.linear2 của x
0:04:24 - 0:04:27, Rồi và sẽ tạo ra là Mu
0:04:27 - 0:04:35, Và trong trường hợp này thì chúng ta cũng không có sử dụng cái activation function
0:04:35 - 0:04:37, Lý do đó là vì cái Mu của mình
0:04:37 - 0:04:42, Thì nó có thể từ giá trị âm cho đến giá trị dương
0:04:43 - 0:04:49, Trong khi nếu chúng ta dùng relu thì nó bị bó hẹp lại cái output của mình là phải từ không trở lên
0:04:49 - 0:04:54, Trong khi Mu của mình nó hoàn toàn có thể là nó là tâm của một cái phân bố xác suất
0:04:54 - 0:04:56, Nó có thể là số âm hay số dương được
0:04:56 - 0:05:01, Do đó ở đây chúng ta sẽ không có để cái activation function relu
0:05:01 - 0:05:10, Về sigma thì sigma của chúng ta là chúng ta sẽ để là self.linear
0:05:11 - 0:05:14, Và truyền vào x
0:05:14 - 0:05:18, Thế thì chúng ta muốn cái sigma của mình nó phải là một cái con số dương
0:05:18 - 0:05:20, Con sigma của mình là con số dương
0:05:20 - 0:05:23, Thì chúng ta có thể để relu ở đây cũng được
0:05:23 - 0:05:26, Còn nếu chúng ta muốn một cách khác
0:05:26 - 0:05:29, Ví dụ chúng ta để một cái hàm miễn sao nó trả ra là số dương
0:05:29 - 0:05:36, Ví dụ như hàm torch.exp
0:05:36 - 0:05:42, Thì nó sẽ ép cái giá trị có thể là từ âm vô cùng đến dương vô cùng về một con số dương
0:05:42 - 0:05:45, Sigma thì bắt buộc phải là con số dương
0:05:45 - 0:05:48, Bước tiếp theo chúng ta sẽ sampling
0:05:48 - 0:05:54, Thế thì như trong slide lý thuyết chúng ta không thể nào mà sampling trực tiếp z
0:05:54 - 0:06:00, Với cái phân bố Gaussian mà có Mu và sigma
0:06:00 - 0:06:01, Giống như ở trên được
0:06:01 - 0:06:06, Tại vì nếu làm như vậy thì nó sẽ không có back propagation được
0:06:06 - 0:06:10, Giờ ở đây chúng ta sẽ dùng cái kỹ thuật reparameterization trick
0:06:10 - 0:06:15, Thì để là Mu cộng cho cái code ở đây nó cũng đã gợi ý ra nè
0:06:15 - 0:06:20, Nhân với lại cái n_sample
0:06:20 - 0:06:25, Thế thì ở đây cái n_sample nó là cái gì
0:06:25 - 0:06:34, Nó chính là cái các cái giá trị đã được lấy theo cái phân bố chuẩn
0:06:34 - 0:06:39, Rồi sau đó thì chúng ta sẽ nhân với lại cái thành phần sigma này
0:06:39 - 0:06:42, Và kích thước của mình thì nó sẽ là bằng Mu.shape luôn
0:06:42 - 0:06:46, Tức là Mu và sigma này sẽ cùng kích thước nhau và cụ thể luôn
0:06:46 - 0:06:48, Trong cái ví dụ này thì size của nó là bằng 2
0:06:48 - 0:06:52, Thì đây chính là reparameterization trick
0:06:52 - 0:07:03, Tiếp theo thì chúng ta sẽ tính cái KL divergence
0:07:03 - 0:07:08, Thì sau khi chúng ta đã có được cái phân bố Mu và sigma rồi
0:07:08 - 0:07:11, Chúng ta sẽ làm một cái thành phần nó gọi là chính quy hóa
0:07:11 - 0:07:16, Thế thì dựa trên cái công thức của cái trang web
0:07:16 - 0:07:17, Ở đây chúng ta có thể tham khảo ha
0:07:17 - 0:07:20, Thì cái công thức của cái KL divergence
0:07:20 - 0:07:23, Nó sẽ là bằng sigma bình phương cộng cho Mu bình phương
0:07:23 - 0:07:25, Trừ cho log của sigma
0:07:25 - 0:07:26, Trừ 1 phần 2
0:07:33 - 0:07:41, Rồi trừ cho torch.log của sigma
0:07:41 - 0:07:44, Tất cả trừ cho 1 phần 2
0:07:44 - 0:07:48, Rồi sau đó tất cả cái thành phần này chúng ta sẽ đi lấy tổng
0:07:49 - 0:07:52, Tại vì nó có rất nhiều mẫu
0:07:52 - 0:07:53, Thì chúng ta sẽ lấy tổng
0:07:53 - 0:07:58, Rồi sau đó chúng ta sẽ chỉ return z thôi
0:07:58 - 0:08:04, Còn không return KL divergence để cho cái bước sau là qua decode
0:08:04 - 0:08:08, Còn KL divergence thì chúng ta sẽ để lại
0:08:08 - 0:08:11, Để chút nữa chúng ta sẽ đưa vào bên trong cái hàm loss
0:08:11 - 0:08:15, Rồi như vậy thì chúng ta sẽ tạo cái Variational Encoder
0:08:15 - 0:08:17, Để chúng ta chạy cái code này
0:08:18 - 0:08:22, Rồi sau đó chúng ta sẽ tạo cái lớp variational autoencoder
0:08:22 - 0:08:27, Trong đó encoder thì lấy từ variational encoder chúng ta vừa mới lập trình xong
0:08:27 - 0:08:33, Còn decoder thì chúng ta sẽ lấy lại lớp mà chúng ta đã cài đặt
0:08:33 - 0:08:36, Ở trong cái phần autoencoder ở phía trước
0:08:36 - 0:08:39, Tức là cái class này
0:08:39 - 0:08:43, Rồi thì bây giờ chúng ta sẽ tạo
0:08:43 - 0:08:47, Và z là cái kết quả của encode
0:08:47 - 0:08:50, Sau đó z được truyền vào cho decode để trả kết quả cuối cùng
0:08:50 - 0:08:52, Và đây chính là cái x mũ
0:08:52 - 0:08:56, Thì chúng ta luôn mong muốn cái x mũ xấp xỉ với cái x đầu vào bắt đầu
0:08:56 - 0:08:57, Thế thì để cái chuyện đó xảy ra
0:08:57 - 0:09:01, Thì chúng ta sẽ có một cái hàm gọi là hàm loss
0:09:01 - 0:09:08, Thì hàm loss này nó sẽ là bằng sai số của cái x hat
0:09:08 - 0:09:13, Trừ cho x tất cả mũ 2
0:09:14 - 0:09:20, Rồi, sau đó thì chúng ta sẽ có thêm cái phần là tính tổng nữa
0:09:20 - 0:09:27, Rồi cái này thì để cho chắc thì chúng ta sẽ để cái dấu mọi thứ đúng không ạ
0:09:27 - 0:09:29, Tức là cái tổng sai số
0:09:29 - 0:09:34, Rồi sau đó chúng ta sẽ đi cộng cho cái thành phần KL divergence
0:09:34 - 0:09:39, Thì nó sẽ lấy ra từ cái auto, cái mô hình là autoencoder này
0:09:39 - 0:09:44, Tại vì nó chỉ nằm trong encoder thôi
0:09:44 - 0:09:49, Chứ không nằm trong cái lớp tức là autoencoder
0:09:49 - 0:09:55, KL thì nó nằm ở trong cái encoder
0:09:55 - 0:09:59, Encoder là con của KL
0:09:59 - 0:10:02, Rồi bây giờ chúng ta sẽ chạy cái thuật toán này
0:10:02 - 0:10:05, Chạy cái hàm này và huấn luyện
0:10:05 - 0:10:08, Latent Dim thì chưa được define
0:10:08 - 0:10:12, Rồi
0:10:12 - 0:10:18, Ok, thì cái Latent Dim này là bằng 2
0:10:18 - 0:10:21, Rồi thì chúng ta sẽ khởi tạo trực tiếp ở đây
0:10:21 - 0:10:28, Chúng ta tách 2 cái đoạn code này ra
0:10:28 - 0:10:30, Data is not defined
0:10:30 - 0:10:35, Thì cái code của cái phần data là chúng ta sẽ lấy trong cái tập dữ liệu MNIST
0:10:35 - 0:10:42, Do đó thì chúng ta gọi cái hàm của scikit-learn để lấy cái data về
0:10:42 - 0:10:44, Đây
0:10:44 - 0:10:48, Autoencoder is not defined
0:10:48 - 0:10:54, Nó sẽ lấy từ torch, không phải từ scikit-learn
0:10:54 - 0:10:56, Rồi util.latch
0:10:56 - 0:10:58, Và đưa về cái thư mục data
0:10:58 - 0:11:02, Và data lúc này của mình thì sẽ chứa dữ liệu
0:11:02 - 0:11:04, Cả nhãn và không có nhãn
0:11:04 - 0:11:09, Thế thì là có cái phần train nhưng mà chưa được gọi
0:11:15 - 0:11:19, Rồi thì ở đây chúng ta không cần phải chạy lại cái hàm train của autoencoder
0:11:19 - 0:11:22, Cái chúng ta cần đó là cái hàm lấy data ở đây
0:11:22 - 0:11:25, Chúng ta sẽ mượn lại đem xuống
0:11:31 - 0:11:34, Và chờ cái mô hình này nó train xong
0:11:34 - 0:11:37, Thì có thể nó tốn của chúng ta là khoảng 2 phút
0:11:37 - 0:11:41, Thế thì chúng ta sẽ cùng xuống dưới để xem cái kết quả
0:11:44 - 0:11:45, Rồi
0:11:45 - 0:11:49, Thì ở đây chúng ta sẽ tạm xóa cái kết quả cũ đã chạy trước đây
0:11:49 - 0:11:53, Để xem tại vì với mỗi lần random có thể ra những cái con số khác nhau
0:11:53 - 0:11:56, Thì cái phần trực quan hóa, cái latent space này
0:11:56 - 0:11:58, Thì chúng ta sẽ dùng cái hàm
0:11:58 - 0:12:00, Là
0:12:00 - 0:12:01, Plot latent
0:12:01 - 0:12:04, Tương tự như của autoencoder
0:12:04 - 0:12:08, Nhưng có điều cái mô hình ở đây chúng ta truyền sẽ là cái mô hình của VAE
0:12:08 - 0:12:13, Và cái data của mình sẽ là lấy cái data của toàn bộ dataset
0:12:13 - 0:12:23, Và chúng ta sẽ in ra cái biểu đồ của cái không gian latent
0:12:23 - 0:12:28, Thì ở đây chúng ta sẽ có một cái thí nghiệm đó là chúng ta sẽ interpolate
0:12:28 - 0:12:32, Tức là sẽ quét trong toàn bộ cái không gian của ảnh
0:12:32 - 0:12:39, Và sau đó thì chúng ta sẽ xác định xem là cái kết quả sau khi chúng ta decode ra
0:12:39 - 0:12:41, Là cái ảnh của mình nó nhìn như thế nào
0:12:41 - 0:12:44, Thì nó sẽ nằm trong cái interpolate
0:12:44 - 0:12:50, Và cái interpolate này thì nó sẽ chạy từ x1 cho đến x2
0:12:50 - 0:12:53, Với x1, x2 là một cái điểm nào đó mà chúng ta lấy
0:12:53 - 0:13:01, Ví dụ như là x1 thì được lấy từ một cái điểm ảnh có cái nhãn là 1 là số 1
0:13:01 - 0:13:06, x2 thì tương ứng là một cái x có cái y là bằng 0
0:13:06 - 0:13:07, Tức là chúng ta lấy một cái con số 0 ra
0:13:07 - 0:13:13, x1 chính là số 1 và x2 chính là một cái hình của con số 0
0:13:13 - 0:13:20, Và sau đó chúng ta sẽ plot trên toàn bộ các cái vector z
0:13:20 - 0:13:25, Vector ẩn từ x1 cho đến x2
0:13:25 - 0:13:30, Thì chúng ta sẽ bàn chi tiết hơn về cái cách thức mà nó chạy
0:13:30 - 0:13:37, Ngoài ra thì chúng ta có cung cấp thêm một cái ảnh gif để cho tạo cái hình động
0:13:37 - 0:13:46, Và cuối cùng chúng ta có thể thử nghiệm với lại các độ dài của các latent space khác nhau
0:13:51 - 0:13:56, Rồi thì sau khoảng 2 phút thì đã đã kết thúc cái quá trình train
0:13:56 - 0:14:02, Và chúng ta sẽ plot cái hàm plot này chúng ta chưa lấy từ ở phía trên xuống
0:14:16 - 0:14:20, Chúng ta có một cái câu hỏi đó là nhận xét gì về cái kết quả này
0:14:20 - 0:14:25, Thì chúng ta thấy nếu như ở trong cái không gian latent ở trên
0:14:25 - 0:14:28, thì chúng ta thấy là nó bị hở khá là nhiều
0:14:28 - 0:14:33, Và cái đường đi của mình nó không phải là dạng hình tròn một cái phân bố Gaussian
0:14:33 - 0:14:36, mà nó đi một cái dạng vòng cung như thế này
0:14:36 - 0:14:41, Trong khi đó ở bên dưới thì chúng ta thấy là nó tròn hơn
0:14:41 - 0:14:45, Đương nhiên nó sẽ có một số cái khu vực
0:14:45 - 0:14:52, Ví dụ như màu số 8 thì nó sẽ là nó hơi dẹp
0:14:52 - 0:14:53, Thực ra là dẹp hay không chúng ta cũng không biết
0:14:53 - 0:14:56, Tại vì nó sẽ ẩn nó bị chồng lên ở đằng sau
0:14:56 - 0:14:59, Lý do tại sao nó bị chồng lên ở khu vực này
0:14:59 - 0:15:03, Đó là vì cái không gian của chúng ta là chọn là 2 chiều
0:15:03 - 0:15:08, Nên nó khá là chật chội dẫn đến là cái mô hình của mình nó có thể bị chồng lấp
0:15:08 - 0:15:15, Nhưng riêng số 1 và số 7 thì nó ít bị chồng lấp hơn
0:15:15 - 0:15:18, Chúng ta thấy là nó nằm ngoài ra ngoài rìa và ít bị chồng lấp hơn
0:15:18 - 0:15:23, Thì số 1 nó có hình thù khá là khác biệt so với những cái con số còn lại
0:15:23 - 0:15:24, Tương tự như vậy số 7
0:15:24 - 0:15:30, Còn các con số ví dụ như là số 8 hoặc là số 3 chẳng hạn
0:15:30 - 0:15:33, Thì chúng ta thấy là nó có những cái nét trùng nhau
0:15:33 - 0:15:38, Vì vậy như số 8 chúng ta bỏ đi cái phần bên tay trái thì sẽ ra số 3
0:15:38 - 0:15:44, Sau đó chúng ta sẽ tìm cách trực quan hóa
0:15:44 - 0:15:51, Thế thì trong cái hình này chúng ta sẽ trực quan hóa trong đoạn từ trừ 3 cho đến 3
0:15:51 - 0:15:58, Thì chúng ta sẽ dùng công cụ snip để cắt cái màn hình này ra
0:15:58 - 0:16:05, Rồi, và trong cái sơ đồ này chúng ta sẽ thấy là
0:16:05 - 0:16:10, Cái khu vực là từ trừ 3 cho đến 3
0:16:10 - 0:16:18, Rồi, thì cái trục hoành của mình là từ trừ 3 cho đến 3
0:16:18 - 0:16:20, Thì đâu đó là khoảng giá trị ở đây
0:16:20 - 0:16:25, Giá trị ở đây cho đến 3, giá trị là ở đây
0:16:26 - 0:16:33, Còn trục tung là từ trừ 3 cho đến 3 thì nó cũng sẽ là từ đây cho đến đây
0:16:33 - 0:16:36, Rồi thì chúng ta chiếu lên
0:16:39 - 0:16:41, Thì nó sẽ là nằm ở đây
0:16:48 - 0:16:52, Thì đây là cái ô vuông mà chúng ta sẽ trực quan
0:16:52 - 0:16:59, Rồi, và chúng ta cũng sẽ chia lưới ra là 12 đường, 12 phần
0:16:59 - 0:17:06, Và với mỗi điểm trên cái mắt lưới này thì chúng ta sẽ đi vẽ ra để xem cái ảnh của nó là như thế nào
0:17:06 - 0:17:10, Thì đó là cái ý tưởng của cái hàm plot reconstructed
0:17:11 - 0:17:23, Thì chúng ta thấy là phía trên bên trái nó sẽ gần với lại số 8 là đúng rồi
0:17:23 - 0:17:26, Tại vì đây là cái khu vực màu vàng lá mạ
0:17:26 - 0:17:31, Thì ở trong cái sơ đồ này là nó gần với lại số 8
0:17:31 - 0:17:34, Nên nó sẽ vẽ ra là số 8
0:17:35 - 0:17:41, Rồi, từ trái sang phải thì chúng ta sẽ thấy là đi vào cái khu vực nó khá là hỗn loạn
0:17:41 - 0:17:46, Nó vừa có màu vàng lá mạ, vừa có màu nâu của số 5 và màu đỏ của số 3
0:17:46 - 0:17:49, Nên chúng ta thấy là nó có bóng dáng của số 3
0:17:49 - 0:17:54, Nhưng mà một cách trực quan thì chúng ta thấy là từ trái sang phải nó đã có cái sự biến đổi
0:17:54 - 0:18:02, Cái đường nét của số 8 nó dần mất 2 cái nét cong ở bên tay trái để chuyển dần thành số 3
0:18:02 - 0:18:07, Rồi ra đây nó sẽ dần dần hòa lại để thành số 0
0:18:07 - 0:18:09, Thì số 0 của mình là màu xanh bên tay phải đây
0:18:11 - 0:18:18, Rồi, và từ trên xuống thì chúng ta thấy là nó đã dần chuyển hóa từ số 8 sang số 1
0:18:20 - 0:18:22, Từ số 8 sang số 1
0:18:22 - 0:18:30, Thì nó đang là đi vào từ vùng màu vàng xuống cái vùng màu cam, màu cam tương ứng với số 1
0:18:30 - 0:18:32, Và nó đi cũng khá là mượt
0:18:32 - 0:18:44, Rồi, tiếp theo thì chúng ta sẽ tìm cách trực quan hóa một chuỗi các con số mà được lấy từ 2 cái vector
0:18:44 - 0:18:47, Từ 1 cho đến 0
0:18:47 - 0:18:51, Thì chúng ta thấy là nó đã có sự dịch chuyển
0:18:51 - 0:18:56, Ban đầu chúng ta sẽ có x1 là hình số 1 như thế này
0:18:57 - 0:19:02, Và trong cái không gian latent z thì ở đây chúng ta sẽ tìm cách trực quan
0:19:10 - 0:19:17, Rồi, thì cái số 1 này thì ở bên trong cái không gian latent nó sẽ nằm ở đây
0:19:17 - 0:19:23, Số 0 này trong không gian latent thì nó nằm ở đây
0:19:24 - 0:19:34, Và cái ý nghĩa của cái hàm interpolate này, nội suy này là từ 2 cái điểm này chúng ta sẽ lấy mẫu đều
0:19:34 - 0:19:42, Và với mỗi điểm này chúng ta sẽ vẽ lên
0:19:42 - 0:19:48, Giống như trên
0:19:48 - 0:19:53, Rồi, thì chúng ta thấy là cái đường đi của nó cũng khá là mượt
0:19:53 - 0:20:03, Rồi, và chúng ta có thể sử dụng cái code interpolate.gif để mà
0:20:03 - 0:20:07, Thay vì chúng ta vẽ thành một giải như thế này thì nó sẽ tạo ra thành một ảnh thôi
0:20:07 - 0:20:11, Và khi chúng ta chạy cái này thì nó sẽ tạo ra một cái file.gif
0:20:11 - 0:20:17, Thì chúng ta có thể tải cái file.gif này về và nhìn thấy được cái sự dịch chuyển của nó
0:20:18 - 0:20:22, Từ số 1 chuyển sang số 0 theo dạng là hoạt hình
0:20:22 - 0:20:31, Và bài tập thực hành tiếp theo cho chúng ta, đó chính là chúng ta thử nghiệm điều gì xảy ra nếu như cái
0:20:31 - 0:20:37, Latent Dimension của mình là bằng 1, tức là cái không gian của mình nó kéo về rất là chật hẹp
0:20:37 - 0:20:42, Ở đây chúng ta mới chọn một cái không gian 2 chiều để mà chúng ta có thể trực quan được thôi
0:20:43 - 0:20:50, Trên cái mặt phẳng đó thì nó sẽ có một cái điểm yếu đó là nó bị chật quá và dẫn đến kéo các con số lại gần nhau như thế này
0:20:50 - 0:21:00, Nhưng mà nhìn chung thì các cái phân bố này là đều dễ hiểu và nó có giống như cái prior distribution đó là dạng Gaussian
0:21:02 - 0:21:09, Rồi và nếu chúng ta đưa về cái không gian 1 chiều thì điều gì sẽ xảy ra thì đó chính là cái bài tập thêm cho chúng ta