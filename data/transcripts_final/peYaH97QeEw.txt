0:00:14 - 0:00:18, Chắc hồi trước thì chúng ta đã cùng tìm hiểu về kiến trúc autoencoder
0:00:18 - 0:00:25, Bây giờ thì chúng ta sẽ cùng tìm hiểu về một biến thể rất là quan trọng, đó chính là Variational Autoencoder
0:00:26 - 0:00:37, Trước hết thì chúng ta sẽ nhắc lại autoencoder là gì và chúng ta sẽ xem có những điểm gì cần phải cải tiến trong kiến trúc này
0:00:37 - 0:00:46, Đầu tiên đó là autoencoder thì sẽ bao gồm hai thành phần, đó là một encoder và một decoder
0:00:46 - 0:00:59, Trong đó, encoder, nhiệm vụ của nó là chúng ta sẽ nén dữ liệu và biểu diễn dữ liệu ở số chiều cao về một vector biểu diễn thấp chiều hơn
0:00:59 - 0:01:10, Nếu chỉ như vậy thì không được, chúng ta sẽ phải tái tạo lại được so với dữ liệu gốc ban đầu là x, chúng ta sẽ phải có một decoder
0:01:10 - 0:01:20, Với decoder này thì nó sẽ giúp chúng ta liên kết về mặt ý nghĩa giữa vector z và dữ liệu gốc ban đầu x
0:01:20 - 0:01:34, Chúng ta tưởng tượng là trong một không gian latent, giống như ở đây thì ảnh của mình sẽ được map vào một không gian và ở đây sẽ là vector z
0:01:34 - 0:01:40, Vector z này qua decoder sẽ khôi phục ngược lại về ảnh gốc ban đầu
0:01:40 - 0:01:46, Câu hỏi đặt ra đó là bây giờ chúng ta sẽ dùng autoencoder này để làm gì?
0:01:46 - 0:01:55, Rõ ràng các mô hình tạo sinh được sử dụng để sinh hình ảnh, do đó chúng ta sẽ tiến hành sinh hình ảnh
0:01:55 - 0:02:08, Ví dụ chúng ta có một vector ở đây và chúng ta sẽ qua hàm decoder, viết tắt chữ D, thì nó sẽ tạo ra thành một tấm hình
0:02:08 - 0:02:23, Với kiến trúc encoder thì nó không có gì đảm bảo rằng ảnh sau khi chúng ta tái tạo lại có thể là một ảnh có ý nghĩa
0:02:23 - 0:02:35, Đó là ý thứ nhất. Ý thứ hai là một điều rất quan trọng, một vector z phải nằm ở gần z trong không gian latent space
0:02:35 - 0:02:54, Khi chúng ta decoder ra thì liệu có gì đảm bảo rằng ảnh mà chúng ta qua decoder phải có tính chất gì đó giống với vector z ở đây không?
0:02:54 - 0:03:08, Tại vì hai vector biểu diễn z và z phải ở trong không gian của mình nằm gần nhau, chúng ta kỳ vọng cái số giải mã ra được là giống như số 3
0:03:08 - 0:03:17, Chứ không thể nào nếu vector z phải gần z, nhưng nếu chúng ta decode mà nó ra số 7 chẳng hạn
0:03:18 - 0:03:27, Thì đây là điều mà chúng ta không mong muốn. Chúng ta mong muốn là z phải gần z, thì khi decode nó phải ra con số giống với con số z này
0:03:27 - 0:03:34, Thì đó mới đúng là một cái không gian latent. Với autoencoder nó không có đảm bảo được cái chuyện đó
0:03:34 - 0:03:44, Nếu như vector z sau khi tôi đã encode, thì khi tôi decode nó sẽ ra lại đúng ảnh ban đầu
0:03:44 - 0:03:54, Nó thiếu đi một tính chất chắn và tính liên quan trong không gian của mình. Đó là những điểm gần nhau sẽ có cùng một ý nghĩa giống nhau
0:03:54 - 0:04:04, Và đó chính là điểm yếu của autoencoder và variational autoencoder sẽ tìm cách giải quyết cái vấn đề này
0:04:04 - 0:04:14, Đó là thay vì với mỗi một cái ảnh khi chúng ta encode thì cũng có một cái module là encode
0:04:14 - 0:04:30, Đó là trong không gian latent. Tuy nhiên thay vì chúng ta ánh xạ sang một điểm cố định giống như trong autoencoder
0:04:30 - 0:04:40, Thì ở đây cái mà chúng ta sẽ ánh xạ sang đó chính là một distribution, một cái phân bố
0:04:40 - 0:04:50, Thế thì tại sao lại là phân bố mà không phải là một điểm cố định? Tại vì nếu chúng ta ánh xạ sang một điểm cố định
0:04:50 - 0:04:58, Thì nó sẽ dễ khiến cái mô hình của mình là học thuộc cái vị trí, tức là cứ ảnh đó thì vị trí đó, ảnh đó thì vị trí đó
0:04:58 - 0:05:10, Mà nó không có cái tính chất tổng quát. Thế thì dẫn đến là khi chúng ta có một cái điểm nào đó gần gần với điểm gì
0:05:10 - 0:05:16, Thì khi chúng ta tạo sinh ra nó không ra cái con số giống con số 3 này. Trong khi đó nếu chúng ta ánh xạ sang một cái phân bố
0:05:16 - 0:05:24, Vì nó là một cái phân bố nên khi chúng ta lấy mẫu ngẫu nhiên một cái vector z nào đó để chúng ta thực hiện cái việc tái tạo lại
0:05:24 - 0:05:30, Thì cái z này nó có thể nằm ở đây, nó có thể nằm ở đây, nó có thể nằm ở đây, nó có thể nằm ở đây
0:05:30 - 0:05:40, Nói chung là xung quanh cái giá trị min này, thì cái vector z này khi chúng ta decode ra nó cũng đều có khả năng là tạo ra được cái số 3 như bạn đọc
0:05:40 - 0:05:54, Thì chính nhờ cái yếu tố ngẫu nhiên nó sẽ khiến cho cái mô hình của mình không có học thuộc
0:05:54 - 0:06:07, Và từ đó là nó sẽ tổng quát hơn và nó thỏa mãn được tính chất mà chúng ta mong muốn đó là hai vector gần nhau
0:06:07 - 0:06:10, Thì khi chúng ta decode nó sẽ giống nhau
0:06:10 - 0:06:17, Thì khi ra một cái phân bố như thế này thì chúng ta random, chúng ta bốc ngẫu nhiên một cái vector z
0:06:17 - 0:06:23, Theo cái phân bố mà chúng ta đã encode được là bao gồm hai tham số là mi và sigma
0:06:23 - 0:06:30, Thì cái đường màu vàng này, ý nghĩa của nó đó chính là cái phép sampling là lấy mẫu
0:06:30 - 0:06:42, Và lấy mẫu với cái phân bố được đại diện bởi hai tham số của cái phân bố đó là mi và sigma
0:06:42 - 0:06:47, Thì đây chính là cái sự khác biệt lớn nhất của autoencoder và decoder
0:06:47 - 0:06:57, Đó là thay vì chúng ta encode về một cái vector z cố định thì chúng ta sẽ đưa về một cái phân bố
0:06:57 - 0:07:00, Và từ cái phân bố này chúng ta mới bắt đầu đi lấy mẫu nó
0:07:00 - 0:07:03, Rồi sau đó cái vector z ở đây chúng ta decode
0:07:03 - 0:07:09, Và cái mục tiêu của nó cũng hoàn toàn tương tự như autoencoder
0:07:09 - 0:07:14, Thì cái module này là hoàn toàn tương tự
0:07:19 - 0:07:22, Hoàn toàn tương tự cái autoencoder
0:07:22 - 0:07:30, Rồi thì đây chính là cái ý tưởng của VAE
0:07:30 - 0:07:34, Thế thì chi tiết chúng ta sẽ đi đến những cái khái niệm
0:07:34 - 0:07:38, Thì mi ở đây là cái min vector
0:07:38 - 0:07:43, Thì chúng ta biết là trong một cái phân bố chuẩn hoặc là phân bố gauss
0:07:43 - 0:07:49, Thì nó sẽ có hai cái tham số để biểu diễn cho cái phân bố gauss này
0:07:49 - 0:07:52, Một đó là mi là min vector
0:07:52 - 0:07:55, Và hai đó là standard deviation vector là sigma
0:07:55 - 0:08:01, Với mi và sigma thì chúng ta có thể tạo ra được cái phân bố gauss của mình
0:08:01 - 0:08:06, Cái variational autoencoder nó chính là một cái biến thể
0:08:06 - 0:08:10, Và biến thể này là mang tính xác suất
0:08:10 - 0:08:12, Đây là một cái biến thể mang tính xác suất của autoencoder
0:08:12 - 0:08:15, Sự khác biệt nó nằm ở chỗ này
0:08:15 - 0:08:17, Thay vì chúng ta đưa đến một cái vector z cố định
0:08:17 - 0:08:22, thì chúng ta sẽ đến một cái cặp giá trị đại diện cho một phân bố, đó là mi và sigma
0:08:22 - 0:08:25, rồi sau đó chúng ta sẽ đi lấy mẫu ngẫu nhiên
0:08:25 - 0:08:29, và xoay xung quanh cái tham số mi và sigma này
0:08:32 - 0:08:36, rồi, và cái việc lấy mẫu này thì từ giá trị trung bình và độ lệch chuẩn
0:08:36 - 0:08:39, để chúng ta lấy cái mẫu tìm ẩn z
0:08:39 - 0:08:43, thì vector z này sẽ là cái vector tìm ẩn trong cái không gian latent của mình
0:08:43 - 0:08:48, thế thì cái giai đoạn tiếp theo của autoencoder đó chính là
0:08:48 - 0:08:54, chúng ta sẽ tối ưu hóa cái VAR này như thế nào
0:08:54 - 0:08:57, tức là chúng ta huấn luyện cái VAR này như thế nào
0:08:57 - 0:09:00, thì ở đây, đối với cái giai quá trình mã hóa
0:09:00 - 0:09:05, thì chúng ta sẽ ký hiệu bởi cái phân bố xác suất đó là Q
0:09:05 - 0:09:10, tức là cho trước x, x là cái ảnh đầu vào
0:09:10 - 0:09:16, thì cái phân bố của z khi cho trước x thì nó được tham số hóa bởi phi
0:09:16 - 0:09:20, thì nhờ có cái tham số phi này nè
0:09:20 - 0:09:22, chúng ta mới có thể tính toán
0:09:22 - 0:09:25, từ x chúng ta sẽ tính toán ra được
0:09:25 - 0:09:29, qua các cái bước mà encode chúng ta sẽ tính toán ra được cái mi và sigma
0:09:29 - 0:09:32, thì đây là chính là tham số của nguồn
0:09:32 - 0:09:40, ở cái quá trình giải mã thì chúng ta sẽ có cái phân bố là p theta
0:09:41 - 0:09:48, khi cho trước x thì chúng ta sẽ có được cái phân bố của z
0:09:48 - 0:09:50, rồi
0:09:50 - 0:10:00, và cái hàm loss cuối cùng của VAR đó là l phi theta và là x
0:10:00 - 0:10:05, thì phi theta và x thì phi chính là cái tham số của encoder
0:10:06 - 0:10:10, và theta chính là cái tham số của decoder
0:10:10 - 0:10:16, và cái hàm loss này, cái hàm lỗi này sẽ bao gồm 2 thành phần
0:10:16 - 0:10:19, đó là sai số tái tạo và thành phần chính quy
0:10:19 - 0:10:21, cái thành phần chính quy này nè
0:10:21 - 0:10:27, cái thành phần để giúp cho chúng ta đưa cái phân bố của mi và sigma về
0:10:27 - 0:10:29, đúng như cái phân bố mà chúng ta mong muốn
0:10:29 - 0:10:32, đó gọi là phân bố tiềm ẩn
0:10:32 - 0:10:38, Tiếp theo thì chúng ta sẽ tính cái sai số tái tạo này
0:10:38 - 0:10:42, đó là cái sai số được tính từ
0:10:42 - 0:10:48, bằng cách đó là so sánh cái x ban đầu với lại cái x mũ sau khi chúng ta đã decode
0:10:48 - 0:10:56, thì chúng ta có thể tính bằng cách là dùng log likelihood hoặc là dùng hàm độ lỗi bình phương
0:10:57 - 0:11:03, chúng ta sẽ lấy x trừ cho x mũ
0:11:03 - 0:11:08, tức là cái x sau khi chúng ta đã decode trừ cho cái giá trị ban đầu
0:11:08 - 0:11:12, rồi tất cả là lấy tổng bình phương các cái sai số
0:11:12 - 0:11:14, thì đây là tổng bình phương
0:11:14 - 0:11:30, Tiếp theo thì liên quan đến cái thành phần chính quy
0:11:30 - 0:11:38, thì chúng ta sẽ đảm bảo rằng là cái phân bố của Q
0:11:38 - 0:11:46, với cho trước x thì cái phân bố của vector z cho trước x là cái phân bố Q
0:11:46 - 0:11:52, và cái phân bố Q này thì được tạo bởi cái tham số đó là phi
0:11:52 - 0:11:54, thì cái phân bố Q này nè
0:11:54 - 0:11:58, tức là đại diện cho được đợt đại diện bởi hai cái tham số mi và sigma nè
0:11:58 - 0:12:04, thì chúng ta mong muốn nó xấp xỉ với lại một cái phân bố ẩn tiềm ẩn
0:12:04 - 0:12:06, một cái phân bố ẩn tiềm ẩn là px
0:12:06 - 0:12:10, thì chúng ta sẽ chính quy hóa thì px nó là cái gì