[
  {
    "page_content": "Quá trình huấn luyện, một mô hình là Neural Machine Translation, tức là cái bài toán về dịch máy, mà có sử dụng CNN. Đó là khi chúng ta đưa vào các giá trị đầu vào, ví dụ như là đưa vào là I'm not sure, thì bắt đầu cái quá trình tính toán sẽ tạo ra các giá trị dự đoán. dựa trên cái ground truth thì chúng ta sẽ tính ra được cái loss cho từ đầu tiên của đoạn văn đích rồi sau đó đến từ thứ 2, chúng ta sẽ có cái loss thứ 2, từ thứ 3, sẽ có cái loss thứ 3 đến từ thứ 6, chúng ta sẽ ra được cái loss",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=--JpgsDEL40",
      "filename": "--JpgsDEL40",
      "title": "[CS431 - Chương 9] Part 1_2: Giới thiệu bài toán Dịch máy",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:06"
    }
  },
  {
    "page_content": "thứ 3 đến từ thứ 6, chúng ta sẽ ra được cái loss thứ 6. Và tổng hợp, à và lưu ý đó là các cái loss thành phần này thì nó được sử dụng là cross entropy loss của cái từ R. Tức là đối với cái văn bản đích là y ở đây thì lẽ ra là chúng ta phải trả ra từ R đúng không? Đây là ground truth nè. Thì ở đây cái dự đoán y này nè, nó sẽ đi so với lại cái vector biểu diễn của từ R để xem xem là hai cái vector đó nó có tương đồng với nhau hay không. Và để tính được cái sai số đó thì chúng ta sẽ sử dụng độ đo",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=--JpgsDEL40",
      "filename": "--JpgsDEL40",
      "title": "[CS431 - Chương 9] Part 1_2: Giới thiệu bài toán Dịch máy",
      "chunk_id": 1,
      "start_timestamp": "0:00:11",
      "end_timestamp": "0:01:25"
    }
  },
  {
    "page_content": "được cái sai số đó thì chúng ta sẽ sử dụng độ đo là cross entropy loss và trong trường hợp này là cross entropy loss cho cái từ R Rồi, tương tự như vậy, đến cái thứ 4 thì chúng ta sẽ là phải đưa ra được cái giá trị dự đoán và nếu như cái giá trị dự đoán này nó khớp với lại cái từ pa trong tiếng pháp, thì cái loss của mình sẽ rất là thấp nhưng nếu không khớp thì loss của mình sẽ là rất là cao Tương tự như vậy, đến từ cuối cùng lẽ ra mình phải trả về kết thúc đó là end, nhưng mình lại trả ra một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=--JpgsDEL40",
      "filename": "--JpgsDEL40",
      "title": "[CS431 - Chương 9] Part 1_2: Giới thiệu bài toán Dịch máy",
      "chunk_id": 2,
      "start_timestamp": "0:01:21",
      "end_timestamp": "0:02:07"
    }
  },
  {
    "page_content": "về kết thúc đó là end, nhưng mình lại trả ra một từ khác thì rõ ràng là loss của mình sẽ là cao tổng hợp toàn bộ loss tương ứng với time step, thời điểm thì mình sẽ ra được là hàm loss như sau đây là trung bình cộng của các loss thành phần và trong quá trình huấn luyện thì từng loss thành phần này sẽ thực hiện thuật toán lan truyền ngược để cập nhật các trọng số trên mô hình ANN với loss số 1 nó sẽ lan truyền độ lỗi lan truyền độ lỗi ngược loss số 2 sẽ lan truyền và toàn bộ các loss này sẽ được",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=--JpgsDEL40",
      "filename": "--JpgsDEL40",
      "title": "[CS431 - Chương 9] Part 1_2: Giới thiệu bài toán Dịch máy",
      "chunk_id": 3,
      "start_timestamp": "0:02:00",
      "end_timestamp": "0:03:00"
    }
  },
  {
    "page_content": "2 sẽ lan truyền và toàn bộ các loss này sẽ được đưa lên lan truyền xuyên suốt toàn bộ mạng của mình và nó sẽ cập nhật các ma trận UVW ví dụ như đây là W, đây là U, đầu ra của mình sẽ là V nó sẽ cập nhật các ma trận trọng số này Để cho mô hình này có khả năng học được đặc trưng cấp cao hơn, chúng ta sẽ sử dụng kiến trúc Deep Stack Encoder Kiến trúc Deep Stack Encoder này, đầu ra của layer thứ Y sẽ là đầu vào của layer thứ Y cộng 1, tức là layer số 1 Layer số 2 sẽ là đầu vào cho Layer số 3 Đây là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=--JpgsDEL40",
      "filename": "--JpgsDEL40",
      "title": "[CS431 - Chương 9] Part 1_2: Giới thiệu bài toán Dịch máy",
      "chunk_id": 4,
      "start_timestamp": "0:02:56",
      "end_timestamp": "0:04:01"
    }
  },
  {
    "page_content": "1 Layer số 2 sẽ là đầu vào cho Layer số 3 Đây là minh họa của seq2seq nhằm giúp chúng ta giải quyết được bài toán dịch máy mà có đặc trưng có thể học được qua các tầng từ tầng cấp thấp cho đến tầng cấp giữa cho đến tầng cấp cao Và thành tựu của Neural Machine Translation đó là nếu như năm 2014, Sutskever và các cộng sự đã đề xuất ra seq2seq thì ngay sau đó chỉ 2 năm, tức là với sự phát triển rất là nhanh thì chỉ sau 2 năm, là Google Translate đã sử dụng và đã chuyển toàn bộ các mô hình dịch máy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=--JpgsDEL40",
      "filename": "--JpgsDEL40",
      "title": "[CS431 - Chương 9] Part 1_2: Giới thiệu bài toán Dịch máy",
      "chunk_id": 5,
      "start_timestamp": "0:03:52",
      "end_timestamp": "0:04:44"
    }
  },
  {
    "page_content": "sử dụng và đã chuyển toàn bộ các mô hình dịch máy Theo hướng tiếp cận truyền thống, đó là học thống kê sang hướng Neural Machine Translation sang dạng dùng ANN Sau đó cũng 2 năm, là đến năm 2018 Gần như tất cả các công ty nào có sử dụng các dịch vụ dịch thuật đều chuyển đổi sử dụng sang mô hình này ví dụ như là Bing Translate của Microsoft cũng đã chuyển sang sử dụng các mô hình về Neural Machine Translation. Như vậy thì điều đó có thể nói là trong một thời gian rất ngắn, Seq2Seq đã tạo ra được",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=--JpgsDEL40",
      "filename": "--JpgsDEL40",
      "title": "[CS431 - Chương 9] Part 1_2: Giới thiệu bài toán Dịch máy",
      "chunk_id": 6,
      "start_timestamp": "0:04:40",
      "end_timestamp": "0:05:21"
    }
  },
  {
    "page_content": "một thời gian rất ngắn, Seq2Seq đã tạo ra được một bước đột phá cả về trong học thuật lẫn trong lĩnh vực về công nghiệp, về công nghệ. Và các công ty công nghệ đã chuyển đổi hoàn toàn sang mô hình Seq2Seq này, thì điều đó chứng tỏ là tính hiệu quả của mô hình này và đồng thời nó có khả năng dễ dàng mở rộng cho rất nhiều những ngôn ngữ khác nhau cũng như là sau này khi có những từ khóa mới thì nó cũng có thể dễ dàng học và cập nhật lại được thì đó chính là thành tựu của Neural Machine",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=--JpgsDEL40",
      "filename": "--JpgsDEL40",
      "title": "[CS431 - Chương 9] Part 1_2: Giới thiệu bài toán Dịch máy",
      "chunk_id": 7,
      "start_timestamp": "0:05:16",
      "end_timestamp": "0:05:59"
    }
  },
  {
    "page_content": "được thì đó chính là thành tựu của Neural Machine Translation Và để đánh giá được mô hình dịch máy thì đây là một trong những bài toán khó trong việc là Đánh giá Tại vì một cái bản dịch của mình Một cái văn bản nguồn của mình Thì nó có khả năng nhiều Cách dịch khác nhau Ví dụ như cũng một cái câu đó Nhưng mà một cái người theo chuyên ngành Về khoa học Thì họ sẽ dịch theo một phong cách Và người theo chuyên ngành về Xã hội thì sẽ dịch theo Một cách hoặc là một người trẻ Và một người lớn tuổi Họ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=--JpgsDEL40",
      "filename": "--JpgsDEL40",
      "title": "[CS431 - Chương 9] Part 1_2: Giới thiệu bài toán Dịch máy",
      "chunk_id": 8,
      "start_timestamp": "0:05:57",
      "end_timestamp": "0:06:43"
    }
  },
  {
    "page_content": "hoặc là một người trẻ Và một người lớn tuổi Họ có thể dịch theo một cái cách khác nhau sau đó thì đánh giá một mô hình dịch máy thì đây là một cái vấn đề khó nhưng mà khó thì không có nghĩa là không có giải pháp và một trong những giải pháp phổ biến hiện nay để mà có thể đánh giá được mô hình dịch máy của mình có tốt hay không đó là sử dụng độ đo BLEU BLEU là viết tắt của chữ bilingual evaluation understudy thì BLEU so sánh cái phiên bản dịch máy với một hoặc là nhiều một thì tuy nhiên rồi,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=--JpgsDEL40",
      "filename": "--JpgsDEL40",
      "title": "[CS431 - Chương 9] Part 1_2: Giới thiệu bài toán Dịch máy",
      "chunk_id": 9,
      "start_timestamp": "0:06:35",
      "end_timestamp": "0:07:14"
    }
  },
  {
    "page_content": "máy với một hoặc là nhiều một thì tuy nhiên rồi, nhưng mà nó phải là để tăng tính khách quan thì nó nên là so với nhiều cái bản dịch khác nhau so với nhiều cái bản dịch khác nhau của các chuyên gia lưu ý là cái bản dịch này cũng phải là của chuyên gia nha chứ còn những người mà không chuyên về ngôn ngữ thì có thể là sẽ dịch không tốt và sau đó thì sẽ tính được cái độ tương đồng giữa cái bản dịch với lại cái bản của chuyên gia và ở đây cái cách mà người ta so sánh đó là sử dụng trung bình điều",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=--JpgsDEL40",
      "filename": "--JpgsDEL40",
      "title": "[CS431 - Chương 9] Part 1_2: Giới thiệu bài toán Dịch máy",
      "chunk_id": 10,
      "start_timestamp": "0:07:05",
      "end_timestamp": "0:08:02"
    }
  },
  {
    "page_content": "mà người ta so sánh đó là sử dụng trung bình điều hòa của các N-gram Precision tức là thay vì chúng ta chỉ so với từng chữ thì ở đây chúng ta sẽ so với cụm N chữ Ví dụ như là bản dịch là một từ, rồi của chuyên gia đó là 1 N-gram 3 từ, đây là 1 N-gram 2 từ và đây là 1 N-gram 1 từ Nó sẽ tính trung bình cho N-gram Precision, trung bình điều hòa Và BLEU thì mặc dù là hiệu quả nhưng mà không có thực sự là hoàn hảo, nó cũng không hoàn hảo Tại vì sao? Tại vì nó sẽ bị bias hay bị chủ quan bởi các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=--JpgsDEL40",
      "filename": "--JpgsDEL40",
      "title": "[CS431 - Chương 9] Part 1_2: Giới thiệu bài toán Dịch máy",
      "chunk_id": 11,
      "start_timestamp": "0:07:57",
      "end_timestamp": "0:08:36"
    }
  },
  {
    "page_content": "sao? Tại vì nó sẽ bị bias hay bị chủ quan bởi các chuyên gia của mình Và như đã đề cập, dịch máy có rất nhiều cách dịch khác nhau, rất là uyển chuyển Mình không thể cố định được một cách dịch Rồi chưa kể là cái yếu tố về tính phức tạp của ngôn ngữ nữa Thế thì có nhiều cái bản dịch tốt, hậu quả đó là gì? Có nhiều cái bản dịch tốt nhưng mà BLEU thì lại cho cái score thấp và cái chuyện này thì cũng không phải là hiếm, chuyện này cũng không phải là hiếm xảy ra Tuy nhiên, cho tới thời điểm hiện tại",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=--JpgsDEL40",
      "filename": "--JpgsDEL40",
      "title": "[CS431 - Chương 9] Part 1_2: Giới thiệu bài toán Dịch máy",
      "chunk_id": 12,
      "start_timestamp": "0:08:27",
      "end_timestamp": "0:09:10"
    }
  },
  {
    "page_content": "hiếm xảy ra Tuy nhiên, cho tới thời điểm hiện tại thì BLEU là một trong những độ đo đánh giá mà tin cậy Nó không hoàn hảo nhưng mà nó vẫn có khả năng thể hiện được sự đối sánh tương đối giữa các phương pháp dịch máy với nhau Nó thể hiện được sự so sánh tương đối, ví dụ như phương pháp này tốt hơn phương pháp kia thì cái BLEU này nó sẽ tốt hơn phương pháp kia Tuy nhiên, nếu giá trị BLEU có thể hiện được bản dịch là thật sự tốt hay không thì nó chưa thể hiện được nhưng nó có thể giúp chúng ta so",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=--JpgsDEL40",
      "filename": "--JpgsDEL40",
      "title": "[CS431 - Chương 9] Part 1_2: Giới thiệu bài toán Dịch máy",
      "chunk_id": 13,
      "start_timestamp": "0:09:01",
      "end_timestamp": "0:09:50"
    }
  },
  {
    "page_content": "thể hiện được nhưng nó có thể giúp chúng ta so được phương pháp này, nó có tốt hơn phương pháp kia hay không Nguyên nhân cho việc score thấp là có ít số lượng N-gram trùng với bản dịch của chuyên gia Nếu chúng ta đưa ra một bản dịch mà không khớp được, không khớp từ nào với các chuyên gia thì nó sẽ có score thấp. Và đây là một ví dụ, đây là bản dịch của máy và đây là bản dịch của một người thì chúng ta sẽ thấy là từ D, ở đây nó sẽ khớp After D, tức là N-gram, trong trường hợp này là N là bằng 2",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=--JpgsDEL40",
      "filename": "--JpgsDEL40",
      "title": "[CS431 - Chương 9] Part 1_2: Giới thiệu bài toán Dịch máy",
      "chunk_id": 14,
      "start_timestamp": "0:09:46",
      "end_timestamp": "0:10:40"
    }
  },
  {
    "page_content": "là N-gram, trong trường hợp này là N là bằng 2 Rồi Attack là 1, N-gram là trong trường hợp này N là bằng 1 Rồi, so với lại cái bản dịch đối với người thứ 2 thì chúng ta thấy là cái cụm từ International Airport NX thì ở đây là một cái bản dịch N-gram đây là khớp N-gram với N là bằng 4 Rồi, tương tự như vậy cho 4 cái bản dịch và từ đó thì chúng ta sẽ tính ra được cái score trung bình, trung bình điều hòa cho 4 cái bản dịch này. Và theo dòng thời gian thì nếu như trước năm 2015, tức là năm 2014 là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=--JpgsDEL40",
      "filename": "--JpgsDEL40",
      "title": "[CS431 - Chương 9] Part 1_2: Giới thiệu bài toán Dịch máy",
      "chunk_id": 15,
      "start_timestamp": "0:10:29",
      "end_timestamp": "0:11:23"
    }
  },
  {
    "page_content": "thì nếu như trước năm 2015, tức là năm 2014 là sự ra đời của Seq2Seq thì đến năm 2015 trở về sau là các hướng tiếp cận của Neural Machine Translation, tức là sử dụng ANN, dựa trên ANN thì nó cho tốc độ tăng trưởng, cho sự gia tăng về độ chính xác tăng lên rất là nhiều và chúng ta có thể thấy là độ dốc, độ dốc của đường màu xanh đậm này làm nó đi rất dốc, tức là sự tăng trưởng về độ chính xác của nó Trong khi đó, các hướng tiếp cận dựa trên thống kê Statistical dựa trên thống kê ví dụ như ở đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=--JpgsDEL40",
      "filename": "--JpgsDEL40",
      "title": "[CS431 - Chương 9] Part 1_2: Giới thiệu bài toán Dịch máy",
      "chunk_id": 16,
      "start_timestamp": "0:11:10",
      "end_timestamp": "0:11:46"
    }
  },
  {
    "page_content": "kê Statistical dựa trên thống kê ví dụ như ở đây có 2 hướng dựa trên syntax và dựa trên phrase thì chúng ta thấy cũng có tăng trưởng nhưng mà tăng trưởng rất là thấp độ dốc của nó rất là thấp, tức là không có sự chênh lệch gì nhiều Và cũng từ 2017 trở về sau, chúng ta cũng thấy là không còn nhiều nghiên cứu sử dụng Phrase-Based hoặc là Syntax-Based Machine Translation theo hướng tiếp cận thống kê nữa. Chúng ta chỉ còn các hướng tiếp cận sử dụng Neural Machine Translation mà thôi. Điều này cho",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=--JpgsDEL40",
      "filename": "--JpgsDEL40",
      "title": "[CS431 - Chương 9] Part 1_2: Giới thiệu bài toán Dịch máy",
      "chunk_id": 17,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Neural Machine Translation mà thôi. Điều này cho thấy là tầm ảnh hưởng của hướng tiếp cận Neural Machine Translation và nó đã đánh bật những phương pháp truyền thống trước đây để tạo ra một hướng đi mới, hiệu quả hơn và thậm chí đã có thể ứng dụng được trong công nghiệp ngay.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=--JpgsDEL40",
      "filename": "--JpgsDEL40",
      "title": "[CS431 - Chương 9] Part 1_2: Giới thiệu bài toán Dịch máy",
      "chunk_id": 18,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề, học sâu, machine learning, ImageNet. Chủ đề, học sâu, machine learning, ImageNet. Đó là thư viện Keras với các cái module như là Layer Trong Layer thì nó sẽ có các cái module như là Embedding Layer, Dense, các cái lớp Fully Connected hoặc là RNN, SimpleRNN, tức là cái module RNN với lại cell của mình là gồm các cái thao tác tính toán trên hidden state và input state, và output state và trong layer này thì cũng có LSTM cell Trong lớp layer này thì nó sẽ có một cái module, có một cái lớp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0DGe4fjr1aw",
      "filename": "0DGe4fjr1aw",
      "title": "[CS431 - Chương 8] Part 4_1: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:15"
    }
  },
  {
    "page_content": "này thì nó sẽ có một cái module, có một cái lớp đối tượng đặc biệt đó chính là input, input sẽ quy định cho chúng ta biết là dữ liệu đầu vào của mình nó sẽ có kích thước là bao nhiêu Ngoài ra thì có các cái hàm module hỗ trợ khác như là load_model Load mô hình đã được huấn luyện trước đó. Model giúp chúng ta đóng gói input và output vào đối tượng tên là Model. Rồi Sequence, Sequence giúp chúng ta có thể chuyển đổi từ có thể làm một số thao tác trên các chuỗi, ví dụ như là padding, tức là cho",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0DGe4fjr1aw",
      "filename": "0DGe4fjr1aw",
      "title": "[CS431 - Chương 8] Part 4_1: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 1,
      "start_timestamp": "0:01:07",
      "end_timestamp": "0:01:52"
    }
  },
  {
    "page_content": "trên các chuỗi, ví dụ như là padding, tức là cho một chuỗi ngắn dài khác nhau, chúng ta sẽ đưa về cùng chuỗi có cùng cái kích thước. Trong nội dung phần ví dụ, chúng ta sẽ được quan sát rõ hơn. Và Keras Dataset IMDB thì trong 2.2.0 này, chúng ta sẽ sử dụng một bộ Dataset IMDB. Đây là một bộ Dataset chứa các review, các bình luận về các bộ phim. Và ở đây thì các trạng thái bình luận của mình thì có 2 trạng thái. Một đó là positive tích cực và hai đó là negative tiêu cực. Thì ở bước đầu tiên,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0DGe4fjr1aw",
      "filename": "0DGe4fjr1aw",
      "title": "[CS431 - Chương 8] Part 4_1: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 2,
      "start_timestamp": "0:01:46",
      "end_timestamp": "0:02:30"
    }
  },
  {
    "page_content": "hai đó là negative tiêu cực. Thì ở bước đầu tiên, chúng ta sẽ load bộ dữ liệu IMDB này lên. thì ở đây chúng ta có thể cho biết vocabulary size, tức là cho biết kích thước của từ điển này của mình. Nếu bình thường thì số từ của mình sẽ rất là lớn, nó có thể lên đến hàng trăm nghìn, thậm chí là hàng triệu từ. Tuy nhiên để cho cái thí nghiệm này của chúng ta có thể thực hiện được trong thời gian ngắn, thì chúng ta sẽ giới hạn từ điển của mình là 5 nghìn từ thôi. Và chúng ta hoàn toàn có thể thay,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0DGe4fjr1aw",
      "filename": "0DGe4fjr1aw",
      "title": "[CS431 - Chương 8] Part 4_1: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 3,
      "start_timestamp": "0:02:22",
      "end_timestamp": "0:03:00"
    }
  },
  {
    "page_content": "nghìn từ thôi. Và chúng ta hoàn toàn có thể thay, tức là các bạn hoàn toàn có thể thay con số 5.000 này bằng một con số khác Thế thì nếu như chúng ta có nhiều hơn 5.000 từ nhưng mà chúng ta lại chỉ lấy có 5.000 từ thì các từ còn lại nó bỏ đâu nó sẽ lưu trong một số đặc biệt, ví dụ như là số 0 Và 5.000 từ này là 5.000 từ nào thì nó sẽ lấy top những từ xuất hiện thường xuyên nhất trong từ điển của mình Rồi, thì như vậy thao tác đầu tiên của mình Đó là chúng ta sẽ tiến hành load cái bộ dữ liệu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0DGe4fjr1aw",
      "filename": "0DGe4fjr1aw",
      "title": "[CS431 - Chương 8] Part 4_1: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 4,
      "start_timestamp": "0:02:58",
      "end_timestamp": "0:03:44"
    }
  },
  {
    "page_content": "Đó là chúng ta sẽ tiến hành load cái bộ dữ liệu này. Chúng ta sẽ tiến hành load cái bộ dữ liệu. Và khi kết quả trả về, chúng ta sẽ trả về hai biến là Xtrain, Ytrain, Xtest và Ytest. Trong đó, Xtrain sẽ chứa chuỗi các giá trị. Nó sẽ chứa chuỗi các giá trị tương ứng với các từ trong câu review. và Ytrain sẽ chứa cái nhãn kết quả này mình sẽ trả về trạng thái của mình là positive, tích cực hay negative, tiêu cực tương tự như vậy cho Xtest và Ytest Rồi, thì ở đây chúng ta sẽ có một cái chuỗi, đó là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0DGe4fjr1aw",
      "filename": "0DGe4fjr1aw",
      "title": "[CS431 - Chương 8] Part 4_1: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 5,
      "start_timestamp": "0:03:38",
      "end_timestamp": "0:04:26"
    }
  },
  {
    "page_content": "thì ở đây chúng ta sẽ có một cái chuỗi, đó là câu bình luận của mình nó có thể dài và ngắn khác nhau Thường thì bình luận của mình là ngắn, đúng không? Tuy nhiên nó cũng không loại trừ có những câu bình luận của mình là dài Có thể là trên 500 chữ Thế thì đối với những câu quá dài thì chúng ta sẽ cắt ngắn Chúng ta sẽ lấy những từ cuối cùng để làm sao cho nó đủ, cho nó đủ 500 từ, 500 chữ Nếu như câu comment của mình ít hơn 500 chữ, thì khi đó chúng ta có thể làm thao tác padding, chúng ta sẽ chèn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0DGe4fjr1aw",
      "filename": "0DGe4fjr1aw",
      "title": "[CS431 - Chương 8] Part 4_1: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 6,
      "start_timestamp": "0:04:21",
      "end_timestamp": "0:05:34"
    }
  },
  {
    "page_content": "ta có thể làm thao tác padding, chúng ta sẽ chèn thêm vào Ví dụ, giả sử như cái X đầu vào của mình, đó là bao gồm các cái chuỗi ký tự Ví dụ như là 10, 20, rồi 23, 90 Lưu ý là cái số 10, 20, 23, 90 này đó chính là thứ tự của từ đó trong từ điển Tức là nó sẽ không lưu cái dạng thô, ví dụ như là This movie is exciting Nó không nói như vậy. Mà cái từ This này nó sẽ được mã hóa bằng có số 10, tức là cái vị trí của từ This trong từ điển là vị trí thứ 10. Movie thì tương ứng vị trí của nó trong từ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0DGe4fjr1aw",
      "filename": "0DGe4fjr1aw",
      "title": "[CS431 - Chương 8] Part 4_1: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 7,
      "start_timestamp": "0:05:33",
      "end_timestamp": "0:06:17"
    }
  },
  {
    "page_content": "10. Movie thì tương ứng vị trí của nó trong từ điển là 20. is, vị trí trong từ điển của nó là 23. Và Exciting, tương ứng trong từ điển của mình là vị trí thứ 90. Thì nó sẽ không lưu dữ liệu thô như thế này, và nó sẽ lưu dưới dạng chỉ số vị trí trong tập từ điển của mình. Đó là ý thứ nhất. Ý thứ hai là phần về Sequence padding. Chúng ta sẽ có kích thước tối đa của mình, chính là max review length. Max review length trong trường hợp này là 500 và chúng ta hoàn toàn có thể thay thế con số 500 này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0DGe4fjr1aw",
      "filename": "0DGe4fjr1aw",
      "title": "[CS431 - Chương 8] Part 4_1: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 8,
      "start_timestamp": "0:06:15",
      "end_timestamp": "0:07:30"
    }
  },
  {
    "page_content": "chúng ta hoàn toàn có thể thay thế con số 500 này bằng một cái con số khác. Nếu thực hiện thao tác padding này thì Xpad sẽ là 0,0,0,0 và phần cuối của mình đó chính là 10, 20, 23 và 90 Thế thì câu hỏi đặt ra là tại sao chúng ta không để padding ở cuối, mà chúng ta lại để padding ở phần đầu như thế này? Rõ ràng khi chúng ta lần lượt thực hiện việc đưa các giá trị vào để xử lý trong mạng RNN, thì nó sẽ xử lý số 10 trước, xử lý số 20, 23 và 90, tức là tương ứng là các từ This movie is exciting,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0DGe4fjr1aw",
      "filename": "0DGe4fjr1aw",
      "title": "[CS431 - Chương 8] Part 4_1: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 9,
      "start_timestamp": "0:07:21",
      "end_timestamp": "0:08:03"
    }
  },
  {
    "page_content": "là tương ứng là các từ This movie is exciting, sau đó nó sẽ đưa vào hàng loạt các con số 0. Rõ ràng là những con số 0 vào sau sẽ được xử lý cuối cùng và nó sẽ làm loãng đi thông tin của những từ đầu. những cái từ mà quan trọng nhất của mình do đó thì cái padding của mình mình sẽ để nó là padding ở phần đầu tại vì khi chúng ta tính toán trên những con số 0 nó sẽ không ra những cái thông tin gì hết và chúng ta sẽ thực sự tính toán trên các cái dữ liệu của mình ở những con số cuối cùng này thôi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0DGe4fjr1aw",
      "filename": "0DGe4fjr1aw",
      "title": "[CS431 - Chương 8] Part 4_1: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 10,
      "start_timestamp": "0:08:01",
      "end_timestamp": "0:09:04"
    }
  },
  {
    "page_content": "liệu của mình ở những con số cuối cùng này thôi thì đó là tại sao chúng ta phải padding ở đằng trước. rồi, bây giờ chúng ta sẽ cùng load cái thư viện của mình Rồi, thì ở đây mình có thực hiện trước thao tác đó là download dữ liệu Rồi, thì ở đây chúng ta sẽ cancel, rồi chạy nè Chúng ta sẽ xem Xtrain của mình là gì Đầu tiên chúng ta sẽ xem kích thước của Xtrain là 25.000 Tương tự như vậy, kích thước của Ytrain cũng là 25.000, như vậy ý nghĩa đó là gì? Đây chính là số mẫu dữ liệu Train, còn số mẫu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0DGe4fjr1aw",
      "filename": "0DGe4fjr1aw",
      "title": "[CS431 - Chương 8] Part 4_1: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 11,
      "start_timestamp": "0:08:59",
      "end_timestamp": "0:09:59"
    }
  },
  {
    "page_content": "gì? Đây chính là số mẫu dữ liệu Train, còn số mẫu dữ liệu Test của mình đó là cũng 25.000 luôn, như vậy tỉ lệ Train và Test là 50-50 Bây giờ chúng ta sẽ quan sát thử cái X đầu vào là cái gì Thì chúng ta sẽ lấy mẫu dữ liệu đầu tiên Như vậy, đây là các con số tương ứng như là chỉ số của từ trong từ điển của mình như đã đề cập ở bên dưới Như chúng ta đã nêu cái ví dụ ở đây thì các con số của mình chính là thứ tự từ trong từ điển của bạn Rồi, chúng ta sẽ lấy thử mẫu thứ 100. Rồi, bây giờ chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0DGe4fjr1aw",
      "filename": "0DGe4fjr1aw",
      "title": "[CS431 - Chương 8] Part 4_1: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 12,
      "start_timestamp": "0:09:56",
      "end_timestamp": "0:10:58"
    }
  },
  {
    "page_content": "ta sẽ lấy thử mẫu thứ 100. Rồi, bây giờ chúng ta sẽ cùng quan sát Y, Y thứ 100, đó là số 0, Y là số 1. Rồi, chúng ta sẽ cùng quan sát 10 mẫu dữ liệu đầu tiên. Rồi, thì 0 và 1, tức là 0 có thể là negative và 1 tức là positive. Đây là ý nghĩa của Xtrain và Ytrain Bây giờ chúng ta sẽ qua thao tác là padding như đã đề cập chúng ta sẽ chèn số 0 vào phía trước Sau khi chèn xong chúng ta sẽ quan sát thử Xtrain của mình Với mẫu dữ liệu đầu tiên nó sẽ là gì? Nó sẽ có chứa một loạt số 0 Nó không chèn vào",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0DGe4fjr1aw",
      "filename": "0DGe4fjr1aw",
      "title": "[CS431 - Chương 8] Part 4_1: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 13,
      "start_timestamp": "0:10:45",
      "end_timestamp": "0:11:34"
    }
  },
  {
    "page_content": "gì? Nó sẽ có chứa một loạt số 0 Nó không chèn vào cuối, tại vì nếu chèn vào cuối thì những thông tin chính của mình sẽ được tính toán trước. Và khi chúng ta lan truyền đến những con số 0 ở phía sau thì thông tin của những comment, review của mình đã bị mai một đi rồi. Bây giờ chúng ta sẽ vào phần chính của mình, đó chính là cài đặt mạng RNN. Ở đây chúng ta sẽ có lớp Embedding Layer. Embedding Layer này chính là mô hình Word2Vec. Một cách tổng quát thì Embedding Layer này chúng ta cũng có thể là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0DGe4fjr1aw",
      "filename": "0DGe4fjr1aw",
      "title": "[CS431 - Chương 8] Part 4_1: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 14,
      "start_timestamp": "0:11:29",
      "end_timestamp": "0:12:34"
    }
  },
  {
    "page_content": "thì Embedding Layer này chúng ta cũng có thể là một phép biến đổi tuyến tính. Nó có thể là một phép biến đổi tuyến tính. và sau đó chúng ta sẽ huấn luyện cái Embedding Layer này. Tức là chúng ta hoàn toàn có thể can thiệp để cho cái Embedding Layer này có thể huấn luyện được thay vì nó là một cái Layer tĩnh. Thì trong cái ví dụ này chúng ta đang xem xét là chúng ta sử dụng Layer tĩnh với cái mô hình Word2Vec đã được huấn luyện trước đó. Ở đây thì Vocab, tức là cái từ điển của mình sẽ có là 5",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0DGe4fjr1aw",
      "filename": "0DGe4fjr1aw",
      "title": "[CS431 - Chương 8] Part 4_1: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 15,
      "start_timestamp": "0:12:10",
      "end_timestamp": "0:13:18"
    }
  },
  {
    "page_content": "thì Vocab, tức là cái từ điển của mình sẽ có là 5 nghìn từ và chuỗi review tối đa là 500 chữ như đã đề cập trước đó Giờ chúng ta sẽ tải về mô hình Word2Vec của FastText sau đó chúng ta sẽ giải nén Lại khi chúng ta khởi tạo (initialize) ngôn ngữ, việc thiết lập các dự án và kết nối thường mất thời gian. Tương tự như trong bài Word2Vec, tổng cộng có thể tốn khoảng 3-4 phút. Trong khi mô hình Word2Vec và Embedding Layer được tải lên, chúng ta sẽ cùng bàn về lớp RNN. Lớp RNN này thì các phương thức",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0DGe4fjr1aw",
      "filename": "0DGe4fjr1aw",
      "title": "[CS431 - Chương 8] Part 4_1: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 16,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "bàn về lớp RNN. Lớp RNN này thì các phương thức như là load_model, save_model, Summary và Predict cũng tương tự như lớp CNN mà chúng ta đã học trong những bài trước. Đây là những phương thức để hiển thị những mô tả kiến trúc của mô hình và dự đoán trên mẫu dữ liệu Xtest mới.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0DGe4fjr1aw",
      "filename": "0DGe4fjr1aw",
      "title": "[CS431 - Chương 8] Part 4_1: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 17,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Cuối cùng, chúng ta sẽ cùng tìm hiểu về các cách thức để sử dụng một mạng huấn luyện sẵn. Thông thường, các mạng CNN được huấn luyện trên những tập dữ liệu rất lớn. Và việc huấn luyện này có thể kéo dài tính bằng ngày hoặc thậm chí tính bằng tháng. Tháng, nó có thể kéo dài đến hàng tháng Và không phải ai cũng có khả năng có thể đủ tài nguyên tính toán để mà có thể thực hiện được công việc huấn luyện này Do đó thì chúng ta sẽ có một kỹ thuật đó là sử dụng những mô hình huấn luyện sẵn để đi giải",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0I8uw0ELYj4",
      "filename": "0I8uw0ELYj4",
      "title": "[CS431 - Chương 4] Part 2: Sử dụng mạng huấn luyện sẵn",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:40"
    }
  },
  {
    "page_content": "sử dụng những mô hình huấn luyện sẵn để đi giải quyết những bài toán của riêng mình Ở đây chúng ta sẽ gọi là kỹ thuật sử dụng các pre-trained model Ở đây có 3 cách chính Cách đầu tiên đó là chúng ta sẽ sử dụng trực tiếp Chúng ta sẽ sử dụng trực tiếp, nghĩa là sao? Nếu như tập data set của mình, đây là tập data set của mình Nó có các cái nhãn, ví dụ như là máy bay, xe, mèo, con ngựa, con quân Và đối tượng cho bài toán mà mình đang quan tâm, đó là cat, dog, horse, tức là mèo, chó, và ngựa Vô tình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0I8uw0ELYj4",
      "filename": "0I8uw0ELYj4",
      "title": "[CS431 - Chương 4] Part 2: Sử dụng mạng huấn luyện sẵn",
      "chunk_id": 1,
      "start_timestamp": "0:00:32",
      "end_timestamp": "0:01:22"
    }
  },
  {
    "page_content": "cat, dog, horse, tức là mèo, chó, và ngựa Vô tình 3 đối tượng này trùng với lại các đối tượng trong tập dataset mà chúng ta đã huấn luyện trước đó Nó đã trùng thì chúng ta sẽ sử dụng trực tiếp luôn Chúng ta sẽ lấy chính cái model đó ra để sử dụng trực tiếp luôn Thì đây là cách ngây thơ nhất, đơn giản nhất để sử dụng Tuy nhiên nó sẽ có 1 vấn đề đó là Đó là, dữ liệu của mình, Cat, Dog và Horse này, nó có khả năng là nó đi theo những cái giống loài mà ở cái khu vực mà mình đang sinh sống. Còn tập",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0I8uw0ELYj4",
      "filename": "0I8uw0ELYj4",
      "title": "[CS431 - Chương 4] Part 2: Sử dụng mạng huấn luyện sẵn",
      "chunk_id": 2,
      "start_timestamp": "0:01:14",
      "end_timestamp": "0:01:54"
    }
  },
  {
    "page_content": "mà ở cái khu vực mà mình đang sinh sống. Còn tập Dataset này thì đó là những tập Dataset chung. Do đó thì có khả năng khi chúng ta sử dụng những cái mô hình mà đã huấn luyện trên tập dữ liệu lớn này, áp dụng trên chính dữ liệu của mình có khả năng là độ chính xác không đạt như chúng ta kỳ vọng. Nhưng mà đây là cách ngây thơ nhất, đơn giản nhất đầu tiên khi chúng ta sử dụng với một cái mạng huấn luyện sẵn Rồi, cách thứ hai, đó là chúng ta sẽ sử dụng cái mạng CNN mà đã được huấn luyện sẵn như là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0I8uw0ELYj4",
      "filename": "0I8uw0ELYj4",
      "title": "[CS431 - Chương 4] Part 2: Sử dụng mạng huấn luyện sẵn",
      "chunk_id": 3,
      "start_timestamp": "0:01:45",
      "end_timestamp": "0:02:28"
    }
  },
  {
    "page_content": "cái mạng CNN mà đã được huấn luyện sẵn như là một cái bộ rút trích đặc trưng Thì ở đây chúng ta sẽ lấy ra một cái hình ảnh ví dụ thôi ha, đó là một cái mạng ResNet 50 Và cái ResNet 50 này nó sẽ có cái phần đầu là cái phần rút trích đặc trưng Nó sẽ là rút trích đặc trưng. Cái phần sau là phần liên quan đến phần lớp. Các nhà khoa học mới phát hiện ra rằng các đặc trưng mà được huấn luyện với những tập dữ liệu lớn trước đây thì khá tổng quát. sau này chúng ta đưa vô một tập dữ liệu bất kỳ hoặc là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0I8uw0ELYj4",
      "filename": "0I8uw0ELYj4",
      "title": "[CS431 - Chương 4] Part 2: Sử dụng mạng huấn luyện sẵn",
      "chunk_id": 4,
      "start_timestamp": "0:02:23",
      "end_timestamp": "0:03:11"
    }
  },
  {
    "page_content": "chúng ta đưa vô một tập dữ liệu bất kỳ hoặc là đưa vô một đối tượng khác thì các đặc trưng này đâu đó vẫn có khả năng sử dụng, tái sử dụng lại được và chúng ta sẽ kết hợp nó, kết hợp với lại một mô hình máy học khác như vậy thì ở đây chúng ta sẽ loại bỏ đi, chúng ta sẽ loại bỏ đi phần lớp cuối cùng và ở đây, đúng không, kết thúc bước feature extraction này chúng ta sẽ ra một feature Chúng ta sẽ ra feature và chúng ta sẽ sử dụng feature này để đi kết hợp với một bộ phân lớp khác ví dụ ở đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0I8uw0ELYj4",
      "filename": "0I8uw0ELYj4",
      "title": "[CS431 - Chương 4] Part 2: Sử dụng mạng huấn luyện sẵn",
      "chunk_id": 5,
      "start_timestamp": "0:03:05",
      "end_timestamp": "0:03:51"
    }
  },
  {
    "page_content": "đi kết hợp với một bộ phân lớp khác ví dụ ở đây chúng ta có thể sử dụng bộ phân lớp là K-Nearest Neighbor chúng ta có thể sử dụng bộ phân lớp là SVM thì feature này nếu mà chiếu trong không gian chúng ta sẽ có các feature như thế này và khi có một cái feature mới cần phải phân loại ví dụ ở đây chúng ta sẽ có một cái feature mới thì chúng ta sẽ chạy cái thuật toán k-nearest-neighbor ví dụ trong trường hợp này k là bằng 3 chúng ta sẽ lấy ra 3 cái feature gần với lại cái điểm mà mình cần phân loại",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0I8uw0ELYj4",
      "filename": "0I8uw0ELYj4",
      "title": "[CS431 - Chương 4] Part 2: Sử dụng mạng huấn luyện sẵn",
      "chunk_id": 6,
      "start_timestamp": "0:03:38",
      "end_timestamp": "0:04:31"
    }
  },
  {
    "page_content": "gần với lại cái điểm mà mình cần phân loại này nhất sau đó chúng ta sẽ xem xem cái nhãn của 3 cái feature này đó là gì? Ví dụ như nếu đây là Dog, đây là Dog, đây là Cat như vậy chúng ta sẽ kết luận cái nhãn của cái điểm này, đó chính là Dog Đây chính là ý tưởng của thuật toán k láng giềng gần nhất, k nearest neighbor tương tự như vậy cho thuật toán SVM là thuật toán phân lớp nhị phân thì chúng ta sẽ có hai tập, ví dụ feature ở đây, tương ứng là cái điểm này và chúng ta sẽ có hai tập là tròn và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0I8uw0ELYj4",
      "filename": "0I8uw0ELYj4",
      "title": "[CS431 - Chương 4] Part 2: Sử dụng mạng huấn luyện sẵn",
      "chunk_id": 7,
      "start_timestamp": "0:04:28",
      "end_timestamp": "0:05:11"
    }
  },
  {
    "page_content": "cái điểm này và chúng ta sẽ có hai tập là tròn và cộng Rồi, chúng ta sẽ nhờ cái máy phân lớp để tìm ra cái đường biên tốt nhất để phân loại ra hai tập dữ liệu này thì đó là thuật toán Support Vector Machine Thì đây là cách sử dụng thứ hai Và cách sử dụng thứ ba Đó là Chúng ta sẽ sử dụng Transfer Learning hay gọi là Học chuyển tiếp Thì học chuyển tiếp ở đây là gì? Như đã đề cập đó là Các lớp đầu tiên đó đóng vai trò là Feature Extraction Còn lớp cuối đó đóng vai trò là Phân lớp Lớp phân lớp này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0I8uw0ELYj4",
      "filename": "0I8uw0ELYj4",
      "title": "[CS431 - Chương 4] Part 2: Sử dụng mạng huấn luyện sẵn",
      "chunk_id": 8,
      "start_timestamp": "0:05:08",
      "end_timestamp": "0:06:04"
    }
  },
  {
    "page_content": "cuối đó đóng vai trò là Phân lớp Lớp phân lớp này dùng cho dữ liệu cũ Dùng để phân lớp cho dữ liệu cũ, do đó mình sẽ không thể tái sử dụng nó được Chúng ta sẽ phải bỏ đi và thay bằng một cái tầng mạng Fully Connected các FC khác và chúng ta lưu ý là chúng ta ở lớp cuối cùng là chúng ta phải điều chỉnh nha ví dụ như ở đây chúng ta có 1000 lớp thì cái output FC này nó sẽ là 1 vector 1000 chiều nhưng mà giả sử như tập dữ liệu của mình nó chỉ có 3 object thôi 3 nhãn thôi thì lúc đó lớp đầu ra của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0I8uw0ELYj4",
      "filename": "0I8uw0ELYj4",
      "title": "[CS431 - Chương 4] Part 2: Sử dụng mạng huấn luyện sẵn",
      "chunk_id": 9,
      "start_timestamp": "0:06:00",
      "end_timestamp": "0:06:47"
    }
  },
  {
    "page_content": "object thôi 3 nhãn thôi thì lúc đó lớp đầu ra của mình nó sẽ phải ra cái vector nó chỉ có 3 chiều thôi chú ý cái chỗ đó là nó sẽ tùy thuộc vào số lượng các loại object của mình sau khi chúng ta bỏ phần lớp ở đây đi và nối với lại lớp FC hay là Neural Network ở đây và lưu ý là phải chỉnh lại tầng cuối cùng sao cho phù hợp với lại kích thước của data set của mình thì mình sẽ tiến hành cách đầu tiên, cách 3.1, đó là chúng ta sẽ đóng băng các lớp đầu này đi Tức là chúng ta sẽ không huấn luyện Chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0I8uw0ELYj4",
      "filename": "0I8uw0ELYj4",
      "title": "[CS431 - Chương 4] Part 2: Sử dụng mạng huấn luyện sẵn",
      "chunk_id": 10,
      "start_timestamp": "0:06:42",
      "end_timestamp": "0:07:44"
    }
  },
  {
    "page_content": "này đi Tức là chúng ta sẽ không huấn luyện Chúng ta sẽ không huấn luyện trên phần rút trích đặc trưng mà chúng ta chỉ huấn luyện ở đây Huấn luyện cái lớp phân lớp, huấn luyện cái việc phân lớp ở đây chúng ta sẽ có một thuật ngữ tinh chỉnh tinh chỉnh hoặc gọi là fine tune các tham số ở những lớp cuối này thôi thì đây là cái cách 3.1 tuy nhiên nếu như dữ liệu của mình đủ lớn cái cách 3.1 này nó chỉ phù hợp cho trường hợp data mới của mình data mới này của mình là nhỏ thôi còn khi mà data mới của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0I8uw0ELYj4",
      "filename": "0I8uw0ELYj4",
      "title": "[CS431 - Chương 4] Part 2: Sử dụng mạng huấn luyện sẵn",
      "chunk_id": 11,
      "start_timestamp": "0:07:39",
      "end_timestamp": "0:08:22"
    }
  },
  {
    "page_content": "này của mình là nhỏ thôi còn khi mà data mới của mình rất là lớn thì chúng ta không cần phải đóng băng cái lớp này chúng ta không cần đóng băng cái lớp rút trích đặc trưng mà chúng ta sẽ huấn luyện luôn trên toàn bộ mạng này luôn tức là chúng ta sẽ huấn luyện trên cả những phần feature extraction lẫn phần mới thêm vào thì đây là hai cái cách thức để học chuyển tiếp và cái cách này nó sẽ phù hợp cho cái trường hợp data của mình data mới của mình nó rất là lớn data mới hờ rồi, như vậy thì hy vọng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0I8uw0ELYj4",
      "filename": "0I8uw0ELYj4",
      "title": "[CS431 - Chương 4] Part 2: Sử dụng mạng huấn luyện sẵn",
      "chunk_id": 12,
      "start_timestamp": "0:08:19",
      "end_timestamp": "0:08:58"
    }
  },
  {
    "page_content": "rất là lớn data mới hờ rồi, như vậy thì hy vọng là qua cái phần số 3 này chúng ta sẽ được giới thiệu chúng ta hiểu qua các cái cách thức để mà sử dụng một cái mạng huấn luyện sẵn trên những tập dữ liệu rất là lớn để đi giải quyết cho các bài toán của cá nhân mình trên những dữ liệu của mình thì trong cách số 1 đó là nếu như dữ liệu của mình mà trùng với lại cái đối tượng mà mình quan tâm mà trùng với lại tập dữ liệu mà mình đã huấn luyện trước đó thì chúng ta sử dụng trực tiếp trong trường hợp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0I8uw0ELYj4",
      "filename": "0I8uw0ELYj4",
      "title": "[CS431 - Chương 4] Part 2: Sử dụng mạng huấn luyện sẵn",
      "chunk_id": 13,
      "start_timestamp": "0:08:53",
      "end_timestamp": "0:09:42"
    }
  },
  {
    "page_content": "thì chúng ta sử dụng trực tiếp trong trường hợp mà dữ liệu của mình nó không giống với dữ liệu mà đã được huấn luyện trước đó nhưng mà đồng thời hoặc là dữ liệu của mình nó giống nhưng mà nó rất khác về cái thể loại ví dụ như chó ở phương Tây nó sẽ khác với chó ở Việt Nam thì chúng ta sẽ sử dụng đến cái cách số 2 và cách số 3 cái cách số 2, đó là chúng ta sẽ sử dụng kết hợp cái cách 2 là chúng ta sẽ kết hợp với các cái mô hình khác và cái cách số 3 đó là chúng ta sẽ học chuyển tiếp Transfer",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0I8uw0ELYj4",
      "filename": "0I8uw0ELYj4",
      "title": "[CS431 - Chương 4] Part 2: Sử dụng mạng huấn luyện sẵn",
      "chunk_id": 14,
      "start_timestamp": "0:09:39",
      "end_timestamp": "0:10:30"
    }
  },
  {
    "page_content": "số 3 đó là chúng ta sẽ học chuyển tiếp Transfer Learning chúng ta sẽ thực hiện gọi là Transfer Learning tức là chúng ta sẽ thiết kế phần sau của kiến trúc mạng CNN của mình sao cho nó phù hợp với lại dữ liệu mới sau đó chúng ta sẽ tinh chỉnh và có 2 cách tinh chỉnh đó là cách 3.1, đó là chúng ta sẽ tinh chỉnh phần cuối đó là phần phân lớp và cách 3.2 đó là chúng ta sẽ đi tinh chỉnh toàn bộ toàn bộ cả mạng thì đây là 3 cái cách thức để chúng ta có thể sử dụng một cái mạng huấn luyện sẵn thì hy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0I8uw0ELYj4",
      "filename": "0I8uw0ELYj4",
      "title": "[CS431 - Chương 4] Part 2: Sử dụng mạng huấn luyện sẵn",
      "chunk_id": 15,
      "start_timestamp": "0:10:22",
      "end_timestamp": "0:10:33"
    }
  },
  {
    "page_content": "có thể sử dụng một cái mạng huấn luyện sẵn thì hy vọng là qua cái bài học này giúp chúng ta có cái góc nhìn toàn diện hơn về những thành tựu của mạng CNN và nắm bắt được một trong những cách thức mà các nhà, các engineer đang sử dụng để ứng dụng trong các công việc của mình là chính là sử dụng phương pháp sử dụng mô hình huấn luyện sẵn.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=0I8uw0ELYj4",
      "filename": "0I8uw0ELYj4",
      "title": "[CS431 - Chương 4] Part 2: Sử dụng mạng huấn luyện sẵn",
      "chunk_id": 16,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chào các bạn, hôm nay chúng ta sẽ cùng đến với bài Transformer và một số ứng dụng của Transformer trong xử lý ngôn ngữ tự nhiên. Đây có thể nói là một trong những bài rất quan trọng, nó sẽ là nền tảng cho chúng ta để có thể học tiếp những cái thành tựu của Transformer trong các lĩnh vực khác, không phải chỉ trong lĩnh vực về xử lý ngôn ngữ tự nhiên, mà cũng có thể dùng trong lĩnh vực về hình ảnh, về xử lý âm thanh. Thì nội dung của bài hôm nay chúng ta sẽ cùng đầu tiên, Đó là chúng ta sẽ ôn tập",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=1tCmeHf1Xk0",
      "filename": "1tCmeHf1Xk0",
      "title": "[CS431 - Chương 10] Part 1: Ôn tập kiến trúc mạng RNN và cơ chế Attention",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:55"
    }
  },
  {
    "page_content": "ta sẽ cùng đầu tiên, Đó là chúng ta sẽ ôn tập về một số cái khái niệm, ví dụ như là attention là gì Rồi chúng ta sẽ tính attention score, attention distribution và attention output là gì Thì trong cái hình ở đây chúng ta thấy là các trạng thái ẩn là S1, S2 cho đến SN của mình thì nó sẽ được gọi là value Còn các cái vector truy vấn thì chúng ta sẽ gọi là query như các ở đây thì được gọi là query và chúng ta sẽ đi lần lượt tính cái giá trị trọng số của cái query với lại cái vector value này để từ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=1tCmeHf1Xk0",
      "filename": "1tCmeHf1Xk0",
      "title": "[CS431 - Chương 10] Part 1: Ôn tập kiến trúc mạng RNN và cơ chế Attention",
      "chunk_id": 1,
      "start_timestamp": "0:00:48",
      "end_timestamp": "0:01:33"
    }
  },
  {
    "page_content": "của cái query với lại cái vector value này để từ đó là chúng ta biết là tại cái vị trí hiện tại chúng ta sẽ quan tâm đến cái từ nào trong cái chuỗi input của mình và sau đó chúng ta sẽ cùng tổng hợp cái attention output để lấy cái trạng thái ẩn của cái từ nào mà chúng ta đang quan tâm nhiều Còn những từ nào mà chúng ta không quan tâm nhiều thì trọng số của nó sẽ thấp hơn. Từ attention output này, chúng ta sẽ được sử dụng để đi tính toán các giá trị output ở mức decoder. Đây chính là ý tưởng của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=1tCmeHf1Xk0",
      "filename": "1tCmeHf1Xk0",
      "title": "[CS431 - Chương 10] Part 1: Ôn tập kiến trúc mạng RNN và cơ chế Attention",
      "chunk_id": 2,
      "start_timestamp": "0:01:30",
      "end_timestamp": "0:02:16"
    }
  },
  {
    "page_content": "output ở mức decoder. Đây chính là ý tưởng của attention. Ngoài ra, chúng ta cũng ôn tập về một số cái mô hình, cách thức mà chúng ta sử dụng mô hình đệ quy cho bài toán NLP. thì đầu tiên đó là cái chuỗi input của mình, nó sẽ được encode 2 chiều, bidirectional có nghĩa là 2 chiều để đảm bảo cho cái đầu vào của mình mình có thể đọc theo trình tự từ trái sang phải và từ phải sang trái chúng ta có đầy đủ được cái thông tin về mặt ngữ cảnh của input một cách đầy đủ Trong mô hình đệ quy, khi chưa có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=1tCmeHf1Xk0",
      "filename": "1tCmeHf1Xk0",
      "title": "[CS431 - Chương 10] Part 1: Ôn tập kiến trúc mạng RNN và cơ chế Attention",
      "chunk_id": 3,
      "start_timestamp": "0:02:06",
      "end_timestamp": "0:03:01"
    }
  },
  {
    "page_content": "một cách đầy đủ Trong mô hình đệ quy, khi chưa có Transformer ra đời, chúng ta sẽ sử dụng kiến trúc RNN với cell LSTM Long-Short-Term Memory. Đây chính là ý tưởng chung, những kiến trúc chung cho các mô hình đệ quy sử dụng cho các bài toán NLP. Và ở trong hình đây thì chúng ta thấy đó là cái bước encoder thì nó sẽ được thực hiện tính toán, đó là 2 chiều, bi directional. Và ở đây chúng ta ký hiệu bằng các cái trạng thái ẩn là bằng các cái vector màu xanh tương ứng là encoder. Rồi, để tính toán",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=1tCmeHf1Xk0",
      "filename": "1tCmeHf1Xk0",
      "title": "[CS431 - Chương 10] Part 1: Ôn tập kiến trúc mạng RNN và cơ chế Attention",
      "chunk_id": 4,
      "start_timestamp": "0:02:51",
      "end_timestamp": "0:03:41"
    }
  },
  {
    "page_content": "màu xanh tương ứng là encoder. Rồi, để tính toán cho cái output thì chúng ta cũng sẽ biểu diễn output dưới dạng chuỗi. dạng chuỗi input của mình nó đã là chuỗi rồi thì output của mình nó cũng sẽ dạng chuỗi và chúng ta sử dụng LSTM chúng ta cũng sử dụng LSTM để sinh ra kết quả, tuy nhiên ở đây chúng ta có một cái nhận xét đó là cái LSTM này thì nó sẽ đi theo một chiều chứ nó không có đi hai chiều tại vì về nguyên tắc là ở cái quá trình output chúng ta sẽ không thấy trước Ví dụ như trong trường",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=1tCmeHf1Xk0",
      "filename": "1tCmeHf1Xk0",
      "title": "[CS431 - Chương 10] Part 1: Ôn tập kiến trúc mạng RNN và cơ chế Attention",
      "chunk_id": 5,
      "start_timestamp": "0:03:33",
      "end_timestamp": "0:04:11"
    }
  },
  {
    "page_content": "ta sẽ không thấy trước Ví dụ như trong trường hợp Encoder chúng ta có thể đi theo chiều ngược lại là vì chúng ta được phép thấy cái dữ kiện của mình ở phía sau truyền lên phía trước và phía trước truyền phía sau. Nhưng mà khi chúng ta tính giá trị output, chúng ta không được phép thấy những giá trị phía sau. Chúng ta chỉ phải lần lượt suy đoán từng từ một. Chúng ta suy đoán ở đây, sau đó mới đến đây, sau đó mới đến đây. Chứ không có chuyện là chúng ta nhận được thông tin từ giá trị cuối truyền",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=1tCmeHf1Xk0",
      "filename": "1tCmeHf1Xk0",
      "title": "[CS431 - Chương 10] Part 1: Ôn tập kiến trúc mạng RNN và cơ chế Attention",
      "chunk_id": 6,
      "start_timestamp": "0:04:05",
      "end_timestamp": "0:05:02"
    }
  },
  {
    "page_content": "ta nhận được thông tin từ giá trị cuối truyền lên đầu. Tại vì lúc đó chúng ta chưa biết cái đáp án. Đó là lý do tại sao phần Output chúng ta sẽ ký hiệu bằng một màu riêng, và chúng ta chỉ có 1 chiều, thay vì là 2 chiều, giống như trên đây, ở trên đây là 2 chiều. Cuối cùng, các mô hình đệ quy cho bài toán NLP có sử dụng một kỹ thuật, đó là Attention để linh hoạt truy xuất bộ nhớ của mình. để linh hoạt truy xuất bộ nhớ của mình. Thì ở đây chúng ta sẽ xét đến cái quá trình chúng ta decode tại cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=1tCmeHf1Xk0",
      "filename": "1tCmeHf1Xk0",
      "title": "[CS431 - Chương 10] Part 1: Ôn tập kiến trúc mạng RNN và cơ chế Attention",
      "chunk_id": 7,
      "start_timestamp": "0:04:51",
      "end_timestamp": "0:05:37"
    }
  },
  {
    "page_content": "sẽ xét đến cái quá trình chúng ta decode tại cái vị trí này. Đúng không? Thì tại đây nếu như không có attention module này thì các thông tin của những từ rất là xa nó sẽ tương tác được nhưng mà phải thông qua số bước di chuyển rất là dài. Trong khi đó nếu nhờ attention, đúng không nhờ module attention này thì chúng ta có thể linh hoạt truy xuất được các thông tin từ đầu, cũng như từ đầu tiên chúng ta có thể truy xuất từ đầu tiên một cách dễ dàng và với số bước rất là ngắn Ví dụ trong cái hình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=1tCmeHf1Xk0",
      "filename": "1tCmeHf1Xk0",
      "title": "[CS431 - Chương 10] Part 1: Ôn tập kiến trúc mạng RNN và cơ chế Attention",
      "chunk_id": 8,
      "start_timestamp": "0:05:29",
      "end_timestamp": "0:06:16"
    }
  },
  {
    "page_content": "và với số bước rất là ngắn Ví dụ trong cái hình này, trong quá trình này, tại cái vị trí này chúng ta có thể truy xuất từ đầu tiên chỉ thông qua một phép biến đổi, đó là tại đây 1 phép biến đổi, trong khi đó nếu như chúng ta thực hiện tại đây không có attention output, chúng ta sẽ phải đi 1 lần, 2 lần, 3 lần, 4 lần, 5 lần chúng ta phải mất 5 lần xử lý, 5 lần biến đổi thì mới đến được đến cái vị trí mà chúng ta cần phải xử lý. Trong khi đó với cái output, với cái attention output thì tại đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=1tCmeHf1Xk0",
      "filename": "1tCmeHf1Xk0",
      "title": "[CS431 - Chương 10] Part 1: Ôn tập kiến trúc mạng RNN và cơ chế Attention",
      "chunk_id": 9,
      "start_timestamp": "0:06:11",
      "end_timestamp": "0:06:37"
    }
  },
  {
    "page_content": "cái output, với cái attention output thì tại đây chúng ta kết nối trực tiếp, rồi sau đó chúng ta sẽ đưa ra giá trị tính toán tiếp theo. Đây chính là ý tưởng của attention và điểm mạnh của attention đó là cho phép mình có thể linh hoạt truy xuất đến cái bộ nhớ của mình.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=1tCmeHf1Xk0",
      "filename": "1tCmeHf1Xk0",
      "title": "[CS431 - Chương 10] Part 1: Ôn tập kiến trúc mạng RNN và cơ chế Attention",
      "chunk_id": 10,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề tập trung vào dự án mô hình seq2seq, sequence-to-sequence và cơ chế attention. Trong bài hôm nay thì chúng ta sẽ cùng tìm hiểu về bài toán dịch máy, mô hình seq2seq, sequence-to-sequence và cơ chế attention. Bài này, chúng ta sẽ dựa trên ý tưởng của mạng RNN trước đây. Và mạng RNN có bao gồm 2 bước biến đổi. Bước số 1, chúng ta sẽ đi tính trạng thái ẩn. Trạng thái ẩn sẽ tổng hợp thông tin của quá khứ và thông tin của hiện tại. Sau đó, từ trạng thái ẩn này, chúng ta sẽ đi tính toán ra giá",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:50"
    }
  },
  {
    "page_content": "thái ẩn này, chúng ta sẽ đi tính toán ra giá trị output. Và một số biến thể của mạng RNN mà chúng ta đã được thảo luận trước đây Đó là biến thể về Long Short-Term Memory LSTM Biến thể về Bidirectional RNN, RNN 2 chiều Và Stacked RNN Thì trong nội dung đầu tiên mà chúng ta sẽ học bài hôm nay đó chính là tìm hiểu về bài toán dịch máy Machine Translation Tiếp theo, đó là chúng ta sẽ tìm hiểu về cơ chế Attention trong việc giải quyết bài toán dịch máy này và một số biến thể của Attention và tại sao",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 1,
      "start_timestamp": "0:00:45",
      "end_timestamp": "0:01:32"
    }
  },
  {
    "page_content": "này và một số biến thể của Attention và tại sao chúng ta lại tìm hiểu về dịch máy mà không phải là các bài toán khác Ví dụ như bài toán tóm tắt văn bản, bài toán phân loại văn bản hoặc là bài toán part-of-speech tagging (gán nhãn từ loại) Bài toán dịch máy có 1 vai trò rất quan trọng Đầu tiên là nó có độ khó Cái độ khó của nó nó thể hiện ở chỗ là dịch máy, chúng ta phải làm việc trên hai cái domain, hai cái không gian khác nhau, đó chính là hai cái ngôn ngữ mà chúng ta cần phải dịch Thế thứ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 2,
      "start_timestamp": "0:01:26",
      "end_timestamp": "0:02:20"
    }
  },
  {
    "page_content": "cái ngôn ngữ mà chúng ta cần phải dịch Thế thứ hai, đó là dịch máy là một cái bài toán mà nó tổng quát Cái kiểu tổng quát của dịch máy đó chính là nó biến đổi từ một cái chuỗi về một cái chuỗi khác Và từ cái dạng chuỗi sang chuỗi này thì nó cũng có thể tương tự để giải quyết cho các bài toán như là bài toán về tóm tắt văn bản, bài toán về chuyển đổi lại, tức là paraphrase một cái văn bản Thậm chí cả bài toán phân loại văn bản thì cái chuỗi đầu ra của mình mới có thể hiểu là một cái giá trị Cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 3,
      "start_timestamp": "0:02:10",
      "end_timestamp": "0:02:57"
    }
  },
  {
    "page_content": "của mình mới có thể hiểu là một cái giá trị Cái chuỗi này có độ dài là một, như vậy thì tính tổng quát của nó nó là cao Và đó là lý do tại sao chúng ta nghiên cứu về bài toán dịch máy và dùng nó như là một kiến thức tổng quát để có thể sau này áp dụng những kiến thức về attention vào cho các bài toán khác Thì định nghĩa bài toán dịch máy là một bài toán cho phép chuyển đổi từ một câu từ ngôn ngữ nguồn sang một ngôn ngữ khác Ví dụ như ở đây chúng ta có đầu vào đầu vào sẽ là một câu thuộc cái văn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 4,
      "start_timestamp": "0:02:48",
      "end_timestamp": "0:03:37"
    }
  },
  {
    "page_content": "ta có đầu vào đầu vào sẽ là một câu thuộc cái văn bản là một ngôn ngữ đó là tiếng Anh và đầu ra của mình, đây chính là cái giá trị output thì nó sẽ ra là một cái văn bản tiếng Pháp thì cái hệ thống dịch máy là làm sao có thể thực hiện được việc chuyển đổi một câu từ ngôn ngữ tiếng Anh sang tiếng Pháp đây là một cái ví dụ ngôn ngữ nó còn hoàn toàn có thể chuyển đổi qua lại giữa tiếng Anh, tiếng Việt, tiếng Pháp, tiếng Tây Ban Nha, v.v. và thậm chí là các hệ thống sau này có khả năng dịch đa ngôn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 5,
      "start_timestamp": "0:03:30",
      "end_timestamp": "0:04:11"
    }
  },
  {
    "page_content": "là các hệ thống sau này có khả năng dịch đa ngôn ngữ tức là chúng ta có thể từ một ngôn ngữ bất kỳ có thể chuyển sang một ngôn ngữ bất kỳ khác thì đó là tầm nhìn về thiết kế các mô hình để cho phép mô hình máy học để có thể dịch được rất nhiều ngôn ngữ qua lại với nhau Tiếp theo chúng ta sẽ nói về hướng tiếp cận Neural Machine Translation Nó là hướng tiếp cận sử dụng RNN là Recurrent Neural Network và Machine Translation chính là tên của bài toán của mình Đây là phương pháp sử dụng một mạng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 6,
      "start_timestamp": "0:04:07",
      "end_timestamp": "0:05:03"
    }
  },
  {
    "page_content": "toán của mình Đây là phương pháp sử dụng một mạng neural network chính là RNN của mình từ đầu đến cuối tức là end to end Chúng ta sẽ đưa vào mạng Neural Network này, mạng RNN này và nó sẽ tạo ra các giá trị output của mình mà không thực hiện những thao tác, thực hiện trung gian qua những loại mô hình khác mà tất cả mọi thứ đều chỉ được thực hiện bởi duy nhất một mô hình đó là Neural Network, Recurrent Neural Network Và kiến trúc sử dụng ở đây chính là kiến trúc sequence to sequence hay gọi tắt",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 7,
      "start_timestamp": "0:04:55",
      "end_timestamp": "0:05:50"
    }
  },
  {
    "page_content": "là kiến trúc sequence to sequence hay gọi tắt là seq2seq thì đây sẽ bao gồm hai cái thành phần, hai cái vai trò là encoder và decoder trong đó encoder thực hiện việc đọc và hiểu cái thông tin input đầu vào decoder là tạo sinh ra cái kết quả trả về Tạo sinh ra kết quả và chúng ta sẽ đến với mô hình seq2seq theo animation như sau Bên trái sẽ là ký hiệu encoder RNN, tức là nguyên những cái phần màu xanh ở đây chính là hidden state, các trạng thái ẩn mà mình sẽ encode câu đầu vào của mình Ví dụ như",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 8,
      "start_timestamp": "0:05:42",
      "end_timestamp": "0:06:34"
    }
  },
  {
    "page_content": "mà mình sẽ encode câu đầu vào của mình Ví dụ như đây chúng ta có đưa một câu của một văn bản nguồn là I'm not sure Thì tại cái mũi tên này, nó là tổng hợp thông tin của toàn bộ nội dung của câu đầu vào Và nó sẽ bắt đầu tiến hành quá trình decode, tức là giải mã Rồi nó sẽ phải truyền vào một cái ký tự đặc biệt Nó sẽ truyền vào một ký tự đặc biệt thì ở đây mình dùng là từ <SOS> <SOS> là để cho hàm ý là có cái ý nghĩa của nó là bắt đầu thôi Còn trong thực tế lúc thực hành chúng ta có thể sử dụng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 9,
      "start_timestamp": "0:06:30",
      "end_timestamp": "0:07:15"
    }
  },
  {
    "page_content": "thực tế lúc thực hành chúng ta có thể sử dụng những cái ký tự đặc biệt Ví dụ như là ký tự <SOS> Ký tự <SOS> hoặc là ký tự đô la dấu thăng đô la rồi dấu thăng Nhưng mà lưu ý, đó là phải có sự đồng nhất Ví dụ như trong toàn bộ những văn bản trong cặp dữ liệu X, Y của mình và trong quá trình huấn luyện và thậm chí là trong quá trình inference, tức là mình thực hiện khi thực hiện mình test mô hình và triển khai thực tế thì chúng ta đều phải thống nhất sử dụng chung một hệ thống ký hiệu này chứ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 10,
      "start_timestamp": "0:07:12",
      "end_timestamp": "0:08:03"
    }
  },
  {
    "page_content": "nhất sử dụng chung một hệ thống ký hiệu này chứ không phải là lúc chúng ta sử dụng là alpha, <SOS> lúc chúng ta sử dụng là dollar thì không được rồi khi chúng ta truyền vô một ký tự đặc biệt để đánh dấu cái này là đánh dấu quá trình decode Đánh dấu quá trình decode và nó sẽ tạo ra dự đoán y hat, đó chính là từ đầu ra Và ở đây chúng ta sẽ dùng hàm argmax Tại vì đầu ra của mình sẽ ra là một cái vector Rồi sau đó mình sẽ tìm coi trọng số nào là trọng số lớn nhất tương ứng với lại cái từ trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 11,
      "start_timestamp": "0:07:58",
      "end_timestamp": "0:08:45"
    }
  },
  {
    "page_content": "trọng số lớn nhất tương ứng với lại cái từ trong tiếng Pháp của mình trong cái embedding tiếng Pháp của mình rồi sau khi chúng ta có được cái từ r thì chúng ta sẽ truyền nối tiếp chúng ta sẽ đưa cái từ r này như là một cái đầu vào cho cái bước tiếp theo cái bước decode tiếp theo truyền cái từ r này và đương nhiên là sử dụng cái thông tin của quá khứ để mà thực hiện cái hàm tính giá trị output nó sẽ tạo ra từ NE rồi từ NE này nó sẽ là đầu vào cho mạng của mình và nó sẽ tạo ra từ suy từ suy này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 12,
      "start_timestamp": "0:08:43",
      "end_timestamp": "0:09:35"
    }
  },
  {
    "page_content": "mạng của mình và nó sẽ tạo ra từ suy từ suy này sẽ truyền vào để tạo ra từ PA cứ như vậy và đến từ kết thúc quá trình decode thì hệ thống này nó phải trả ra một từ đặc biệt đó là <EOS> và cũng tương tự như <SOS> thì <EOS> này là để đánh dấu Kết thúc quá trình decode Và chúng ta sẽ phải sử dụng một ký tự đặc biệt Ví dụ như là nếu ở đây đã sử dụng <SOS> rồi thì ở đây chúng ta có thể sử dụng là Ký tự là <EOS> Và phải có sự đồng nhất từ đầu đến cuối Ở bên tay trái, encoder thực hiện công việc đó là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 13,
      "start_timestamp": "0:09:29",
      "end_timestamp": "0:10:22"
    }
  },
  {
    "page_content": "Ở bên tay trái, encoder thực hiện công việc đó là tổng hợp thông tin của toàn bộ câu văn nguồn của mình Còn ở bên tay phải là decoder đóng vai trò như là một mô hình ngôn ngữ Là một language model để tạo ra cái văn bản đích, tạo ra câu văn đích Và dựa trên decoder này nó thực hiện được là dựa trên thông tin đã được tổng hợp từ câu văn nguồn Rồi, như vậy thì tính linh hoạt của Seq2Seq nó sẽ thể hiện ở những cái ví dụ sau. Đầu tiên đó là bất cứ cái văn bản ở dạng chuỗi, input nào mà ở dạng chuỗi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 14,
      "start_timestamp": "0:10:16",
      "end_timestamp": "0:10:53"
    }
  },
  {
    "page_content": "văn bản ở dạng chuỗi, input nào mà ở dạng chuỗi và output nào ở dạng chuỗi thì chúng ta đều có thể sử dụng được Seq2Seq này. Thì cái ý đầu tiên này nó minh họa cho cái việc là tính linh hoạt của Seq2Seq Và do đó thì một cái mạng Neural Network mà nhận cái input và tạo ra một cái vector biểu diễn Một cái mạng Neural Network thì nhận cái input và tạo ra một cái vector biểu diễn Một cái mạng Neural Network thì nhận cái input và tạo ra một cái vector biểu diễn Rồi mạng Neural Network khác nó sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 15,
      "start_timestamp": "0:10:48",
      "end_timestamp": "0:11:37"
    }
  },
  {
    "page_content": "biểu diễn Rồi mạng Neural Network khác nó sẽ sinh ra cái output và từ cái vector biểu diễn trên Thì thực ra đây chính là cái quá trình encode Quá trình encode và đây là quá trình decode Mạng Neural sẽ nhận toàn bộ nội dung đầu vào, nội dung input Sau khi đã đọc hết toàn bộ thông tin đó thì nó sẽ tạo ra 1 vector Và vector này sẽ tích hợp toàn bộ thông tin của input Và với thông tin của input này, sẽ sinh ra output từ vector biểu diễn trên, tức là vector này. Nó sẽ sinh ra output và đây chính là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 16,
      "start_timestamp": "0:11:35",
      "end_timestamp": "0:12:11"
    }
  },
  {
    "page_content": "vector này. Nó sẽ sinh ra output và đây chính là quá trình decode. Seq2Seq không chỉ hiệu quả cho bài toán dịch máy, mà nó còn hiệu quả cho cả những bài toán khác. Bài toán tóm tắt văn bản, summarization Bài toán tóm tắt văn bản là gì? Đầu vào của mình cũng sẽ là một chuỗi, một đoạn văn rất là dài Và đầu ra của mình sẽ là một đoạn văn ngắn Mô tả lại toàn bộ nội dung của đoạn văn dài Tóm tắt lại nội dung chính của đoạn văn dài này Cho bài toán hội thoại hay dialogue Input của mình sẽ là lời",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 17,
      "start_timestamp": "0:12:08",
      "end_timestamp": "0:12:47"
    }
  },
  {
    "page_content": "hội thoại hay dialogue Input của mình sẽ là lời thoại trước Và output của mình sẽ là cái lời thoại sau, giống như là khi chúng ta đặt cái... cái... cái... mình sẽ chat với lại một cái chatbot thì mình sẽ cung cấp cho nó một cái lời thoại trước và nó sẽ trả lời mình và nó sẽ trò chuyện với mình thì đây chính là cái lời thoại sau và thì cái này phù hợp cho bài toán là chatbot Bài toán phân tích cú pháp thì đầu vào của mình sẽ là một cái chuỗi đoạn văn, một cái câu văn và đầu ra của mình cũng sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 18,
      "start_timestamp": "0:12:43",
      "end_timestamp": "0:13:03"
    }
  },
  {
    "page_content": "văn, một cái câu văn và đầu ra của mình cũng sẽ là một cái chuỗi để mô tả cái cây cú pháp tương ứng với các đoạn văn trên rồi bài toán Code Synthesis, Code Generation thì đầu vào của mình sẽ là mô tả cái chức năng của một chương trình hoặc là chức năng của một thuật toán mà mình đang muốn cài đặt và đầu ra là nó sẽ tạo ra một cái mã nguồn theo một cái ngôn ngữ nào đó mà chúng ta đã được chọn lựa ví dụ như mã nguồn cho ngôn ngữ Python Tại sao, toàn bộ nội dung của Slide này thể hiện là tính linh",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 19,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "bộ nội dung của Slide này thể hiện là tính linh hoạt của sequence to sequence có thể xử lý được bất cứ bài toán nào đầu vào của mình là ở dạng chuỗi và đầu ra của mình cũng ở dạng chuỗi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4EdX3Ga9YoM",
      "filename": "4EdX3Ga9YoM",
      "title": "[CS431 - Chương 9] Part 1_1: Giới thiệu bài toán Dịch máy",
      "chunk_id": 20,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chúng ta sẽ cùng tìm hiểu về phân đoạn ngữ nghĩa cho đối tượng. Chúng ta sẽ có định nghĩa bài toán phân đoạn ngữ nghĩa đối tượng là chúng ta sẽ xác định vị trí của đối tượng, các đối tượng cần quan tâm, và chính xác đến cấp độ điểm ảnh. tức là trước đây nếu như chúng ta phát hiện đối tượng object detection, thì đây chúng ta chỉ cần chỉ ra cái bounding box bao xung quanh cái đối tượng. thì ở đây chúng ta sẽ phải chỉ đến cái cấp độ đó là pixel. tức là pixel này thì nó sẽ thuộc về đối tượng là con",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:44"
    }
  },
  {
    "page_content": "là pixel này thì nó sẽ thuộc về đối tượng là con bò. Còn pixel này thì sẽ thuộc về đối tượng là cái bãi cỏ. Tương tự như vậy. Ở đây, trước đây thì chúng ta sẽ chỉ ra cái Bounding Box. Còn bây giờ thì chúng ta sẽ chỉ ra chi tiết đến từng cái pixel. Vì vậy thì cái bài toán phân đoạn ngữ nghĩa đối tượng, nó sẽ giúp chúng ta giải quyết triệt để hơn cái bài toán Object Detection. Tại vì trong Object Detection thì nó sẽ có cái vùng của cái Bounding Box mà không thực sự là thuộc đối tượng. Ví dụ như",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 1,
      "start_timestamp": "0:00:40",
      "end_timestamp": "0:01:22"
    }
  },
  {
    "page_content": "mà không thực sự là thuộc đối tượng. Ví dụ như trong ảnh ở đây, chúng ta thấy mặc dù nó nằm trong bounding box nhưng nó không thực sự là nằm trong đối tượng là con bò. Trong số những cái cách tiếp cận cho phân đoạn ngữ nghĩa đối tượng thì kiến trúc U-Net, mặc dù nó ra đời từ khoảng năm 2015 2016 rất là lâu rồi. Nhưng có thể nói cho đến nay, đây là một trong những kiến trúc rất tổng quát và được tái sử dụng cho rất nhiều mô hình và ý tưởng của nó đó chính là sử dụng các skip connection. Rồi, thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 2,
      "start_timestamp": "0:01:16",
      "end_timestamp": "0:02:11"
    }
  },
  {
    "page_content": "đó chính là sử dụng các skip connection. Rồi, thì tại sao lại như vậy? Đầu tiên nếu như không có skip connection này thì ảnh của mình sẽ được down sample, tức là được giảm độ phân giải xuống. Ví dụ ban đầu là 572 x 572, sau đó sẽ giảm xuống còn 284 x 284, rồi kéo xuống một hồi sẽ còn là 30 x 30, 28 x 28. Và đây là kết thúc quá trình encode. Để mà có thể tái tạo lại feature map hoặc là kết quả có độ phân giải giống với độ phân giải ban đầu, thì buộc feature map ở đây sẽ phải tăng kích thước của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 3,
      "start_timestamp": "0:02:04",
      "end_timestamp": "0:02:54"
    }
  },
  {
    "page_content": "feature map ở đây sẽ phải tăng kích thước của 2 chiều không gian bề ngang và bề cao lên. 2 chiều không gian bề ngang và bề cao lên, ví dụ ở đây chúng ta thấy là từ 28 x 28 tăng lên là 56 x 56 rồi, sau đó là tăng lên 112, tăng lên 200 x 200, rồi tăng lên 392 nhân cho 392. thì cái thao tác mà upsample hay là upsampling là nó sẽ làm giảm cái độ phân giải của tấm hình của mình. nó sẽ làm giảm cái độ phân giải. Tức là đường nét của mình không còn sắc nét nữa. Thì cái này là một cái việc rất là bình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 4,
      "start_timestamp": "0:02:52",
      "end_timestamp": "0:03:34"
    }
  },
  {
    "page_content": "nét nữa. Thì cái này là một cái việc rất là bình thường. Khi chúng ta từ một cái không gian mà nhiều thông tin nén xuống không gian ít thông tin, xong từ không gian ít thông tin mở rộng trở lại, thì nó sẽ bị thiếu sót thông tin. Do đó, nó sẽ có cái Skip Connection này. Skip Connection này sẽ tận dụng được cái thông tin gốc, tận dụng được độ chi tiết và nó sẽ giữ được độ phân giải. Từ đó là nó sẽ kết nối với lại feature map ở các lớp đã được upsampling từ giai đoạn encode. Sau đó nó sẽ concat,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 5,
      "start_timestamp": "0:03:31",
      "end_timestamp": "0:04:28"
    }
  },
  {
    "page_content": "từ giai đoạn encode. Sau đó nó sẽ concat, kết nối với lại feature map tại lớp trước đó Trước khi thực hiện quá trình encode, như vậy ở đây sẽ giúp chúng ta giữ được độ phân giải. Về lý thuyết của ResNet, với Residual Block thì nó cũng sẽ có các skip connection. Và cái skip connection này ngoài việc giữ được độ phân giải của feature map output, thì mình sẽ còn có một tính năng nữa đó là giúp cho quá trình huấn luyện nhanh hơn. Nó đỡ tránh được hiện tượng Vanishing Gradient. Không bị hiện tượng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 6,
      "start_timestamp": "0:04:21",
      "end_timestamp": "0:05:17"
    }
  },
  {
    "page_content": "tượng Vanishing Gradient. Không bị hiện tượng Vanishing Gradient. Rồi, để có thể thực hiện được các thao tác upsampling lên, thì chúng ta sẽ có các phép là Unpooling và Deconvolution. Nếu như pooling, chúng ta lưu giá trị nhỏ nhất hoặc giá trị lớn nhất hoặc giá trị trung bình tại đây, thì khi chúng ta tái tạo, chúng ta sẽ không biết phải thế giá trị này vào vị trí nào. Do đó, trong quá trình pooling, chúng ta sẽ lưu các switch variables để lưu vị trí giá trị lớn nhất hoặc giá trị nhỏ nhất hoặc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 7,
      "start_timestamp": "0:05:10",
      "end_timestamp": "0:05:54"
    }
  },
  {
    "page_content": "trí giá trị lớn nhất hoặc giá trị nhỏ nhất hoặc giá trị trung bình đó. Ví dụ, ở đây chúng ta biết giá trị này là giá trị lớn nhất, Thứ nhất, chúng ta sẽ đưa giá trị đó vào pool map, nhưng đồng thời đánh dấu là cái vị trí này là chứa giá trị mà mình vừa mới được thực hiện pooling. Khi quá trình unpooling, chúng ta sẽ lấy giá trị này chép ngược trở lại về vị trí này. Và lưu ý là 3 giá trị ở đây sẽ để là 3 con số 0, tại vì nó không có thông tin để trả giá trị về. Rồi, đối với phép deconvolution,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 8,
      "start_timestamp": "0:05:50",
      "end_timestamp": "0:06:33"
    }
  },
  {
    "page_content": "trả giá trị về. Rồi, đối với phép deconvolution, thì nó tổng hợp thông tin từ một vùng giả sử như đây là kích thước 3x3. Nó sẽ tổng hợp thông tin về một cái ô. Thì deconvolution là cái công việc ngược lại. Tức là từ một cái ô có kích thước là 1x1, nó sẽ lan truyền cái thông tin này đến cái vùng có kích thước là 3x3 ở feature map output. và nó cũng sẽ có các bộ filter để thực hiện phép convolution, tương tự như phép deconvolution. Một kiến trúc khác cũng rất là nổi tiếng và đó chính là DeepLab",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 9,
      "start_timestamp": "0:06:25",
      "end_timestamp": "0:07:24"
    }
  },
  {
    "page_content": "khác cũng rất là nổi tiếng và đó chính là DeepLab V3. Ý tưởng của DeepLab V3 sẽ dựa trên phép tính toán, đó là atrous convolution hoặc là tên khác, đó là dilated convolution. Nếu như phép biến đổi Convolution là, nếu như đây là Input, đây là Output, Rồi, thì nó sẽ tổng hợp thông tin của vùng có kích thước là 3 x 3 để tổng hợp thông tin và điền vào 1 cái điểm ở trên feature map ở đây, thì cái việc này sẽ dẫn đến vấn đề đó là nó sẽ không tổng hợp được thông tin ở những vùng có kích thước lớn hơn.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 10,
      "start_timestamp": "0:07:18",
      "end_timestamp": "0:08:01"
    }
  },
  {
    "page_content": "thông tin ở những vùng có kích thước lớn hơn. muốn tổng hợp thông tin ở những vùng lớn hơn thì chúng ta phải thực hiện cái phép convolution liên tiếp nhiều lần. còn cái phép atrous, và đương nhiên cái việc mà chúng ta thực hiện nhiều lần như vậy thì nó sẽ tăng chi phí tính toán đồng thời là không giải quyết được vấn đề về scale, scale, vấn đề về độ bất biến trong phân đoạn ngữ nghĩa với những đối tượng nhỏ hoặc là những đối tượng rất lớn. Thì ở đây, phép atrous convolution, thay vì chúng ta chỉ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 11,
      "start_timestamp": "0:07:50",
      "end_timestamp": "0:08:42"
    }
  },
  {
    "page_content": "phép atrous convolution, thay vì chúng ta chỉ lấy các vùng ba nhân ba liên tiếp nhau thì chúng ta có thể skip, chúng ta sẽ bỏ qua những cái ô ở giữa. thì ở trong cái ví dụ này thì cái rate rate nó thể hiện là cái skip của mình là bằng 2. trong cái ví dụ này thì cái rate của mình rate là bằng 1, tức là nó chỉ nhảy cóc 1 đơn vị, còn cái này là nhảy cóc 2 đơn vị để lấy cái giá trị để tổng hợp thông tin lên đây. và cái phép atrous convolution Nó là sự kết hợp, concat của thông tin tại rất nhiều",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 12,
      "start_timestamp": "0:08:36",
      "end_timestamp": "0:09:30"
    }
  },
  {
    "page_content": "là sự kết hợp, concat của thông tin tại rất nhiều vùng với các cái kích thước khác nhau. Ví dụ như là từ một cái vùng 1 x 1, từ cái vùng là 3 x 3, Thực ra cái 1 x 1 convolution này nếu như mình nhìn một cái góc độ nào đó Nó chính là rate bằng 0. Tức là các giá trị của mình không có nhảy ra để tổng hợp thông tin mà nó cứ đứng yên vậy đó. Rate là không. Còn convolution, 3x3 convolution với rate là bằng 6, tức là khoảng cách giữa các điểm mà mình lấy mẫu để tổng hợp thông tin, thì khoảng cách này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 13,
      "start_timestamp": "0:09:14",
      "end_timestamp": "0:10:11"
    }
  },
  {
    "page_content": "mẫu để tổng hợp thông tin, thì khoảng cách này là 6. Rate là 12, Rate là 18, thì khoảng cách giữa 2 điểm ảnh trên feature map để tổng hợp thông tin là 18. thì nhắc lại lần nữa là cái 1x1 convolution này hiểu một cách là extreme case thì nó chính là 3x3 nhưng mà rate bằng không là cái điểm mà mình lấy mẫu không có nhảy ra ngoài mà cứ đứng yên một chỗ, đồng thời nó sẽ kết hợp với lại cái phép biến đổi là image pooling như bình thường, nó sẽ downsampling như bình thường. Như vậy thì cả 5 feature",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 14,
      "start_timestamp": "0:10:03",
      "end_timestamp": "0:11:30"
    }
  },
  {
    "page_content": "như bình thường. Như vậy thì cả 5 feature maps này sẽ concat lại với nhau, tức là nó sẽ trồng lại lên nhau, và đồng thời nó sẽ thực hiện phép 1x1 convolution để tổng hợp thông tin về 1 feature map. Tức là ban đầu các phép tính 1x1 convolution sẽ tạo ra 1 feature map như thế này, Phép 3x3 convolution với rate là bằng 6, nó sẽ tạo ra một cái feature map mới, rồi với rate là 12, với rate là 18. Thở Thở Rồi, thì sau khi chúng ta thực hiện phép 1x1 convolution thì nó sẽ tổng hợp lại thành duy nhất",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 15,
      "start_timestamp": "0:11:22",
      "end_timestamp": "0:12:17"
    }
  },
  {
    "page_content": "convolution thì nó sẽ tổng hợp lại thành duy nhất ờ, nó sẽ tổng hợp lại thành duy nhất một cái feature map mà thôi. Rồi, sau đó nó sẽ thực hiện phép upsampling, Bilinear upsampling, chứ là không có tham số để tái tạo ra một cái ảnh kết quả mà có cái độ phân giải cao. Đây là ý tưởng DeepLabV3. Ý chính của DeepLabV3 chính là phép Atrous Convolution hay Dilated Convolution. Nằm trong Atrous Spatial Pyramid Pooling. Nó sẽ concat thông tin khi thực hiện Atrous Convolution với rất nhiều rate khác",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 16,
      "start_timestamp": "0:12:06",
      "end_timestamp": "0:12:50"
    }
  },
  {
    "page_content": "hiện Atrous Convolution với rất nhiều rate khác nhau. Sau đó chúng ta tổng hợp lại thông qua phép concat kết hợp với 1x1 Convolution. Rồi, như vậy thì trên đây, đó là chúng ta đã tóm tắt rất nhiều những cái ứng dụng kinh điển, điển hình của mạng CNN, từ các cái ứng dụng liên quan đến bài toán phân loại đối tượng trên những cái loại đối tượng mà có cái ý rất là mịn. Tức là, thay vì chúng ta nhận diện hoa so với lại các đối tượng khác như là cây cối, thì ở đây hoa, chúng ta sẽ phân ra rất nhiều",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 17,
      "start_timestamp": "0:12:43",
      "end_timestamp": "0:13:17"
    }
  },
  {
    "page_content": "cối, thì ở đây hoa, chúng ta sẽ phân ra rất nhiều loài hoa. Tương tự như vậy, đối với xe hơi, chúng ta cũng sẽ có rất nhiều loại xe hơi, các dòng xe hơi, các niên đại của nó. Rồi, đối với bài toán nhận diện gương mặt, chúng ta sẽ phải phân biệt được định danh của người này với người kia. thì đó là cái ứng dụng trong bài toán Classification nhưng mà ở cấp độ là fine-grained Classification. Và cái ứng dụng tiếp theo đó là cho cái bài toán truy vấn, tức là tấm ảnh của mình nó sẽ được convert sang",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 18,
      "start_timestamp": "0:13:11",
      "end_timestamp": "0:13:50"
    }
  },
  {
    "page_content": "tức là tấm ảnh của mình nó sẽ được convert sang cái dạng Embedding Vector. Và cái Embedding Vector này sẽ được sử dụng để đi so sánh với lại các Embedding Vector của những cái tấm ảnh khác trong cơ sở dữ liệu. Và cái việc so sánh này thì cũng tương tự như là các cái thao tác truy vấn bình thường. Đó là chúng ta có thể sử dụng các độ đo tích vô hướng, cosine hoặc là sử dụng độ đo khoảng cách. Rồi sau đó lấy top các giá trị mà có độ tương đồng cao để chúng ta trả về. Và cái ứng dụng nữa đó chính",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 19,
      "start_timestamp": "0:13:45",
      "end_timestamp": "0:14:30"
    }
  },
  {
    "page_content": "để chúng ta trả về. Và cái ứng dụng nữa đó chính là có thể thực hiện các thao tác liên quan đến phát hiện đối tượng. Tức là chúng ta sẽ chỉ ra chính xác, chúng ta có thể chỉ ra được vị trí của đối tượng đến cấp độ là bounding box. Và đối với bài toán SEMANTIC SEGMENTATION, tức là phân đoạn ngữ nghĩa đối tượng, thì chúng ta có thể chỉ ra được vị trí của đối tượng đến cấp độ là pixel. Và trong các kiến trúc tiếp cận thì hướng tiếp cận U-Net với cấu trúc encoder và decoder, đó là một trong những",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 20,
      "start_timestamp": "0:14:22",
      "end_timestamp": "0:15:14"
    }
  },
  {
    "page_content": "trúc encoder và decoder, đó là một trong những kiến trúc cho đến bây giờ vẫn được sử dụng rất là nhiều. Có rất nhiều những biến thể khác nhau nhưng mà ý tưởng chung đó là có skip connection giữa lớp encoder sang lớp decoder để đảm bảo được độ phân giải giữa ảnh đầu vào với ảnh output nó có độ phân giải và đường nét sắc nét và độ chính xác cao. Bên cạnh các ứng dụng trên thì còn rất nhiều những ứng dụng khác, ví dụ như là ứng dụng tăng độ phân giải ảnh, tức là từ một ảnh có kích thước và ví dụ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 21,
      "start_timestamp": "0:15:04",
      "end_timestamp": "0:15:42"
    }
  },
  {
    "page_content": "ảnh, tức là từ một ảnh có kích thước và ví dụ như 200 x 200, Sau khi thực hiện phép Super Resolution xong, nó có thể tạo thành ảnh có kích thước lên đến 1 ngàn, nhân với 1 ngàn. Ví dụ vậy, nó sẽ tăng độ phân giải của ảnh lên. Và đương nhiên nó vẫn phải giữ được tính chất, sự sắc nét của ảnh, chứ nó không phải chỉ sử dụng thao tác upsampling theo kiểu Bilinear, tức là song tuyến, một cách gọi là phi tham số để mà repeat, tức là chỉ lặp lại các điểm ảnh nhiều lần tạo ra cái ảnh không được sắc nét",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 22,
      "start_timestamp": "0:15:34",
      "end_timestamp": "0:16:14"
    }
  },
  {
    "page_content": "ảnh nhiều lần tạo ra cái ảnh không được sắc nét cho lắm. Chuyển đổi phong cách ảnh, tức là chúng ta sẽ có thể chuyển đổi một cái domain, một cái không gian của ảnh tại cái domain hiện tại sang một cái domain khác. Ví dụ như chúng ta có được một cái tấm ảnh trong đời thực của mình, hoặc chúng ta có thể chuyển sang cái phong cách đó là ảnh hoạt hình. thì đây có lẽ là một trong những ứng dụng mà có rất nhiều sự chú ý của cộng đồng trong thời gian gần đây liên quan đến cái mạng generative AI. Tiếp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 23,
      "start_timestamp": "0:16:08",
      "end_timestamp": "0:16:41"
    }
  },
  {
    "page_content": "đây liên quan đến cái mạng generative AI. Tiếp theo đó là các bài toán liên quan đến theo dõi đối tượng, tức là một đối tượng nó sẽ xuất hiện trong suốt rất nhiều frame của video và làm sao mình biết đối tượng này cũng chính là đối tượng này, Đối tượng này, cái ID của đối tượng, thông qua cái ID của đối tượng khi di chuyển qua nhiều frame. Đối tượng này cũng chính là đối tượng này. Thì đó là bài toán Tracking. Và các cái kiến trúc mạng trong các cái bài toán này thì có cái mức độ ảnh hưởng lớn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 24,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "cái bài toán này thì có cái mức độ ảnh hưởng lớn và nó sẽ dẫn dắt cái ý tưởng chủ đạo cho các kiến trúc về sau. Mặc dù những kiến trúc, phương pháp, thuật toán, mô hình mà được giới thiệu trong bài học ngày hôm nay cũng có từ cách đây 4-5 năm trước. Tuy nhiên, những ý tưởng này vẫn như đã đề cập, đối với kiến trúc U-Net hoặc một số kiến trúc như là phát hiện đối tượng với một giai đoạn hoặc phát hiện đối tượng với hai giai đoạn. Các hướng tiếp cận gần đây vẫn kế thừa những ý tưởng chủ đạo đó để",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 25,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "gần đây vẫn kế thừa những ý tưởng chủ đạo đó để mà tạo ra những cái công trình nghiên cứu mới nhất.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=4p0L74qD7Lg",
      "filename": "4p0L74qD7Lg",
      "title": "[CS431 - Chương 5] Part 4: Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
      "chunk_id": 26,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Nội dung này thì chúng ta gồm có 3 phần đầu tiên đó là chúng ta sẽ cùng tìm hiểu về kiến trúc Transformer. Sau đó chúng ta sẽ cùng tìm hiểu về một số cái điểm yếu của Transformer và cuối cùng đó là một số ứng dụng cũng như là thành tựu. Đầu tiên thì về kiến trúc thì chúng ta sẽ xét đến cái động lực tại sao chúng ta cần phải có cái kiến trúc mạng Transformer. Động lực đầu tiên xuất phát từ việc đó là giữa hai cái từ bất kỳ trong đoạn văn input của mình chúng ta tương tác với nhau, tương tác về",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5DE5HXG8FWk",
      "filename": "5DE5HXG8FWk",
      "title": "[CS431 - Chương 10] Part 2: Động lực của kiến trúc Transformer",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:42"
    }
  },
  {
    "page_content": "mình chúng ta tương tác với nhau, tương tác về mặt thông tin với nhau, chúng ta phải tốn rất nhiều thao tác Ví dụ, ở đây chúng ta sẽ có hai cái từ này Rõ ràng là trong xử lý ngôn ngữ tự nhiên, nó sẽ có tình huống đó là các cái từ phải có sự liên hệ về mặt ý nghĩa với nhau thì từ đó chúng ta mới có thể hiểu rõ được nội dung của input của mình là gì khi đó chúng ta mới có thể đi tính toán ra các giá trị output cho phù hợp thì ở đây cũng vậy nếu như không có module attention này thì giữa hai từ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5DE5HXG8FWk",
      "filename": "5DE5HXG8FWk",
      "title": "[CS431 - Chương 10] Part 2: Động lực của kiến trúc Transformer",
      "chunk_id": 1,
      "start_timestamp": "0:00:36",
      "end_timestamp": "0:01:16"
    }
  },
  {
    "page_content": "như không có module attention này thì giữa hai từ bất kỳ trong một câu của mình nó tương tác với nhau thông qua số từ trong câu của mình tức là Sequence Length Chúng ta sẽ nói kỹ hơn về động lực của Transformer là làm sao chúng ta có thể tối thiểu hóa, tức là giảm bớt độ dài của sự tương tác này, giảm bớt được độ dài của sự tương tác giữa hai từ bất kỳ trong câu. và chúng ta sẽ tối đa hóa thao tác song song tại vì Deep Learning muốn hiệu quả thì nó phải khai thác được sức mạnh của thiết bị tính",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5DE5HXG8FWk",
      "filename": "5DE5HXG8FWk",
      "title": "[CS431 - Chương 10] Part 2: Động lực của kiến trúc Transformer",
      "chunk_id": 2,
      "start_timestamp": "0:01:14",
      "end_timestamp": "0:02:07"
    }
  },
  {
    "page_content": "nó phải khai thác được sức mạnh của thiết bị tính toán song song nhưng hiện tại thì nếu như chúng ta thực hiện tính toán tuần tự từ trái sang phải hoặc từ phải sang trái thì khi đó không có khai thác được điểm mạnh của GPU của các bộ xử lý song song Đối với ý tối thiểu hóa độ dài tương tác giữa các cặp từ, chúng ta sẽ lấy một ví dụ sau In France, I had a great time and I... Ở đây chúng ta sẽ điền vô chỗ trống Language Ở đây chúng ta sẽ thấy có từ Language từ France và từ này chúng ta cần phải",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5DE5HXG8FWk",
      "filename": "5DE5HXG8FWk",
      "title": "[CS431 - Chương 10] Part 2: Động lực của kiến trúc Transformer",
      "chunk_id": 3,
      "start_timestamp": "0:02:03",
      "end_timestamp": "0:02:50"
    }
  },
  {
    "page_content": "từ Language từ France và từ này chúng ta cần phải điền vào thì khi đó chúng ta đang muốn điền cái thông tin vào cái chỗ trống này chúng ta cần phải có cái sự tương tác thông tin giữa từ France và từ Language và cả cái từ mà chúng ta cần phải điền vào chỗ trống này thì khi đó là thông tin của từ France khi mà lan truyền được đến đây, khi mà lan truyền được đến cái vị trí này vị trí này thì nó đã tốn một cái chi phí đó là Sequence Length. Sequence Length là chiều dài của chuỗi. Trong các hệ thống",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5DE5HXG8FWk",
      "filename": "5DE5HXG8FWk",
      "title": "[CS431 - Chương 10] Part 2: Động lực của kiến trúc Transformer",
      "chunk_id": 4,
      "start_timestamp": "0:02:41",
      "end_timestamp": "0:03:27"
    }
  },
  {
    "page_content": "Length là chiều dài của chuỗi. Trong các hệ thống ký hiệu của mình, Sequence Length của mình là T hay thường được gọi là, thay vì O(Sequence Length) thì chúng ta sẽ ký hiệu là O(T), tức là chúng ta sẽ tốn T bước. Trong quá trình mà thông tin của từ France nó lan truyền đến được đây thì nó đã bị mất mát thông tin rất là nhiều rồi. Với kiến trúc hiện tại là tuần tự, thì rất khó để huấn luyện do có sự phụ thuộc dài từ Language, rồi chỗ trống ở đây sẽ phụ thuộc vào từ France để điền vào cái này là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5DE5HXG8FWk",
      "filename": "5DE5HXG8FWk",
      "title": "[CS431 - Chương 10] Part 2: Động lực của kiến trúc Transformer",
      "chunk_id": 5,
      "start_timestamp": "0:03:19",
      "end_timestamp": "0:04:06"
    }
  },
  {
    "page_content": "sẽ phụ thuộc vào từ France để điền vào cái này là từ French. Muốn có được thông tin ở đây thì chúng ta phải có được thông tin từ France. Và cái việc khó huấn luyện này nó xuất phát từ cái vấn đề về Vanishing Gradient Tức là khi cái hàm biến đổi của mình mà càng dài thì các cái đạo hàm thành phần của mình càng bé Các cái đạo hàm thành phần của mình là bé thì khi chúng ta nhân lần lượt tất cả các cái đạo hàm thành phần này lại với nhau thì các cái giá trị bé nó nhân lại với nhau nó sẽ tạo ra",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5DE5HXG8FWk",
      "filename": "5DE5HXG8FWk",
      "title": "[CS431 - Chương 10] Part 2: Động lực của kiến trúc Transformer",
      "chunk_id": 6,
      "start_timestamp": "0:04:01",
      "end_timestamp": "0:04:41"
    }
  },
  {
    "page_content": "cái giá trị bé nó nhân lại với nhau nó sẽ tạo ra những cái giá trị vô cùng bé nó làm giảm mất cái gọi là bước nhảy của cái tham số của mình. Thì đó là cái lý do tại sao khi có cái sự phụ thuộc dài thì cái mô hình của mình huấn luyện không còn hiệu quả nữa. Rồi, và ý tiếp theo của cái động lực tại sao chúng ta phải có, phải đề xuất ra cái kiến trúc mạng Transformer đó chính là chúng ta phải tối đa hóa số phép xử lý song song. Trong quá trình Feed Forward hoặc Backward, chúng ta cần tốn chi phí,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5DE5HXG8FWk",
      "filename": "5DE5HXG8FWk",
      "title": "[CS431 - Chương 10] Part 2: Động lực của kiến trúc Transformer",
      "chunk_id": 7,
      "start_timestamp": "0:04:33",
      "end_timestamp": "0:05:36"
    }
  },
  {
    "page_content": "Forward hoặc Backward, chúng ta cần tốn chi phí, tốn O(T), phép toán không song song. Thì ở trong hình này, chúng ta sẽ thấy nếu thực hiện từ trái sang phải, forward hoặc sau này khi chúng ta huấn luyện là backward thì ở bên trong ô này chúng ta sẽ ký hiệu là nó sẽ phụ thuộc vào những phép tính trước đó là cần phải phụ thuộc vào bao nhiêu phép tính trước đó Ví dụ, tại đây chúng ta thấy là giá trị là 1 là vì nó bị phụ thuộc vào một phép tính trước đó là đây Còn ở đây là bằng 0 là vì chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5DE5HXG8FWk",
      "filename": "5DE5HXG8FWk",
      "title": "[CS431 - Chương 10] Part 2: Động lực của kiến trúc Transformer",
      "chunk_id": 8,
      "start_timestamp": "0:05:28",
      "end_timestamp": "0:06:04"
    }
  },
  {
    "page_content": "đó là đây Còn ở đây là bằng 0 là vì chúng ta tính trực tiếp luôn, chúng ta không có bị phụ thuộc vô phép tính nào trước đó Thì ở đây chúng ta sẽ bị phụ thuộc 2 phép tính do bị phụ thuộc ở đây là một phép tính Và cái phép tính ở đây, giá trị output ở đây nó lại bị phụ thuộc bởi một phép tính trước đó Vì vậy, khi chúng ta tính toán đến phần tử cuối cùng, đến trạng thái ẩn cuối cùng, thì chúng ta cần phải thực hiện các phép tính trước đó, tức là có sự phụ thuộc. Tuy nhiên, ở đây chúng ta sẽ thấy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5DE5HXG8FWk",
      "filename": "5DE5HXG8FWk",
      "title": "[CS431 - Chương 10] Part 2: Động lực của kiến trúc Transformer",
      "chunk_id": 9,
      "start_timestamp": "0:05:54",
      "end_timestamp": "0:06:45"
    }
  },
  {
    "page_content": "sự phụ thuộc. Tuy nhiên, ở đây chúng ta sẽ thấy là GPU là một vi xử lý song song, thì nó chỉ có thể thực hiện được các phép độc lập, tức là nó sẽ phân rã các thao tác tính toán cho từng lõi xử lý và các cái lõi xử lý nó phải độc lập nhau thì khi đó nó mới tính toán được kết quả. Trong khi đó RNN hoặc là các biến thể của RNN thì cái trạng thái ẩn, các trạng thái ẩn của mình trong quá khứ nó sẽ tính xong thì khi đó mới tính được những trạng thái hiện tại, tức là trạng thái hiện tại sẽ bị phụ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5DE5HXG8FWk",
      "filename": "5DE5HXG8FWk",
      "title": "[CS431 - Chương 10] Part 2: Động lực của kiến trúc Transformer",
      "chunk_id": 10,
      "start_timestamp": "0:06:36",
      "end_timestamp": "0:07:10"
    }
  },
  {
    "page_content": "hiện tại, tức là trạng thái hiện tại sẽ bị phụ thuộc vào trạng thái ẩn trong quá khứ. Dẫn đến là không thể huấn luyện trên những data set cực lớn, tức là những kiến trúc biến thể RNN không khai thác được GPU. Dẫn đến là sau này chúng ta không thể sử dụng được sức mạnh của GPU để tính toán trên data set cực lớn với số tham số cực lớn. Đó chính là động lực tại sao chúng ta cần phải có Transformer.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5DE5HXG8FWk",
      "filename": "5DE5HXG8FWk",
      "title": "[CS431 - Chương 10] Part 2: Động lực của kiến trúc Transformer",
      "chunk_id": 11,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Xin chào mừng các bạn đến với môn học CS431 Các Kỹ thuật học sâu và ứng dụng. Hôm nay chúng ta sẽ đến với bài học đầu tiên, bài mở đầu. Và tôi xin tự giới thiệu với các bạn. Tôi tên là Nguyễn Minh Tiệp, hiện đang là Trưởng phòng Thí nghiệm truyền thông Đa Phương Tiện, Đại học Công nghệ Thông tin Đại học Quốc gia thành phố Hồ Chí Minh. của Quốc gia thành phố Hồ Chí Minh. Thì nội dung chính của bài học ngày hôm nay sẽ gồm có 3 phần. Đầu tiên sẽ là giới thiệu tổng quan về môn học. Sau đó thì chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6sTFEFKDDqI",
      "filename": "6sTFEFKDDqI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.1: Giới thiệu môn học",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:40"
    }
  },
  {
    "page_content": "giới thiệu tổng quan về môn học. Sau đó thì chúng ta sẽ cùng lướt qua lịch sử hình thành và thảo luận về một số thành tựu của trí tuệ nhân tạo nói chung cũng như là học sâu, Deep Learning nói riêng. Và cuối cùng đó là chúng ta sẽ cùng ôn tập kiến thức toán nền tảng. Trong số rất nhiều những kiến thức toán nền tảng, thì có cái phần là về đại số tuyến tính và giải tích là hai kiến thức toán có liên quan trực tiếp đến môn học này của chúng ta. Đầu tiên, đó là giới thiệu về môn học. Môn học này của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6sTFEFKDDqI",
      "filename": "6sTFEFKDDqI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.1: Giới thiệu môn học",
      "chunk_id": 1,
      "start_timestamp": "0:00:37",
      "end_timestamp": "0:01:14"
    }
  },
  {
    "page_content": "đó là giới thiệu về môn học. Môn học này của chúng ta sẽ được diễn ra trong vòng 10 tuần. Và dưới đây là timeline về các nội dung sẽ diễn ra trong vòng 10 tuần. Tuần số 1, chúng ta sẽ giới thiệu và ôn tập kiến thức toán nền tảng. Như đã đề cập, đó là chúng ta sẽ ôn tập về kiến thức toán nền tảng. Sang tuần thứ hai, chúng ta sẽ cùng học về một mô hình học tổng quát Và dựa trên mô hình học tổng quát này, chúng ta sẽ phát triển sang các mô hình học nông Trước khi đi sang học sâu, chúng ta sẽ học",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6sTFEFKDDqI",
      "filename": "6sTFEFKDDqI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.1: Giới thiệu môn học",
      "chunk_id": 2,
      "start_timestamp": "0:01:09",
      "end_timestamp": "0:01:44"
    }
  },
  {
    "page_content": "nông Trước khi đi sang học sâu, chúng ta sẽ học các mô hình học nông trong đó có đại diện Chúng ta sẽ có các mô hình ví dụ như là mô hình Linear Regression, Logistic Regression, Softmax Regression Và chúng ta sẽ tiếp cận sang mô hình học sâu đầu tiên, đó chính là mạng Neural Network Và sang tuần thứ 3 thì chúng ta sẽ phát triển lên Đó là chúng ta sẽ học về một kiến trúc mạng CNN Convolutional Neural Network Thì đây là một trong những kiến trúc mạng rất là kinh điển Và có rất nhiều ứng dụng hiện",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6sTFEFKDDqI",
      "filename": "6sTFEFKDDqI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.1: Giới thiệu môn học",
      "chunk_id": 3,
      "start_timestamp": "0:01:37",
      "end_timestamp": "0:02:13"
    }
  },
  {
    "page_content": "rất là kinh điển Và có rất nhiều ứng dụng hiện nay Và để giúp cho các bạn có thể hiểu rõ hơn Về cái kiến trúc mạng này Thì chúng ta sẽ có cái phần gọi là trực quan hóa mạng CNN Sang tuần thứ 4 thì chúng ta sẽ có một số Biến thể của mạng CNN Và đây là những biến thể rất là nổi tiếng và có rất nhiều thành tựu hiện nay. Sang tuần thứ 5, chúng ta sẽ được học về áp dụng mạng CNN để vào giải quyết một số bài toán trong xử lý ảnh ví dụ như là bài toán phân loại hình ảnh, bài toán phát hiện và phân",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6sTFEFKDDqI",
      "filename": "6sTFEFKDDqI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.1: Giới thiệu môn học",
      "chunk_id": 4,
      "start_timestamp": "0:02:09",
      "end_timestamp": "0:02:48"
    }
  },
  {
    "page_content": "phân loại hình ảnh, bài toán phát hiện và phân đoạn ngữ nghĩa đối tượng. Sang tuần thứ 6, chúng ta sẽ được tìm hiểu về mạng Recurrent Neural Network, RNN và một biến thể khác của nó, đó chính là LSTM. thì đây mặc dù là hai cái kiến trúc mạng rất là kinh điển tuy nhiên nó cũng có những cái ứng dụng tại thời kỳ này và sang tuần thứ 7 thì chúng ta sẽ được học về hai cái cơ chế một đó là Attention và hai đó là kiến trúc mạng Transformer thì đây là hai cái cơ chế và kiến trúc mạng rất là hiện đại",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6sTFEFKDDqI",
      "filename": "6sTFEFKDDqI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.1: Giới thiệu môn học",
      "chunk_id": 5,
      "start_timestamp": "0:02:43",
      "end_timestamp": "0:03:20"
    }
  },
  {
    "page_content": "hai cái cơ chế và kiến trúc mạng rất là hiện đại hiện nay có rất nhiều những cái ứng dụng đã áp dụng vào trong cuộc sống sử dụng hai cái cơ chế này Sang tuần thứ 8 thì chúng ta sẽ được học qua một số ứng dụng của Mô Hình Attention và Transformer này. Và đến tuần thứ 9, tuần thứ 10 thì chúng ta sẽ dành 2 thời lượng của 2 tuần này để cho báo cáo đồ án cuối kỳ cũng như là ôn tập trước khi thi cuối kỳ. Tiếp theo đó chính là hình thức đánh giá của môn học này. Môn học này sẽ có 3 thành phần chính.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6sTFEFKDDqI",
      "filename": "6sTFEFKDDqI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.1: Giới thiệu môn học",
      "chunk_id": 6,
      "start_timestamp": "0:03:15",
      "end_timestamp": "0:03:59"
    }
  },
  {
    "page_content": "học này. Môn học này sẽ có 3 thành phần chính. Đầu tiên là bài tập chiếm 30% số điểm. Để có được cái cột điểm này thì chúng ta sẽ phải làm các cái bài quiz, tức là các cái bài tập trắc nghiệm hàng tuần. Tiếp theo đó là chúng ta sẽ làm các cái bài thực hành để kiểm tra và vận dụng kiến thức lý thuyết ở trên lớp. Cột điểm thứ 2 đó chính là đồ án, cũng chiếm 30% số điểm và để có được cái cột điểm này thì chúng ta sẽ phải làm đồ án theo nhóm trình bày ở trên lớp. Và chúng ta sẽ nộp báo cáo vào thời",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6sTFEFKDDqI",
      "filename": "6sTFEFKDDqI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.1: Giới thiệu môn học",
      "chunk_id": 7,
      "start_timestamp": "0:03:50",
      "end_timestamp": "0:04:35"
    }
  },
  {
    "page_content": "ở trên lớp. Và chúng ta sẽ nộp báo cáo vào thời điểm cuối của học kỳ. Và cuối cùng đó chính là cột điểm cuối kỳ, chiếm 40% số điểm là cao nhất. Và để có được cái cột điểm này thì chúng ta sẽ phải thi lý thuyết, được tổ chức tập trung và thực hiện với đề đóng. Quy ước chung của môn học này đó chính là chúng ta sẽ sử dụng ngôn ngữ lập trình Python phiên bản 3.9 trở lên Và framework Deep Learning thì có rất nhiều nhưng chúng ta sẽ làm các bài tập của môn học này là framework TensorFlow Còn ngoài",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6sTFEFKDDqI",
      "filename": "6sTFEFKDDqI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.1: Giới thiệu môn học",
      "chunk_id": 8,
      "start_timestamp": "0:04:26",
      "end_timestamp": "0:05:03"
    }
  },
  {
    "page_content": "của môn học này là framework TensorFlow Còn ngoài ra trong phần đồ án nếu như chúng ta có sử dụng một framework khác ví dụ như PyTorch thì cũng được Nhưng mà bài tập thì chúng ta sẽ sử dụng framework đó là TensorFlow Tiếp theo, để có thể giúp cho các bạn hiểu được bài lý thuyết kỹ hơn thì chúng ta không một cách nào khác, đó chính là phải làm bài tập đầy đủ hàng tuần. Và một trong những cái công việc cũng rất là quan trọng, đó chính là chúng ta phải tích cực đặt các câu hỏi cho giảng viên ở",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6sTFEFKDDqI",
      "filename": "6sTFEFKDDqI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.1: Giới thiệu môn học",
      "chunk_id": 9,
      "start_timestamp": "0:04:56",
      "end_timestamp": "0:05:35"
    }
  },
  {
    "page_content": "ta phải tích cực đặt các câu hỏi cho giảng viên ở trên lớp hoặc là trên kênh website của môn học hoặc là trên kênh YouTube của lớp. Về tài liệu tham khảo, thì tài liệu tham khảo chính của môn này đó chính là giáo trình các kỹ thuật học sâu và ứng dụng do tôi là Nguyễn Minh Tiệp và viết cùng với lại thầy Lê Đường Di và thầy Dương Anh Đức. Ngoài ra thì chúng ta cũng còn một cái tài liệu tiếng nước ngoài cũng rất là nổi tiếng. Đó chính là cuốn sách về Deep Learning hay còn gọi là học sâu của tác",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6sTFEFKDDqI",
      "filename": "6sTFEFKDDqI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.1: Giới thiệu môn học",
      "chunk_id": 10,
      "start_timestamp": "0:05:29",
      "end_timestamp": "0:05:56"
    }
  },
  {
    "page_content": "về Deep Learning hay còn gọi là học sâu của tác giả Bengio và các cộng sự. Và các cái tài liệu tham khảo này thì cũng không phải là tất cả. Và chúng ta cũng có thể tham khảo các cái tài liệu khác. Bên cạnh việc là đọc sách thì chúng ta cũng có thể tham khảo các cái khóa học online Ví dụ như Coursera là một trong những nền tảng học trực tuyến rất là nổi tiếng hiện nay và chúng ta sẽ học các khóa học liên quan đến môn học sâu.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6sTFEFKDDqI",
      "filename": "6sTFEFKDDqI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.1: Giới thiệu môn học",
      "chunk_id": 11,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Trong phiên bản của Transformer gốc, nó sẽ sử dụng các hàm sin và cos, lần lượt các hàm sin và cos, và giá trị y. Bên trong hàm sin của mình là y chia cho 10.000 bình phương nhân cho 1 phần d. Thế thì, cái PE này của mình sẽ có kích thước là d chiều. nó sẽ có kích thước là d chiều tại vì ở đây là d chia hai ở đây là 1, 1 là d chia hai thì như vậy là nó sẽ là d chia hai mà nhân 2 lên tại vì nó sẽ là một cặp nó sẽ là một cặp d chia hai mỗi cái này là cho một cái chỉ số là cho một cái chỉ số ví dụ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7AZr_li6ZtA",
      "filename": "7AZr_li6ZtA",
      "title": "[CS431 - Chương 2] Part 4_3: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:53"
    }
  },
  {
    "page_content": "là cho một cái chỉ số là cho một cái chỉ số ví dụ đây là 1, 1 Tiếp theo sẽ là 2, 2, đến đây sẽ là d/2, d/2 thì chúng ta sẽ có tất cả là d/2 cái cặp như vậy d/2 cặp, thì d/2 nhân 2 sẽ là bằng d như vậy thì kích thước output của positional embedding này sẽ là một cái vector d chiều ý tiếp theo chúng ta cần phải đề cập đến đây đó là cái chỉ số i cái chỉ số i này của mình tương ứng là index vị trí của từ và với việc chúng ta cho mẫu số là 10.000 nó sẽ giúp cho khả năng là các positional embedding",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7AZr_li6ZtA",
      "filename": "7AZr_li6ZtA",
      "title": "[CS431 - Chương 2] Part 4_3: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 1,
      "start_timestamp": "0:00:52",
      "end_timestamp": "0:01:39"
    }
  },
  {
    "page_content": "sẽ giúp cho khả năng là các positional embedding không lặp lại với y của mình chạy từ 0 cho đến 10.000 tại vì với y chạy từ 0 cho đến 10.000 thì giá trị này sẽ là từ 0 Nó nhảy lên là 1 phần 10.000 Rồi nhảy lên 2 phần 10.000, vân vân nhảy cho đến 1 Nó cứ nhảy lên, thì việc mà chúng ta đang xét sin, cos, sin, cos này Nó sẽ có thêm một cái tác dụng nữa Tức là việc giúp tránh giá trị PE trùng nhau Đó là cái ý thứ nhất Ý thứ 2 đó là đảm bảo cho PE nó sẽ đi theo phân bố Là phân bố chuẩn Các phần tử",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7AZr_li6ZtA",
      "filename": "7AZr_li6ZtA",
      "title": "[CS431 - Chương 2] Part 4_3: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 2,
      "start_timestamp": "0:01:35",
      "end_timestamp": "0:02:31"
    }
  },
  {
    "page_content": "sẽ đi theo phân bố Là phân bố chuẩn Các phần tử của PE, các phần tử trong vector PE này tuân theo phân bố chuẩn Ừ Ở đây là ưu điểm là hàm tuần hoàn cho thấy vị trí tương đối không quan trọng Tức là chúng ta hoàn toàn có thể thay hàm tuần hoàn này bằng một hàm khác, hàm ý của nó vẫn đúng. Chúng ta có thể sử dụng giá trị của mình sẽ là thay đổi lên xuống, lên xuống, lên xuống Như vậy thì thông tin về mặt vị trí tương đối không quan trọng Thế là thông tin về mặt chỉ số y, y cộng 1, y cộng 2, v.v.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7AZr_li6ZtA",
      "filename": "7AZr_li6ZtA",
      "title": "[CS431 - Chương 2] Part 4_3: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 3,
      "start_timestamp": "0:02:21",
      "end_timestamp": "0:03:13"
    }
  },
  {
    "page_content": "tin về mặt chỉ số y, y cộng 1, y cộng 2, v.v. Thì lẽ ra nó phải tăng, nếu mà xét về mặt vị trí tương đối thì nó phải tăng Nhưng mà hàm tuần hoàn thì nó lại là lên xuống, lên xuống Thì như vậy là nó khẳng định cái việc đó là khi chúng ta chọn với hàm tuần hoàn mà độ chính xác của hệ thống này nó vẫn tốt. Tức là cái vị trí tương đối, sự tăng dần của cái chỉ số này cho cái Positional Embedding là không cần thiết. Tức là PE của mình nó phải là một cái hàm tăng là không cần thiết. ưu điểm thứ 2 là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7AZr_li6ZtA",
      "filename": "7AZr_li6ZtA",
      "title": "[CS431 - Chương 2] Part 4_3: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 4,
      "start_timestamp": "0:03:03",
      "end_timestamp": "0:03:45"
    }
  },
  {
    "page_content": "cái hàm tăng là không cần thiết. ưu điểm thứ 2 là nó có thể biểu diễn được chuỗi rất dài thì thay đổi qua 10.000 thì y của mình thay đổi thì giá trị này sẽ tăng theo và thậm chí cho đến khi y chạm được đến 10.000 và vượt qua khoảng 10.000 thì các giá trị này của mình, vector PE của mình cũng sẽ không lặp lại nó không có trùng nhau Tại vì để trùng thì nó sẽ phải có thêm một cái đại lượng là pi nữa Nó phải có thêm một cái đại lượng là pi Còn ở đây là không có pi vô nên cái khả năng nó trùng rất",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7AZr_li6ZtA",
      "filename": "7AZr_li6ZtA",
      "title": "[CS431 - Chương 2] Part 4_3: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 5,
      "start_timestamp": "0:03:39",
      "end_timestamp": "0:04:31"
    }
  },
  {
    "page_content": "là không có pi vô nên cái khả năng nó trùng rất là thấp Rồi Và đương nhiên nó cũng sẽ có một số khuyết điểm như chúng ta thấy ở đây Và cái vector positional embedding này là một cái vector cố định Với một cái Y cố định thì chúng ta sẽ có một cái PE cố định Và cái này nó là một cái hàm do chúng ta thiết kế, tổ hợp của các hàm tuần hoàn Nó không phải học từ dữ liệu, nó không học được từ dữ liệu Thì đây chính là cái điểm yếu của cái cách biểu diễn vị trí dưới dạng các hàm sin Và ở đây thì chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7AZr_li6ZtA",
      "filename": "7AZr_li6ZtA",
      "title": "[CS431 - Chương 2] Part 4_3: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 6,
      "start_timestamp": "0:04:24",
      "end_timestamp": "0:05:11"
    }
  },
  {
    "page_content": "trí dưới dạng các hàm sin Và ở đây thì chúng ta sẽ xuất hiện thêm một cái khái niệm nữa, đó là multi-head self-attention. Trước đây thì là self-attention, còn bây giờ chúng ta sẽ làm multi-head self-attention. Thì ở đây nó xuất phát từ một cái góc nhìn, đó là một từ nó sẽ có thể có nhiều cái mối quan hệ trong câu. và chúng ta sẽ thực hiện cái self-attention này nhiều lần ứng với một lần, nó sẽ thể hiện trong một mối quan hệ của một từ trong câu và chúng ta sẽ kết hợp các kết quả này lại với",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7AZr_li6ZtA",
      "filename": "7AZr_li6ZtA",
      "title": "[CS431 - Chương 2] Part 4_3: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 7,
      "start_timestamp": "0:05:06",
      "end_timestamp": "0:05:57"
    }
  },
  {
    "page_content": "và chúng ta sẽ kết hợp các kết quả này lại với nhau lấy ví dụ, tôi có hẹn với Bảo nhưng anh ấy nhắn đến muộn thì chúng ta thấy cái từ anh ấy nó sẽ có hai mối quan hệ Mối quan hệ đầu tiên sẽ là tham chiếu đến từ là Bảo Đồng thời, anh ấy sẽ là chủ thể cho hành động nhắn tin Với một từ này, nó có đến những hai mối quan hệ Một cách tổng quát, một từ có rất nhiều mối quan hệ trong câu do đó chúng ta sẽ không sử dụng single head attention mà chúng ta sẽ sử dụng multi head thì ở đây chúng ta vẫn sẽ là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7AZr_li6ZtA",
      "filename": "7AZr_li6ZtA",
      "title": "[CS431 - Chương 2] Part 4_3: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 8,
      "start_timestamp": "0:05:51",
      "end_timestamp": "0:07:03"
    }
  },
  {
    "page_content": "sử dụng multi head thì ở đây chúng ta vẫn sẽ là hệ thống ký hiệu V, K và Q tương ứng là Value, Key và Query chúng ta đưa qua linear, linear này bản chất chính là phép nhân tuyến tính sau khi chúng ta nhân tuyến tính xong, chúng ta thực hiện scaled dot-product Chúng ta thực hiện Scaled Dot-Product để mà cuối cùng là chúng ta sẽ nhân với lại tuyến tính để tính ra giá trị vector tổng hợp Đây chính là Attention output Bên này là multi-head self-attention, đây là một cái lát cắt, chúng ta sẽ thực",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7AZr_li6ZtA",
      "filename": "7AZr_li6ZtA",
      "title": "[CS431 - Chương 2] Part 4_3: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 9,
      "start_timestamp": "0:06:43",
      "end_timestamp": "0:07:35"
    }
  },
  {
    "page_content": "đây là một cái lát cắt, chúng ta sẽ thực hiện trên một cái khía cạnh của một cái từ trong câu của mình và chúng ta sẽ thực hiện nhiều khía cạnh khác nhau sau đó đến đây, chúng ta sẽ concat thông tin của các scaled dot-product attention lại với nhau và sau đó chúng ta mới thực hiện phép biến đổi linear để tổng hợp thông tin thì nó gọi là multi-head self-attention và multi-head self-attention thì nó sẽ có công thức như sau trong self-attention bình thường chỉ có ma trận Q, K và V bây giờ có nhiều",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7AZr_li6ZtA",
      "filename": "7AZr_li6ZtA",
      "title": "[CS431 - Chương 2] Part 4_3: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 10,
      "start_timestamp": "0:07:27",
      "end_timestamp": "0:08:31"
    }
  },
  {
    "page_content": "thường chỉ có ma trận Q, K và V bây giờ có nhiều đầu, ở đây sẽ có thêm chỉ số L ở phía dưới các ma trận này đều có kích thước đó là d nhân cho d chia h trong đó h là số đầu attention của mình hay số lượng head của mình rồi, và l của mình nó sẽ chạy từ 1 cho đến h tức là mỗi bộ QL, KL, VL tương ứng là một cái tương ứng là một cái bộ này Bộ tham số cho phép biến đổi tuyến tính ở đây Ở đây có bao nhiêu heads? Ở đây có 3 heads Trong trường hợp này, H của mình là bằng 3 Và L của mình sẽ là một cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7AZr_li6ZtA",
      "filename": "7AZr_li6ZtA",
      "title": "[CS431 - Chương 2] Part 4_3: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 11,
      "start_timestamp": "0:08:29",
      "end_timestamp": "0:09:15"
    }
  },
  {
    "page_content": "H của mình là bằng 3 Và L của mình sẽ là một cái chỉ số chạy L trong trường hợp này là bằng 1 L trong trường hợp này là bằng 2 L trong trường hợp này là bằng 3 L là cái chỉ số chạy Mỗi một cái lát cắt như vậy, chúng ta sẽ có một cái bộ trọng số bộ tham số trong hình cần phải huấn luyện và output lúc này theo từng head của mình đó sẽ là softmax của Q_l K_l chuyển vị chia cho, đây chính là scaled dot-product rồi nhân với V_l và như vậy output này sẽ ra là một vector có kích thước là d chia h và ở",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7AZr_li6ZtA",
      "filename": "7AZr_li6ZtA",
      "title": "[CS431 - Chương 2] Part 4_3: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 12,
      "start_timestamp": "0:09:12",
      "end_timestamp": "0:10:08"
    }
  },
  {
    "page_content": "ra là một vector có kích thước là d chia h và ở cái bước tổng hợp này, ở cái bước concat này Chúng ta sẽ tổng hợp lại output 1, output 2, output h, chúng ta sẽ concat lại với nhau sau đó chúng ta sẽ nhân tuyến tính với lại cái ma trận Y để tổng hợp thông tin thì đây sẽ là cái thông tin tổng hợp cuối cùng của multi-head của nhiều đầu Vì vậy, mỗi đầu sẽ có một góc nhìn của ngữ cảnh như đã giải thích ở trên. Và như vậy thì đến đây, kiến trúc encoder của chúng ta đã tương đối là một phần hoàn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7AZr_li6ZtA",
      "filename": "7AZr_li6ZtA",
      "title": "[CS431 - Chương 2] Part 4_3: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 13,
      "start_timestamp": "0:09:58",
      "end_timestamp": "0:10:42"
    }
  },
  {
    "page_content": "của chúng ta đã tương đối là một phần hoàn chỉnh, và chúng ta đã tìm được một khối hoàn chỉnh. Và như vậy thì đến đây, kiến trúc encoder của chúng ta đã đầy đủ rồi. Vậy thì decoder của mình sẽ là sao? Decoder sẽ được tiến hành truy vấn trên đặc trưng được lấy từ encoder Encoder tổng hợp thông tin, tính toán xong Đến tầng thứ 6, chúng ta sẽ tính toán và bắt đầu quá trình giải mã cho Encoder",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7AZr_li6ZtA",
      "filename": "7AZr_li6ZtA",
      "title": "[CS431 - Chương 2] Part 4_3: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 14,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Tiếp theo, chúng ta sẽ tìm hiểu về một số cái vấn đề đối với các mạng CNN và một số giải pháp để tạm thời khắc phục những vấn đề đó. Đầu tiên, chúng ta sẽ cùng khảo sát lại sơ đồ của thuật toán RNN này. Từ trái sang phải, chúng ta sẽ lần lượt feed các giá trị trong chuỗi vào. Các từ thứ t trừ 1 cho đến t cộng 1 vào. Và tại thời điểm thứ t, chúng ta sẽ có một giá trị loss thành phần, đó là Lt. Trong slide trước, chúng ta dùng hệ thống ký hiệu là trị tuyệt đối V, nhưng do nó sẽ dễ nhầm lẫn với V",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8-3xv_NElG0",
      "filename": "8-3xv_NElG0",
      "title": "[CS431 - Chương 7] Part 3_1: Một số vấn đề của mạng RNN",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:46"
    }
  },
  {
    "page_content": "trị tuyệt đối V, nhưng do nó sẽ dễ nhầm lẫn với V nên ở đây chúng ta sẽ ký hiệu nó bằng một từ tổng quát hơn, đó là k trong đó k là độ dài của cái vector của mình k của mình là độ dài của vector output hoặc vector dự đoán của mình Rồi, và chúng ta sẽ có cái hàm loss của mô hình sẽ là công thức như sau. Thì cái này là chúng ta đã được trình bày trong cái slide trước. Như vậy thì đến cái giai đoạn số 3, cái bước số 3 của cái quá trình mà xây dựng một cái mô hình máy học đó là chúng ta sẽ phải đi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8-3xv_NElG0",
      "filename": "8-3xv_NElG0",
      "title": "[CS431 - Chương 7] Part 3_1: Một số vấn đề của mạng RNN",
      "chunk_id": 1,
      "start_timestamp": "0:00:38",
      "end_timestamp": "0:01:28"
    }
  },
  {
    "page_content": "một cái mô hình máy học đó là chúng ta sẽ phải đi tính đạo hàm và có được đạo hàm rồi thì chúng ta sẽ đi cập nhật các tham số. thì công thức cho việc tính đạo hàm thì chúng ta sẽ có công thức như ở dưới đây đạo hàm của hàm loss theo ma trận U thì nó sẽ là bằng tổng của các đạo hàm thành phần tức là tổng của đạo hàm của Lt theo ma trận U đạo hàm của tổng bằng tổng các đạo hàm và tương tự như vậy cho các ma trận V và ma trận W kể sau Bây giờ chúng ta sẽ xem chi tiết hơn. Giả sử như hàm loss của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8-3xv_NElG0",
      "filename": "8-3xv_NElG0",
      "title": "[CS431 - Chương 7] Part 3_1: Một số vấn đề của mạng RNN",
      "chunk_id": 2,
      "start_timestamp": "0:01:21",
      "end_timestamp": "0:02:19"
    }
  },
  {
    "page_content": "ta sẽ xem chi tiết hơn. Giả sử như hàm loss của mình là Lt này, mình gọi lại thay vì là Lt. Thế thì chúng ta sẽ để Lt là một cái hàm hợp bao gồm các hàm Fn, Fn-1, Fn-2 cho đến F2, F1 và W. và ở đây chúng ta đang xem xét cái việc tính đạo hàm này theo ma trận W tương tự, một cách tương tự cho cái việc chúng ta tính đạo hàm theo U và V bây giờ chúng ta sẽ tính theo W trước rồi, thì khi đó cái công thức đạo hàm hàm hợp chain rule là đạo hàm của Lt theo ma trận W thì sẽ là bằng đạo hàm của hàm thứ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8-3xv_NElG0",
      "filename": "8-3xv_NElG0",
      "title": "[CS431 - Chương 7] Part 3_1: Một số vấn đề của mạng RNN",
      "chunk_id": 3,
      "start_timestamp": "0:02:07",
      "end_timestamp": "0:03:10"
    }
  },
  {
    "page_content": "theo ma trận W thì sẽ là bằng đạo hàm của hàm thứ Fn theo Fn-1 đạo hàm của Fn-1 theo Fn-2 đạo hàm của F2 theo F1, đạo hàm của F1 theo W thì công thức này đã rất là quen thuộc rồi và khi chúng ta thực hiện lan truyền thì chúng ta sẽ đi thực hiện từ cái bước cuối cùng trước Ví dụ như Fn và Fn-1 Bản chất chính là những thao tác ở bước cuối Ví dụ như là chúng ta tính hàm loss đó là sai số giữa y ngã, tức là giá trị dự đoán và giá trị thực tế Đến Fn-2, xin lỗi Fn, đạo hàm của Fn-1 theo Fn-2 thì đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8-3xv_NElG0",
      "filename": "8-3xv_NElG0",
      "title": "[CS431 - Chương 7] Part 3_1: Một số vấn đề của mạng RNN",
      "chunk_id": 4,
      "start_timestamp": "0:03:02",
      "end_timestamp": "0:03:50"
    }
  },
  {
    "page_content": "xin lỗi Fn, đạo hàm của Fn-1 theo Fn-2 thì đó sẽ là cái thao tác Softmax và tổng hợp thông tin y ngã t từ s_t cứ như vậy thì nó sẽ lan truyền cho đến những cái thao tác tính đầu tiên cho đến những cái thao tác tính đầu tiên lan truyền, lan truyền nó sẽ lan truyền ngược về và cập nhật các cái đạo hàm để từ đó sẽ tính cho các cái tham số kèm theo cái ma trận W và ở đây chúng ta sẽ có cái nhận xét Đầu tiên đó chính là đa số các đạo hàm thành phần đều có giá trị tuyệt đối nhỏ hơn 1. Hay là cái công",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8-3xv_NElG0",
      "filename": "8-3xv_NElG0",
      "title": "[CS431 - Chương 7] Part 3_1: Một số vấn đề của mạng RNN",
      "chunk_id": 5,
      "start_timestamp": "0:03:48",
      "end_timestamp": "0:04:47"
    }
  },
  {
    "page_content": "có giá trị tuyệt đối nhỏ hơn 1. Hay là cái công thức là trị tuyệt đối của đạo hàm F' là bé hơn 1. Tức là các thành phần này, đa số của nó sẽ là bé hơn 1. Thì điều này là tại sao chúng ta sẽ lấy cái ví dụ sau Chúng ta có cái công thức tính cho cái s_t, tức là cái trạng thái ở s_t là bằng sigmoid của U*x_t cộng cho W*s_{t-1} Và khi chúng ta triển khai cái đạo hàm này, thì nó sẽ là bằng đạo hàm của sigmoid, thì sẽ là bằng sigmoid nhân cho 1 trừ cho sigmoid. Mà chúng ta biết rằng là sigmoid của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8-3xv_NElG0",
      "filename": "8-3xv_NElG0",
      "title": "[CS431 - Chương 7] Part 3_1: Một số vấn đề của mạng RNN",
      "chunk_id": 6,
      "start_timestamp": "0:04:32",
      "end_timestamp": "0:05:28"
    }
  },
  {
    "page_content": "cho sigmoid. Mà chúng ta biết rằng là sigmoid của mình là một cái hàm mà dải giá trị của mình là từ 0 cho đến 1. Từ 0 cho đến 1, do đó cái giá trị này sẽ là từ 0 cho đến 1. Giá trị này sẽ là từ 0 cho đến 1. Và 1 trừ cho một cái giá trị từ 0 cho đến 1 thì nguyên cái này cũng sẽ là từ 0 cho đến 1. Như vậy tích của hai cái giá trị này cũng sẽ là từ 0 cho đến 1. Ma trận W, ma trận W của mình cũng vậy. Ma trận W cũng sẽ là các cái giá trị random ban đầu là lấy xung quanh cái phân bố chuẩn. do đó nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8-3xv_NElG0",
      "filename": "8-3xv_NElG0",
      "title": "[CS431 - Chương 7] Part 3_1: Một số vấn đề của mạng RNN",
      "chunk_id": 7,
      "start_timestamp": "0:05:24",
      "end_timestamp": "0:06:10"
    }
  },
  {
    "page_content": "đầu là lấy xung quanh cái phân bố chuẩn. do đó nó cũng sẽ từ 0 đến 1 như vậy, tổng thể thì tất cả các cái giá trị thành phần ở đây đều là các giá trị từ 0 đến 1 rồi và từ cái nhận xét số 1 đó, nó sẽ nảy sinh ra cho chúng ta đến cái nhận xét số 2 khi chúng ta xử lý văn bản, đúng không? thì cái t này thường là những cái con số rất là lớn Có những cái đoạn văn bản dài lên đến hàng ngàn chữ. Ví dụ như cho bài toán tóm tắt văn bản, chúng ta sẽ đọc nguyên một cái cuốn truyện và chúng ta sẽ phải tổng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8-3xv_NElG0",
      "filename": "8-3xv_NElG0",
      "title": "[CS431 - Chương 7] Part 3_1: Một số vấn đề của mạng RNN",
      "chunk_id": 8,
      "start_timestamp": "0:06:06",
      "end_timestamp": "0:06:47"
    }
  },
  {
    "page_content": "một cái cuốn truyện và chúng ta sẽ phải tổng hợp để tóm tắt lại cái nội dung chính của cái cuốn truyện đó là gì. Hoặc là trong một số cái bài toán dịch máy, thì cái cụm từ của mình nó có thể lên đến vài chục chữ, và vài chục từ. Và với vài chục từ này, thì nó cũng được xem là lớn rồi. Nó cũng đã xem được là lớn rồi. Và khi t này lớn thì n sẽ càng lớn và khi t càng lớn, tức là sao? Đối với cái văn bản của mình, khi t này càng lớn thì cái đạo hàm thành phần này cũng sẽ càng nhỏ Tại vì cứ một cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8-3xv_NElG0",
      "filename": "8-3xv_NElG0",
      "title": "[CS431 - Chương 7] Part 3_1: Một số vấn đề của mạng RNN",
      "chunk_id": 9,
      "start_timestamp": "0:06:43",
      "end_timestamp": "0:07:43"
    }
  },
  {
    "page_content": "thành phần này cũng sẽ càng nhỏ Tại vì cứ một cái từ, chúng ta sẽ phải thực hiện tổ hợp rất nhiều thao tác Tổng hợp thông tin từ quá khứ, hiện tại qua hàm Softmax, nó sẽ còn nhiều cái hàm con như vậy t mà càng lớn tức là số từ mà càng lớn thì số lượng cái hàm f của cái hàm hợp này sẽ càng lớn mà nó càng lớn thì sẽ dẫn đến là cái đạo hàm này sẽ càng tiến về 0 và ở đây chúng ta có một cái để ý đó là với cái loss L2 tức là t nhỏ Đối với L2, hàm hợp của nó sẽ ngắn hơn hàm hợp của Lt Tại vì sao? Từ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8-3xv_NElG0",
      "filename": "8-3xv_NElG0",
      "title": "[CS431 - Chương 7] Part 3_1: Một số vấn đề của mạng RNN",
      "chunk_id": 10,
      "start_timestamp": "0:07:32",
      "end_timestamp": "0:08:22"
    }
  },
  {
    "page_content": "của nó sẽ ngắn hơn hàm hợp của Lt Tại vì sao? Từ L2, chúng ta lan truyền ngược lại, thì ở đây chúng ta chỉ mất có 3 bước Trong khi đó, ở đây là t, khi chúng ta lan truyền ngược, chúng ta sẽ phải thực hiện hàm hợp này rất nhiều lần Do đó thì n của Lt này sẽ là rất là lớn. Thì cụ thể trong trường hợp này nó sẽ phải là đến t cộng 1 bước, đúng không? 1 bước, 2 bước cho đến đây là t cộng 1 bước. Rồi, và tại sao nó lại có cái việc là đạo hàm của mình? Tiến về 0? Thì như cái nhận xét số 1 chúng ta đã",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8-3xv_NElG0",
      "filename": "8-3xv_NElG0",
      "title": "[CS431 - Chương 7] Part 3_1: Một số vấn đề của mạng RNN",
      "chunk_id": 11,
      "start_timestamp": "0:08:19",
      "end_timestamp": "0:09:08"
    }
  },
  {
    "page_content": "Tiến về 0? Thì như cái nhận xét số 1 chúng ta đã thấy là các cái giá trị đạo hàm thành phần này đại đa số là những con số bé hơn 1. là những con số bé hơn 1 và các con số bé hơn 1 và nhân với nhau thì nó sẽ có xu hướng là tiến đến 0 trong chương trình cấp 3, chúng ta đã từng học một kiến thức đó là lim của n tiến đến vô cùng của a mũ n với a là con số với a là con số bé hơn 1 lớn hơn 0 thì lim của nó sẽ là bằng 0 Còn nếu như chúng ta muốn kiểm chứng bằng một cách gọi là trực quan và nó có thể",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8-3xv_NElG0",
      "filename": "8-3xv_NElG0",
      "title": "[CS431 - Chương 7] Part 3_1: Một số vấn đề của mạng RNN",
      "chunk_id": 12,
      "start_timestamp": "0:09:04",
      "end_timestamp": "0:09:22"
    }
  },
  {
    "page_content": "chứng bằng một cách gọi là trực quan và nó có thể tính toán được hay thì chúng ta sẽ lấy một con số rất gần số 1 thôi là 0.9 và chúng ta thử lũy thừa n với n ở đây chúng ta có thể cho là khoảng 20 đi thì chúng ta thấy cái con số này là con số rất là bé nó sẽ tiến về, tiến về 0 Rồi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8-3xv_NElG0",
      "filename": "8-3xv_NElG0",
      "title": "[CS431 - Chương 7] Part 3_1: Một số vấn đề của mạng RNN",
      "chunk_id": 13,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề, học sâu, machine learning, logistic regression, ImageNet. Chủ đề, học sâu, machine learning, ImageNet. Trong phần trước, chúng ta đã tìm hiểu qua các tensor không chiều, một chiều, hai chiều, ba chiều và nhiều chiều. thì chúng ta đã biết rằng các tensor này được sử dụng để biểu diễn cho các loại dữ liệu thích hợp. Sang nội dung tiếp theo. Chúng ta sẽ tìm hiểu về một số thao tác biến đổi, một số phép toán trên các tensor này và tương ứng nó sẽ được sử dụng để biến đổi dữ liệu. Phép đầu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=88y0wnXxCUo",
      "filename": "88y0wnXxCUo",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.3: Ôn tập nền tảng đại số tuyến tính (Part 3)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:50"
    }
  },
  {
    "page_content": "nó sẽ được sử dụng để biến đổi dữ liệu. Phép đầu tiên là một trong những phép rất thú vị, đó chính là phép chuyển vị. Phép chuyển vị này là nhằm biến các vector từ dạng cột sang dạng hàng. Ví dụ như đây là ma trận A, thì chuyển vị của ma trận A tương ứng sẽ xoay vector A2. Ví dụ ở đây chúng ta thấy vector A2 từ dạng cột sẽ được chuyển sang dạng hàng. Và ở hình minh họa ở đây thì chúng ta thấy đó là ma trận A thì bao gồm các giá trị ví dụ như là 1, 2, 3, 4, 5, 6, 7, 8 và khi chuyển vị thì nó sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=88y0wnXxCUo",
      "filename": "88y0wnXxCUo",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.3: Ôn tập nền tảng đại số tuyến tính (Part 3)",
      "chunk_id": 1,
      "start_timestamp": "0:00:45",
      "end_timestamp": "0:01:32"
    }
  },
  {
    "page_content": "1, 2, 3, 4, 5, 6, 7, 8 và khi chuyển vị thì nó sẽ tương ứng là một cái thao tác đối xứng qua trục đường chéo và cái ma trận sau khi đã thực hiện đối xứng qua cái trục đường chéo này đó là a chuyển vị, nó sẽ có giá trị là 1, 5, 9, 13 thì đây là một ví dụ để minh họa cách thức thực hiện thao tác chuyển vị và chuyển vị thì thường được sử dụng để giúp cho việc tính toán nó thuận tiện và đi theo một quy tắc thống nhất Thao tác quan trọng tiếp theo là thao tác cộng vector Hai vector x và y để có thể",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=88y0wnXxCUo",
      "filename": "88y0wnXxCUo",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.3: Ôn tập nền tảng đại số tuyến tính (Part 3)",
      "chunk_id": 2,
      "start_timestamp": "0:01:26",
      "end_timestamp": "0:02:18"
    }
  },
  {
    "page_content": "thao tác cộng vector Hai vector x và y để có thể thực hiện được phép cộng với nhau, nó nhất thiết phải có cùng số chiều ví dụ trong trường hợp này là cùng là vector có 1 chiều nhưng mà có n phần tử Lưu ý là cùng chiều và cùng số phần tử thì ở đây là chúng ta sẽ có 1 chiều nhưng mà 1 chiều với n phần tử Hai vector này muốn cộng được thì nó phải cùng chiều là 1 và số phần tử là n và khi cộng thì chúng ta sẽ phải thực hiện từng phần tử với nhau ví dụ như phần tử thứ nhất thì sẽ cộng với phần tử",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=88y0wnXxCUo",
      "filename": "88y0wnXxCUo",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.3: Ôn tập nền tảng đại số tuyến tính (Part 3)",
      "chunk_id": 3,
      "start_timestamp": "0:02:12",
      "end_timestamp": "0:03:00"
    }
  },
  {
    "page_content": "dụ như phần tử thứ nhất thì sẽ cộng với phần tử thứ nhất và để ra phần tử thứ nhất của vector kết quả, tương tự như vậy cho phần tử thứ hai thì ở đây chúng ta có thể lấy một cái ví dụ minh họa là vector 1, 2, 3 và khi cộng với lại cái vector là 3, 2, 1 ví dụ vậy thì chúng ta sẽ ra một cái vector 3 cộng 1 là 4, 2 cộng 2 là 4, 3 cộng 1 là 4 Như vậy chúng ta sẽ ra một cái vector gồm toàn số 4 Đây là một ví dụ để minh họa cho phép cộng trên vector. Tiếp theo, đó chính là phép cộng trên ma trận cũng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=88y0wnXxCUo",
      "filename": "88y0wnXxCUo",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.3: Ôn tập nền tảng đại số tuyến tính (Part 3)",
      "chunk_id": 4,
      "start_timestamp": "0:02:53",
      "end_timestamp": "0:03:48"
    }
  },
  {
    "page_content": "theo, đó chính là phép cộng trên ma trận cũng tương tự như là cộng trên vector. Để hai ma trận A và B có thể thực hiện được thao tác cộng, thì nó nhất thiết phải có cùng số chiều và cùng số phần tử. ở đây trong trường hợp này là hai chiều và số phần tử là m và n thì ở đây chúng ta lấy một cái ví dụ là ma trận a là tại phần tử thứ a mười hai thì nó sẽ phải cộng với lại cái phần tử là b mười hai để ra một cái ma trận kết quả và giá trị tại vị trí mười hai giá trị ở đây sẽ là tổng của hai cái phần",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=88y0wnXxCUo",
      "filename": "88y0wnXxCUo",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.3: Ôn tập nền tảng đại số tuyến tính (Part 3)",
      "chunk_id": 5,
      "start_timestamp": "0:03:42",
      "end_timestamp": "0:04:34"
    }
  },
  {
    "page_content": "hai giá trị ở đây sẽ là tổng của hai cái phần tử này thì ở đây chúng ta có thể lấy một cái ví dụ nhỏ thôi, ví dụ như là 1, 2, 3, 4 chúng ta cộng với lại ma trận là 3, 0, 1, 0 thì chúng ta sẽ ra được một cái ma trận có cùng kích thước mà 1 cộng 3 sẽ là 4, 2 cộng 0 sẽ là 2, 3 cộng 1 sẽ là 4 và 4 cộng 0 sẽ là 4 và đây là ma trận kết quả Ứng dụng của phép cộng ma trận này, chúng ta có thể lấy một ví dụ như sau Nếu như chúng ta cộng trung bình 3 kênh màu Red, Green, Blue tương ứng là 3 ma trận RGB",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=88y0wnXxCUo",
      "filename": "88y0wnXxCUo",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.3: Ôn tập nền tảng đại số tuyến tính (Part 3)",
      "chunk_id": 6,
      "start_timestamp": "0:04:24",
      "end_timestamp": "0:05:16"
    }
  },
  {
    "page_content": "màu Red, Green, Blue tương ứng là 3 ma trận RGB và tính trung bình, thì kết quả của mình sẽ ra là một cái ma trận Và ma trận này chính là ảnh trung bình, là ảnh mức xám để biểu diễn cho tấm ảnh Phép tiếp theo là phép tích Hadamard. Phép tích Hadamard cũng tương tự như phép cộng. Hai ma trận A và B muốn thực hiện phép tích Hadamard phải có cùng số chiều và cùng số phần tử trên các chiều đó. Ở đây, các phần tử sẽ được nhân tương ứng và hai phần tử này nhân vào thì sẽ ra giá trị ở đây. thực hiện",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=88y0wnXxCUo",
      "filename": "88y0wnXxCUo",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.3: Ôn tập nền tảng đại số tuyến tính (Part 3)",
      "chunk_id": 7,
      "start_timestamp": "0:05:13",
      "end_timestamp": "0:06:06"
    }
  },
  {
    "page_content": "này nhân vào thì sẽ ra giá trị ở đây. thực hiện là phép nhân lần lượt theo từng phần tử Và đây là một ví dụ vui Trên ảnh gốc là img chúng ta sẽ thực hiện thao tác tích Hadamard với một ảnh gốc gọi là ảnh mask hay ảnh mặt nạ Nếu như các giá trị trên vùng trắng này nhận giá trị là 1 và các vùng trên vùng đen nhận giá trị là 0 thì cái kết quả khi nhân img với cái ảnh mask mà nhân tích Hadamard thì nó sẽ ra cái ảnh kết quả như sau và cái ý nghĩa của cái ảnh này đó chính là chúng ta xóa cái dây giày",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=88y0wnXxCUo",
      "filename": "88y0wnXxCUo",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.3: Ôn tập nền tảng đại số tuyến tính (Part 3)",
      "chunk_id": 8,
      "start_timestamp": "0:06:00",
      "end_timestamp": "0:06:41"
    }
  },
  {
    "page_content": "cái ảnh này đó chính là chúng ta xóa cái dây giày này đi giống như là tưởng tượng người này đang bay thì tại sao cái ảnh kết quả này nó ra được cái hình như thế này là vì các cái giá trị trên cái vùng trắng này mà nhận giá trị là 1 thì 1 nhân với lại các cái phần tử ở đây thì nó sẽ giữ nguyên cái giá trị là các phần tử ở đây do đó thì tại vì 1 nhân với bao nhiêu thì cũng chính là bằng số đó như vậy tương ứng cái vùng màu trắng sẽ là nguyên giá trị mà riêng cái vùng của cái dây ở đây thì nhân",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=88y0wnXxCUo",
      "filename": "88y0wnXxCUo",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.3: Ôn tập nền tảng đại số tuyến tính (Part 3)",
      "chunk_id": 9,
      "start_timestamp": "0:06:35",
      "end_timestamp": "0:07:22"
    }
  },
  {
    "page_content": "trị mà riêng cái vùng của cái dây ở đây thì nhân cho cái vùng màu đen tức là giá trị bằng không thì không nhân với bao nhiêu cũng bằng không do nó sẽ triệt tiêu đi và dẫn đến là cái vùng này, nó sẽ có màu là màu đen màu đen nó sẽ trùng với màu nền ở vùng này để tạo ra cái hiệu ứng đó là chúng ta đang muốn xóa bỏ dây giày ở đây đi một cái thao tác nhân khác được sử dụng phổ biến hơn đó chính là phép nhân vector và nhân ở đây đó là nhân vô hướng hay còn gọi là tích vô hướng Hai vector a và b có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=88y0wnXxCUo",
      "filename": "88y0wnXxCUo",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.3: Ôn tập nền tảng đại số tuyến tính (Part 3)",
      "chunk_id": 10,
      "start_timestamp": "0:07:18",
      "end_timestamp": "0:08:26"
    }
  },
  {
    "page_content": "hay còn gọi là tích vô hướng Hai vector a và b có thể thực hiện được tích vô hướng với nhau nếu như nó có cùng số phần tử Ví dụ như ở đây chúng ta sẽ ký hiệu đó là a chuyển vị nhân với b Vì a mặc định đó là viết dưới dạng là cột a là viết dưới dạng cột để đảm bảo quy tắc nhân giữa hai vector, chúng ta sẽ ký hiệu đó là a chuyển vị để chuyển từ dạng cột sang dạng hàng sau đó chúng ta sẽ nhân với lại vector b, quy tắc nhân đó là nhân theo hàng và cột Từng phần tử ở đây sẽ nhân lại với nhau Sau đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=88y0wnXxCUo",
      "filename": "88y0wnXxCUo",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.3: Ôn tập nền tảng đại số tuyến tính (Part 3)",
      "chunk_id": 11,
      "start_timestamp": "0:08:20",
      "end_timestamp": "0:09:20"
    }
  },
  {
    "page_content": "Từng phần tử ở đây sẽ nhân lại với nhau Sau đó chúng ta sẽ thực hiện tổng tất cả các tích này a1, b1, b2, an, bn Biểu diễn dưới dạng hình học của thao tác nhân này chính là phép chiếu thì ở đây chúng ta sẽ có hai vector A và B tích vô hướng của hai vector A và B thì nó tương ứng sẽ thực hiện phép chiếu vector A xuống vector B và giá trị của phép tích vô hướng này nó sẽ có giá trị như trên và chúng ta chú ý đó là tích vô hướng của a chuyển vị b thì nó cũng tương ứng sẽ là bằng tích vô hướng của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=88y0wnXxCUo",
      "filename": "88y0wnXxCUo",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.3: Ôn tập nền tảng đại số tuyến tính (Part 3)",
      "chunk_id": 12,
      "start_timestamp": "0:09:14",
      "end_timestamp": "0:10:27"
    }
  },
  {
    "page_content": "nó cũng tương ứng sẽ là bằng tích vô hướng của b chuyển vị nhân với a Tức là nó sẽ có thể thực hiện cái thao tác gọi là giao hoán với nhau A chuyển vị nhân B thì cũng sẽ là bằng B chuyển vị nhân A Thì đây là một cái ví dụ của phép nhân vector với vector Nếu như chúng ta có cái dữ liệu các thành phần vitamin A, vitamin C và chất xơ của các loại rau củ đó là cà rốt, bắp cải và dưa leo Ví dụ như 35, 0.5 và 0.5 ở đây tương ứng là vector biểu diễn cho vitamin A có trong các loại cà rốt, bắp cải và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=88y0wnXxCUo",
      "filename": "88y0wnXxCUo",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.3: Ôn tập nền tảng đại số tuyến tính (Part 3)",
      "chunk_id": 13,
      "start_timestamp": "0:10:11",
      "end_timestamp": "0:11:18"
    }
  },
  {
    "page_content": "vitamin A có trong các loại cà rốt, bắp cải và dưa leo Nếu chúng ta gọi x, y, z là số kg cà rốt, bắp cải và dưa leo chúng ta mua được trong lần đi chợ đó thì khi đó lượng vitamin A mà chúng ta có bổ sung cho cái bữa ăn ngày hôm đó sẽ là 35, 0.5, 0.5 nhân theo hàng và cột thì đây chính là lượng vitamin A mà chúng ta sẽ cung cấp cho cái bữa ăn ngày hôm đó Tiếp theo, đó chính là thực hiện thao tác nhân ma trận với vector Thì để ma trận A có thể nhân được với lại vector x thì nó sẽ phải đảm bảo đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=88y0wnXxCUo",
      "filename": "88y0wnXxCUo",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.3: Ôn tập nền tảng đại số tuyến tính (Part 3)",
      "chunk_id": 14,
      "start_timestamp": "0:11:10",
      "end_timestamp": "0:12:07"
    }
  },
  {
    "page_content": "được với lại vector x thì nó sẽ phải đảm bảo đó là số hàng, số cột Đây là số cột của a, nó tương ứng là số phần tử trong chiều thành phần của vector x. Như vậy n ở đây phải tương ứng bằng n ở đây. Và khi đó tích y là bằng a nhân với x thì nó sẽ là một vector. Nói như vậy kết quả của thao tác nhân giữa ma trận với một vector, nó sẽ là một vector. thì tại sao là như vậy? nếu như chúng ta thực hiện theo đúng nguyên tắc nhân hàng với cột thì chúng ta thấy a chúng ta sẽ thực hiện nhân theo hàng ở",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=88y0wnXxCUo",
      "filename": "88y0wnXxCUo",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.3: Ôn tập nền tảng đại số tuyến tính (Part 3)",
      "chunk_id": 15,
      "start_timestamp": "0:12:01",
      "end_timestamp": "0:12:11"
    }
  },
  {
    "page_content": "ta thấy a chúng ta sẽ thực hiện nhân theo hàng ở đây a chúng ta có tất cả là n hàng rồi lưu ý là ở đây chúng ta có một sự nhầm lẫn ở đây sẽ là có m hàng và mỗi một vector A1, A2 ở đây thì nó sẽ thuộc Rn, tức là gồm có n phần tử thì khi đó chúng ta sẽ nhân theo hàng và x thì chúng ta sẽ nhân theo cột thì kết quả của thao tác nhân này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=88y0wnXxCUo",
      "filename": "88y0wnXxCUo",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.3: Ôn tập nền tảng đại số tuyến tính (Part 3)",
      "chunk_id": 16,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Mô hình Continuous Bag of Words. Continuous Bag of Words sẽ đi đoán từ ở giữa khi cho trước từ xung quanh. Trước đây là cho trước từ ở giữa đoán từ xung quanh. Bây giờ mình sẽ đi đoán từ ở giữa khi biết từ xung quanh. Lúc này là mũi tên của mình sẽ là ngược lại. Từ những từ t-2, t-1, chúng ta sẽ đi đoán từ thứ t. Và chúng ta sẽ có công thức của hàm loss của mình là bằng trừ là bằng trung bình cộng. là âm của trung bình cộng, của log, của P(Wt | Wt-m, ..., Wt-1, Wt+1, ..., Wt+m). Lưu ý là từ đây,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=AkHEcgasvkw",
      "filename": "AkHEcgasvkw",
      "title": "[CS431 - Chương 6] Part 4_2: Mô hình Word2Vec",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:03"
    }
  },
  {
    "page_content": "..., Wt-1, Wt+1, ..., Wt+m). Lưu ý là từ đây, ở cái chỗ này nè, thì chúng ta sẽ không có Wt. Chúng ta sẽ không có Wt. Từ Wt-m nó sẽ nhảy lên Wt+1 luôn. Rồi, và ở đây thì tương tự như Skip-gram thì chúng ta cũng sẽ sử dụng một cái mạng Neural Network cho cái mô hình Continuous Bag of Words. Và dựa trên các cái đầu vào thì chúng ta sẽ tính cái vector h bằng cách đó là lấy tổng hoặc trung bình cộng của các cái vector tương ứng. Với mỗi khối này, nó sẽ ánh xạ từ x1k về một dạng vector, x2k về một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=AkHEcgasvkw",
      "filename": "AkHEcgasvkw",
      "title": "[CS431 - Chương 6] Part 4_2: Mô hình Word2Vec",
      "chunk_id": 1,
      "start_timestamp": "0:00:56",
      "end_timestamp": "0:01:58"
    }
  },
  {
    "page_content": "sẽ ánh xạ từ x1k về một dạng vector, x2k về một dạng vector, xk về một dạng vector. và chúng ta sẽ tính tổng tất cả các vector đó để tạo ra h. Và như vậy thì chúng ta sẽ có công thức là h sẽ là bằng W chuyển vị nhân với lại x1. cộng cho x2 cộng cho xt. Rồi, và qua đó thì chúng ta sẽ Tính lớp Output Layer bằng hàm tương tự như Skip-gram, y ngã, bằng softmax của W và W' chuyển vị. Cái này là tương tự Skip-gram. Và khi đó hàm lỗi của mình lúc này sẽ đơn giản hơn. Tại vì nó sẽ không phải tính trên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=AkHEcgasvkw",
      "filename": "AkHEcgasvkw",
      "title": "[CS431 - Chương 6] Part 4_2: Mô hình Word2Vec",
      "chunk_id": 2,
      "start_timestamp": "0:01:50",
      "end_timestamp": "0:03:18"
    }
  },
  {
    "page_content": "đơn giản hơn. Tại vì nó sẽ không phải tính trên tổng của dự đoán từ t-1, t-2 cho đến t+1, t+2 nữa. Mà nó chỉ đoán tại thời điểm thứ t. Và do đó thì cái vector này nó sẽ là bằng y ngã [chỉ số của Wt]. Thì nó sẽ là lấy cái phần tử có chỉ số là chỉ số của cái từ thứ Wt. Tức là cái từ ở giữa, cái từ ở giữa nó sẽ có một cái chỉ số. Vì vậy, từ đó sẽ có một cái chỉ số, ví dụ như từ đó nó nằm ở đây. Thì ở đây chúng ta sẽ tính là log của y ngã. Ví dụ trong vị trí này là 012, chỉ số của mình là 2. Tương",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=AkHEcgasvkw",
      "filename": "AkHEcgasvkw",
      "title": "[CS431 - Chương 6] Part 4_2: Mô hình Word2Vec",
      "chunk_id": 3,
      "start_timestamp": "0:03:08",
      "end_timestamp": "0:04:06"
    }
  },
  {
    "page_content": "vị trí này là 012, chỉ số của mình là 2. Tương ứng là vị trí của cái từ Wt. Thì khi đó chúng ta sẽ lấy y ngã[2]. Rồi, và chúng ta tối ưu hóa cái hàm loss này, chúng ta sẽ tìm được các bộ trọng số W và W'. thì tương tự như trong ký hiệu hồi nãy, chúng ta đã nói theta của mình nó sẽ bao gồm một bộ là W và W'. theta của mình sẽ bao gồm một bộ tham số. Rồi, bây giờ thì công việc đó là huấn luyện. chúng ta đã có hàm mô hình cho từng mô hình Skip-gram và Continuous Bag of Words. chúng ta có hàm loss",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=AkHEcgasvkw",
      "filename": "AkHEcgasvkw",
      "title": "[CS431 - Chương 6] Part 4_2: Mô hình Word2Vec",
      "chunk_id": 4,
      "start_timestamp": "0:04:02",
      "end_timestamp": "0:04:48"
    }
  },
  {
    "page_content": "và Continuous Bag of Words. chúng ta có hàm loss cho từng mô hình, vậy thì dữ liệu lấy ở đâu? Dữ liệu được thu thập từ các trang Wikipedia và những trang web mà uy tín khác. Sau đó thì chúng ta sẽ huấn luyện trong nhiều tuần với rất nhiều GPU. Thực sự để huấn luyện được các mô hình Skip-gram và Continuous Bag of Words này thì phải dựa trên sức mạnh tính toán của các tập đoàn công nghệ được trang bị rất nhiều những server siêu máy tính mới có thể thực hiện được. Cụ thể ở đây chúng ta biết là thư",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=AkHEcgasvkw",
      "filename": "AkHEcgasvkw",
      "title": "[CS431 - Chương 6] Part 4_2: Mô hình Word2Vec",
      "chunk_id": 5,
      "start_timestamp": "0:04:40",
      "end_timestamp": "0:05:37"
    }
  },
  {
    "page_content": "thực hiện được. Cụ thể ở đây chúng ta biết là thư viện của FastText là của Facebook, là nơi cung cấp rất nhiều word embedding cho các từ của các ngôn ngữ phổ biến nhất hiện nay. sau khi huấn luyện xong, chúng ta lưu ý là có ma trận W và W'. thì ma trận W sẽ chứa toàn bộ các word vector của các từ trong từ điển. Ví dụ, cái ma trận W của mình có kích thước là V nhân N. Đây là V và đây là N. Giả sử như chúng ta có một cái từ là 'k'. Chúng ta muốn biết cái word embedding của từ 'k' là gì. Thì chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=AkHEcgasvkw",
      "filename": "AkHEcgasvkw",
      "title": "[CS431 - Chương 6] Part 4_2: Mô hình Word2Vec",
      "chunk_id": 6,
      "start_timestamp": "0:05:28",
      "end_timestamp": "0:06:42"
    }
  },
  {
    "page_content": "cái word embedding của từ 'k' là gì. Thì chúng ta sẽ tra coi 'k' trong tập từ điển của mình. Chỉ số của từ 'k' xuất hiện trong từ điển tại vị trí thứ y. Tương ứng trong ma trận W, chúng ta sẽ dò đến vị trí thứ y. và chúng ta sẽ trích ra vector dòng này. Và cái vector này sẽ có n phần tử. Thì đây chính là cái word vector của từ 'k'. Đó chính là mô hình Word2Vec và lưu ý là với mỗi mô hình Skip-gram hoặc là Continuous Bag of Words, thì chúng ta sẽ có một ma trận riêng, tức là mỗi mô hình, chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=AkHEcgasvkw",
      "filename": "AkHEcgasvkw",
      "title": "[CS431 - Chương 6] Part 4_2: Mô hình Word2Vec",
      "chunk_id": 7,
      "start_timestamp": "0:06:32",
      "end_timestamp": "0:07:34"
    }
  },
  {
    "page_content": "có một ma trận riêng, tức là mỗi mô hình, chúng ta có thể tiếp cận bằng hai cách khác nhau. Mỗi mô hình sẽ cho sản sinh ra một bộ trọng số. Từ mỗi trọng số này thì chúng ta sẽ lấy vector biểu diễn cho từ đó. Và mỗi một vector biểu diễn của một từ tương ứng là một hàng trong ma trận W. Khi người ta trực quan hóa các vector biểu diễn của các từ trong không gian, thì người ta mới thấy là có những cái mối quan hệ rất là thú vị. ví dụ người ta vẽ cái... người ta biểu diễn các cái từ như là king,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=AkHEcgasvkw",
      "filename": "AkHEcgasvkw",
      "title": "[CS431 - Chương 6] Part 4_2: Mô hình Word2Vec",
      "chunk_id": 8,
      "start_timestamp": "0:07:29",
      "end_timestamp": "0:08:17"
    }
  },
  {
    "page_content": "cái... người ta biểu diễn các cái từ như là king, queen trong không gian và princess, hero, heroine, rồi he, she, v.v. thì chúng ta thấy là các cái từ mà có thể hiện cái vector ánh xạ từ queen sang king, Từ Princess sang Prince, rồi từ She sang He, hình như nó đều có cái vector giống nhau. Nó đều có cái vector giống nhau. Và bây giờ người ta sẽ nảy sinh ra một cái ý tưởng đó là, Nếu như mình lấy cái vector từ Men sang Woman, đúng không? Mình lấy cái vector Woman trừ cho vector Men, mình có cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=AkHEcgasvkw",
      "filename": "AkHEcgasvkw",
      "title": "[CS431 - Chương 6] Part 4_2: Mô hình Word2Vec",
      "chunk_id": 9,
      "start_timestamp": "0:08:11",
      "end_timestamp": "0:09:03"
    }
  },
  {
    "page_content": "cái vector Woman trừ cho vector Men, mình có cái vector này. Lấy vector này đem xuống đây, sau đó chúng ta sẽ lấy vector biểu diễn của từ king. Ánh xạ lên với cùng vector giống như là từ Men sang Woman thì hỏi Vector x ở đây nó sẽ là cái từ nào trong không gian embedding? Và công thức chúng ta sẽ mô hình hóa ý tưởng này dưới dạng công thức sau: Đó là Vector(woman) - Vector(man) = Vector(x) - Vector(king). Giả sử đây là hai cái vector biểu diễn nha. Đây chính là cái vector, đây là cái word",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=AkHEcgasvkw",
      "filename": "AkHEcgasvkw",
      "title": "[CS431 - Chương 6] Part 4_2: Mô hình Word2Vec",
      "chunk_id": 10,
      "start_timestamp": "0:08:57",
      "end_timestamp": "0:09:37"
    }
  },
  {
    "page_content": "nha. Đây chính là cái vector, đây là cái word vector của từ woman nha. Đây là word vector của từ man. Vector(woman) trừ cho Vector(man) là Vector(x) trừ cho Vector(king). từ đó chúng ta sẽ đem trừ king qua bên kia. Vector(x) là Vector(woman) trừ Vector(man) cộng Vector(king). Và như vậy chúng ta sẽ lấy vector biểu diễn của từ man, lấy vector biểu diễn của từ woman, từ man và từ king. chúng ta thực hiện với các công thức Vector(woman) trừ Vector(man) sau đó cộng cho Vector(king), chúng ta sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=AkHEcgasvkw",
      "filename": "AkHEcgasvkw",
      "title": "[CS431 - Chương 6] Part 4_2: Mô hình Word2Vec",
      "chunk_id": 11,
      "start_timestamp": "0:09:36",
      "end_timestamp": "0:10:25"
    }
  },
  {
    "page_content": "sau đó cộng cho Vector(king), chúng ta sẽ được vector. Và với cái vector này, thì chúng ta sẽ xem xem từ nào nằm gần với cái từ mà biểu diễn bởi cái vector x này nhất. thì rất là thú vị, đó chính là từ nữ hoàng. Thì ở đây nếu mà dịch sang một cái ngữ nghĩa nào đó, thì chúng ta có thể thấy là, nếu như cái người đàn ông mà quyền lực là vua, gọi là vua, thì hỏi người đàn bà quyền lực thì gọi là gì? thì người đàn bà quyền lực đó chính là nữ hoàng. Thì đây chính là một mối quan hệ về mặt ngữ nghĩa",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=AkHEcgasvkw",
      "filename": "AkHEcgasvkw",
      "title": "[CS431 - Chương 6] Part 4_2: Mô hình Word2Vec",
      "chunk_id": 12,
      "start_timestamp": "0:10:21",
      "end_timestamp": "0:11:24"
    }
  },
  {
    "page_content": "Thì đây chính là một mối quan hệ về mặt ngữ nghĩa rất là thú vị. Thì mối quan hệ ở đây chính là mối quan hệ về mặt giới tính. mối quan hệ về mặt giới tính. Và tương tự như vậy, thì chúng ta sẽ có các mối quan hệ ngữ nghĩa khác. Lấy ví dụ, apple và apples, thì ở đây nó chính là mối quan hệ về số ít, số nhiều. Số nhiều bên đây là số ít của một từ danh từ. Và với công thức này, Vector(x) sẽ bằng Vector(apples) trừ cho Vector(apple) cộng Vector(car). Như vậy thì câu hỏi đặt ra đó là, Từ nào gần với",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=AkHEcgasvkw",
      "filename": "AkHEcgasvkw",
      "title": "[CS431 - Chương 6] Part 4_2: Mô hình Word2Vec",
      "chunk_id": 13,
      "start_timestamp": "0:11:15",
      "end_timestamp": "0:12:25"
    }
  },
  {
    "page_content": "Như vậy thì câu hỏi đặt ra đó là, Từ nào gần với vector biểu diễn của vector x này nhất thì rất là thú vị. Đó chính là từ cars. Và tương tự như vậy cho các mối quan hệ về tính từ, và adverb. Rồi, mối quan hệ về đất nước và thủ đô, đây là thủ đô nè. Đây là đất nước nè. Thì nếu như thủ đô của Đức là Berlin thì thủ đô của Pháp là gì? Vì vậy, vector x này cũng cho ra được một kết quả rất là thú vị, đó chính là Paris. Rồi, chúng ta sẽ còn rất rất nhiều những mối quan hệ ngữ nghĩa khác và nó cũng đều",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=AkHEcgasvkw",
      "filename": "AkHEcgasvkw",
      "title": "[CS431 - Chương 6] Part 4_2: Mô hình Word2Vec",
      "chunk_id": 14,
      "start_timestamp": "0:12:17",
      "end_timestamp": "0:13:06"
    }
  },
  {
    "page_content": "những mối quan hệ ngữ nghĩa khác và nó cũng đều thỏa mãn được kiến thức trong thực tế như vậy. Một mô hình Word2Vec khi được train trên một kho dữ liệu cực kỳ lớn thì nó vẫn sẽ lưu được những cái thông tin, mối quan hệ khác bên cạnh mối quan hệ về mặt ngữ pháp. nó vẫn có những mối quan hệ khác nữa. Và mối quan hệ về thủ đô đất nước, mối quan hệ về so sánh hơn, rồi mối quan hệ về đất nước, rồi mối quan hệ về kim loại và ký hiệu và ký hiệu trong cái bảng phân loại tuần hoàn, rồi mối quan hệ về",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=AkHEcgasvkw",
      "filename": "AkHEcgasvkw",
      "title": "[CS431 - Chương 6] Part 4_2: Mô hình Word2Vec",
      "chunk_id": 15,
      "start_timestamp": "0:13:03",
      "end_timestamp": "0:13:50"
    }
  },
  {
    "page_content": "cái bảng phân loại tuần hoàn, rồi mối quan hệ về các công ty sản phẩm v.v. thì mô hình Word2Vec thể hiện được trong không gian embedding. Và như vậy thì chúng ta đã tìm hiểu về mô hình, một trong những mô hình rất quan trọng cho lĩnh vực xử lý ngôn ngữ tự nhiên trở về sau. Tại vì Word2Vec sẽ là đầu vào cho các mô hình máy học. Chúng ta sẽ phải biến các từ thành một dạng vector biểu diễn. Và có vector biểu diễn này rồi thì các mô hình máy học bản chất chính là các phép toán, Các hàm toán học",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=AkHEcgasvkw",
      "filename": "AkHEcgasvkw",
      "title": "[CS431 - Chương 6] Part 4_2: Mô hình Word2Vec",
      "chunk_id": 16,
      "start_timestamp": "0:13:46",
      "end_timestamp": "0:14:24"
    }
  },
  {
    "page_content": "bản chất chính là các phép toán, Các hàm toán học trên đại số tuyến tính. Thực hiện cộng trừ nhân chia thì nó phải thực hiện trên đối tượng vector này. Nó không thể nào thực hiện phép cộng trừ nhân chia với các từ ở dưới dạng là chuỗi được, nó phải chuyển sang dạng vector. Và từ nay trở về sau thì chúng ta sử dụng Word2Vec như là một trong những công cụ để làm đầu vào cho các mô hình về sau. Và có rất nhiều mô hình Word2Vec hiện nay và nổi tiếng và cho độ chính xác cao. Đó chính là GloVe, là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=AkHEcgasvkw",
      "filename": "AkHEcgasvkw",
      "title": "[CS431 - Chương 6] Part 4_2: Mô hình Word2Vec",
      "chunk_id": 17,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "và cho độ chính xác cao. Đó chính là GloVe, là viết tắt của chữ Global Vectors. Các thư viện của Python hiện giờ đều cho phép hỗ trợ GloVe embedding. Và đây là các tài liệu tham khảo sử dụng trong bài học của ngày hôm nay.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=AkHEcgasvkw",
      "filename": "AkHEcgasvkw",
      "title": "[CS431 - Chương 6] Part 4_2: Mô hình Word2Vec",
      "chunk_id": 18,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Tiếp theo, chúng ta sẽ xem tham số theta của mình. Thế thì muốn xem tham số theta thì mình sẽ phải cung cấp thêm cho nó một hàm nữa. Một cái phương thức nữa đó là self.get_weights(). Rồi, chúng ta sẽ return self.model.layers Thì layer số 0 là input, mình sẽ không xem layer đó, mà mình sẽ xem layer số 1, chính là lớp Dense input, thì mình sẽ không xem cái này, mà mình sẽ xem từ số 1 đến đây, chính là lớp Dense và get_weights() Chúng ta phải chạy lại và train lại mô hình này Cũng may là chương",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CqnM7BT7oSU",
      "filename": "CqnM7BT7oSU",
      "title": "[CS431 - Chương 2] Part 2b_3: Cài đặt mô hình linear regression",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:02"
    }
  },
  {
    "page_content": "lại và train lại mô hình này Cũng may là chương trình của mình chạy khá là nhanh Cảm ơn quý vị đã theo dõi, hãy đăng ký để ủng hộ kênh nhé! Ở đây nó có 2, nó sẽ có một Array, trong đó chúng ta có thể quan sát được nhanh, có 2 mảng con Vì kiến trúc của Keras tổng quát hơn, nó sẽ tách thành phần bias và thành phần trọng số của lớp Fully Connected, đó là lớp kết nối đầy đủ 3,13 là tham số cho phần kết nối đầy đủ này, không bao gồm bias và bias được lưu trong một mảng riêng 6.6 là tham số cho bias",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CqnM7BT7oSU",
      "filename": "CqnM7BT7oSU",
      "title": "[CS431 - Chương 2] Part 2b_3: Cài đặt mô hình linear regression",
      "chunk_id": 1,
      "start_timestamp": "0:00:58",
      "end_timestamp": "0:02:31"
    }
  },
  {
    "page_content": "lưu trong một mảng riêng 6.6 là tham số cho bias 3,13 là tham số cho phần trọng số Do ở đây là chúng ta chỉ có duy nhất một cái feature thôi Nên cái mảng này của chúng ta cũng sẽ có duy nhất một cái tham số thôi Bây giờ chúng ta sẽ lấy thành phần theta Không Nó sẽ là bằng W W[1], đó chính là cái thành phần bias Rồi Không Và theta 1 Nó sẽ là W[0] và nó sẽ phải thêm truy xuất cho hai cái phần tử hai cái phần tử là (0,0) Rồi, bây giờ chúng ta sẽ in ra theta 0, và theta 1 Giá trị đương nhiên sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CqnM7BT7oSU",
      "filename": "CqnM7BT7oSU",
      "title": "[CS431 - Chương 2] Part 2b_3: Cài đặt mô hình linear regression",
      "chunk_id": 2,
      "start_timestamp": "0:02:25",
      "end_timestamp": "0:03:31"
    }
  },
  {
    "page_content": "in ra theta 0, và theta 1 Giá trị đương nhiên sẽ giống như những gì chúng ta nhìn và bây giờ chúng ta sẽ trực quan hóa chúng ta sẽ cùng trực quan hóa Các bạn có thể thấy giá trị 3 và 6 đã khớp với 3 và 8 Sở dĩ không đạt được giá trị 3 và 8 là vì có một ít nhiễu (noise) theta 0 và theta 1 Mặc dù tham số có khác đôi chút, nhưng mô hình vẫn học được về đúng dạng đường thẳng. Tiếp theo thì chúng ta sẽ thử sử dụng các phương thức, ví dụ như là phương thức predict Trước khi sử dụng phương thức",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CqnM7BT7oSU",
      "filename": "CqnM7BT7oSU",
      "title": "[CS431 - Chương 2] Part 2b_3: Cài đặt mô hình linear regression",
      "chunk_id": 3,
      "start_timestamp": "0:03:28",
      "end_timestamp": "0:04:55"
    }
  },
  {
    "page_content": "phương thức predict Trước khi sử dụng phương thức predict thì chúng ta sẽ lưu model này xuống Rồi, chúng ta sẽ truyền vô một đường dẫn, ví dụ như là MyModel.h5 Đặt tên gì cũng được Chúng ta sẽ quan sát cái thư mục ở đây Nó tạo ra là MyModel trong model này sẽ có các file đi kèm sẽ có các file bổ trợ đi kèm Bây giờ chúng ta sẽ load model này lên để minh chứng cho việc load hoàn toàn từ file chúng ta sẽ tạo ra một cái biến mới ở đây sẽ là new_model sau đó, new_model.load từ model đã được lưu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CqnM7BT7oSU",
      "filename": "CqnM7BT7oSU",
      "title": "[CS431 - Chương 2] Part 2b_3: Cài đặt mô hình linear regression",
      "chunk_id": 4,
      "start_timestamp": "0:04:51",
      "end_timestamp": "0:06:05"
    }
  },
  {
    "page_content": "sau đó, new_model.load từ model đã được lưu trước đó là MyModel sau đó, chúng ta sẽ cùng predict ví dụ như chúng ta tính giá trị là tại 7 khi chúng ta dóng lên, thì chiếu bên đây đâu đó phải ra là 27 hay 28 gì đấy thì nó mới đúng, bây giờ chúng ta sẽ truyền vô giá trị là 7 Rồi, nó báo sai ở cái dòng này ở đây nó sẽ không thể truyền vào giá trị Scalar mà chúng ta phải truyền vào giá trị dạng Numpy Array np.array x.reshape(-1, 1) x là 7 rồi chúng ta sẽ để chạy thử Vậy thì nó sẽ là 28 đúng như hồi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CqnM7BT7oSU",
      "filename": "CqnM7BT7oSU",
      "title": "[CS431 - Chương 2] Part 2b_3: Cài đặt mô hình linear regression",
      "chunk_id": 5,
      "start_timestamp": "0:06:01",
      "end_timestamp": "0:07:19"
    }
  },
  {
    "page_content": "sẽ để chạy thử Vậy thì nó sẽ là 28 đúng như hồi nãy chúng ta dự đoán nếu giá trị 7 chiếu lên trên đường thẳng này sau đó chiếu qua đây thì nó sẽ phải ra giá trị 27-28 với mô hình của mình là 28.6 Vì vậy, qua cái demo này, chúng ta đã tiến hành cài đặt mô hình Linear Regression với 3 phiên bản. Phiên bản đầu tiên, đó chính là phiên bản tham số rời rạc. Thì cái phiên bản này có một cái điểm yếu, đó chính là chúng ta sẽ phải đi triển khai cho từng tham số. Thì điều gì xảy ra nếu như mô hình của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CqnM7BT7oSU",
      "filename": "CqnM7BT7oSU",
      "title": "[CS431 - Chương 2] Part 2b_3: Cài đặt mô hình linear regression",
      "chunk_id": 6,
      "start_timestamp": "0:07:10",
      "end_timestamp": "0:07:55"
    }
  },
  {
    "page_content": "tham số. Thì điều gì xảy ra nếu như mô hình của mình lên đến hàng triệu tham số Tức là chúng ta sẽ phải cập nhật cái này hàng triệu lần Tức là 1 triệu tham số thì chúng ta sẽ phải có 1 triệu dòng cập nhật như thế này Rất là bất tiện. Vì vậy chúng ta phải chuyển sang dạng thứ 2, đó là dạng vector hóa Vector hóa này thì mỗi tham số sẽ được đóng gói trong một biến theta Tuy nhiên thì cách làm này nó lại có một điểm yếu đó là chúng ta phải đi tính cái công thức chúng ta sẽ phải đi tính công thức",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CqnM7BT7oSU",
      "filename": "CqnM7BT7oSU",
      "title": "[CS431 - Chương 2] Part 2b_3: Cài đặt mô hình linear regression",
      "chunk_id": 7,
      "start_timestamp": "0:07:50",
      "end_timestamp": "0:08:27"
    }
  },
  {
    "page_content": "cái công thức chúng ta sẽ phải đi tính công thức đạo hàm một cách tường minh. Trong khi đó, với cái phiên bản mà dùng Keras, chúng ta có thể quan sát thấy ở trong cái mã nguồn của mình không hề có một bước nào đi tính đạo hàm hết, mà mình chỉ quy định cho nó kiến trúc là đầu vào, kích thước bao nhiêu, thực hiện phép biến đổi gì, activation là gì, rồi có sử dụng bias hay không, kết thúc. Rồi mình quy ước cho nó là sử dụng hàm lỗi (loss function) là gì. và thậm chí chúng ta cũng không cần phải",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CqnM7BT7oSU",
      "filename": "CqnM7BT7oSU",
      "title": "[CS431 - Chương 2] Part 2b_3: Cài đặt mô hình linear regression",
      "chunk_id": 8,
      "start_timestamp": "0:08:24",
      "end_timestamp": "0:09:07"
    }
  },
  {
    "page_content": "là gì. và thậm chí chúng ta cũng không cần phải cài đặt hàm lỗi Nó cũng đã có một số hàm lỗi phổ biến rồi như MSE, Cross-Entropy Rồi chúng ta cũng sẽ chỉ cho nó biết optimizer là gì và chuyện còn lại là Deep Learning framework nó sẽ tự tính toán đạo hàm, sẽ tự cập nhật cho mình Đây chính là điểm lợi của việc dùng Keras Và từ nay trở về sau, từ bài logistic trở đi chúng ta sẽ sử dụng cách cài đặt này cho nó đơn giản và việc tính đạo hàm đã được Deep Learning Framework ngầm thực hiện cho chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CqnM7BT7oSU",
      "filename": "CqnM7BT7oSU",
      "title": "[CS431 - Chương 2] Part 2b_3: Cài đặt mô hình linear regression",
      "chunk_id": 9,
      "start_timestamp": "0:08:59",
      "end_timestamp": "0:09:12"
    }
  },
  {
    "page_content": "Learning Framework ngầm thực hiện cho chúng ta rồi. Chúng ta chỉ tập trung vào việc xây dựng mô hình mà thôi.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CqnM7BT7oSU",
      "filename": "CqnM7BT7oSU",
      "title": "[CS431 - Chương 2] Part 2b_3: Cài đặt mô hình linear regression",
      "chunk_id": 10,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề tập 3. Nền tảng toán Tiếp theo, chúng ta sẽ cùng ôn tập nền tảng toán. Như đã gặp trước đây, đại số tuyến tính và giải tích là 2 nền tảng toán rất quan trọng và liên quan trực tiếp đến mô hình này. Đầu tiên, chúng ta sẽ đặt câu hỏi là tại sao chúng ta cần phải có kiến thức đại số tuyến tính. Công dụng đầu tiên của nó chính là dùng để biểu diễn dữ liệu. Như chúng ta đã biết, trong lĩnh vực về máy học, công cụ, thành phần đầu tiên chúng ta phải có chính là dữ liệu. Và làm sao để dữ liệu này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CYWH9ZBN5Gc",
      "filename": "CYWH9ZBN5Gc",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.1: Ôn tập nền tảng đại số tuyến tính (Part 1)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:54"
    }
  },
  {
    "page_content": "có chính là dữ liệu. Và làm sao để dữ liệu này có khả năng tính toán được thì chúng ta phải dùng các công cụ đại số. thì ở đây, từ thấp lên cao thì chúng ta sẽ có các khái niệm tensor 0 chiều tên gọi khác chính là scalar, tensor 1 chiều, tên gọi khác là vector, tensor 2 chiều, tên gọi khác là ma trận và tensor N chiều, hay là tensor nhiều chiều thì các khái niệm scalar, vector, ma trận và tensor nhiều chiều này nó sẽ được sử dụng để biểu diễn cho những loại dữ liệu khác nhau thì chi tiết chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CYWH9ZBN5Gc",
      "filename": "CYWH9ZBN5Gc",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.1: Ôn tập nền tảng đại số tuyến tính (Part 1)",
      "chunk_id": 1,
      "start_timestamp": "0:00:48",
      "end_timestamp": "0:01:29"
    }
  },
  {
    "page_content": "những loại dữ liệu khác nhau thì chi tiết chúng ta sẽ cùng thảo luận trong những slide tiếp theo. Một công dụng khác đó chính là dùng để biến đổi dữ liệu. Sau khi chúng ta đã biểu diễn rồi, thì chúng ta sẽ biến dữ liệu đó thành dữ liệu phái sinh, hay còn gọi là đặc trưng. Các đặc trưng sẽ giúp cho mô hình máy học có thể hiểu dữ liệu ở nhiều khía cạnh khác nhau là sử dụng các đặc trưng này để phục vụ cho mục đích của các bài toán ví dụ như mục đích phân lớp, ví dụ như là mục đích chuyển đổi từ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CYWH9ZBN5Gc",
      "filename": "CYWH9ZBN5Gc",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.1: Ôn tập nền tảng đại số tuyến tính (Part 1)",
      "chunk_id": 2,
      "start_timestamp": "0:01:24",
      "end_timestamp": "0:02:09"
    }
  },
  {
    "page_content": "phân lớp, ví dụ như là mục đích chuyển đổi từ dữ liệu dạng này sang dạng khác. Đầu tiên chúng ta sẽ tìm hiểu về khái niệm scalar hay là tensor 0 chiều. Bản chất của các giá trị scalar đó chính là các số thực. Nếu như chúng ta biểu diễn dưới dạng là các ký hiệu toán học thì scalar sẽ là biểu diễn bởi một cái ký tự viết bình thường và scalar thì thường được sử dụng để biểu diễn cho các cái đại lượng vô hướng tức là không có hướng, ví dụ như là tốc độ thể hiện sự nhanh chậm, khối lượng thể hiện sự",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CYWH9ZBN5Gc",
      "filename": "CYWH9ZBN5Gc",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.1: Ôn tập nền tảng đại số tuyến tính (Part 1)",
      "chunk_id": 3,
      "start_timestamp": "0:02:04",
      "end_timestamp": "0:02:57"
    }
  },
  {
    "page_content": "độ thể hiện sự nhanh chậm, khối lượng thể hiện sự nặng nhẹ chiều dài thể hiện chiều dài là ngắn dài và những cái tên gọi khác nó không phổ biến lắm của tensor 0 chiều hay là scalar đó chính là scalar tensor, zero-dimensional tensor, 0D tensor thì các cái tên gọi này ít được gặp và trong thư viện NumPy hoặc là Python thì chúng ta sẽ có hai cái thông số rất là quan trọng hay là hai thuộc tính hai thuộc tính rất là quan trọng đó chính là ndim và shape dựa vào ndim hoặc shape thì sẽ giúp chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CYWH9ZBN5Gc",
      "filename": "CYWH9ZBN5Gc",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.1: Ôn tập nền tảng đại số tuyến tính (Part 1)",
      "chunk_id": 4,
      "start_timestamp": "0:02:49",
      "end_timestamp": "0:03:40"
    }
  },
  {
    "page_content": "dựa vào ndim hoặc shape thì sẽ giúp chúng ta kiểm tra xem cái biến của mình nó là kiểu scalar hay không thì đầu tiên chúng ta sẽ khởi tạo một cái biến x là np.array là có giá trị là 12 và x.ndim thì nó sẽ có giá trị là 0 và x.shape thì nó sẽ có giá trị là rỗng Để kiểm tra giá trị scalar, chúng ta sẽ kiểm tra xem các giá trị ndim có bằng 0 hay không, hoặc là shape có bằng rỗng hay không Tiếp theo, đó chính là khái niệm Vector hay là tensor 1 chiều Chúng ta sẽ xét một cái ví dụ, đó là chúng ta sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CYWH9ZBN5Gc",
      "filename": "CYWH9ZBN5Gc",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.1: Ôn tập nền tảng đại số tuyến tính (Part 1)",
      "chunk_id": 5,
      "start_timestamp": "0:03:38",
      "end_timestamp": "0:04:24"
    }
  },
  {
    "page_content": "Chúng ta sẽ xét một cái ví dụ, đó là chúng ta sẽ có một cái bảng dữ liệu về các thông tin của các nhà trong một khu phố Và thông tin này thì sẽ bao gồm là diện tích, số phòng ngủ và giá Thì chúng ta sẽ lấy ra một dòng dữ liệu Thì các thông tin về một dòng dữ liệu Nó sẽ hình thành một vector Và cái vector này sẽ là đại diện cho căn nhà thứ 3 Để biểu diễn cho căn nhà này, chúng ta sẽ dùng một vector 3 chiều trong đó bao gồm 3 giá trị là 2430369 thì tương ứng nó chính là diện tích số phòng ngủ và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CYWH9ZBN5Gc",
      "filename": "CYWH9ZBN5Gc",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.1: Ôn tập nền tảng đại số tuyến tính (Part 1)",
      "chunk_id": 6,
      "start_timestamp": "0:04:21",
      "end_timestamp": "0:05:21"
    }
  },
  {
    "page_content": "tương ứng nó chính là diện tích số phòng ngủ và giá trị của căn nhà về ký hiệu thì khác với lại scalar, vector sẽ được ký hiệu viết nhỏ và in đậm sẽ là viết thường và tô đậm Nếu mà scalar thì nó sẽ là viết thường nhưng mà không có tô đậm Còn vector thì sẽ là viết thường nhưng mà có in đậm Và số chiều của mình ở đây sẽ là N Tương ứng là số phần tử của vector Và chiều không gian của mình sẽ có 1 chiều Và vector thì thường sẽ bao gồm một tập hợp các giá trị scalar, ví dụ đây là vector x, vector x",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CYWH9ZBN5Gc",
      "filename": "CYWH9ZBN5Gc",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.1: Ôn tập nền tảng đại số tuyến tính (Part 1)",
      "chunk_id": 7,
      "start_timestamp": "0:05:18",
      "end_timestamp": "0:06:11"
    }
  },
  {
    "page_content": "giá trị scalar, ví dụ đây là vector x, vector x sẽ gồm có N giá trị và mỗi giá trị này chính là 1 giá trị scalar về mặt ký hiệu, vector sẽ thường biểu diễn dưới dạng cột trong các tài liệu chúng ta thấy vector sẽ biểu diễn dưới dạng cột Về lập trình, chúng ta sẽ khởi tạo một vector là np.array hay NumPy array và tạo ra một list, trong đó tham số của mình sẽ là một list cái list gồm các giá trị là 12, 3, 6 và 14 và list này sẽ được tự động ép kiểu sang kiểu vector và để kiểm tra số chiều cũng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CYWH9ZBN5Gc",
      "filename": "CYWH9ZBN5Gc",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.1: Ôn tập nền tảng đại số tuyến tính (Part 1)",
      "chunk_id": 8,
      "start_timestamp": "0:06:06",
      "end_timestamp": "0:06:50"
    }
  },
  {
    "page_content": "sang kiểu vector và để kiểm tra số chiều cũng như là số phần tử Trong cái vector này thì chúng ta sẽ dùng hai cái phương thức, đó là ndim và shape Với cái vector ví dụ ở đây, số chiều của mình sẽ là 1 Tức là chiều không gian thì chúng ta sẽ có 1 chiều Nhưng mà số phần tử thì chúng ta sẽ có là 4 Đối với vector thì chúng ta sẽ chú ý như vậy Và vector dùng để biểu diễn cái gì? Vector thì thường sử dụng để biểu diễn văn bản Ví dụ như chúng ta có mô hình là Word2Vec là biểu diễn các cái câu trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CYWH9ZBN5Gc",
      "filename": "CYWH9ZBN5Gc",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.1: Ôn tập nền tảng đại số tuyến tính (Part 1)",
      "chunk_id": 9,
      "start_timestamp": "0:06:47",
      "end_timestamp": "0:07:44"
    }
  },
  {
    "page_content": "hình là Word2Vec là biểu diễn các cái câu trong mỗi một cái từ trong văn bản dưới dạng là một vector Ví dụ như từ King sẽ biểu diễn bởi vector 4 chiều này từ Man sẽ biểu diễn bởi vector 4 chiều này Và trong một số các cái mô hình biểu diễn khác Ví dụ như mô hình Bag of Words Mỗi một đoạn văn hoặc văn bản sẽ được biểu diễn bởi một vector nhiều chiều Đây là ứng dụng của vector trong biểu diễn dữ liệu Ngoài ra, đối với dữ liệu dạng sóng, ví dụ sóng âm thanh có thể biểu diễn dưới dạng là một mảng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CYWH9ZBN5Gc",
      "filename": "CYWH9ZBN5Gc",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.1: Ôn tập nền tảng đại số tuyến tính (Part 1)",
      "chunk_id": 10,
      "start_timestamp": "0:07:40",
      "end_timestamp": "0:08:31"
    }
  },
  {
    "page_content": "âm thanh có thể biểu diễn dưới dạng là một mảng hoặc vector thì đây là một hình minh họa cho sóng âm thanh và nó sẽ được lấy mẫu và biểu diễn dưới dạng là vector trong máy tính Tiếp theo thì chúng ta sẽ nói về khái niệm ma trận thì cũng lấy ví dụ trong ví dụ trước là tập hợp các căn nhà trong khu phố thì ở đây nếu như chúng ta lấy hết toàn bộ tất cả các căn nhà này ra để biểu diễn thì đây chính là một cái ma trận để đại diện cho thông tin của các căn nhà trong một khu phố thì đây là ma trận và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CYWH9ZBN5Gc",
      "filename": "CYWH9ZBN5Gc",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.1: Ôn tập nền tảng đại số tuyến tính (Part 1)",
      "chunk_id": 11,
      "start_timestamp": "0:08:24",
      "end_timestamp": "0:09:10"
    }
  },
  {
    "page_content": "căn nhà trong một khu phố thì đây là ma trận và nếu xét về ký hiệu thì ma trận được biểu diễn bởi một ký tự viết hoa nhưng mà không tô đậm và ở đây chúng ta sẽ thấy là nó sẽ có hai chiều thành phần không gian và chiều thứ nhất thì sẽ gồm có M phần tử chiều thứ 2 thì sẽ có N phần tử thì một cách nói khác đó chính là đây là một ma trận 2 chiều không gian và gồm M dòng N cột và ma trận thì có thể được hiểu đó là một tập hợp các cái vector cùng kích thước ví dụ như đây là một vector đây là một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CYWH9ZBN5Gc",
      "filename": "CYWH9ZBN5Gc",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.1: Ôn tập nền tảng đại số tuyến tính (Part 1)",
      "chunk_id": 12,
      "start_timestamp": "0:09:07",
      "end_timestamp": "0:10:04"
    }
  },
  {
    "page_content": "kích thước ví dụ như đây là một vector đây là một vector và đây là một vector tất cả cái vector này đều có cùng kích thước đó là gồm có M chiều và trong lập trình Python thì đây là cách thức để chúng ta khai báo một cái ma trận lưu ý là ở đây chúng ta sẽ có hai cái dấu ngoặc vuông ở đây và tương ứng thì chúng ta sẽ kiểm tra xem một cái biến nó có phải là ma trận hay không thông qua cái phương thức là ndim và shape Trong cái ví dụ này chúng ta thấy số chiều của mình sẽ là 2, trong đó số phần tử",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CYWH9ZBN5Gc",
      "filename": "CYWH9ZBN5Gc",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.1: Ôn tập nền tảng đại số tuyến tính (Part 1)",
      "chunk_id": 13,
      "start_timestamp": "0:09:46",
      "end_timestamp": "0:10:39"
    }
  },
  {
    "page_content": "số chiều của mình sẽ là 2, trong đó số phần tử theo từng chiều đó là 3 và 4 Thì đây là cách thức để chúng ta lập trình sử dụng Python để tạo ra một ma trận Tiếp theo thì chúng ta sẽ cùng thảo luận về việc sử dụng ma trận để biểu diễn dữ liệu thì nếu như là ma trận, thì chúng ta có thể dùng ma trận để biểu diễn cho một cái ảnh mức xám thì chúng ta thấy cái tấm hình này là một cái dạng phóng to và với cái độ phân giải thấp hình của một cái gương mặt người và nếu như mỗi một cái ô này, chúng ta sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CYWH9ZBN5Gc",
      "filename": "CYWH9ZBN5Gc",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.1: Ôn tập nền tảng đại số tuyến tính (Part 1)",
      "chunk_id": 14,
      "start_timestamp": "0:10:37",
      "end_timestamp": "0:11:21"
    }
  },
  {
    "page_content": "người và nếu như mỗi một cái ô này, chúng ta sẽ biểu diễn bởi một cái số và cái số này nó có cái miền giá trị là từ 0 cho đến 255 hay còn gọi là chúng ta sẽ sử dụng một byte dữ liệu đó để biểu diễn thì như vậy từng cái phần tử ở đây nó sẽ được minh họa cho một con số và cái số này người ta còn gọi là mức xám số mà càng nhỏ thì màu sẽ càng tối số sẽ càng lớn thì màu sẽ càng sáng ví dụ như chúng ta thấy ở đây cái vị trí này là giá trị gần với lại con số là 255 thì giá trị này là 251 và nó tương",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CYWH9ZBN5Gc",
      "filename": "CYWH9ZBN5Gc",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.1: Ôn tập nền tảng đại số tuyến tính (Part 1)",
      "chunk_id": 15,
      "start_timestamp": "0:11:18",
      "end_timestamp": "0:11:29"
    }
  },
  {
    "page_content": "con số là 255 thì giá trị này là 251 và nó tương ứng là cho vùng màu sáng Còn giá trị ở đây, ví dụ như là 10 thì chúng ta thấy là nó gần với số 0, tức là vùng màu tối Và đây sẽ là cái biểu diễn dưới dạng ma trận của tấm ảnh gốc đầu vào này Như vậy thì chúng ta thấy là cái công dụng của ma trận Công dụng đầu tiên của ma trận đó chính là để biểu diễn hình ảnh Và lưu ý ảnh ở đây là ảnh không có màu, ảnh mức xám",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CYWH9ZBN5Gc",
      "filename": "CYWH9ZBN5Gc",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.1: Ôn tập nền tảng đại số tuyến tính (Part 1)",
      "chunk_id": 16,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chúng ta sẽ tiến hành cài đặt một cái mạng Neural Network. Đặc tính của cái mạng Neural Network là nó giúp chúng ta giải quyết được các bài toán non-linear. Trong trường hợp này, chúng ta sẽ lấy tình huống đơn giản nhất của non-linear là các tập điểm hình tam giác và hình tròn. thì hai tập điểm này không thể chia tách ra được bởi một đường thẳng do đó thì đường đúng sẽ phải là một đường tròn như thế này thì nó mới có thể phân ra làm hai phần riêng biệt được Thì đây là một bài toán phi tuyến và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DGNdZGdwihs",
      "filename": "DGNdZGdwihs",
      "title": "[CS431 - Chương 2] Part 5b_1: Cài đặt mạng neural network",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:50"
    }
  },
  {
    "page_content": "biệt được Thì đây là một bài toán phi tuyến và để giải quyết bài toán này thì chúng ta cũng không cần thiết phải sử dụng một mạng Neural Network quá phức tạp thì ở đây chỉ cần có một lớp ẩn thôi Đây là một hidden layer. Mạng Neural Network đúng của chúng ta có thể có một, hai hoặc rất nhiều hidden layer. Nhưng trong trường hợp này, chúng ta chỉ cần một hidden layer. Thứ hai là tập điểm này chỉ có hai thành phần. Vì vậy, ở đây chúng ta sẽ có duy nhất một node output cuối cùng. ở đây là chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DGNdZGdwihs",
      "filename": "DGNdZGdwihs",
      "title": "[CS431 - Chương 2] Part 5b_1: Cài đặt mạng neural network",
      "chunk_id": 1,
      "start_timestamp": "0:00:44",
      "end_timestamp": "0:01:28"
    }
  },
  {
    "page_content": "nhất một node output cuối cùng. ở đây là chúng ta sẽ có một lớp input, một hidden layer và một output và cái output này do giá trị của mình chỉ có một phân lớp, xin gọi nó có hai phân lớp nên ở đây chúng ta không sử dụng hàm Softmax mà chúng ta sẽ sử dụng hàm Sigmoid tại vì Sigmoid sẽ đưa miền giá trị của mình về đoạn từ 0 cho đến 1 Lúc này giá trị y và y_hat này sẽ sử dụng độ đo là binary cross-entropy Đây là biến thể đơn giản của mạng Neural Network Tiếp theo, chúng ta sẽ tiến hành cài đặt",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DGNdZGdwihs",
      "filename": "DGNdZGdwihs",
      "title": "[CS431 - Chương 2] Part 5b_1: Cài đặt mạng neural network",
      "chunk_id": 2,
      "start_timestamp": "0:01:19",
      "end_timestamp": "0:02:17"
    }
  },
  {
    "page_content": "Network Tiếp theo, chúng ta sẽ tiến hành cài đặt cho ví dụ này Rồi, thì cũng tương tự, chúng ta sẽ có đoạn code để khởi tạo cho các tập điểm nằm trong và nằm bên ngoài vùng tròn Thì ở đây chúng ta có một thư viện là scikit-learn Nó sẽ có cái hàm, gọi là hàm `make_circles` Và cái hàm `make_circles` này thì nó sẽ giúp chúng ta tạo ra các cái điểm nằm trong và nằm ngoài hình tròn Các cái điểm nằm trong thì chúng ta sẽ được đánh dấu bằng màu đỏ Và các cái điểm nằm ngoài được đánh dấu bằng các cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DGNdZGdwihs",
      "filename": "DGNdZGdwihs",
      "title": "[CS431 - Chương 2] Part 5b_1: Cài đặt mạng neural network",
      "chunk_id": 3,
      "start_timestamp": "0:02:12",
      "end_timestamp": "0:03:00"
    }
  },
  {
    "page_content": "các cái điểm nằm ngoài được đánh dấu bằng các cái điểm màu xanh lá Và các điểm có nhãn `y` thì được đánh dấu bằng màu đỏ Và những cái điểm nào màu đỏ thì sẽ được gắn nhãn là bằng 0 Và những cái điểm nào màu xanh lá thì sẽ được gắn nhãn là bằng 1 Và tất cả thì đều được ép về kiểu số thực Rồi, thì `x` của mình, cái tọa độ `x` của mình nó chính là tập dữ liệu tọa độ theo trục x1 và x2 Tức là bao gồm hai chiều Y là cái nhãn, hoặc là nhận giá trị 0, hoặc là nhận giá trị là 1. Bây giờ về phần cài đặt",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DGNdZGdwihs",
      "filename": "DGNdZGdwihs",
      "title": "[CS431 - Chương 2] Part 5b_1: Cài đặt mạng neural network",
      "chunk_id": 4,
      "start_timestamp": "0:02:55",
      "end_timestamp": "0:03:46"
    }
  },
  {
    "page_content": "là nhận giá trị là 1. Bây giờ về phần cài đặt thuật toán. Cũng tương tự cho các mô hình Logistic Regression và Support Vector Machine, chúng ta sẽ có một bộ khung. Ở đây, chúng ta sẽ có một hàm `get_weights`, đó là hàm `get_weights`. Triển khai hàm `get_weights` này thì chúng ta phải viết lại so với các mô hình trước đây Tại vì trong mạng Neural Network thì chúng ta sẽ có nhiều layer Nếu chúng ta muốn quan sát tham số của layer nào thì chúng ta phải truyền thêm chỉ số của layer đó vào Vì vậy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DGNdZGdwihs",
      "filename": "DGNdZGdwihs",
      "title": "[CS431 - Chương 2] Part 5b_1: Cài đặt mạng neural network",
      "chunk_id": 5,
      "start_timestamp": "0:03:39",
      "end_timestamp": "0:04:41"
    }
  },
  {
    "page_content": "phải truyền thêm chỉ số của layer đó vào Vì vậy chúng ta sẽ có thêm một phương thức để viết lại phương thức `get_weights` này Bây giờ, đối với hàm `build`, chúng ta sẽ có Input Dimension và Output Dimension Chúng ta cũng tương tự cài đặt, là `Input`, với `Shape` là bằng `Input` Tiếp theo là lớp Hidden Layer, để tạo ra `hidden` Lớp Hidden Layer sẽ được thực hiện bởi phép biến đổi Fully Connected Từ lớp input sang lớp Hidden này, nó được kết nối đầy đủ Activation là sử dụng hàm Sigmoid và sử dụng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DGNdZGdwihs",
      "filename": "DGNdZGdwihs",
      "title": "[CS431 - Chương 2] Part 5b_1: Cài đặt mạng neural network",
      "chunk_id": 6,
      "start_timestamp": "0:04:26",
      "end_timestamp": "0:06:06"
    }
  },
  {
    "page_content": "đủ Activation là sử dụng hàm Sigmoid và sử dụng Bias Layer là Dense Output là 8 nodes Activation là Sigmoid UseBias là True Chúng ta phải truyền đầu vào cho nó là `hidden` Và với Output Layer, chúng ta lại một lần nữa Một lần nữa thì chúng ta sẽ đưa qua lớp biến đổi là Fully Connected tại vì bản chất Tất cả các cái node đầu vào và các cái node đầu ra thì nó kết nối đầy đủ với nhau Và ở đây nó cũng là một cái Dense layer Và cái Dense này thì cái Output của mình chỉ có duy nhất một node tại sao",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DGNdZGdwihs",
      "filename": "DGNdZGdwihs",
      "title": "[CS431 - Chương 2] Part 5b_1: Cài đặt mạng neural network",
      "chunk_id": 7,
      "start_timestamp": "0:06:00",
      "end_timestamp": "0:07:01"
    }
  },
  {
    "page_content": "Output của mình chỉ có duy nhất một node tại sao một node? tại vì ở đây chúng ta phân lớp nhị phân rồi ở đây sẽ có là Output là bằng Dense, trong đó chỉ có một node Activation thì chúng ta sẽ để là Sigmoid rồi sử dụng bias bằng True và input của nó chính là cái `hidden` ở phía trước Bây giờ chúng ta sẽ đóng gói cả cái này vào trong cái biến `model` và chúng ta sẽ trả về cho `self.model` Ở đây thì chúng ta sẽ không cần phải return cái gì ra bên ngoài Rồi, tương tự như vậy, ở đây chúng ta sẽ có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DGNdZGdwihs",
      "filename": "DGNdZGdwihs",
      "title": "[CS431 - Chương 2] Part 5b_1: Cài đặt mạng neural network",
      "chunk_id": 8,
      "start_timestamp": "0:06:57",
      "end_timestamp": "0:08:24"
    }
  },
  {
    "page_content": "ngoài Rồi, tương tự như vậy, ở đây chúng ta sẽ có optimizer sẽ là bằng `tf.keras.optimizers.SGD` với learning rate là bằng 0.1 để train cho nó nhanh và ở đây chúng ta sẽ có sử dụng momentum momentum sẽ giúp cho quá trình huấn luyện của mình nhanh hơn là bằng 0.9 Thông thường mặc định sẽ là 0, nhưng theo kinh nghiệm momentum sẽ là 0.9 Bây giờ chúng ta sẽ `compile` vào `model` Optimizer là `optimizer` Loss sẽ là `tf.keras.losses.BinaryCrossentropy` Chúng ta sẽ có thêm một tham số nữa, đó là số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DGNdZGdwihs",
      "filename": "DGNdZGdwihs",
      "title": "[CS431 - Chương 2] Part 5b_1: Cài đặt mạng neural network",
      "chunk_id": 9,
      "start_timestamp": "0:07:58",
      "end_timestamp": "0:09:38"
    }
  },
  {
    "page_content": "Chúng ta sẽ có thêm một tham số nữa, đó là số epoch, sẽ có thêm số epoch nữa, Tham số `num_epochs` sẽ là bằng `num_epochs`. Đối với hàm `get_weights`, chúng ta phải truyền vào layer số mấy Chúng ta sẽ truyền vào `self.model.layers` và truyền vào chỉ số của layer Rồi, chấm `get_weights` Rồi, bây giờ chúng ta sẽ tiến hành chạy thử đoạn training này, may quá không có lỗi Và để khởi tạo thì chúng ta sẽ tạo một đối tượng tên là `NeuralNetwork` Rồi, `neural_network.build` Chúng ta sẽ truyền vào số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DGNdZGdwihs",
      "filename": "DGNdZGdwihs",
      "title": "[CS431 - Chương 2] Part 5b_1: Cài đặt mạng neural network",
      "chunk_id": 10,
      "start_timestamp": "0:09:30",
      "end_timestamp": "0:11:25"
    }
  },
  {
    "page_content": "`neural_network.build` Chúng ta sẽ truyền vào số input là 2, số output là 1, tại vì chúng ta phân lớp nhị phân Rồi, chấm `summary()` Ở đây mô hình này có tất cả 33 tham số, trong đó có một lớp ẩn và một lớp output cuối cùng Lớp ẩn thì gồm có 8 neuron Chúng ta để số epoch là 1000, nhưng chúng ta giảm số epoch này xuống, vì chúng ta dùng momentum, nên bước cập nhật của mình rất nhanh Rồi, ta không hiểu `epoch` Mình sẽ gọi vào hàm `fit` `self.model.fit` và sẽ `fit` `X_train` và `y_train` cùng số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DGNdZGdwihs",
      "filename": "DGNdZGdwihs",
      "title": "[CS431 - Chương 2] Part 5b_1: Cài đặt mạng neural network",
      "chunk_id": 11,
      "start_timestamp": "0:11:17",
      "end_timestamp": "0:12:41"
    }
  },
  {
    "page_content": "và sẽ `fit` `X_train` và `y_train` cùng số lượng epoch Rồi, chúng ta sẽ train lại bắt đầu chúng ta thấy là loss rất là cao là 0.7 sau đó thì loss đã giảm xuống còn dưới 0.1 nó đã ra là 0.28 Bây giờ chúng ta sẽ vẽ `history` Quá trình Train này thực hiện rất nhanh, chúng ta sẽ chạy lại cái này Chúng ta sẽ để là `history` Chút nữa chúng ta sẽ quan sát giá trị loss đã giảm như thế nào Chúng ta sẽ trả về loss của quá trình train Chúng ta thấy giá trị của loss liên tục giảm xuống Hãy subscribe cho",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DGNdZGdwihs",
      "filename": "DGNdZGdwihs",
      "title": "[CS431 - Chương 2] Part 5b_1: Cài đặt mạng neural network",
      "chunk_id": 12,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "của loss liên tục giảm xuống Hãy subscribe cho kênh Ghiền Mì Gõ để không bỏ lỡ những video hấp dẫn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DGNdZGdwihs",
      "filename": "DGNdZGdwihs",
      "title": "[CS431 - Chương 2] Part 5b_1: Cài đặt mạng neural network",
      "chunk_id": 13,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề này đưa ra thêm một số điểm yếu của kiến trúc Transformer. Chủ đề này đưa ra thêm một số điểm yếu của kiến trúc Transformer. tại vì ở đây chúng ta thấy là cái thao tác self-attention của mình nó sẽ phải đi thực hiện trên tất cả các cái cặp. Ví dụ cái chuỗi này mà càng dài, chuỗi này là có T. Đây là 1, 2, 3. Thì trong cái bước tiếp theo, chúng ta sẽ phải đi tính tất cả các cái tính attention trên tất cả các cái từ của mình. Và chúng ta sẽ phải thực hiện cái việc này trên tất cả các cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fEGw6eEre2I",
      "filename": "fEGw6eEre2I",
      "title": "[CS431 - Chương 10] Part 6: Một số vấn đề của Transformer",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:54"
    }
  },
  {
    "page_content": "phải thực hiện cái việc này trên tất cả các cái cặp. Vậy như vậy chi phí của mình sẽ là O(T bình phương). Chưa kể là chúng ta sẽ phải có thêm cái D là số chiều vector này của mình. Thì đây chính là cái điểm yếu đầu tiên là chi phí tính toán. Nó sẽ thay đổi theo cái độ dài của cái chuỗi của mình. Do phải tính trên tất cả các cặp tương tác của self-attention. Và vấn đề tiếp theo, đó là vấn đề về biểu diễn vị trí. Trong positional embedding, chúng ta đã từng nhận xét đó là vị trí tuyệt đối sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fEGw6eEre2I",
      "filename": "fEGw6eEre2I",
      "title": "[CS431 - Chương 10] Part 6: Một số vấn đề của Transformer",
      "chunk_id": 1,
      "start_timestamp": "0:00:51",
      "end_timestamp": "0:01:38"
    }
  },
  {
    "page_content": "ta đã từng nhận xét đó là vị trí tuyệt đối sẽ không quan trọng. Vị trí tuyệt đối không quan trọng. Mà đôi khi vị trí tương đối trong self-attention là quan trọng. Vị trí tương đối là gì? Là vị trí giữa một cái từ thứ T với lại những cái từ xung quanh đó đó là từ thứ T trừ 1, T trừ 2, rồi T cộng 1, T cộng 2. Thì vị trí tương đối của mình trong trường hợp này nó sẽ là trừ 1, trừ 2, cộng 1, cộng 2, là những cái vị trí tương đối so với những cái từ xung quanh. Nó mới là những cái thể hiện được ý",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fEGw6eEre2I",
      "filename": "fEGw6eEre2I",
      "title": "[CS431 - Chương 10] Part 6: Một số vấn đề của Transformer",
      "chunk_id": 2,
      "start_timestamp": "0:01:36",
      "end_timestamp": "0:02:23"
    }
  },
  {
    "page_content": "xung quanh. Nó mới là những cái thể hiện được ý nghĩa trong yếu tố về mặt vị trí, chứ không phải là cái con số tuyệt đối là 1, 2, 3 cho đến T ở đây. Và cái bài báo của Shaw và các cộng sự vào năm 2018 cho thấy là cái vai trò của vị trí tương đối trong attention của mình quan trọng như thế nào. Và nó đã giúp cho cải thiện độ chính xác của hệ thống lên trong một số task rất là đáng kể. Tiếp theo là vị trí của mình sẽ dựa trên cây cú pháp học, tức là chúng ta sẽ có chủ ngữ, động từ, tân ngữ v.v.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fEGw6eEre2I",
      "filename": "fEGw6eEre2I",
      "title": "[CS431 - Chương 10] Part 6: Một số vấn đề của Transformer",
      "chunk_id": 3,
      "start_timestamp": "0:02:11",
      "end_timestamp": "0:03:08"
    }
  },
  {
    "page_content": "là chúng ta sẽ có chủ ngữ, động từ, tân ngữ v.v. Tính từ v.v. thì ở đây sẽ là cây cú pháp và tùy theo vị trí trong cây cú pháp này của mình và mình sẽ có cái... Ở đây sẽ có một cái ví dụ thôi nha, chứ không chắc là cái cây này đúng nha. Thì tùy vào cái cấu trúc của cái cây này nè, thì mình sẽ có được cái thông tin về mặt vị trí khác nhau, chứ chúng ta không phải dựa trên cái chỉ số. Chúng ta sẽ dựa trên cái vai trò về mặt ngữ pháp, về mặt ngữ pháp trong câu. Rồi chúng ta sẽ có những cái phương",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fEGw6eEre2I",
      "filename": "fEGw6eEre2I",
      "title": "[CS431 - Chương 10] Part 6: Một số vấn đề của Transformer",
      "chunk_id": 4,
      "start_timestamp": "0:03:04",
      "end_timestamp": "0:03:41"
    }
  },
  {
    "page_content": "trong câu. Rồi chúng ta sẽ có những cái phương pháp cải tiến khác, như là Rotary Embedding, Mover. Thì đây là những phương pháp biểu diễn vị trí và cho phép chúng ta có thể đạt được độ chính xác cao và cho phép mô hình của mình có thể học và tạo ra được những positional embedding. Đúng vậy chứ không phải là những cái hằng số, những cái vector ở dạng hằng số không có sự thay đổi. Thì đây chính là những cái vấn đề lớn của Transformer. Và khi nói về vấn đề giảm chi phí tính toán trên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fEGw6eEre2I",
      "filename": "fEGw6eEre2I",
      "title": "[CS431 - Chương 10] Part 6: Một số vấn đề của Transformer",
      "chunk_id": 5,
      "start_timestamp": "0:03:35",
      "end_timestamp": "0:04:19"
    }
  },
  {
    "page_content": "Và khi nói về vấn đề giảm chi phí tính toán trên self-attention thì Linformer đây là một kiến trúc do Wang và các cộng sự năm 2020. Thay vì chúng ta tính toán trên cái không gian T chiều thì chúng ta sẽ giảm số chiều này xuống còn K chiều. Và khi đó thì độ phức tạp của mình lúc này chỉ còn là K bình D. Và K này là con số nhỏ hơn so với T rất là nhiều. Và có thể là con số cố định luôn. Tức là khi T thay đổi thì K này vẫn có thể là cố định. Khi đó thì chúng ta thấy với cái sơ đồ này, ở đây chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fEGw6eEre2I",
      "filename": "fEGw6eEre2I",
      "title": "[CS431 - Chương 10] Part 6: Một số vấn đề của Transformer",
      "chunk_id": 6,
      "start_timestamp": "0:04:12",
      "end_timestamp": "0:04:58"
    }
  },
  {
    "page_content": "thì chúng ta thấy với cái sơ đồ này, ở đây chúng ta sẽ có cái cặp số là độ dài của chuỗi và cái max size. Thì ở đây chúng ta thấy là với cái Transformer phiên bản gốc, đây là Transformer gốc nè, thì cái độ phức tạp khi chúng ta inference, cái thời gian chúng ta inference của mình tăng lên. Nhưng với Linformer, khi K cố định chúng ta thấy là gần như không thay đổi, nó đi ngang. Thì cái thời gian inference của mình là gần như không đổi. Và cái module chính của nó đó chính là cái module projection",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fEGw6eEre2I",
      "filename": "fEGw6eEre2I",
      "title": "[CS431 - Chương 10] Part 6: Một số vấn đề của Transformer",
      "chunk_id": 7,
      "start_timestamp": "0:04:51",
      "end_timestamp": "0:05:35"
    }
  },
  {
    "page_content": "chính của nó đó chính là cái module projection ở đây. Đó là biến từ, chiếu từ cái không gian T chiều về cái không gian nhỏ hơn. Đó là ý tưởng của Linformer. Với BigBird, thay vì chúng ta sẽ phải tính tất cả các cặp, nếu như chúng ta vẽ trong cái ma trận, tức là chúng ta sẽ phải tính trên tất cả những cặp tương tác. Chúng ta sẽ phải tính full trên toàn bộ cặp tương tác. Thế thì chúng ta sẽ sử dụng một tổ hợp các cặp tương tác, ví dụ như random, tức là chúng ta sẽ random các vị trí, các cặp của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fEGw6eEre2I",
      "filename": "fEGw6eEre2I",
      "title": "[CS431 - Chương 10] Part 6: Một số vấn đề của Transformer",
      "chunk_id": 8,
      "start_timestamp": "0:05:32",
      "end_timestamp": "0:06:15"
    }
  },
  {
    "page_content": "tức là chúng ta sẽ random các vị trí, các cặp của mình. Chúng ta kết hợp với lại Window, tức là những cái cặp nào mà gần nhau thôi. Ví dụ như tại vị trí này, chúng ta sẽ lấy những cái từ trước đó và từ sau đó. Đó là những cái cặp mà cục bộ, ở gần nhau. Là Window và Global, tức là chúng ta sẽ có những cái cặp tương tác mà lấy được tất cả những cái từ đầu cho đến cuối. Thì nó gọi là Global Attention. Tuy nhiên lúc chúng ta sẽ không lấy dày đặc hết. Tại vì nếu lấy dày đặc hết, thì nó không khác gì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fEGw6eEre2I",
      "filename": "fEGw6eEre2I",
      "title": "[CS431 - Chương 10] Part 6: Một số vấn đề của Transformer",
      "chunk_id": 9,
      "start_timestamp": "0:06:11",
      "end_timestamp": "0:06:50"
    }
  },
  {
    "page_content": "Tại vì nếu lấy dày đặc hết, thì nó không khác gì Transformer bình thường. Chúng ta sẽ lấy từ đầu đến cuối, nhưng ở phía những phần tử đầu tiên, ở hàng đầu tiên và hai cột cuối cùng thôi. BigBird chính là sự kết hợp của ba loại attention này, như hình bên đây. Với BigBird thì nó sẽ giúp cho chúng ta tăng tốc độ tính toán self-attention nhưng nó sẽ không có, tức là nó sẽ lấy ra được những cặp quan trọng nhưng nó sẽ không lấy hết, nó sẽ không lấy hết tất cả các cặp tương tác. Nó vừa có đủ yếu tố",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fEGw6eEre2I",
      "filename": "fEGw6eEre2I",
      "title": "[CS431 - Chương 10] Part 6: Một số vấn đề của Transformer",
      "chunk_id": 10,
      "start_timestamp": "0:06:46",
      "end_timestamp": "0:07:35"
    }
  },
  {
    "page_content": "hết tất cả các cặp tương tác. Nó vừa có đủ yếu tố về random, vừa có yếu tố về mặt vị trí cục bộ ở trong những lân cận xung quanh, mà vừa có yếu tố là lấy được toàn cục, lấy hết. Đó là phương pháp BigBird. Cuối cùng, một cái nhận xét cuối cùng là có rất nhiều biến thể khác nhau của Transformer đã được thử nghiệm. Gần như tất cả những biến thể đó đều không cải thiện nhiều về độ chính xác. Và độ chính xác, nhưng chúng ta nhìn thấy đây là cái độ chính xác của mình dao động quanh con số là 26 phẩy,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fEGw6eEre2I",
      "filename": "fEGw6eEre2I",
      "title": "[CS431 - Chương 10] Part 6: Một số vấn đề của Transformer",
      "chunk_id": 11,
      "start_timestamp": "0:07:23",
      "end_timestamp": "0:07:57"
    }
  },
  {
    "page_content": "xác của mình dao động quanh con số là 26 phẩy, 26,8 mấy, 26,27, tức là nó sẽ không có sự dao động nhiều. 26,27, 26,27, các biến thể này không có làm thay đổi độ chính xác của mình một cách đáng kể. Điều này cho thấy là Transformer ổn định, cho thấy Transformer phổ quát. Đó chính là những nhận xét về ưu khuyết điểm cũng như là một số cải tiến của Transformer.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fEGw6eEre2I",
      "filename": "fEGw6eEre2I",
      "title": "[CS431 - Chương 10] Part 6: Một số vấn đề của Transformer",
      "chunk_id": 12,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Đối với mô hình Softmax regression, chúng ta cũng sẽ phát triển từ mô hình logistic regression. Đầu tiên chúng ta sẽ xem điều kiện, đó là nhãn của dữ liệu Y, nó thuộc một tập C. Trong đó, C này, số lượng phần tử K là lớn hơn 2. Đối với mô hình logistic regression, K là bằng 2, trong trường hợp nhiều hơn 2 lớp thì chúng ta sẽ sử dụng mô hình Softmax. Và ở đây chúng ta sẽ thấy có 3 tập điểm. Thì chúng ta hy vọng rằng là cái output y này của mình nếu như mà nó chỉ có gắn duy nhất một nhãn, một nhãn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:49"
    }
  },
  {
    "page_content": "như mà nó chỉ có gắn duy nhất một nhãn, một nhãn duy nhất. Và y của mình nó chỉ có thể là có một nhãn thì chúng ta sẽ sử dụng cái vector nó gọi là one-hot. Nó gọi là one-hot. Còn trong trường hợp mà đa nhãn, tức là y của mình có thể vừa thuộc một lớp, có thể thuộc hai lớp, có thể thuộc ba lớp, thì chúng ta cũng sẽ sử dụng vector biểu diễn dạng 0,1 như thế này. Nhưng lúc này nó không còn gọi là vector one-hot nữa, mà gọi là binary coding. Rồi, thì đây là cách để biểu diễn cái y trong trường hợp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 1,
      "start_timestamp": "0:00:47",
      "end_timestamp": "0:01:32"
    }
  },
  {
    "page_content": "đây là cách để biểu diễn cái y trong trường hợp mà nó có một nhãn hoặc nó có nhiều nhãn. Đối với mô hình phân loại nhị phân, chúng ta đã học trước đây, chúng ta sử dụng mô hình logistic regression, thì việc tìm ra được một bộ tham số theta tương đương với việc tìm ra được một đường thẳng để phân tách hai tập điểm này ra làm 2. Đường thẳng này được tạo bởi tham số theta. Với công thức đó là, ví dụ trong trường hợp này là x1, x2. Công thức cho phương trình đường thẳng này là theta0, theta1, x1,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 2,
      "start_timestamp": "0:01:24",
      "end_timestamp": "0:02:11"
    }
  },
  {
    "page_content": "trình đường thẳng này là theta0, theta1, x1, theta2, x2 là bằng 0. Đây là phương trình đường thẳng. Đại diện cho đường thẳng này là một tham số theta. Nếu chúng ta mở rộng mô hình logistic, K mô hình logistic cho K lớp, trong ví dụ này chúng ta lấy K bằng 3, thì với dữ liệu đầu vào x, chúng ta sẽ có nhân với lại một tham số để tạo ra giá trị z1. Sau đó, z1 qua hàm sigmoid, Chúng ta sẽ tạo ra giá trị y_mũ 1. Đây chính là một cái logistic model. Đây là một cái mô hình logistic. Và ứng với cái mô",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 3,
      "start_timestamp": "0:02:08",
      "end_timestamp": "0:03:02"
    }
  },
  {
    "page_content": "là một cái mô hình logistic. Và ứng với cái mô hình logistic này, thì chúng ta sẽ có một cái đường phân lớp màu cam ở đây. Tương tự như vậy, chúng ta sẽ học ra một cái mô hình logistic thứ 2. Và chúng ta sẽ có một cái đường phân lớp. Và với mô hình Logistic thứ K, chúng ta sẽ có một đường phân lớp ở đây. Và ở đây chúng ta sẽ đưa ra quyết định là rốt cuộc thì nó sẽ thuộc về lớp Tam giác, lớp tròn, hay là lớp X. Với một giả định đó là trong trường hợp này chúng ta sẽ sử dụng đơn nhãn. Trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 4,
      "start_timestamp": "0:02:56",
      "end_timestamp": "0:03:43"
    }
  },
  {
    "page_content": "hợp này chúng ta sẽ sử dụng đơn nhãn. Trong trường hợp này, chúng ta đơn nhãn tức là 1 điểm chỉ được gắn vào duy nhất 1 trong 3 lớp tròn, tam giác và X. Vì vậy, làm sao chúng ta chọn ra được nó thuộc về lớp tròn, tam giác hay X? Thì chúng ta sẽ chọn trong K mô hình này, chúng ta sẽ chọn mô hình nào mà có cái xác suất đầu ra là cao nhất. Tức là y_mũ 1, y_mũ 2 cho đến y_mũ K. Thì giá trị nào cao nhất thì chúng ta sẽ nói nó thuộc về phân lớp đó. Tuy nhiên nếu mà làm như thế này thì nó sẽ nảy sinh",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 5,
      "start_timestamp": "0:03:38",
      "end_timestamp": "0:04:31"
    }
  },
  {
    "page_content": "nhiên nếu mà làm như thế này thì nó sẽ nảy sinh ra một số vấn đề. Và vấn đề đó là nếu như cái điểm của mình nó nằm ở trong cái khu vực, mà nó sẽ nằm trong cái tam giác này thì nếu dựa trên cái đường phân chia màu cam, thì nó sẽ nói là không thuộc tam giác. Đối với đường màu đen, nó sẽ nói là không phải là hình tròn. Đối với đường màu xanh, nó sẽ nói không thuộc dấu X. Như vậy, rốt cuộc điểm này sẽ thuộc về lớp nào? Tương tự như vậy, nó sẽ ở cái vùng nhiễu. Nó sẽ thuộc về lớp tam giác. Cái này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 6,
      "start_timestamp": "0:04:27",
      "end_timestamp": "0:05:14"
    }
  },
  {
    "page_content": "vùng nhiễu. Nó sẽ thuộc về lớp tam giác. Cái này sẽ nói về lớp X. Vì vậy, chúng ta sẽ kết luận điểm này thuộc về lớp nào? Đó chính là cái vùng gây nhập nhằng khó khăn cho chúng ta. Để giải quyết vấn đề này, chúng ta sẽ có 3 giá trị này. Chúng ta có thể sử dụng cái mô hình đó là chúng ta sẽ gọi hàm Max của các giá trị y này. Tuy nhiên, nếu chúng ta dùng hàm Max này, thì nó sẽ nảy sinh ra một vấn đề đó là hàm Max này là một hàm khó tính đạo hàm. Và hàm khó tính đạo hàm thì cái bước số 3 của chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 7,
      "start_timestamp": "0:05:08",
      "end_timestamp": "0:06:02"
    }
  },
  {
    "page_content": "hàm khó tính đạo hàm thì cái bước số 3 của chúng ta khi mà chúng ta dùng cái thuật toán Gradient Descent nó cũng sẽ khó tính. Như vậy thì giải pháp của mình trong trường hợp này đó chính là mô hình Softmax. Thay vì dùng hàm Max thì chúng ta sẽ sử dụng một cái hàm gọi là hàm Softmax. Thì chúng ta ý tưởng đó là bỏ hết tất cả các cái node sigmoid ở đây, mà chúng ta sẽ thay nó bằng một cái hàm duy nhất đó là hàm Softmax. Với các dữ liệu đầu vào là z1, z2, ..., zK, đầu ra sẽ lần lượt tương ứng là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 8,
      "start_timestamp": "0:05:57",
      "end_timestamp": "0:06:42"
    }
  },
  {
    "page_content": "z1, z2, ..., zK, đầu ra sẽ lần lượt tương ứng là y_mũ 1, y_mũ 2 cho đến y_mũ K. Và công thức để thiết kế hàm dự đoán cũng rất đơn giản. Fθx sẽ bằng Softmax của theta chuyển vị nhân x. Bình thường ở đây sẽ là sigmoid, thì ở đây chúng ta sẽ bỏ đi và thay bằng một hàm Softmax. Và nếu chúng ta đặt z là bằng theta x, là cái vector z bao gồm các thành phần z1, z2, ..., zK, thì khi đó công thức của Softmax sẽ có công thức như sau: Softmax z sẽ là bằng y_mũ, tức là giá trị dự đoán, Với công thức của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 9,
      "start_timestamp": "0:06:37",
      "end_timestamp": "0:07:33"
    }
  },
  {
    "page_content": "y_mũ, tức là giá trị dự đoán, Với công thức của thành phần thứ i của y_mũ sẽ là bằng e mũ z_i chia cho tổng của K, chạy từ 1 cho đến K lớn e mũ z_k. Công thức này nhìn có vẻ phức tạp, tuy nhiên chúng ta có thể đưa ra một số con số để chúng ta có thể tính thử. Ví dụ như z của mình, nó sẽ là bằng giá trị là 1, 2, 3. Mình cố tình đưa ra cái con số này để cho nó tăng liên tục và nó dễ tính. Thì cái giá trị y_mũ của mình sẽ là một cái vector. Trong đó, cái thành phần đầu tiên của mình sẽ là e mũ.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 10,
      "start_timestamp": "0:07:25",
      "end_timestamp": "0:08:20"
    }
  },
  {
    "page_content": "đó, cái thành phần đầu tiên của mình sẽ là e mũ. Thì ở đây, thành phần tử đầu tiên sẽ là e mũ 1. Chia cho tổng của các e mũ k, tức là e mũ 1 cộng cho e mũ 2 cộng cho e mũ 3. Tương tự như vậy, thành phần thứ 2 của vector dự đoán y_mũ sẽ là e mũ 2, chia cho tổng e mũ 1 cộng cho e mũ 2 cộng cho e mũ 3. Và thành phần cuối cùng nó sẽ là e mũ 3, chia cho e mũ 1, cộng cho e mũ 2, cộng cho e mũ 3. Vì vậy thì chúng ta có thể thấy nó có một tính chất đó là gì? Các giá trị y_mũ i này đều là những con số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 11,
      "start_timestamp": "0:08:13",
      "end_timestamp": "0:09:05"
    }
  },
  {
    "page_content": "là gì? Các giá trị y_mũ i này đều là những con số lớn hơn 0 và bé hơn 1. Đó là những con số lớn hơn 0 tại vì e mũ x là một hàm mà miền giá trị của nó là dương. Tất cả các phần tử này đều là dương. Rồi tổng, tất cả các cái y_mũ i này thì nó sẽ là bằng một. Ví dụ như trong ví dụ này thì chúng ta thấy là tổng của nó sẽ là bằng e mũ một, cộng cho e mũ hai, cộng cho e mũ ba. Tất cả chia cho cái mẫu số chung, đó là e mũ một, cộng cho e mũ hai, cộng cho e mũ ba. Thì cái này nó sẽ là bằng một. Vì vậy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 12,
      "start_timestamp": "0:09:01",
      "end_timestamp": "0:09:45"
    }
  },
  {
    "page_content": "e mũ ba. Thì cái này nó sẽ là bằng một. Vì vậy thì cái hàm Softmax nó có một tính chất khá hay đó là nó sẽ ánh xạ các giá trị z1, z2, ..., zK, thuộc dải giá trị là từ trừ vô cùng, dương vô cùng. Nó sẽ ánh xạ cái giá trị z này, đúng không? Trên cái miền giá trị là từ trừ vô cùng, dương vô cùng. Về cái giá trị y_mũ i, nó sẽ thuộc cái đoạn là từ 0 cho đến 1. Và tổng tất cả các giá trị y_mũ này đều bằng một. Đây chính là một cái không gian xác suất. Tại vì trong cái không gian xác suất, mỗi một cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 13,
      "start_timestamp": "0:09:42",
      "end_timestamp": "0:10:16"
    }
  },
  {
    "page_content": "Tại vì trong cái không gian xác suất, mỗi một cái phần tử này, mỗi một cái giá trị dự đoán này, nó là cái xác suất để thuộc về một cái lớp số 1. Cái này sẽ là xác suất thuộc về cái lớp số 2. Và cái này sẽ là cái xác suất thuộc về cái lớp số K. Và các cái xác suất này đều tuân theo tính chất từng phần tử này. Từng xác suất này đều là những con số từ 0 cho đến 1. Và tổng tất cả các biến cố, tổng tất cả các xác suất đều là bằng 1. Đây là tính chất rất hay của Softmax. Đồng thời, Softmax của mình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 14,
      "start_timestamp": "0:10:12",
      "end_timestamp": "0:10:58"
    }
  },
  {
    "page_content": "rất hay của Softmax. Đồng thời, Softmax của mình là một hàm liên tục, và đạo hàm của nó cũng sẽ tính rất dễ dàng. Tại sao mình nói có thể tính dễ dàng? Vì các bạn thấy ở trong đây nó có sử dụng các hàm là e mũ, mà chúng ta biết rồi, e mũ x, đạo hàm cũng chính là bằng e mũ x. Thì chính nhờ tính chất này nên khi chúng ta tính đạo hàm của Softmax thì nó sẽ ra công thức rất là đẹp. Tuy nhiên trong phạm vi của bước 2 và bước 3 thì chúng ta sẽ không tính đạo hàm của Softmax. Và như cũng đã giới thiệu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 15,
      "start_timestamp": "0:10:53",
      "end_timestamp": "0:11:52"
    }
  },
  {
    "page_content": "đạo hàm của Softmax. Và như cũng đã giới thiệu thì Softmax đã được hỗ trợ tính đạo hàm thông qua các thư viện của Deep Learning Framework rồi. Thì ở đây mình sẽ không có đề cập đến chi tiết, mà sẽ sử dụng các Deep Learning Framework sau. Và đối với bước số 2, tức là cái bước để mà thiết kế hàm lỗi, thì đối với trường hợp một mẫu, chúng ta sẽ có công thức hàm lỗi. Cái công thức này gọi là Cross-entropy. Thì thực ra công thức này là công thức dạng tổng quát của Binary Cross-entropy. Với y_mũ của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 16,
      "start_timestamp": "0:11:45",
      "end_timestamp": "0:12:42"
    }
  },
  {
    "page_content": "tổng quát của Binary Cross-entropy. Với y_mũ của mình, nó sẽ là một cái vector dự đoán. Còn y của mình thì nó sẽ là cái vector dữ liệu thực tế. Vì vậy, bây giờ mình sẽ lấy một trường hợp ví dụ, đó là y của mình là có ba thành phần thôi, đó là 0, 1, 0. Tức là y này nó đang nói là mẫu dữ liệu của mình nó đang thuộc về lớp thứ hai. Ví dụ đây là y, trong trường hợp này là 0, 1, 0. Y trong trường hợp này là 1, 0, 0. Y trong trường hợp này là 0, 0, 1. Thì ở đây hàm ý là cái nhãn của mình là cái nhãn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 17,
      "start_timestamp": "0:12:40",
      "end_timestamp": "0:13:36"
    }
  },
  {
    "page_content": "Thì ở đây hàm ý là cái nhãn của mình là cái nhãn tam giác. Rồi, đây là giá trị thực tế. Còn giá trị dự đoán, y_mũ. Nếu như cái y_mũ này của mình mà khớp với giá trị dự đoán 0, 0, 1, Thì khi chúng ta thế vô cái công thức này, nó sẽ là bằng 0 nhân log(0). Có dấu trừ ở đằng trước nữa. Trong cái công thức này thì nó thiếu dấu trừ nha. Nó sẽ có cái dấu trừ ở đằng trước. Cộng cho 1 nhân log(1), cộng cho 0 nhân log(0). Thì 0 nhân log(0) (có thể xem là 0), và 1 nhân log(1) (tức là log của 1 là 0). Vì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 18,
      "start_timestamp": "0:13:34",
      "end_timestamp": "0:14:13"
    }
  },
  {
    "page_content": "0), và 1 nhân log(1) (tức là log của 1 là 0). Vì vậy thì lỗi của mình sẽ là 0 trong trường hợp dự đoán đúng. Đây là trường hợp dự đoán đúng. Trường hợp dự đoán sai, tức là y là bằng 0, 1, 0. Nhưng mà y_mũ của mình thì nó lại là bằng 1, 0, 1 đi. 1, 0, 0 đi. Lúc này, loss của mình sẽ là bằng trừ của (0 nhân log(1) + 1 nhân log(0) + 0 nhân log(0)). Thì rõ ràng là 0 nhân với mấy cũng bằng 0. Nhân với mấy cũng bằng 0. Và lúc này thì cái loss của mình, nó sẽ là bằng trừ của (1 nhân log(0)). Thì trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 19,
      "start_timestamp": "0:14:07",
      "end_timestamp": "0:14:49"
    }
  },
  {
    "page_content": "nó sẽ là bằng trừ của (1 nhân log(0)). Thì trong bài Cross-entropy chúng ta biết rồi, log(0) nó chính là bằng trừ vô cùng. Do đó thì trừ của trừ nó sẽ ra là dương vô cùng. Nó sẽ tạo ra một con số vô cùng lớn. Và sở dĩ có con số vô cùng lớn, nó sẽ giúp cho đạo hàm lớn. Đạo hàm lớn thì việc cập nhật tham số sẽ nhanh hơn. Thế nên là nhắc lại ý tưởng cũ. Rồi, thì đây là cho trường hợp một mẫu dữ liệu. Đối với trường hợp mà toàn bộ mẫu dữ liệu, tức là dữ liệu đầu vào là một tập hợp các mẫu. Vì ở đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 20,
      "start_timestamp": "0:14:38",
      "end_timestamp": "0:15:35"
    }
  },
  {
    "page_content": "dữ liệu đầu vào là một tập hợp các mẫu. Vì ở đây chúng ta có N mẫu, với N mẫu chúng ta sẽ tính trung bình cộng. Lưu ý là có dấu trừ ở đằng trước. Và với từng mẫu, chúng ta lại tính công thức cho từng phần tử của vector nhãn thực tế y và vector dự đoán y_mũ. Với mẫu thứ i, chúng ta sẽ có vector nhãn thực tế y^(i). Với mẫu thứ i, chúng ta sẽ có vector dự đoán y_mũ^(i). Rồi qua công thức Cross-entropy này, thực hiện element-wise, tức là thực hiện trên từng phần tử, chúng ta sẽ ra một giá trị lỗi.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 21,
      "start_timestamp": "0:15:28",
      "end_timestamp": "0:15:37"
    }
  },
  {
    "page_content": "từng phần tử, chúng ta sẽ ra một giá trị lỗi. Và chúng ta sẽ tính trung bình cộng tất cả các lỗi này trên toàn bộ N mẫu. Thì đây là công thức toàn bộ mẫu và không có vector hóa. Đối với trường hợp nhiều mẫu, toàn bộ mẫu mà có vector hóa, thì chúng ta sẽ có công thức rất gọn như sau: Đó là bằng trung bình cộng của Cross-entropy, của Cross-entropy và đầu vào của mình sẽ là Softmax của theta chuyển vị nhân x. Và đây là giá trị thực tế y. Đây là dự đoán, đây là giá trị thực. Và chúng ta sẽ thực",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 22,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "dự đoán, đây là giá trị thực. Và chúng ta sẽ thực hiện Cross-entropy trên từng mẫu y, trên từng phần tử, sau đó tính trung bình cộng lại.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G4lcEPrfETo",
      "filename": "G4lcEPrfETo",
      "title": "[CS431 - Chương 2] Part 4a: Mô hình hồi quy Softmax (SoftmaxRegression)",
      "chunk_id": 23,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Tiếp theo, chúng ta sẽ tiến hành cài đặt mô hình Softmax Regression. Đối với mô hình Softmax Regression, chúng ta phải phân lớp với số phân lớp lớn hơn 2. Trong trường hợp này, số phân lớp của mình chọn đó chính là bằng 4. Và ở đây thì chúng ta sẽ có một cái mô hình dưới dạng là đồ thị của Softmax Regression thì cũng tương tự như hai mô hình Linear Regression và Logistic Regression chúng ta sẽ có cái lớp Input trong đó thì cái lớp Input nó sẽ bao gồm hai cái feature là X1, X2 và kèm theo một cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G71D3dacAds",
      "filename": "G71D3dacAds",
      "title": "[CS431 - Chương 2] Part 4b: Cài đặt mô hình softmax regression",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:42"
    }
  },
  {
    "page_content": "gồm hai cái feature là X1, X2 và kèm theo một cái thành phần là Bias và cái lớp Output của mình thì nhìn có vẻ lớn Nhìn có vẻ lớn nhưng thực ra ở đây chúng ta chỉ có duy nhất một lớp, gọi là lớp Fully Connected Và cái Softmax này nó chính là cái activation của mình Đó chính là cái hàm activation Và Dense ở đây thì nó sẽ khác so với Linear Regression và Logistic Regression là output của nó là nó đầu ra có đến K cái output Nó sẽ K output, trong trường hợp này K của mình chính là bằng 4 Tiếp theo,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G71D3dacAds",
      "filename": "G71D3dacAds",
      "title": "[CS431 - Chương 2] Part 4b: Cài đặt mô hình softmax regression",
      "chunk_id": 1,
      "start_timestamp": "0:00:39",
      "end_timestamp": "0:01:23"
    }
  },
  {
    "page_content": "hợp này K của mình chính là bằng 4 Tiếp theo, chúng ta sẽ tiến hành cài đặt mô hình Softmax Activation. Đầu tiên, chúng ta sẽ tiến hành tạo dữ liệu. Chúng ta sẽ có trước đoạn code để tạo dữ liệu. Ý tưởng tạo dữ liệu, chúng ta sẽ dựa trên một số điểm tâm S1 có tọa độ là 10-2 S2 có tọa độ là 2-8 S3 có tọa độ là 12-8 S4 có tọa độ là 2-0 Và với mỗi tâm này, nó sẽ tương ứng với lại một phần lớp Và với mỗi tâm, ví dụ S1, chúng ta sẽ xây dựng các điểm xoay xung quanh tâm này Với noise của mình đó là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G71D3dacAds",
      "filename": "G71D3dacAds",
      "title": "[CS431 - Chương 2] Part 4b: Cài đặt mô hình softmax regression",
      "chunk_id": 2,
      "start_timestamp": "0:01:15",
      "end_timestamp": "0:02:25"
    }
  },
  {
    "page_content": "xoay xung quanh tâm này Với noise của mình đó là mean của mình là 0 Độ lệch chuẩn sẽ là 1,5 và số mẫu của mình sẽ là 50 cho mỗi class. Rồi, sau đó thì chúng ta gom toàn bộ các cái điểm PT1, PT2, PT3 và PT4 để tạo thành cái feature đầu vào X, và nhãn Y. Thì nó tương ứng chính là các cái nhãn 0, 1, 2, 3. Và với mỗi cái giá trị 0 này thì chúng ta sẽ nhân với Nsample, 50 giá trị 0 cho class số 1, 50 giá trị 1 cho class số 2, 50 giá trị 2, 50 giá trị 3 cho class số 3-4. Để sử dụng được độ đo về hàm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G71D3dacAds",
      "filename": "G71D3dacAds",
      "title": "[CS431 - Chương 2] Part 4b: Cài đặt mô hình softmax regression",
      "chunk_id": 3,
      "start_timestamp": "0:02:18",
      "end_timestamp": "0:03:11"
    }
  },
  {
    "page_content": "3 cho class số 3-4. Để sử dụng được độ đo về hàm loss như cross-entropy, thì Y của mình ban đầu ở dạng nhãn sẽ được chuyển về dạng one-hot encoding. chúng ta sẽ sử dụng cái hàm sau để đưa nó về cái dạng One-Hot Encoding. One-Hot Encoding có nghĩa là sao? Tức là, ví dụ như cái nhãn của mình là 0, thì khi đưa về One-Hot Encoding, nó sẽ có cái dạng như sau, đó là 1000, nữa. Điều như cái nhãn của mình đó là 2, thì nó sẽ đưa về cái dạng One-Hot Encoding đó là 0010. Thì đây là One-Hot Encoding. Rồi,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G71D3dacAds",
      "filename": "G71D3dacAds",
      "title": "[CS431 - Chương 2] Part 4b: Cài đặt mô hình softmax regression",
      "chunk_id": 4,
      "start_timestamp": "0:03:06",
      "end_timestamp": "0:04:04"
    }
  },
  {
    "page_content": "đó là 0010. Thì đây là One-Hot Encoding. Rồi, và chúng ta sẽ chạy lại cái đoạn code trên. Rồi, tương tự như vậy thì mỗi một cái điểm nó sẽ có một cái màu. Vì ứng với nguồn, có vài điểm hơi giao thoa, nhưng không ảnh hưởng nhiều đến việc huấn luyện. Ở phần cài đặt cho thuật toán thì chúng ta sẽ sử dụng bộ phương thức tương tự như Linear Regression và Logistic Regression. Chúng ta sẽ chủ yếu cài đặt cho hai phương thức đó là Build và Train. Đối với Build, đầu vào của mình sẽ có input dimension,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G71D3dacAds",
      "filename": "G71D3dacAds",
      "title": "[CS431 - Chương 2] Part 4b: Cài đặt mô hình softmax regression",
      "chunk_id": 5,
      "start_timestamp": "0:03:51",
      "end_timestamp": "0:04:53"
    }
  },
  {
    "page_content": "Build, đầu vào của mình sẽ có input dimension, input dimension Và cho Softmax Regression, chúng ta sẽ phải có thêm một tham số nữa, đó là tham số Output Dimension. Hay nói cách khác đó chính là tham số K của mình. Vậy thì ở đây, chúng ta sẽ tiến hành bổ sung thêm một tham số nữa đó là Output Dimension. và tương tự, chúng ta sẽ có input là bằng input, shape là bằng input, tên của mình sẽ là input name, đó là một character Về phần output, nó chỉ là kết quả của một phép biến đổi kết nối đầy đủ là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G71D3dacAds",
      "filename": "G71D3dacAds",
      "title": "[CS431 - Chương 2] Part 4b: Cài đặt mô hình softmax regression",
      "chunk_id": 6,
      "start_timestamp": "0:04:46",
      "end_timestamp": "0:06:06"
    }
  },
  {
    "page_content": "kết quả của một phép biến đổi kết nối đầy đủ là Dense và đầu ra của mình bình thường mình để nó 1, thì bây giờ đầu ra của mình chính là Output Output dimension Activation, thì mình sẽ phải để hàm đó là Softmax Use Bias thì chúng ta sẽ để là bằng True Và ở đây là chúng ta mới chỉ khởi tạo cho cái lớp biến đổi, chúng ta sẽ phải truyền đầu vào cho nó chính là đối tượng tên là Input Bây giờ chúng ta sẽ đóng gói Input và Output lại vào một đối tượng tên Model và chúng ta sẽ trả về self.model. Và hàm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G71D3dacAds",
      "filename": "G71D3dacAds",
      "title": "[CS431 - Chương 2] Part 4b: Cài đặt mô hình softmax regression",
      "chunk_id": 7,
      "start_timestamp": "0:05:55",
      "end_timestamp": "0:07:04"
    }
  },
  {
    "page_content": "Model và chúng ta sẽ trả về self.model. Và hàm này thì chúng ta sẽ không trả về kết quả gì hết. Rồi đối với phương thức train thì chúng ta cũng sẽ có số epoch. Bởi vì mô hình phức tạp hơn, số epoch có thể cho số lớn hơn như 1000 epoch. Tương tự như vậy, optimizer sẽ là bằng tf.keras.optimizers. Chúng ta sẽ sử dụng Stochastic Gradient Descent. Tuy nhiên, nếu chúng ta muốn thì chúng ta cũng có thể sử dụng Adam, nó sẽ nhanh hơn. Rồi, Learning Rate là bằng 0.01. Bây giờ chúng ta sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G71D3dacAds",
      "filename": "G71D3dacAds",
      "title": "[CS431 - Chương 2] Part 4b: Cài đặt mô hình softmax regression",
      "chunk_id": 8,
      "start_timestamp": "0:07:00",
      "end_timestamp": "0:08:24"
    }
  },
  {
    "page_content": "Learning Rate là bằng 0.01. Bây giờ chúng ta sẽ self.model.compile để tích hợp cái optimizer này vào. Chúng ta sẽ cần phải khai báo hàm loss Trước trước, chúng ta sử dụng mse, chúng ta sẽ sử dụng Categorical Cross-Entropy Và để train thì chúng ta sẽ để là self.model.fit(X_train, Y_train) Rồi số epoch thì chúng ta sẽ để là epochs và num_epochs Rồi, như vậy là chúng ta đã cài đặt xong lớp đối tượng là Softmax Regression Tương tự như vậy thì chúng ta sẽ tiến hành khởi tạo, build và train mô hình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G71D3dacAds",
      "filename": "G71D3dacAds",
      "title": "[CS431 - Chương 2] Part 4b: Cài đặt mô hình softmax regression",
      "chunk_id": 9,
      "start_timestamp": "0:08:21",
      "end_timestamp": "0:09:10"
    }
  },
  {
    "page_content": "ta sẽ tiến hành khởi tạo, build và train mô hình Khởi tạo thì chúng ta sẽ có Softmax Regression Rồi, không có tham số Rồi, chúng ta sẽ gọi hàm build.build Lưu ý là ở đây chúng ta sẽ có hai tham số đầu vào là input_dimension và output_dimension Do đó thì input_dimension thì chúng ta sẽ có hai là do cái điểm trong không gian 2 chiều Output của mình thì ở trên đây số dữ liệu của mình đó là 4, K là 4, đúng không? Như vậy thì chúng ta sẽ truyền vào, đây chính là K, K trong trường hợp này là 4 Rồi,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G71D3dacAds",
      "filename": "G71D3dacAds",
      "title": "[CS431 - Chương 2] Part 4b: Cài đặt mô hình softmax regression",
      "chunk_id": 10,
      "start_timestamp": "0:09:04",
      "end_timestamp": "0:09:59"
    }
  },
  {
    "page_content": "đây chính là K, K trong trường hợp này là 4 Rồi, và chúng ta sẽ xem thử cái model này nó sẽ có cấu hình như mình mong muốn chưa input dimension là input của mình là một vector 2 chiều và số tham số bằng 0 và output của mình sẽ là lớp Dense với output neurons của mình chính là 4 và số tham số của mình sẽ là 12 thì tại sao lại là 12? 12 là bằng 2 cộng 1, tức là thêm phần bias. Đầu vào của mình sẽ có input của mình là 1 là bias và X1 X2. Như vậy tổng tổng của mình là có 3 cái đầu vào. Đầu ra của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G71D3dacAds",
      "filename": "G71D3dacAds",
      "title": "[CS431 - Chương 2] Part 4b: Cài đặt mô hình softmax regression",
      "chunk_id": 11,
      "start_timestamp": "0:09:56",
      "end_timestamp": "0:11:06"
    }
  },
  {
    "page_content": "tổng của mình là có 3 cái đầu vào. Đầu ra của mình thì K trong trường hợp này K là bằng 4. Như vậy là 3 nhân 4 chính là 12 tham số. Rồi, tổng số tham số là 12. Bây giờ mình sẽ tiến hành train mô hình này Soft.train với X Lưu ý ở đây là Y_train Nhưng mà Y thì chúng ta sẽ phải lấy là Y_OneHot Ở đây dữ liệu là X Chúng ta sẽ lấy X và Y_OneHot Rồi, X về Y_one_hot. Rồi, chúng ta cũng quan sát loss và thấy là loss ban đầu là khá là lớn. Sau đó thì, loss bắt đầu sẽ là A, sau đó thì có xu hướng giảm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G71D3dacAds",
      "filename": "G71D3dacAds",
      "title": "[CS431 - Chương 2] Part 4b: Cài đặt mô hình softmax regression",
      "chunk_id": 12,
      "start_timestamp": "0:10:55",
      "end_timestamp": "0:12:25"
    }
  },
  {
    "page_content": "loss bắt đầu sẽ là A, sau đó thì có xu hướng giảm dần. Là còn 2, rồi 1, 0.1. Và do số lượng dữ liệu cũng nhiều và mô hình có nhiều tham số hơn một chút, nên chúng ta sẽ thấy mô hình của mình train sẽ lâu hơn. Để trực quan hóa mô hình này của mình, chúng ta sẽ thấy là cái Loss của mình đã giảm xuống 0.05. Và để trực quan hóa thì cái cách thức để trực quan hóa cái mô hình này của mình, và để trực quan hóa, cách thức để trực quan hóa mô hình này là lấy một cái lưới, Lấy một cái lưới Ví dụ chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G71D3dacAds",
      "filename": "G71D3dacAds",
      "title": "[CS431 - Chương 2] Part 4b: Cài đặt mô hình softmax regression",
      "chunk_id": 13,
      "start_timestamp": "0:12:21",
      "end_timestamp": "0:13:27"
    }
  },
  {
    "page_content": "lấy một cái lưới, Lấy một cái lưới Ví dụ chúng ta sẽ có giá trị X1 là từ X1 min cho đến X1 max Rồi X2 thì sẽ có là X2 min cho đến X2 max Và chúng ta sẽ lấy lưới, tức là chúng ta sẽ lấy chi lưới Cứ mỗi điểm trên cái lưới này, chúng ta sẽ gọi hàm mô hình của mình để xem nó sẽ được xếp vào điểm màu xanh lá, xanh dương, màu vàng, hay là màu đỏ. Để có thể trực quan hóa được thì ở đây chúng ta sẽ để Softmax Regression Và chúng ta sẽ gọi cái hàm predict để đưa ra cái Y Trong đó cái X_test của mình là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G71D3dacAds",
      "filename": "G71D3dacAds",
      "title": "[CS431 - Chương 2] Part 4b: Cài đặt mô hình softmax regression",
      "chunk_id": 14,
      "start_timestamp": "0:13:23",
      "end_timestamp": "0:14:39"
    }
  },
  {
    "page_content": "để đưa ra cái Y Trong đó cái X_test của mình là lấy các giá trị từ X và Y trong đó XX và YY sẽ lấy trên một cái lưới với X sẽ lấy từ 8 cho đến 17 thực ra cái này phải đúng là X1 rồi, cái này sẽ là X2 rồi X1, X2 rồi thì chúng ta sẽ có XX1 và XX2 X_grid và Y_grid Chúng ta sẽ truyền các giá trị X1, X2, X3 Len của X, X1, X, X1 Rồi, chúng ta thấy là một cái lưới khá lớn, ở đây nó sẽ chạy từ trừ 8 cho đến 17, và X2 sẽ cũng chạy từ trừ 8 cho đến 17, và chúng ta sẽ có một lưới các cái điểm. Với mỗi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G71D3dacAds",
      "filename": "G71D3dacAds",
      "title": "[CS431 - Chương 2] Part 4b: Cài đặt mô hình softmax regression",
      "chunk_id": 15,
      "start_timestamp": "0:14:36",
      "end_timestamp": "0:15:21"
    }
  },
  {
    "page_content": "và chúng ta sẽ có một lưới các cái điểm. Với mỗi điểm này, chúng ta sẽ chạy hàm Softmax Regression.predict. Sau đó sẽ ra được cái Y. Từ cái Y này, chúng ta sẽ biết được cái nhãn tương ứng của nó là gì. Chúng ta sẽ gọi qua hàm np.argmax. Tại vì cái Output của mình sẽ ra là một vector. Output sẽ ra một vector 4 chiều. Mình sẽ lấy thành phần Max. Tương ứng với thành phần Maximum, sẽ có được chỉ số để gọi vô hàm color. Color ở đây thì chúng ta đã thiết lập sẵn các cái giá trị, các cái hiệu về màu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G71D3dacAds",
      "filename": "G71D3dacAds",
      "title": "[CS431 - Chương 2] Part 4b: Cài đặt mô hình softmax regression",
      "chunk_id": 16,
      "start_timestamp": "0:15:09",
      "end_timestamp": "0:16:07"
    }
  },
  {
    "page_content": "lập sẵn các cái giá trị, các cái hiệu về màu sắc và cái gì. Rồi, thì chúng ta thấy là cái Softmax Regression cũng đã phân ra các tập màu xanh dương, xanh lá, màu vàng và màu đỏ thành các cái vùng khá là phù hợp. Vì vậy, trong bài Softmax Regression này, chúng ta đã tiến hành cài đặt mô hình sử dụng thư viện Keras. Thư viện của Softmax Regression cũng tương tự như thư viện của Linear Regression và Logistic Regression. Nó cũng chỉ có một input đầu vào và một output đầu ra là kết quả phép biến đổi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G71D3dacAds",
      "filename": "G71D3dacAds",
      "title": "[CS431 - Chương 2] Part 4b: Cài đặt mô hình softmax regression",
      "chunk_id": 17,
      "start_timestamp": "0:16:02",
      "end_timestamp": "0:16:18"
    }
  },
  {
    "page_content": "vào và một output đầu ra là kết quả phép biến đổi fully connected. Điểm khác là phải sử dụng hàm Activation là Softmax. Output dimension bình thường là 1, số lớp lớn hơn 2, giờ ta sẽ phải có Output dimension ở đây. Loss sẽ sử dụng Categorical Cross-Entropy. Điểm thú vị khác là trong trực quan hóa, chúng ta sẽ phải lấy grid và ứng với từng cái điểm trong cái grid, trong cái lưới này thì chúng ta sẽ gọi cái hàm predict rồi từ cái giá trị output Y này thì chúng ta sẽ suy ra được cái nhãn cái màu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G71D3dacAds",
      "filename": "G71D3dacAds",
      "title": "[CS431 - Chương 2] Part 4b: Cài đặt mô hình softmax regression",
      "chunk_id": 18,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "này thì chúng ta sẽ suy ra được cái nhãn cái màu sắc và cái ký hiệu tương ứng để vẽ lên.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=G71D3dacAds",
      "filename": "G71D3dacAds",
      "title": "[CS431 - Chương 2] Part 4b: Cài đặt mô hình softmax regression",
      "chunk_id": 19,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chào các bạn, hôm nay chúng ta sẽ đến với bài số 2, tên là Máy học tổng quát và một số mô hình cơ bản. Nội dung của buổi hôm nay sẽ bao gồm các phần như sau. Mô hình máy học tổng quát, đây là một trong những nội dung quan trọng và đi xuyên suốt trong toàn bộ môn học này. Chúng ta sẽ học về kiến trúc chung của các mô hình máy học có giám sát. Từ đó, chúng ta sẽ phát triển lên các mô hình như Linear Regression, Logistic Regression, Softmax Regression. Cuối cùng, chúng ta sẽ đến mô hình học sâu đầu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:48"
    }
  },
  {
    "page_content": "Cuối cùng, chúng ta sẽ đến mô hình học sâu đầu tiên mà chúng ta sẽ học trong môn học này, chính là mạng Neural Network. Đối với mô hình máy học tổng quát, chúng ta sẽ nhận dữ kiện đầu vào. Đây sẽ là cái thông tin để giúp cho chúng ta đưa ra quyết định Và quyết định của mình thì nó ở dạng là dự đoán và là một cái giá trị y ngã Và y ngã này là kết quả của một cái hàm số fθx Thì đây là một cái mô hình máy học Và fθx thì θ (theta) chính là cái tham số của mô hình Còn x chính là cái dữ kiện đầu vào",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 1,
      "start_timestamp": "0:00:44",
      "end_timestamp": "0:01:27"
    }
  },
  {
    "page_content": "số của mô hình Còn x chính là cái dữ kiện đầu vào chúng ta đã nhận được Và cái giá trị dự đoán này chúng ta luôn mong muốn nó xấp xỉ với lại cái giá trị thực tế Ví dụ như khi chúng ta đoán cái giá của một cái cổ phiếu Thì chúng ta mong muốn đoán giá chính xác với lại cái giá trong tương lai Hoặc chúng ta dự đoán giá nhà Chúng ta mong muốn đoán với cái giá trị thật của cái căn nhà một cách chính xác nhất Thì để đảm bảo cho cái điều kiện là cái y ngã xấp xỉ với lại cái Y Thì chúng ta sẽ phải có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 2,
      "start_timestamp": "0:01:21",
      "end_timestamp": "0:01:57"
    }
  },
  {
    "page_content": "ngã xấp xỉ với lại cái Y Thì chúng ta sẽ phải có một cái hàm, nó gọi là hàm loss Hàm này gọi là tên tiếng Việt, có thể là hàm mất mát hoặc là hàm độ lỗi Chúng ta có thể có hai cách gọi khác nhau L này là viết tắt của chữ LOSS Chúng ta có một lưu ý đó là nếu như trong hàm mô hình máy học thì biến số của mình chính là dữ kiện đầu vào thì đối với hàm mất mát, biến số của mình chính là θ (theta). θ (theta) sẽ là biến số, chứ nó không phải là tham số. Còn các giá trị x và y ở đây, thì bình thường",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 3,
      "start_timestamp": "0:01:49",
      "end_timestamp": "0:02:44"
    }
  },
  {
    "page_content": "số. Còn các giá trị x và y ở đây, thì bình thường theo cách ký hiệu trong chương trình phổ thông, chúng ta hay dùng x cho biến số, nhưng trong trường hợp này, x, y chính là tham số. Tại sao nó gọi là tham số? Bởi vì x và y trong trường hợp này chính là những cặp dữ liệu mà mình được sử dụng để huấn luyện. chúng ta sẽ đưa vào các cặp dữ liệu (x, y) mà mình thu thập được trong thực tế và mình hy vọng là chúng ta sẽ học và tìm ra được một hàm mô hình máy học và khi mà cái hàm mô hình máy học này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 4,
      "start_timestamp": "0:02:35",
      "end_timestamp": "0:03:12"
    }
  },
  {
    "page_content": "máy học và khi mà cái hàm mô hình máy học này đã học đảm bảo được giá trị mất mát và giá trị độ lỗi này là thấp nhất thì sau đây chúng ta sẽ sử dụng mô hình máy học để đi dự đoán cho những mẫu thực tế Thế thì ba công việc chính cần phải làm khi thiết kế một mô hình đó chính là đầu tiên chúng ta sẽ thiết kế một cái mô hình dự đoán, tức là thiết kế một cái hàm fθx. Công việc thứ hai, đó là chúng ta sẽ thiết kế cái hàm lỗi. Hàm lỗi của cái việc dự đoán đó chính là hàm Lθxi. Và công việc cuối cùng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 5,
      "start_timestamp": "0:03:07",
      "end_timestamp": "0:03:51"
    }
  },
  {
    "page_content": "đoán đó chính là hàm Lθxi. Và công việc cuối cùng đó là chúng ta sẽ tìm ra cái tham số θ (theta) để cho cái hàm độ lỗi này là nhỏ nhất. Tại vì chúng ta mong muốn tìm một cái hàm mô hình fθx sao cho giá trị dự đoán y ngã xấp xỉ y thì việc này tương đương với việc là chúng ta sẽ có hàm độ lỗi là thấp nhất hoặc là sai số mất mát là nhỏ nhất. Vì vậy trong 3 công việc này chúng ta sẽ tìm hiểu công việc thứ 3 trước tiên. Tại sao là như vậy? Tại vì các mô hình về các thư viện hiện nay đều đã hỗ trợ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 6,
      "start_timestamp": "0:03:45",
      "end_timestamp": "0:04:27"
    }
  },
  {
    "page_content": "mô hình về các thư viện hiện nay đều đã hỗ trợ cho chúng ta tìm θ (theta) sao cho hàm độ lỗi này nhỏ nhất rồi. và chúng ta sẽ sử dụng một thuật toán và sau đây chúng ta sẽ tìm hiểu đó là thuật toán Gradient Descent. Đây là một trong những thuật toán mà rất là hiệu quả trong cái việc là tìm một cái tham số θ (theta) sao cho cái độ lỗi này là nhỏ nhất. Và khi cái công việc này mà đã giải quyết rồi thì từ nay trở về sau chúng ta chỉ quan tâm đến hai cái công việc đầu tiên đó là thiết kế cái hàm mô",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 7,
      "start_timestamp": "0:04:23",
      "end_timestamp": "0:05:10"
    }
  },
  {
    "page_content": "cái công việc đầu tiên đó là thiết kế cái hàm mô hình và thiết kế cái hàm lỗi. Đầu tiên, chúng ta sẽ vẽ một biểu đồ để minh họa cho hàm lỗi Lθ Trong trường hợp này X_i, chúng ta sẽ không xem xét nữa, tại vì X_i là các dữ kiện đầu vào đóng góp trong việc hình thành hàm lỗi Lθ Chúng ta sẽ chọn một hàm lỗi tương đối đơn giản Trong trường hợp hàm lỗi phức tạp, chúng ta sẽ bàn thêm sau Hàm này sẽ có một nhận định là tại một vị trí bất kỳ Chúng ta nhận thấy, dấu của đạo hàm sẽ ngược hướng với điểm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 8,
      "start_timestamp": "0:05:07",
      "end_timestamp": "0:05:57"
    }
  },
  {
    "page_content": "thấy, dấu của đạo hàm sẽ ngược hướng với điểm cực tiểu. Điểm cực tiểu của chúng ta trong trường hợp này chính là cái điểm này. Nếu tại vị trí θ (theta) này, điểm cực tiểu nằm ở bên tay phải. Tức là lẽ ra chúng ta phải đi về phía tay phải để tìm được điểm cực tiểu. thì trong trường hợp này đạo hàm sẽ hướng như thế nào? chúng ta thấy tại vị trí θ (theta) này thì đồ thị của hàm số đang dốc xuống nó đang có độ dốc, tức là đạo hàm của mình là âm và đạo hàm âm tức là hướng của nó sẽ hướng về phía bên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 9,
      "start_timestamp": "0:05:53",
      "end_timestamp": "0:06:29"
    }
  },
  {
    "page_content": "hàm âm tức là hướng của nó sẽ hướng về phía bên tay trái trong khi đó điểm cực tiểu thì nó lại nằm bên phía tay phải như vậy ở đây nó đang ngược hướng với lại hướng của điểm cực tiểu Bây giờ chúng ta sẽ xét cái tình huống θ (theta) nó nằm ở phía bên trái, bên đây Thì cái điểm cực tiểu nó nằm ở bên phía tay trái của θ (theta) Vì vậy thì lẽ ra chúng ta sẽ phải đi về hướng này để hướng đến cái điểm cực tiểu Thì trong trường hợp này chúng ta thấy cái hàm của mình nó đang đồng biến, nó đang đi lên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 10,
      "start_timestamp": "0:06:24",
      "end_timestamp": "0:07:07"
    }
  },
  {
    "page_content": "hàm của mình nó đang đồng biến, nó đang đi lên Như vậy đạo hàm của mình sẽ là dương Mà đạo hàm dương, tức là cái hướng của đạo hàm nó sẽ hướng về hướng này Hướng về tay phải, như vậy ở trong trường hợp này thì đạo hàm cũng ngược hướng với lại hướng của điểm cực tiểu Do đó chúng ta sẽ có một cái bước để cập nhật cái θ (theta) này Đó là chúng ta sẽ di chuyển ngược hướng với lại hướng của đạo hàm và nó thể hiện ở cái dấu trừ Còn đây là ký hiệu của đạo hàm Và chúng ta sẽ đi ngược phương, thì đây là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 11,
      "start_timestamp": "0:07:00",
      "end_timestamp": "0:07:56"
    }
  },
  {
    "page_content": "hàm Và chúng ta sẽ đi ngược phương, thì đây là phiên bản đầu tiên Thế thì bây giờ nó sẽ có một cái vấn đề nảy sinh, đó là nếu như cái giá trị của đạo hàm nó lớn, nó quá lớn thì sao? Ví dụ như chúng ta có cái điểm θ (theta) nằm ở đây Và tại cái vị trí này, cái độ dốc của nó nó rất là lớn, do đó cái giá trị của đạo hàm khiến bước nhảy quá lớn, và nếu chúng ta cập nhật, chúng ta sẽ lấy θ (theta) trừ cho giá trị đạo hàm này. θ (theta) sẽ nhảy qua bên này, θ (theta) sẽ nhảy qua bên này. Và như vậy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 12,
      "start_timestamp": "0:07:51",
      "end_timestamp": "0:08:30"
    }
  },
  {
    "page_content": "này, θ (theta) sẽ nhảy qua bên này. Và như vậy thì nó đã vượt qua cái giá trị điểm cực tiểu Như vậy thì ở đây chúng ta gọi là θ (theta) đã được cập nhật và đi quá đà Đi quá cái điểm cực tiểu của mình nằm ở đây Thế thì để đảm bảo cho cái việc cập nhật nó không có bị đi quá đà Thì chúng ta sẽ nhân với một cái hệ số learning rate alpha Thì cái này hiểu nôm na, đó là chậm mà chắc Lúc này, chúng ta sẽ có công thức cập nhật, đó là θ (theta), thì sẽ là bằng θ (theta) trừ cho alpha nhân với đạo hàm.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 13,
      "start_timestamp": "0:08:22",
      "end_timestamp": "0:09:13"
    }
  },
  {
    "page_content": "là bằng θ (theta) trừ cho alpha nhân với đạo hàm. Đây là công thức cho việc cập nhật các tham số. Thay vì nó nhảy quá đà qua đây, thì nó sẽ nhảy với một đại lượng đủ nhỏ, rồi sau đó lại nhảy một đại lượng đủ nhỏ, hướng về điểm cực tiểu. Bây giờ, chúng ta sẽ nảy sinh ra thêm một cái vấn đề tiếp theo đó là cập nhật cho đến khi nào thì dừng cập nhật θ (theta) cho đến khi nào thì dừng θ (theta) ban đầu nó nằm ở đây và chúng ta sẽ cập nhật cập nhật nhưng mà cập nhật cho đến khi nào thì chúng ta quan",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 14,
      "start_timestamp": "0:09:01",
      "end_timestamp": "0:09:59"
    }
  },
  {
    "page_content": "mà cập nhật cho đến khi nào thì chúng ta quan sát là khi θ (theta) mà càng tiến gần đến cái điểm cực tiểu thì cái độ dốc của hàm này Độ dốc sẽ càng thấp và đỉnh điểm là khi đạt đến được điểm cực tiểu thì độ dốc của mình trong trường hợp này chính là bằng 0. Độ dốc của mình sẽ bằng 0. Nhưng mà khi tiến được đến 0 này thì bước nhảy của nó gần như rất thấp, gần như không di chuyển. Vì vậy chúng ta sẽ có một cái điểm dừng. Giải pháp số 1 là chúng ta sẽ dừng khi độ dốc gần như bằng 0 hoặc là khi đạo",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 15,
      "start_timestamp": "0:09:48",
      "end_timestamp": "0:10:41"
    }
  },
  {
    "page_content": "sẽ dừng khi độ dốc gần như bằng 0 hoặc là khi đạo hàm đủ nhỏ, tức là chúng ta sẽ dừng tại vị trí như thế này. Độ dốc này nó bé hơn một ngưỡng epsilon, thì chúng ta sẽ dừng. Một giải pháp thứ 2 là chúng ta sẽ dừng khi đạt được một số vòng lặp nhất định. Khi chúng ta thấy việc lặp đi lặp lại tốn quá nhiều thời gian, chúng ta sẽ xét cố định một số lần lặp cố định đủ lớn. Khi chúng ta đạt được số lần lặp, chúng ta sẽ dừng. Vì vậy, chúng ta sẽ có hai giải pháp để kết thúc quá trình cập nhật tham số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 16,
      "start_timestamp": "0:10:27",
      "end_timestamp": "0:11:10"
    }
  },
  {
    "page_content": "giải pháp để kết thúc quá trình cập nhật tham số θ (theta). Rồi, và như vậy thì các bước lặp đi lặp lại, lặp đi lặp lại θ (theta) thì càng về sau θ (theta) sẽ càng tiến về điểm cực tiểu. Thì đây chính là thuật toán và chúng ta sẽ có một số điểm khởi tạo. Đầu tiên là điểm θ0. Thứ hai, chúng ta sẽ có một cái ngưỡng để cho biết là khi đạt được đến độ dốc như thế nào đó thì chúng ta sẽ kết thúc quá trình lặp. Thông thường 2 cái tham số này sẽ là các cái siêu tham số. Ngoài ra thì chúng ta sẽ còn có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 17,
      "start_timestamp": "0:11:00",
      "end_timestamp": "0:11:41"
    }
  },
  {
    "page_content": "cái siêu tham số. Ngoài ra thì chúng ta sẽ còn có thêm một cái tham số nữa, đó là learning rate, tức là cái hệ số mà hồi nãy mình nói nó là chậm mà chắc. Hệ số Learning Rate này khi chúng ta không biết là nó là khoảng bao nhiêu thì thông thường tham số mặc định chúng ta có thể sử dụng đó là 0.0001 Tức là khoảng 0.0001 khi chúng ta không biết mô hình của mình nó nên sử dụng tham số Learning Rate alpha là bao nhiêu thì chúng ta sử dụng tham số này Tuy nhiên trong bài toán này thì chúng ta có một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 18,
      "start_timestamp": "0:11:36",
      "end_timestamp": "0:12:11"
    }
  },
  {
    "page_content": "Tuy nhiên trong bài toán này thì chúng ta có một chút kinh nghiệm thì chúng ta có thể chọn tham số Learning Rate đủ nhỏ thôi Ví dụ như là khoảng 0.01 là được rồi Chứ còn nếu mà cho tham số Learning Rate Alpha này quá nhỏ thì thuật toán của mình sẽ chạy chậm Cái này là quá chậm Rồi, như vậy thì chúng ta sẽ xong cái bước khởi tạo đó là một giá trị θ0 Đó là cái điểm bắt đầu của θ (theta), learning rate và ngưỡng dừng Ngưỡng dừng epsilon thì chúng ta thường cũng chọn một cái ngưỡng đủ nhỏ Ví dụ như",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 19,
      "start_timestamp": "0:12:08",
      "end_timestamp": "0:12:59"
    }
  },
  {
    "page_content": "thường cũng chọn một cái ngưỡng đủ nhỏ Ví dụ như là 10 mũ trừ 5, ví dụ vậy Và chúng ta sẽ đến cái quá trình lặp Thì cái quá trình lặp chúng ta sẽ thực hiện 2 cái công việc thôi Đó là cập nhật θ (theta) θ (theta) sẽ là bằng θ (theta) trừ cho alpha nhân cho đạo hàm của hàm loss Và chúng ta sẽ dừng khi mà cái đạo hàm này đủ nhỏ Thì đây là cho cái giải pháp số 1 Đây chính là cái giải pháp số 1 Còn cho cái giải pháp số 2 thì chúng ta có thể viết một cái vòng lặp for for i từ 1 đến K Ví dụ như chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 20,
      "start_timestamp": "0:12:54",
      "end_timestamp": "0:13:42"
    }
  },
  {
    "page_content": "cái vòng lặp for for i từ 1 đến K Ví dụ như chúng ta cho nó lặp 100 lần, rồi thì chúng ta chỉ việc cập nhật θ (theta) sẽ là bằng θ (theta) trừ cho alpha nhân cho đạo hàm. Là xong, thì đây là cái giải pháp thứ 2. Đây chính là cái giải pháp thứ 2. Và đây là thuật toán Gradient Descent. Trong trường hợp nếu như hàm phức tạp hơn thì có nhiều điểm cực tiểu. Chúng ta sẽ lấy một trường hợp, đó là chúng ta có hai điểm cực tiểu. Điều gì sẽ xảy ra nếu chúng ta khởi tạo ngay tại vị trí này? Nếu chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 21,
      "start_timestamp": "0:13:27",
      "end_timestamp": "0:14:24"
    }
  },
  {
    "page_content": "ta khởi tạo ngay tại vị trí này? Nếu chúng ta khởi tạo giá trị θ0 ở đây, thì khi giả sử chúng ta nhìn cái này dưới góc độ là một cái góc nhìn vật lý chúng ta sẽ có một cái viên bi đặt ở đây và khi chúng ta thả cái viên bi này ra nó sẽ từ từ nó rớt xuống khi nó chạm được đến cái điểm cực tiểu của vùng này nó sẽ dừng tại sao nó dừng? tại vì khi chạm được đến cái điểm cực tiểu này thì cái đạo hàm của mình nó sẽ xấp xỉ bằng 0 mà khi đạo hàm xấp xỉ bằng 0 thì cái bước nhảy của mình lúc này Bước nhảy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 22,
      "start_timestamp": "0:14:16",
      "end_timestamp": "0:14:59"
    }
  },
  {
    "page_content": "0 thì cái bước nhảy của mình lúc này Bước nhảy sẽ là bằng 0, tức là θ (theta) sẽ là bằng θ (theta) trừ 0, tức là θ (theta) không cập nhật gì nữa, tức là nó sẽ đến đây nó sẽ dừng. Dưới góc nhìn vật lý, nếu như chúng ta có nhiều điểm cực tiểu và dùng phiên bản này thì rõ ràng là không ổn. Vậy thì giải pháp đầu tiên là chúng ta sẽ chạy nhiều lần và chúng ta sẽ lấy giá trị nhỏ nhất. Ví dụ, thì chúng ta sẽ cập nhật và nó sẽ dừng tại đây. Như vậy thì chúng ta sẽ có hai lần chạy và chúng ta sẽ lấy giá",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 23,
      "start_timestamp": "0:14:55",
      "end_timestamp": "0:15:45"
    }
  },
  {
    "page_content": "ta sẽ có hai lần chạy và chúng ta sẽ lấy giá trị nào mà nhỏ nhất. Thì điểm yếu của phương pháp này là nó tốn tài nguyên do chúng ta phải lặp đi lặp lại quá trình khởi tạo ngẫu nhiên này nhiều lần. Giải pháp thứ hai được sử dụng rất là nhiều trong các framework hiện nay, Trong các Deep Learning Framework hiện nay, chúng ta sẽ sử dụng momentum hay còn gọi là quán tính. Khi giả sử chúng ta khởi tạo tại cái vị trí này, tức là tại điểm cực tiểu, gần điểm cực tiểu, mà không phải là điểm cực tiểu nhỏ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 24,
      "start_timestamp": "0:15:42",
      "end_timestamp": "0:16:29"
    }
  },
  {
    "page_content": "điểm cực tiểu, mà không phải là điểm cực tiểu nhỏ nhất, thì khi chúng ta cập nhật và đến cái vị trí này, thì nó vẫn còn một cái độ quán tính. nó sẽ giúp cho chúng ta thoát ra khỏi cái vị trí này và để rớt xuống cái vị trí điểm cực tiểu nhỏ hơn. Thì đây chính là cái giải pháp dùng momentum. Và một trong những thuật toán mà có khai thác yếu tố về momentum này chính là thuật toán ADAM. Và thuật toán ADAM thì đã được cài đặt trong các thư viện như là TensorFlow và PyTorch. Cài đặt rất là đầy đủ và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 25,
      "start_timestamp": "0:16:21",
      "end_timestamp": "0:17:08"
    }
  },
  {
    "page_content": "TensorFlow và PyTorch. Cài đặt rất là đầy đủ và sử dụng rất là dễ dàng. Vì vậy, nếu chúng ta tìm tham số θ (theta) sao cho hàm loss Lθ nhỏ nhất, chúng ta sẽ nghĩ ngay đến giải pháp sử dụng thuật toán ADAM. Đây giống như là một trick, một mẹo để chúng ta, khi chúng ta tìm giá trị nhỏ nhất của một hàm, thì chúng ta nghĩ ngay đến thuật toán ADAM. Chi tiết cách sử dụng thuật toán ADAM như thế nào thì chúng ta sẽ trình bày trong phần thực hành sau. Với mô hình máy học tổng quát này, chúng ta nhắc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 26,
      "start_timestamp": "0:16:58",
      "end_timestamp": "0:17:31"
    }
  },
  {
    "page_content": "Với mô hình máy học tổng quát này, chúng ta nhắc lại 3 công việc cần phải thực hiện. Đó là thiết kế hàm dự đoán, hàm mô hình máy học fθx. Chúng ta sẽ phải thiết kế lại hàm lỗi Lθxi. và chúng ta phải tìm θ (theta) sao cho hàm lỗi này là nhỏ nhất và chúng ta có một lưu ý đó là các thư viện Deep Learning hiện tại đều đã giải quyết rất tốt công việc số 3 này rồi như vậy công việc số 3 này chúng ta sẽ không còn quan tâm nữa và khi dùng thì chúng ta sẽ nghĩ ngay đến giải thuật từ thư viện đó chính là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 27,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "nghĩ ngay đến giải thuật từ thư viện đó chính là ADAM, hay còn gọi là Adam Optimizer Nhớ đến cái keyword này. Và như vậy thì từ nay về sau chúng ta chỉ còn giải quyết hai cái công việc thôi. Đó là thiết kế cái hàm dự đoán fθx và thiết kế cái hàm lỗi Lθxi. Và chúng ta sẽ thiết kế thì tùy theo cái tính chất của y nó phụ thuộc như thế nào với x. thì chúng ta sẽ có những cách thiết kế khác nhau, ví dụ đối với bài toán Tuyến tính, đối với bài toán Hồi quy, chúng ta sẽ thiết kế theo một cách, đối với",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 28,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "quy, chúng ta sẽ thiết kế theo một cách, đối với bài toán Phân loại, chúng ta sẽ thiết kế theo một cách khác và đối với bài toán Phi tuyến, chúng ta sẽ thiết kế theo một cách khác nữa. Tùy vào tính chất của dữ liệu (x, y) này để chúng ta sẽ thiết kế hai cái hàm này.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=GdKIVY6CsTw",
      "filename": "GdKIVY6CsTw",
      "title": "[CS431 - Chương 2] Part 1: Mô hình học tổng quát",
      "chunk_id": 29,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Nói ra là, nếu chúng ta có một cái vùng đen và bóng, nó sẽ thay đổi nhiều hơn nếu chúng ta có một cái vùng đen và bóng mà ít thay đổi hơn nếu chúng ta có một cái vùng đen và bóng. thì đốm sáng này cũng sẽ tiến về phía trên của feature map. Rồi, chúng ta cũng để ý thêm là tỷ lệ của vùng gương mặt này nó bằng một nửa so với gương mặt này thì đốm sáng này cũng bằng một nửa. Như vậy thì điều này nó thể hiện tính chất gì? Nó thể hiện tính chất đó là feature map của mình nó sẽ bất biến về trình tự",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gmQTGRTHH2o",
      "filename": "gmQTGRTHH2o",
      "title": "[CS431 - Chương 3] Part 4_2: Trực quan hóa mạng CNN",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:56"
    }
  },
  {
    "page_content": "feature map của mình nó sẽ bất biến về trình tự không gian. Nó sẽ bất biến về trình tự không gian, nghĩa là Cái gương mặt bên tay trái, nó nằm bên tay trái, cái gương mặt bên tay phải thì cái đốm sáng tương ứng nó cũng nằm bên tay phải. Cái gương mặt này nó nằm ở phía trên so với gương mặt này thì cái đốm sáng tương ứng của nó cũng nằm phía trên so với cái đốm sáng này. Và cái tỷ lệ nó sẽ bất biến về phép tỷ lệ, bất biến đối với cái yếu tố về mặt tỷ lệ. Gương mặt này bằng khoảng một nửa so với",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gmQTGRTHH2o",
      "filename": "gmQTGRTHH2o",
      "title": "[CS431 - Chương 3] Part 4_2: Trực quan hóa mạng CNN",
      "chunk_id": 1,
      "start_timestamp": "0:00:52",
      "end_timestamp": "0:01:28"
    }
  },
  {
    "page_content": "tỷ lệ. Gương mặt này bằng khoảng một nửa so với gương mặt này thì đốm sáng này cũng sẽ bằng một nửa so với đốm sáng này. Và dựa trên tính chất này, chúng ta có thể dùng nó để cho bài toán đó là bài toán Object Detection hoặc là bài toán Segmentation, bài toán phát hiện đối tượng hoặc là bài toán phân đoạn ngữ nghĩa đối tượng. Ví dụ, đối với bài toán phát hiện gương mặt thì chúng ta có feature map này rồi đúng không? chúng ta sẽ dùng cái phương pháp đó là phân ngưỡng để lấy ra những cái khu vực",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gmQTGRTHH2o",
      "filename": "gmQTGRTHH2o",
      "title": "[CS431 - Chương 3] Part 4_2: Trực quan hóa mạng CNN",
      "chunk_id": 2,
      "start_timestamp": "0:01:21",
      "end_timestamp": "0:01:58"
    }
  },
  {
    "page_content": "đó là phân ngưỡng để lấy ra những cái khu vực đốm sáng hai cái đốm sáng này và sáng hơn một cái ngưỡng cho trước chúng ta sẽ có cái tọa độ của hai cái đốm sáng này sau đó chúng ta sẽ lấy ra được cái Bounding Box cái Bounding Box tức là cái hình chữ nhật bao xung quanh hai cái đốm sáng này từ cái tọa độ của cái hình chữ nhật hai cái đốm sáng này chúng ta sẽ nội suy lên trên cái tọa độ nội suy cái tọa độ ở phía trên này ảnh gốc Và như vậy chúng ta sẽ có hai cái bounding box, hai cái hình chữ nhật",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gmQTGRTHH2o",
      "filename": "gmQTGRTHH2o",
      "title": "[CS431 - Chương 3] Part 4_2: Trực quan hóa mạng CNN",
      "chunk_id": 3,
      "start_timestamp": "0:01:55",
      "end_timestamp": "0:02:29"
    }
  },
  {
    "page_content": "sẽ có hai cái bounding box, hai cái hình chữ nhật bao xung quanh hai cái gương mặt này. Thì đó chính là cái ý tưởng của việc ứng dụng mạng CNN cho giải quyết bài toán Object Detection. Và thậm chí nó có thể giải quyết luôn cả bài toán segmentation. Bây giờ chúng ta sẽ mở rộng thêm cái thí nghiệm này. bằng cách đó là cũng với feature map hồi nãy, chúng ta sẽ đưa vào một ảnh con mèo. Với feature map mà thể hiện là gương mặt như hồi nãy, chúng ta thấy feature map này có tính chất gì? Tính chất đầu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gmQTGRTHH2o",
      "filename": "gmQTGRTHH2o",
      "title": "[CS431 - Chương 3] Part 4_2: Trực quan hóa mạng CNN",
      "chunk_id": 4,
      "start_timestamp": "0:02:25",
      "end_timestamp": "0:03:10"
    }
  },
  {
    "page_content": "feature map này có tính chất gì? Tính chất đầu tiên là nó vẫn phát sáng, nhưng tính chất thứ hai là nó phát sáng yếu hơn hẳn so với trước đây. Với con mèo, chúng ta thấy nó có phát sáng nhưng mà nó không có rực rỡ, không có sáng như cái mặt của con người. Vì vậy, ý nghĩa của cái concept này phải thêm một cái nữa, đó là nó phải là mặt của con người. Nhưng mà câu hỏi đặt ra là tại sao mặt con mèo thì nó vẫn sáng nhưng mà nó hơi hơi sáng. Vì vậy, con mèo nó không có rực rỡ, không có sáng như cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gmQTGRTHH2o",
      "filename": "gmQTGRTHH2o",
      "title": "[CS431 - Chương 3] Part 4_2: Trực quan hóa mạng CNN",
      "chunk_id": 5,
      "start_timestamp": "0:03:03",
      "end_timestamp": "0:03:48"
    }
  },
  {
    "page_content": "con mèo nó không có rực rỡ, không có sáng như cái mặt của con người. Nhưng mà câu hỏi đặt ra là tại sao gương mặt con mèo thì nó vẫn sáng nhưng mà nó hơi hơi sáng thì có thể giải thích đó là cái gương mặt của con mèo thì nó cũng sẽ có một số tính chất giống như con người ví dụ mặt con mèo nó cũng sẽ có mắt, có lỗ mũi tuy nhiên cái phần miệng của con mèo các bạn thấy là cái phần miệng con mèo nó rất là bé nó chúm chím, nó rất là bé ở đây trong khi đó, cái miệng của con người các bạn thấy rồi nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gmQTGRTHH2o",
      "filename": "gmQTGRTHH2o",
      "title": "[CS431 - Chương 3] Part 4_2: Trực quan hóa mạng CNN",
      "chunk_id": 6,
      "start_timestamp": "0:03:43",
      "end_timestamp": "0:04:20"
    }
  },
  {
    "page_content": "đó, cái miệng của con người các bạn thấy rồi nó to hơn nhiều và nó dài hơn nhiều so với cái miệng của con mèo và chính vì chỉ có 2 trên 3 cái đặc điểm trên nên cái feature map ở đây nó chỉ hơi sáng tức là hàm ý, nó chỉ có đâu đó có cái đặc trưng về mắt và về mũi thôi còn cái phần về miệng nó hơi yếu nên đâm ra là cái feature map này nó sáng yếu thì ý nghĩa của cái feature map này đó là Nó sẽ dựa trên yếu tố về hình học. Nó sẽ coi có cái dáng của ánh mắt hay không. Nó có cái dáng của cái lỗ mũi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gmQTGRTHH2o",
      "filename": "gmQTGRTHH2o",
      "title": "[CS431 - Chương 3] Part 4_2: Trực quan hóa mạng CNN",
      "chunk_id": 7,
      "start_timestamp": "0:04:16",
      "end_timestamp": "0:04:59"
    }
  },
  {
    "page_content": "ánh mắt hay không. Nó có cái dáng của cái lỗ mũi hay không. Nếu có thì nó sẽ tăng trọng số lên cho feature map đó. Nhưng mà cái phần miệng thì cái tỷ lệ của nó, cái dáng của nó ít xuất hiện. Nên tỷ lệ của feature map này nó cũng sẽ sáng ít hơn so với mặt của con người. Rồi, và bây giờ chúng ta sẽ cùng qua một số cái ví dụ khác. Ví dụ như ở đây, chúng ta quan sát thấy có một cái feature map khá là sáng, và nó tương ứng là hai cái đốm này. Hai cái đốm này. Thì các bạn đoán xem, ý nghĩa của chín",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gmQTGRTHH2o",
      "filename": "gmQTGRTHH2o",
      "title": "[CS431 - Chương 3] Part 4_2: Trực quan hóa mạng CNN",
      "chunk_id": 8,
      "start_timestamp": "0:04:53",
      "end_timestamp": "0:05:45"
    }
  },
  {
    "page_content": "đốm này. Thì các bạn đoán xem, ý nghĩa của chín cái tấm ảnh, mà làm cho cái feature map này nó sáng nhất. Ý nghĩa của chín tấm ảnh này là gì? Các bạn sẽ đoán đó chính là có sự xuất hiện của quần áo, đúng không ạ? Có sự xuất hiện của quần áo, nhưng mà cái ý nghĩa của cái concept này không phải là như vậy. Ý nghĩa của đó là cái nếp nhăn của quần áo, ví dụ vậy. Khi nào trên tấm hình có cái nếp nhăn thì nó sẽ phát sáng. Thì ở đây, nhân vật làm demo ở đây, họ sẽ tiến hành một thí nghiệm đó là Anh",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gmQTGRTHH2o",
      "filename": "gmQTGRTHH2o",
      "title": "[CS431 - Chương 3] Part 4_2: Trực quan hóa mạng CNN",
      "chunk_id": 9,
      "start_timestamp": "0:05:38",
      "end_timestamp": "0:06:37"
    }
  },
  {
    "page_content": "ở đây, họ sẽ tiến hành một thí nghiệm đó là Anh này mới tìm cách phủi cho cái tay áo bên tay phải, phủi cái áo làm cho nó nhẵn hơn và không còn cái nếp nhăn nữa Nếu bạn nói là ý nghĩa của cái concept này là quần áo thì vô lý tại vì ở đây anh này vẫn đang mặc cái áo này mà nhưng mà cái chỗ này nó không còn phát sáng nữa nó không còn phát sáng nhiều nữa khi anh này làm cho cái áo nhẵn hơn thì tương ứng chỗ này nó sẽ đốm sáng biến mất nó nhạt dần đi Còn bên tay trái thì các bạn thấy là đốm sáng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gmQTGRTHH2o",
      "filename": "gmQTGRTHH2o",
      "title": "[CS431 - Chương 3] Part 4_2: Trực quan hóa mạng CNN",
      "chunk_id": 10,
      "start_timestamp": "0:06:25",
      "end_timestamp": "0:07:14"
    }
  },
  {
    "page_content": "đi Còn bên tay trái thì các bạn thấy là đốm sáng của mình vẫn còn rõ hơn là do nó còn nhiều cái nếp nhăn ở bên áo bên tay trái. Thì ý nghĩa của feature map này chính là cái nếp nhăn. Rồi, bây giờ chúng ta sẽ đến với feature map. Đó chính là cái feature map này Thì hiện tại trên khu vực này chúng ta thấy là nó tối thui, tức là nó không có phát sáng Đó Thì top chín cái tấm ảnh mà làm cho feature map này phát sáng Đó chính là chín cái tấm ảnh này Và các bạn cũng đoán xem ý nghĩa điểm chung của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gmQTGRTHH2o",
      "filename": "gmQTGRTHH2o",
      "title": "[CS431 - Chương 3] Part 4_2: Trực quan hóa mạng CNN",
      "chunk_id": 11,
      "start_timestamp": "0:07:12",
      "end_timestamp": "0:07:48"
    }
  },
  {
    "page_content": "Và các bạn cũng đoán xem ý nghĩa điểm chung của chín tấm ảnh này là đó là gì Các bạn sẽ đoán Đó chính là điểm chung của chín tấm ảnh này đó là có hình tròn, đúng không ạ? hoặc các bạn sẽ nói là nó sẽ có cái chữ Canon ở nằm bên trong hình tròn nó có cái chữ nằm bên trong hình tròn vậy thì chúng ta sẽ cùng giải đáp xem Rồi, khi anh này đưa vào một cái bìa của một cái cuốn sách thì chúng ta thấy là cái khu vực này nó phát sáng và cái này nó không hề có hình tròn nào hết đúng không? Nó không hề có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gmQTGRTHH2o",
      "filename": "gmQTGRTHH2o",
      "title": "[CS431 - Chương 3] Part 4_2: Trực quan hóa mạng CNN",
      "chunk_id": 12,
      "start_timestamp": "0:07:46",
      "end_timestamp": "0:08:29"
    }
  },
  {
    "page_content": "có hình tròn nào hết đúng không? Nó không hề có hình tròn nào hết như vậy, cái ý nghĩa của cái concept, của cái feature map này Cái feature map này đó chính là phải có cái chữ viết Và khi có cái chữ thì nó sẽ phát sáng Do đó điểm chung của tất cả chín cái tấm hình này đó chính là concept về chữ viết Rồi, khi anh này đưa thêm một cái bìa của một cái cuốn tập hay là một cái sổ khác Thì chúng ta cũng có thể thấy cái khu vực này nó lại tiếp tục phát sáng và nó tách ra làm hai cái đốm sáng riêng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gmQTGRTHH2o",
      "filename": "gmQTGRTHH2o",
      "title": "[CS431 - Chương 3] Part 4_2: Trực quan hóa mạng CNN",
      "chunk_id": 13,
      "start_timestamp": "0:08:24",
      "end_timestamp": "0:09:14"
    }
  },
  {
    "page_content": "sáng và nó tách ra làm hai cái đốm sáng riêng biệt thì điều này một lần nữa khẳng định đó là cái concept của feature map này đó chính là có sự xuất hiện của các dòng chữ và nếu như dựa trên ý tưởng của feature map này các bạn đoán xem chúng ta có thể ứng dụng feature map này để làm gì đó là dùng cho bài toán OCR Optical Character Recognition Tức là chúng ta sẽ phát hiện xem vị trí khu vực nào có sự xuất hiện của chữ viết Rồi khi chúng ta sẽ tách ra được vị trí có chữ viết rồi chúng ta sẽ trích",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gmQTGRTHH2o",
      "filename": "gmQTGRTHH2o",
      "title": "[CS431 - Chương 3] Part 4_2: Trực quan hóa mạng CNN",
      "chunk_id": 14,
      "start_timestamp": "0:09:04",
      "end_timestamp": "0:09:47"
    }
  },
  {
    "page_content": "ra được vị trí có chữ viết rồi chúng ta sẽ trích xuất ra và dùng các thuật toán nhận diện để xem mặt chữ của nó là gì Đó là toàn bộ nội dung của bài Deep Visualization Toolbox Và hy vọng là qua cái demo ngắn gọn này trực quan hóa các feature map cũng như trực quan hóa các filter thì sẽ giúp cho các bạn hiểu rõ hơn cái concept của các feature map ý nghĩa của nó là gì Thông qua việc thống kê top chín tấm hình mà làm cho feature map này sáng nhất Hi vọng rằng nếu các bạn có thể hiểu rõ hơn mạng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gmQTGRTHH2o",
      "filename": "gmQTGRTHH2o",
      "title": "[CS431 - Chương 3] Part 4_2: Trực quan hóa mạng CNN",
      "chunk_id": 15,
      "start_timestamp": "0:09:37",
      "end_timestamp": "0:09:47"
    }
  },
  {
    "page_content": "Hi vọng rằng nếu các bạn có thể hiểu rõ hơn mạng CNN này rồi thì sau này chúng ta có thể sử dụng được mạng CNN này cho các bài toán bên lĩnh vực thị giác máy tính. Rất là hiệu quả, vì bộ đặc trưng feature map này cực kỳ phong phú và cực kỳ tổng quát khi chúng ta huấn luyện trên một tập dữ liệu đủ lớn.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gmQTGRTHH2o",
      "filename": "gmQTGRTHH2o",
      "title": "[CS431 - Chương 3] Part 4_2: Trực quan hóa mạng CNN",
      "chunk_id": 16,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Vấn đề đầu tiên mà chúng ta sẽ bàn về mạng RNN. Trong hai slide trước, chúng ta đưa ra những nhận xét. Những nhận xét đó sẽ là tiền đề để giải thích cho các vấn đề của mạng RNN. Vấn đề đầu tiên đó chính là sự phụ thuộc dài. Vấn đề đầu tiên đó là sự phụ thuộc dài hay còn gọi là long term dependency. Chúng ta lấy một cái ví dụ sau. Một cái ví dụ tiếng Anh sau. In France, I had a great time and I learned some of the... Chúng ta sẽ để trống. Language. Và nhiệm vụ của chúng ta cần phải đoán xem là.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IKD0O35NOUI",
      "filename": "IKD0O35NOUI",
      "title": "[CS431 - Chương 7] Part 3_2: Một số vấn đề của mạng RNN",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:42"
    }
  },
  {
    "page_content": "Và nhiệm vụ của chúng ta cần phải đoán xem là. Trong cái chỗ trống này đó là gì. Đúng không? Thì chúng ta thấy là cái mô hình RNN. Nó không có cái cơ chế để cho phép chúng ta nắm bắt sự phụ thuộc dài của từ. Tức là nói chứ truyền xt sang xt cộng 1, xt cộng 1 sang xt cộng 2, vân vân. Nhưng mà nó không có cơ chế để cho chúng ta có thể lưu thông tin đối với những từ rất xa trước đó. Nó không có cơ chế nào để ý thức được những từ ở đằng xa, đúng không? Ví dụ từ France. Nó sẽ có khoảng cách xa đến",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IKD0O35NOUI",
      "filename": "IKD0O35NOUI",
      "title": "[CS431 - Chương 7] Part 3_2: Một số vấn đề của mạng RNN",
      "chunk_id": 1,
      "start_timestamp": "0:00:39",
      "end_timestamp": "0:01:25"
    }
  },
  {
    "page_content": "Ví dụ từ France. Nó sẽ có khoảng cách xa đến chỗ chúng ta cần dự đoán hơn rất nhiều. so với những từ như là từ sum, từ of, từ the. thì nó không có cơ chế đó. và trong khi cái từ mà chúng ta cần dự đoán. thì nó lại phụ thuộc vào những cái từ đó. thì ở đây chúng ta đang nói về language. như vậy thì khả năng cao là cái từ này sẽ là. từ French, từ tiếng Pháp. nhưng mà muốn biết được cái từ này là từ French. nó phải bám vào cái từ. ở cách nó rất xa. xa so với những từ of, the, some, learn. đó chính",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IKD0O35NOUI",
      "filename": "IKD0O35NOUI",
      "title": "[CS431 - Chương 7] Part 3_2: Một số vấn đề của mạng RNN",
      "chunk_id": 2,
      "start_timestamp": "0:01:22",
      "end_timestamp": "0:02:04"
    }
  },
  {
    "page_content": "xa so với những từ of, the, some, learn. đó chính là cái từ France này. thì cái từ cần dự đoán phụ thuộc vào cái từ này. rất là xa. và RNN không có cơ chế để cho mình nắm bắt sự phụ thuộc dài này. Để giải thích cho cái việc này. đó là chúng ta sẽ dựa trên công thức của cái hàm độ lỗi. chúng ta sẽ dựa trên công thức của hàm độ lỗi. là chúng ta sẽ xét hai cái loss. xin lỗi, chúng ta sẽ xét hai cái loss là L2 và L_t. thì chúng ta sẽ có hai cái đạo hàm thành phần. và ở đây chúng ta cũng giả sử là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IKD0O35NOUI",
      "filename": "IKD0O35NOUI",
      "title": "[CS431 - Chương 7] Part 3_2: Một số vấn đề của mạng RNN",
      "chunk_id": 3,
      "start_timestamp": "0:01:59",
      "end_timestamp": "0:02:54"
    }
  },
  {
    "page_content": "hàm thành phần. và ở đây chúng ta cũng giả sử là chúng ta chỉ xét với cái biến W nha. hoàn toàn tương tự cho hai cái biến là V và U. Thì cái nhận xét đó là cái thành phần L2 nó sẽ đóng vai trò quan trọng hơn so với lại cái thành phần L_t. Trong cái công thức của cái hàm độ lỗi này, nó là bằng trung bình cộng của các cái hàm thành phần. Nhưng khi tính đạo hàm thì cái thành phần L2 nó lại đóng vai trò quan trọng hơn. Thì điều này là tại sao? Và cái việc này thì nó dẫn đến là cái từ thứ hai, là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IKD0O35NOUI",
      "filename": "IKD0O35NOUI",
      "title": "[CS431 - Chương 7] Part 3_2: Một số vấn đề của mạng RNN",
      "chunk_id": 4,
      "start_timestamp": "0:02:41",
      "end_timestamp": "0:03:36"
    }
  },
  {
    "page_content": "cái việc này thì nó dẫn đến là cái từ thứ hai, là cái từ gần, nó sẽ có ảnh hưởng hơn so với lại cái từ thứ t. Thì cái điều này nó cũng chính là ý nghĩa cho cái việc là phụ thuộc dài đó. Thì điều này giải thích tại sao? Tại vì cái hàm L2, nó gần, nó gần hơn nên cái hàm này, cái hàm hợp của nó, nó sẽ ít phép biến đổi hơn. Hàm L_t ở xa hơn nên nó sẽ nhiều phép biến đổi hơn. Hàm hợp nào mà càng nhiều phép biến đổi thì các con số. Ví dụ 0.9...0.9...0.9... Chuỗi này càng dài thì nó sẽ càng tiến đến",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IKD0O35NOUI",
      "filename": "IKD0O35NOUI",
      "title": "[CS431 - Chương 7] Part 3_2: Một số vấn đề của mạng RNN",
      "chunk_id": 5,
      "start_timestamp": "0:03:24",
      "end_timestamp": "0:04:25"
    }
  },
  {
    "page_content": "Chuỗi này càng dài thì nó sẽ càng tiến đến 0. Vậy thì vấn đề đặt ra là khi đạo hàm này tiến đến 0. Tức là nó đóng góp vào bên trong công thức của hàm tổng của chúng ta, tức là cái thằng này nó sẽ đóng góp ít. Trong khi đó L2 đóng góp vào hàm tổng này là đóng góp nhiều. mà cái hàm này thì nó lại là bằng trung bình cộng. nó lại bằng trung bình cộng của cái tổng. đó thì dẫn đến là cái thành phần mà đóng góp nhiều. nhưng mà hệ số của nó vẫn tương đương với lại cái hệ số của cái thằng đóng góp ít.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IKD0O35NOUI",
      "filename": "IKD0O35NOUI",
      "title": "[CS431 - Chương 7] Part 3_2: Một số vấn đề của mạng RNN",
      "chunk_id": 6,
      "start_timestamp": "0:04:20",
      "end_timestamp": "0:05:03"
    }
  },
  {
    "page_content": "với lại cái hệ số của cái thằng đóng góp ít. thì đó chính là cái vấn đề và nó gây ra cái sự phụ thuộc dài. có những cái từ T càng dài. Còn cái T mà càng dài thì đóng góp cho công thức đạo hàm là càng ít. Trong khi đó những từ rất là ngắn. những từ rất là ngắn. thì những từ ở đầu tiên thì lại đóng góp nhiều hơn. Nó tạo ra sự mất cân xứng. Trong nguyên một câu của mình thì rõ ràng là. câu nào, từ nào. tại những vị trí nào cũng đều có những giá trị nhất định. và hiện tượng thứ 2 đó chính là hiện",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IKD0O35NOUI",
      "filename": "IKD0O35NOUI",
      "title": "[CS431 - Chương 7] Part 3_2: Một số vấn đề của mạng RNN",
      "chunk_id": 7,
      "start_timestamp": "0:05:00",
      "end_timestamp": "0:05:47"
    }
  },
  {
    "page_content": "nhất định. và hiện tượng thứ 2 đó chính là hiện tượng vanishing gradient hoặc là exploding gradient. thì cũng dựa trên công thức của hàm hợp ở các slide trước. chúng ta có công thức đạo hàm của hàm hợp như sau. và nhận xét đó là khi văn bản của mình mà càng dài. tức là t này có thể tiến đến từ trừ vài trăm cho đến vài nghìn. thì đạo hàm tại L_t sẽ tiến đến 0. thì điều này, như chúng ta đã từng đề cập trước đó. cái công thức này, chúng ta có công thức đạo hàm của 1 cái hàm là sigmoid của Uxt.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IKD0O35NOUI",
      "filename": "IKD0O35NOUI",
      "title": "[CS431 - Chương 7] Part 3_2: Một số vấn đề của mạng RNN",
      "chunk_id": 8,
      "start_timestamp": "0:05:43",
      "end_timestamp": "0:06:43"
    }
  },
  {
    "page_content": "thức đạo hàm của 1 cái hàm là sigmoid của Uxt. cộng cho Ws_t-1. công thức này là công thức của s_t. Thì khi chúng ta tính đạo hàm của nó thì nó sẽ ra cái công thức này. Nếu mà chỉ tính s_t theo s_t-1 thì các thành phần này đều là các con số nhỏ hơn 1. Xin lỗi là từ 0 cho đến 1. Con số này là từ 0 cho đến 1. W của mình ban đầu nó cũng sẽ khởi tạo bởi một cái giá trị random. Nó là một cái ma trận bởi các giá trị random trong mean là bằng 0 và standard deviation là bằng 1. Như vậy nó cũng là các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IKD0O35NOUI",
      "filename": "IKD0O35NOUI",
      "title": "[CS431 - Chương 7] Part 3_2: Một số vấn đề của mạng RNN",
      "chunk_id": 9,
      "start_timestamp": "0:06:36",
      "end_timestamp": "0:07:25"
    }
  },
  {
    "page_content": "deviation là bằng 1. Như vậy nó cũng là các cái con số rất là nhỏ. Và các cái con số mà nhỏ thì khi nhân với nhau nó sẽ nảy sinh ra cái vấn đề đó. Vì vậy, ở đây chúng ta sẽ có giải pháp để giải quyết vấn đề về vanishing gradient, vấn đề về tiêu biến gradient. Đó là thay vì chúng ta sử dụng hàm sigmoid, thay vì chúng ta sử dụng hàm sigmoid, thì chúng ta sẽ sử dụng hàm khác. Có thể là sử dụng hàm tanh. Nhưng mà lưu ý là với hàm tanh thì dải giá trị của mình. Thay vì là từ 0 đến 1 thì nó sẽ là từ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IKD0O35NOUI",
      "filename": "IKD0O35NOUI",
      "title": "[CS431 - Chương 7] Part 3_2: Một số vấn đề của mạng RNN",
      "chunk_id": 10,
      "start_timestamp": "0:07:20",
      "end_timestamp": "0:08:06"
    }
  },
  {
    "page_content": "của mình. Thay vì là từ 0 đến 1 thì nó sẽ là từ trừ 1 cho đến 1. Thì suy cho cùng nó cũng là những con số với giá trị tuyệt đối. Bé hơn 1. Như vậy thì sigmoid và tanh không giúp cho mình giảm bớt hiện tượng vanishing này. Mà chúng ta sẽ sử dụng cái hàm ReLU. Tại vì sao? Hàm ReLU là có cái công thức như sau. là bằng max của 0 và x. như vậy thì hàm ReLU nó sẽ có cái đạo hàm với x mà lớn hơn 0. thì đạo hàm của nó sẽ là bằng 1. đạo hàm của nó sẽ là bằng 1. như vậy nó sẽ ngăn. nó sẽ giúp cho mình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IKD0O35NOUI",
      "filename": "IKD0O35NOUI",
      "title": "[CS431 - Chương 7] Part 3_2: Một số vấn đề của mạng RNN",
      "chunk_id": 11,
      "start_timestamp": "0:08:04",
      "end_timestamp": "0:08:47"
    }
  },
  {
    "page_content": "bằng 1. như vậy nó sẽ ngăn. nó sẽ giúp cho mình ngăn ngừa. nó sẽ giúp cho mình ngăn ngừa cái đạo hàm của mình. đạo hàm f_n, f_n-1. nó sẽ ngăn cho cái đạo hàm của mình bị tiêu biến dần. cho đến ma trận W. Thì đây cũng là một lý do tại sao từ năm 2012 sau cuộc thi MNIST thì tất cả, gần như tất cả các mô hình học sâu. đều chuyển từ sigmoid sang sử dụng các hàm ReLU hoặc các biến thể của ReLU. Và tiếp theo thì chúng ta sẽ giải quyết cái vấn đề liên quan đến ma trận W. Sigmoid thì chúng ta đã giải",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IKD0O35NOUI",
      "filename": "IKD0O35NOUI",
      "title": "[CS431 - Chương 7] Part 3_2: Một số vấn đề của mạng RNN",
      "chunk_id": 12,
      "start_timestamp": "0:08:44",
      "end_timestamp": "0:09:38"
    }
  },
  {
    "page_content": "quan đến ma trận W. Sigmoid thì chúng ta đã giải quyết rồi. do cái dải giá trị sigmoid là từ 0 cho đến 1. do đó chúng ta thay thế bằng ReLU. bây giờ đối với ma trận W thì ban đầu là chúng ta dùng cái phân bố là 0,1. thì các giá trị random của mình nó sẽ thường là sẽ nhỏ hơn 1 và lớn hơn 0. thì bây giờ W của mình mình sẽ cố định nó luôn là bằng một cái ma trận đơn vị. Ma trận đơn vị này, khi nhân với lại một cái ma trận khác, thì nó sẽ giúp ngăn ngừa cho việc thay đổi giá trị của cái ma trận. Ví",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IKD0O35NOUI",
      "filename": "IKD0O35NOUI",
      "title": "[CS431 - Chương 7] Part 3_2: Một số vấn đề của mạng RNN",
      "chunk_id": 13,
      "start_timestamp": "0:09:34",
      "end_timestamp": "0:10:21"
    }
  },
  {
    "page_content": "cho việc thay đổi giá trị của cái ma trận. Ví dụ, I nhân với A thì nó sẽ bằng chính là A luôn. Và nó sẽ giúp ngăn chặn sự tiêu biến giá trị. Và cụ thể là giá trị của cái biến này. Nó ngăn giảm giá trị này xuống, giá trị theo kiểu tuyệt đối. Như vậy thì ở đây chúng ta sẽ có hai giải pháp. Giải pháp đầu tiên là thay thế hàm sigmoid hoặc là hàm tanh bằng ReLU. Và giải pháp thứ hai, đó là giá trị w chúng ta sẽ khởi tạo nó bằng một cái ma trận đơn vị. Và đây là hai cách để giúp chúng ta chống lại",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IKD0O35NOUI",
      "filename": "IKD0O35NOUI",
      "title": "[CS431 - Chương 7] Part 3_2: Một số vấn đề của mạng RNN",
      "chunk_id": 14,
      "start_timestamp": "0:10:13",
      "end_timestamp": "0:10:52"
    }
  },
  {
    "page_content": "vị. Và đây là hai cách để giúp chúng ta chống lại hiện tượng vanishing gradient. Và ngoài ra thì chúng ta sẽ còn một số cái vấn đề khác, ví dụ như cái vấn đề về Exploding Gradient. Exploding Gradient nó là ngược của Vanishing Gradient nếu như cái đạo hàm của mình mà lớn quá. các cái con số mà lớn thì khi nhân với nhau nó cũng sẽ có xu hướng là tiến đến cộng vô cùng. Như vậy thì ở đây người ta sẽ có một cái kỹ thuật để chống lại hiện tượng Exploding Gradient. Mình sẽ sử dụng clipping. Ngưỡng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IKD0O35NOUI",
      "filename": "IKD0O35NOUI",
      "title": "[CS431 - Chương 7] Part 3_2: Một số vấn đề của mạng RNN",
      "chunk_id": 15,
      "start_timestamp": "0:10:50",
      "end_timestamp": "0:11:39"
    }
  },
  {
    "page_content": "Gradient. Mình sẽ sử dụng clipping. Ngưỡng chặn. Tức là gradient mà quá lớn thì mình sẽ lấy nó. làm một mức trần thôi. Thì đó là exploding gradient. Bên cạnh. các giải pháp về gradient. thì người ta có một số. phương pháp khác. đó là chúng ta. thay các cái node. trong cái mạng. Recurrent Neural Network. thay vì chúng ta sử dụng. Cái cell ở dạng đơn giản, chúng ta có thể thay thế bằng các cái cổng. Chúng ta sẽ thay thế bằng các cái cổng để kiểm soát thông tin. Ví dụ đối với Cell này, thì các hàm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IKD0O35NOUI",
      "filename": "IKD0O35NOUI",
      "title": "[CS431 - Chương 7] Part 3_2: Một số vấn đề của mạng RNN",
      "chunk_id": 16,
      "start_timestamp": "0:11:26",
      "end_timestamp": "0:11:39"
    }
  },
  {
    "page_content": "thông tin. Ví dụ đối với Cell này, thì các hàm sigmoid của mình. Với các hàm tanh hoặc hàm sigmoid của mình khi chúng ta thực hiện. Thì nó sẽ dễ tiêu biến và dễ tiêu biến có khả năng là nó làm cho thông tin của mình bị mất mát đi. Do đó thì chúng ta sẽ sử dụng LSTM cell để điều tiết thông tin, nhớ cái cần nhớ và quên cái cần quên.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IKD0O35NOUI",
      "filename": "IKD0O35NOUI",
      "title": "[CS431 - Chương 7] Part 3_2: Một số vấn đề của mạng RNN",
      "chunk_id": 17,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Trong vòng tiếp theo, chúng ta sẽ cùng tìm hiểu về ứng dụng của Transformer cũng như các thành tựu của nó. Đầu tiên, đó là các mô hình nền tảng BERT và GPT. Đây là hai mô hình ngôn ngữ được huấn luyện trên tập dữ liệu lớn. Ở hai bước của Transformer BERT là cho giai đoạn encoder và GPT là cho giai đoạn decoder. BERT là viết tắt của chữ Bidirectional Encoder Representation from Transformer. Chúng ta thấy đều có cái từ là Transformer. GPT là Generative. Generative này chính là decoder. Pre-trained",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:48"
    }
  },
  {
    "page_content": "Generative này chính là decoder. Pre-trained Transformer. Chúng ta cũng thấy cả hai mô hình này đều base, đều dựa trên kiến trúc của Transformer. Và điểm chung đó là đều thuộc cái nhóm tự học self-supervised learning. Tức là học khi không có dữ liệu gắn nhãn Học khi không có dữ liệu gắn nhãn Đều sử dụng dữ liệu không gắn nhãn Rồi đều dùng để biểu diễn Thì ở đây có thể biểu diễn gì? Chúng ta có thể sử dụng để biểu diễn từ Biểu diễn từ Tức là khi chúng ta đưa vào một câu Nó sẽ dựa trên ngữ cảnh",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 1,
      "start_timestamp": "0:00:40",
      "end_timestamp": "0:01:23"
    }
  },
  {
    "page_content": "chúng ta đưa vào một câu Nó sẽ dựa trên ngữ cảnh của những từ xung quanh Để đưa ra biểu diễn của từ đó Nó gọi là contextual embedding Tại sao nó lại có khái niệm này? Và tại sao phải có yếu tố về mặt ngữ cảnh? Đó là vì, ví dụ cái từ Apple Nếu như chúng ta không có ngữ cảnh của những từ xung quanh chúng ta sẽ không biết Apple ở đây là trái táo hay Apple ở đây là tên của một công ty của một công ty, chúng ta phải có những cái từ xung quanh chúng ta mới biết được. Thế thì đó chính là công dụng của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 2,
      "start_timestamp": "0:01:20",
      "end_timestamp": "0:02:06"
    }
  },
  {
    "page_content": "mới biết được. Thế thì đó chính là công dụng của contextual embedding, đó là biểu diễn từ khi có yếu tố bề mặt ngữ cảnh. Rồi, cả hai BERT và GPT đều sử dụng transformer như đã đề cập. Và nó đều có sử dụng, có thể sử dụng để làm cho các cái downstream task. Downstream task có nghĩa là gì? Đó là những cái task mà không phải là task chính của BERT và GPT. Đó là những task phụ không được... Tức là trong quá trình huấn luyện BERT và GPT, nó không được huấn luyện để giải quyết các nhiệm vụ này. BERT",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 3,
      "start_timestamp": "0:02:01",
      "end_timestamp": "0:02:45"
    }
  },
  {
    "page_content": "huấn luyện để giải quyết các nhiệm vụ này. BERT và GPT được sử dụng để huấn luyện cho bài toán khác, đó là bài toán dự đoán từ. BERT thì dự đoán từ ở giữa, từ bị che. Còn GPT thì để dự đoán từ tiếp theo. Nó không được huấn luyện để giải quyết các task, ví dụ task phân loại văn bản (sentiment analysis), sentiment analysis, hoặc là cho task như là QA, question answering, trả lời cái câu hỏi, hoặc là dịch máy, translation. Rõ ràng là các mô hình như BERT và GPT không được huấn luyện để giải quyết",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 4,
      "start_timestamp": "0:02:42",
      "end_timestamp": "0:03:35"
    }
  },
  {
    "page_content": "BERT và GPT không được huấn luyện để giải quyết các task này nhưng khi chúng ta sử dụng mô hình đã được pre-trained, chúng ta có thể sử dụng và khai thác nó để giải quyết task này BERT không sinh ra để giải quyết task này, nó huấn luyện để giải quyết bài toán đoán từ nhưng chúng ta có thể sử dụng mô hình này để cho downstream task khác Đó là ý nghĩa của ý cuối này. Đối với mô hình BERT, đó là một mô hình ngôn ngữ masked language model, masked language model. Còn GPT là mô hình ngôn ngữ tự hồi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 5,
      "start_timestamp": "0:03:29",
      "end_timestamp": "0:04:07"
    }
  },
  {
    "page_content": "model. Còn GPT là mô hình ngôn ngữ tự hồi quy, auto-regressive, tức là chúng ta sẽ đoán ra cái từ tiếp theo. Còn masked language model, tức là chúng ta sẽ che đi một từ ở giữa, một từ bất kỳ, một từ ngẫu nhiên. Ta sẽ phải đoán cái từ đó bị che là từ gì. Đó là hai cái mô hình. Cấu tạo thì BERT là bao gồm encoder trong transformer và GPT thì là cấu tạo bởi decoder trong transformer. Nhiệm vụ BERT là đoán cái từ bị che là mask word còn GPT sẽ là đoán cái từ tiếp theo là next word. Và các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 6,
      "start_timestamp": "0:04:06",
      "end_timestamp": "0:04:51"
    }
  },
  {
    "page_content": "sẽ là đoán cái từ tiếp theo là next word. Và các downstream task, những task mà có thể sử dụng để giải quyết với mô hình BERT, đó là phân loại văn bản, trả lời câu hỏi, tóm tắt văn bản hoặc là nhận diện thực thể Named Entity Recognition. Còn các downstream task cho GPT mà nó khá phù hợp đó chính là dịch máy và tạo sinh nội dung tự động. Rồi, để sử dụng hai mô hình nền tảng này, để sử dụng được các mô hình nền tảng thì chúng ta sẽ có hai cách. Cách đầu tiên đó là fine-tuning. Fine-tuning hiểu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 7,
      "start_timestamp": "0:04:44",
      "end_timestamp": "0:06:03"
    }
  },
  {
    "page_content": "Cách đầu tiên đó là fine-tuning. Fine-tuning hiểu một cách nôm na đó chính là chúng ta sẽ huấn luyện lại hoặc là chúng ta sẽ thay đổi các tham số của mô hình. Còn prompting là chúng ta sẽ gần như tạo ra các chỉ thị, chỉ dẫn cho mô hình, chỉ dẫn cho mô hình có thể thực thi, cho mô hình đưa ra các phán đoán. và quan trọng đó là không làm thay đổi tham số của mô hình. Với phương pháp fine-tuning thì chúng ta phải sử dụng một thuật toán gradient descent để tối ưu lại trọng số cho một task nào đó.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 8,
      "start_timestamp": "0:05:48",
      "end_timestamp": "0:06:40"
    }
  },
  {
    "page_content": "để tối ưu lại trọng số cho một task nào đó. và chúng ta sẽ có những cách để fine-tune. Chúng ta sẽ fine-tune lại toàn bộ mô hình. Chúng ta sẽ tạo một đầu ra của mô hình. Với fine-tune toàn bộ này, chúng ta sẽ có cách gọi là Readout Head. Chúng ta sẽ tạo ra cái module tại đầu ra, sau đó chúng ta sẽ fine-tune trên toàn bộ mô hình. Sau đó chúng ta cũng có thể sử dụng một kỹ thuật Cái kỹ thuật đó gọi là một kỹ thuật khác thay cho Readout Head, đó chính là adapter. Với tất cả các phương pháp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 9,
      "start_timestamp": "0:06:31",
      "end_timestamp": "0:07:18"
    }
  },
  {
    "page_content": "đó chính là adapter. Với tất cả các phương pháp fine-tune ở đây, chúng ta đều phải thay đổi tham số của mô hình, dù ít dù nhiều. Ở đây là thay đổi toàn bộ, hai cái này là thay đổi toàn bộ. Còn adapter thì có thể là chúng ta chỉ thay đổi cho một phần của mô hình mà thôi. Còn prompting thì chúng ta sẽ phải thiết kế những prompt đặc biệt để gợi ý và ràng buộc mô hình để giải quyết một task nào đó. Và ở đây là chúng ta sẽ không cần cập nhật tham số mô hình. Và ở đây chúng ta sẽ thay đổi cách thức",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 10,
      "start_timestamp": "0:07:14",
      "end_timestamp": "0:08:07"
    }
  },
  {
    "page_content": "mô hình. Và ở đây chúng ta sẽ thay đổi cách thức sử dụng mô hình. Bên đây là chúng ta sẽ thay đổi tham số mô hình, còn bên đây là chúng ta sẽ thay đổi cách sử dụng mô hình thông qua prompt. Đối với phương pháp Fine-tuning với đầu ra của mô hình tức là Readout Head, chúng ta sẽ thêm đầu ra và hàm kích hoạt phù hợp để giải quyết một bài toán. Ví dụ, tại đầu ra của mô hình BERT, chúng ta sẽ đưa thêm qua một linear Kết hợp với lại một softmax để tính toán được class của nhãn đầu vào. Ví dụ ở đây là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 11,
      "start_timestamp": "0:08:04",
      "end_timestamp": "0:08:47"
    }
  },
  {
    "page_content": "toán được class của nhãn đầu vào. Ví dụ ở đây là nhãn chẳng hạn. Còn cho bài toán phân loại văn bản, ở đây chúng ta sẽ có một linear module và cộng với lại, thay vì ở đây là phân loại đa lớp thì ở đây chúng ta chỉ cần là softmax. Nếu như ở đây chúng ta phân lớp ra là positive và negative thôi, thì ở đây chúng ta sẽ là hàm sigmoid thôi. Còn nếu như ở đây là phân loại văn bản nhưng mà cho nhiều lớp thì chúng ta có thể là softmax. Vì vậy, tùy vào đầu ra của task của mình là gì thì mình sẽ có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 12,
      "start_timestamp": "0:08:34",
      "end_timestamp": "0:09:32"
    }
  },
  {
    "page_content": "vào đầu ra của task của mình là gì thì mình sẽ có activation tương ứng cho nó phù hợp và module linear cho nó phù hợp. Còn cho bài toán trả lời câu hỏi thì ở đây mình sẽ phải làm bài toán regression. Tức là chúng ta sẽ có cái start và cái end, và cái span, tức là start end, tức là cái đoạn thông tin ở bên trong đoạn văn đầu vào của mình. Span là cái mở rộng ra để đưa ra câu trả lời tương ứng với cái câu hỏi của mình. Vì vậy, chúng ta sẽ phải thích ứng theo từng task của mình để từ đó chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 13,
      "start_timestamp": "0:09:25",
      "end_timestamp": "0:10:09"
    }
  },
  {
    "page_content": "ứng theo từng task của mình để từ đó chúng ta thiết kế đầu ra tại Readout Head cho phù hợp. Và ở đây chúng ta lưu ý là chúng ta sẽ fine-tune toàn bộ mô hình. Vì vậy, phương pháp này có khả năng chi phí tính toán của mình sẽ rất là lớn. Và fine-tune với adapter là chúng ta sẽ thêm một module nhỏ vào mô hình ngôn ngữ. và chúng ta chỉ fine-tune cho một module vừa được thêm. Vì vậy thì phương pháp này sẽ rất tiết kiệm chi phí tính toán cũng như là có thể dễ dàng lưu được mô hình của mình. Ví dụ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 14,
      "start_timestamp": "0:10:02",
      "end_timestamp": "0:10:46"
    }
  },
  {
    "page_content": "có thể dễ dàng lưu được mô hình của mình. Ví dụ trong phương pháp prefix tuning, bình thường chúng ta sẽ có các ma trận W_Q, W_K, W_V. Ở đây chúng ta vẫn sẽ cố định W_Q, W_K, W_V, nhưng chúng ta sẽ gắn thêm một phần nữa vào đầu. Đầu prefix, chúng ta sẽ gắn thêm một cái ma trận vào đầu, hoặc là concatenate vào đầu cái ma trận K, concatenate một cái ma trận khác vào đầu ma trận V. Sau đó chúng ta chỉ đi fine-tune trên cái bộ tham số của cái phần ma trận màu hồng này và màu tím này thôi. Chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 15,
      "start_timestamp": "0:10:37",
      "end_timestamp": "0:11:32"
    }
  },
  {
    "page_content": "trận màu hồng này và màu tím này thôi. Chúng ta không có fine-tune cho Q, K, V cũ, chúng ta chỉ fine-tune cho những cái phần mới thêm vào này. Low-rank adaptation là một cái phương pháp rất là nổi tiếng. Tại mỗi ma trận W, chúng ta sẽ có tách ra. Đây là cái đã được pre-train. Đây là cái ma trận đã được pre-train. Chúng ta sẽ kết hợp với hai cái ma trận là A và B là hai cái ma trận. Trong đó, mục tiêu của A là để giảm chiều và mục tiêu của B là để khôi phục lại chiều ban đầu. Và đây là phương",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 16,
      "start_timestamp": "0:11:26",
      "end_timestamp": "0:12:11"
    }
  },
  {
    "page_content": "để khôi phục lại chiều ban đầu. Và đây là phương pháp low rank. sau khi chúng ta huấn luyện A và B này xong, chúng ta sẽ cộng lại để ra được một cái ma trận tổng của ma trận pre-trained và A nhân B và cái A nhân B này sẽ giúp chúng ta giải quyết cái bài toán mới Tương tự như vậy, hai cái ma trận màu tím và màu hồng ở đây thì cũng là giúp chúng ta đi giải quyết bài toán mới. Đây là hai phương pháp điển hình của Fine-tune với Adapter. Và đối với phương pháp về prompting, chúng ta sẽ cho mô hình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 17,
      "start_timestamp": "0:12:04",
      "end_timestamp": "0:12:53"
    }
  },
  {
    "page_content": "phương pháp về prompting, chúng ta sẽ cho mô hình nó học từ ngữ cảnh. Nghĩa là sao? Ở đây chúng ta sẽ cho một số ví dụ về task chúng ta cần giải Và mô hình sẽ tự tìm ra cách giải quyết Lấy ví dụ, ở đây chúng ta sẽ cho trước một số cặp input và output Ví dụ như ở đây chúng ta sẽ có cái input là một câu Circulation revenue has increased by 5% in Finland Thì ở đây chúng ta sẽ có cái output của mình là positive Paying off the national debt will be extremely painful. Thì ở đây chúng ta sẽ có cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 18,
      "start_timestamp": "0:02:49",
      "end_timestamp": "0:13:28"
    }
  },
  {
    "page_content": "extremely painful. Thì ở đây chúng ta sẽ có cái output của mình là negative. Ở đây chúng ta sẽ có neutral. Và ở đây chúng ta sẽ để thêm là the company anticipated its operating profit to improve. Thì chúng ta sẽ để dấu gạch chéo để trống. Mô hình sẽ tự biết là ở trên đây là positive, ở đây là neutral, ở đây là negative thì tự điền vô chỗ trống này là positive hay negative hay neutral và nó sẽ tự biết mối quan hệ giữa cặp input output này Mô hình sẽ tự học theo ngữ cảnh Đây là 3 cái context để",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 19,
      "start_timestamp": "0:13:19",
      "end_timestamp": "0:14:17"
    }
  },
  {
    "page_content": "sẽ tự học theo ngữ cảnh Đây là 3 cái context để giúp cho mình đưa ra cái phán đoán tại vị trí output cho cái sample mới này Tương tự như vậy cho cái bài toán là phân loại văn bản Chúng ta sẽ cho trước, đây là chủ đề về Finance, chủ đề về Sport, chủ đề về Tech thì bên đây nó sẽ từ input này nó sẽ đưa ra phán đoán cái output của mình thì đây là phương pháp Prompting và chúng ta sẽ có một số thuật ngữ trong phương pháp này, đó là Zero-shot Prompting tức là chúng ta sẽ không cần cho mẫu này luôn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 20,
      "start_timestamp": "0:14:09",
      "end_timestamp": "0:15:08"
    }
  },
  {
    "page_content": "tức là chúng ta sẽ không cần cho mẫu này luôn Chúng ta sẽ không cần cho mẫu, chúng ta sẽ hỏi nó là The committee anticipated its operating profit to improve Thì nó là positive, negative hay neutral Cái sentiment của nó là positive, negative hay neutral Chúng ta sẽ hỏi nó luôn One-shot Tức là chúng ta sẽ cho nó một mẫu và few-shot là chúng ta sẽ cho nó nhiều mẫu Few-shot. Và chúng ta sẽ có phương pháp kết hợp giữa fine-tune với lại prompting, nó gọi là instruction tuning Chúng ta sẽ vừa có sự",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 21,
      "start_timestamp": "0:14:53",
      "end_timestamp": "0:15:40"
    }
  },
  {
    "page_content": "gọi là instruction tuning Chúng ta sẽ vừa có sự tinh chỉnh mô hình, nhưng ở đây chúng ta sẽ tinh chỉnh mô hình để trả lời cho các câu hỏi Và mô hình sẽ tự tổng quát hóa để giải cho các task khác, chưa từng thấy. Chúng ta sẽ có pre-trained language model Chúng ta sẽ instruction tune trên các task BCD Đây là task cũ Chúng ta sẽ thay đổi, chúng ta sẽ tinh chỉnh cái tham số của mô hình Và sang giai đoạn suy luận chúng ta có thể thực hiện trên task mới hoàn toàn, đó là task A Và cái template cho",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 22,
      "start_timestamp": "0:15:34",
      "end_timestamp": "0:16:19"
    }
  },
  {
    "page_content": "mới hoàn toàn, đó là task A Và cái template cho mình đó sẽ bao gồm là premise, tức là cái nội dung ngữ cảnh mình đưa vào, hypothesis và các cái option output của mình là gì? thì đây chúng ta sẽ có 4 cái template, ví dụ cho các task để mà chúng ta fine-tune mô hình. Chúng ta đưa vô, chúng ta fine-tune xong, thì cái mô hình đã được chỉnh sửa, đã được tinh chỉnh tham số, thì nó sẽ có thể, khả năng là giải quyết được cho cái task mới, task A. Và Transformer nó không chỉ làm cho dữ liệu văn bản,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 23,
      "start_timestamp": "0:16:10",
      "end_timestamp": "0:17:00"
    }
  },
  {
    "page_content": "Transformer nó không chỉ làm cho dữ liệu văn bản, không chỉ làm cho dữ liệu văn bản mà Transformer còn có thể mở rộng cho các dữ liệu dạng chuỗi khác chúng ta có thể kể đến ví dụ như là Transformer thực hiện được trên dữ liệu âm thanh và điển hình cho dữ liệu âm thanh đó là chúng ta có mô hình Whisper của OpenAI thì đây là một trong những mô hình state-of-the-art cho bài toán là Speech-to-Text bài toán nhận diện giọng nói, từ giọng nói, biến thành văn bản Rồi, ở đây thì mình ghi nhầm đó là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 24,
      "start_timestamp": "0:16:53",
      "end_timestamp": "0:17:58"
    }
  },
  {
    "page_content": "thành văn bản Rồi, ở đây thì mình ghi nhầm đó là chúng ta sẽ có mô hình Vision Transformer Mô hình Vision Transformer, và dữ liệu chuỗi ở đây chúng ta cũng có thể hiểu đó là dữ liệu ảnh đó là chuỗi các cái pixel hoặc là chuỗi các cái patch, patch này đến trước, patch này đến sau Và ở đây chúng ta sẽ lưu ý yếu tố đó là 2 chiều. Chuỗi này của chúng ta là đi theo 2 chiều. Rồi, và cuối cùng đó chính là chúng ta có một ví dụ đó là trên multimodal. Tức là vừa có sự kết hợp của cả ảnh và text. Thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 25,
      "start_timestamp": "0:17:46",
      "end_timestamp": "0:18:52"
    }
  },
  {
    "page_content": "Tức là vừa có sự kết hợp của cả ảnh và text. Thì trong cái model là Stable Diffusion, chúng ta thấy là có sự tham gia của text là đóng vai trò là conditioning để can thiệp vào không gian latent để cho chúng ta có thể chỉnh sửa nội dung của tấm ảnh theo mong muốn của text, nội dung text này. Đó chính là một số thành tựu của Transformer không chỉ trên lĩnh vực về văn bản mà nó còn có thể làm trên các loại dữ liệu như là âm thanh, hình ảnh hoặc là multimodal, ví dụ như hình ảnh, kết hợp với văn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 26,
      "start_timestamp": "0:18:46",
      "end_timestamp": "0:18:56"
    }
  },
  {
    "page_content": "multimodal, ví dụ như hình ảnh, kết hợp với văn bản. Vì vậy, trong bài ngày hôm nay, chúng ta đã tìm hiểu qua về motivation của kiến trúc Transformer. Rồi, chúng ta đồng thời cũng đã tìm hiểu về kiến trúc kinh điển của Transformer. Chúng ta đã tìm hiểu qua về các khuyết điểm, một số cái vấn đề cần tồn tại và một số giải pháp ban đầu của Transformer. Và cuối cùng đó là những ứng dụng của Transformer Vì vậy, chúng ta có thể fine-tune để giải quyết các task, giải quyết downstream task. Chúng ta có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 27,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "các task, giải quyết downstream task. Chúng ta có thể prompting để chỉ dẫn cho mô hình hiểu cái context, hiểu cái ngữ cảnh. và chúng ta có kiểu là Zero-shot, One-shot, Few-shot đồng thời chúng ta có thể áp dụng trong lĩnh vực không chỉ là văn bản mà nó có thể áp dụng trong loại dữ liệu là âm thanh, hình ảnh và kết hợp cả ảnh cộng với lại văn bản Đó chính là nội dung của bài học ngày hôm nay.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=iMfkIHkU6NM",
      "filename": "iMfkIHkU6NM",
      "title": "[CS431 - Chương 10] Part 7: Một số ứng dụng của kiến trúc mạng Transformer",
      "chunk_id": 28,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Trước tiên chúng ta sẽ visualize hàm loss như thế nào. Ở đây sẽ là plt.plot. Và để truy xuất vô hàm loss thì chúng ta sẽ để là his.history. Rồi, để đây sẽ là hàm loss. và tương tự như vậy chúng ta sẽ có và vẽ rồi, legend thì chúng ta sẽ phải vẽ plt.legend rồi Về Validation Về Validation Về Validation Màu xanh là tương ứng cho loss của tập train. Validation sẽ được vẽ cho màu cam. Màu cam, Validation nằm ở phía trên so với tập train. Tiến hành trực quan hóa mô hình. Chúng ta sẽ copy đoạn code để",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=istYhrhklqs",
      "filename": "istYhrhklqs",
      "title": "[CS431 - Chương 2] Part 3b_2: Cài đặt mô hình logistic regression",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:53"
    }
  },
  {
    "page_content": "quan hóa mô hình. Chúng ta sẽ copy đoạn code để vẽ các data point. Chúng ta clear đi để nó gọn. Rồi, vẽ lại các dữ liệu bắt đầu. Bây giờ chúng ta phải vẽ mô hình này. Về cách chúng ta có thể vẽ được mô hình này, chúng ta phải quay qua bên đây để xem phương trình đường thẳng này là gì. Thì nếu mà thông thường thì phương trình cho model này sẽ ở dạng là theta0 cộng theta1 x1 cộng theta2 x2. và phương trình đường thẳng này sẽ là bằng 0. Tất cả những điểm màu xanh, dù ở đây trong trường hợp này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=istYhrhklqs",
      "filename": "istYhrhklqs",
      "title": "[CS431 - Chương 2] Part 3b_2: Cài đặt mô hình logistic regression",
      "chunk_id": 1,
      "start_timestamp": "0:01:43",
      "end_timestamp": "0:02:49"
    }
  },
  {
    "page_content": "điểm màu xanh, dù ở đây trong trường hợp này tương ứng y là bằng 1 và ở đây tương ứng y là bằng 0, thì tất cả những điểm nằm về cùng phía với màu xanh sẽ khiến cho bộ giá trị này lớn bằng 0. Còn nếu như với những điểm màu cam, tức là cho nhãn y bằng 0, thì nó sẽ làm cho theta 0 cộng cho theta1 x1 cộng cho theta2 x2 sẽ bằng 0. Còn những cái điểm nào, x1, x2 nào mà nằm trên đường thẳng này, thì khi thế vô nó sẽ có giá trị bằng 0. Và để trực quan hóa cái mô hình của mình, thì chúng ta sẽ phải đi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=istYhrhklqs",
      "filename": "istYhrhklqs",
      "title": "[CS431 - Chương 2] Part 3b_2: Cài đặt mô hình logistic regression",
      "chunk_id": 2,
      "start_timestamp": "0:02:46",
      "end_timestamp": "0:03:25"
    }
  },
  {
    "page_content": "hóa cái mô hình của mình, thì chúng ta sẽ phải đi vẽ phương trình đường thẳng này. Tuy nhiên, để vẽ được phương trình đường thẳng này, thì chúng ta sẽ phải dùng một cái trick, đó là chúng ta sẽ đưa về cái dạng phương trình giống như thời xưa là y bằng ax cộng b. Chúng ta không có khái niệm y và x, giống như hồi cấp 2 mà chúng ta phải đưa về khái niệm y của mình chẳng hạn x2 của mình. Vì vậy, chúng ta sẽ chuyển vế theta2 x2 bằng trừ theta0 trừ cho theta1 x1. Chia cho Theta2, chúng ta sẽ có công",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=istYhrhklqs",
      "filename": "istYhrhklqs",
      "title": "[CS431 - Chương 2] Part 3b_2: Cài đặt mô hình logistic regression",
      "chunk_id": 3,
      "start_timestamp": "0:03:19",
      "end_timestamp": "0:04:30"
    }
  },
  {
    "page_content": "theta1 x1. Chia cho Theta2, chúng ta sẽ có công thức đó là x2 là bằng trừ Theta1 phần Theta2 x1 Trừ cho Theta0 chia cho Theta2. Và dựa trên công thức này, khi chúng ta vẽ lên trên đồ thị, chúng ta sẽ pick ra các giá trị x1. Ví dụ như pick ra một giá trị x1 ở đây, pick ra giá trị x1 ở đây. Thế vào chúng ta sẽ có x2 và từ đó chúng ta sẽ vẽ được 2 cái điểm này. Và khi có 2 điểm này rồi, nối lại thì chúng ta sẽ có đường thẳng. Rồi, như vậy thì ở đây chúng ta sẽ phải tìm coi cái theta 0, theta 1,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=istYhrhklqs",
      "filename": "istYhrhklqs",
      "title": "[CS431 - Chương 2] Part 3b_2: Cài đặt mô hình logistic regression",
      "chunk_id": 4,
      "start_timestamp": "0:04:22",
      "end_timestamp": "0:05:48"
    }
  },
  {
    "page_content": "chúng ta sẽ phải tìm coi cái theta 0, theta 1, theta 2 nó là cái gì. Thì muốn vậy thì chúng ta sẽ gọi cái hàm get_weights. Chúng ta sẽ xem W này giá trị gì. Thì W thành phần đầu tiên là array bao gồm 2 con số trừ 0.9 và 0.79. thì nó tương ứng là tham số cho 2 thành phần x1 và x2. Thì bias sẽ nằm trong array cuối, như vậy thì mình sẽ có theta 0 là thành phần cuối, nó sẽ là W1. Theta 1 là bằng W[0], lấy phần tử đầu tiên. Tương tự như vậy cho Theta2, chúng ta sẽ lấy W[0] và lấy phần tử ở vị trí",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=istYhrhklqs",
      "filename": "istYhrhklqs",
      "title": "[CS431 - Chương 2] Part 3b_2: Cài đặt mô hình logistic regression",
      "chunk_id": 5,
      "start_timestamp": "0:05:39",
      "end_timestamp": "0:07:14"
    }
  },
  {
    "page_content": "chúng ta sẽ lấy W[0] và lấy phần tử ở vị trí thứ 2, chúng ta sẽ có Theta0, Theta1, Theta2. Bây giờ chúng ta sẽ lần lượt vẽ. Thì chúng ta sẽ thế các giá trị là tại trừ 1. Thế giá trị x1 tại trừ 1. Và thế giá trị x1 tại giá trị là 6. Thì chúng ta sẽ có 2 giá trị là trừ 1 và 6. Đây chính là x1. Tương ứng tọa độ trục x2 tương ứng của nó thì chúng ta sẽ dựa trên công thức này. Chúng ta sẽ dựa trên công thức là x2 là bằng trừ theta 1 chia cho theta 2 nhân x1. Trừ cho theta 0 chia cho theta 2. Vì vậy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=istYhrhklqs",
      "filename": "istYhrhklqs",
      "title": "[CS431 - Chương 2] Part 3b_2: Cài đặt mô hình logistic regression",
      "chunk_id": 6,
      "start_timestamp": "0:07:09",
      "end_timestamp": "0:08:13"
    }
  },
  {
    "page_content": "nhân x1. Trừ cho theta 0 chia cho theta 2. Vì vậy thì ở đây mình sẽ có một cái mảng. Trừ theta 1 chia cho theta 2 nhân cho x1. x1 là trường hợp này là trừ 1. rồi chúng ta copy. à quên chúng ta còn một thành phần nữa là trừ theta. 0 chia cho theta2. sau đó chúng ta sẽ copy. cái công thức này. khi chúng ta thế với cái điểm. Thứ 2 là điểm 6. Rồi ở đây sẽ là điểm 6. Rồi ở đây là plt.plot. Khi chúng ta vẽ lên trên. Khi chúng ta vẽ lên trên thì. Chúng ta sẽ thấy là. Nó tạo ra một cái đường thẳng.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=istYhrhklqs",
      "filename": "istYhrhklqs",
      "title": "[CS431 - Chương 2] Part 3b_2: Cài đặt mô hình logistic regression",
      "chunk_id": 7,
      "start_timestamp": "0:08:11",
      "end_timestamp": "0:09:24"
    }
  },
  {
    "page_content": "ta sẽ thấy là. Nó tạo ra một cái đường thẳng. Đường thẳng này tách ra làm 2. Đây là vẽ đường thẳng phân chia từ tham số của mô hình. Vì vậy, trong bài cài đặt này, chúng ta đã tiến hành sử dụng Keras cài đặt cho mô hình Logistic Regression và kế thừa được những phương thức như save, load, summary, predict. Cách gọi những hàm này cũng hoàn toàn tương tự như bài Linear Regression. Save thì chúng ta chỉ cần truyền đường dẫn vào file. Load thì chúng ta cũng phải đưa đường dẫn của file mà nó đã lưu.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=istYhrhklqs",
      "filename": "istYhrhklqs",
      "title": "[CS431 - Chương 2] Part 3b_2: Cài đặt mô hình logistic regression",
      "chunk_id": 8,
      "start_timestamp": "0:09:19",
      "end_timestamp": "0:10:05"
    }
  },
  {
    "page_content": "ta cũng phải đưa đường dẫn của file mà nó đã lưu. Và đồng thời trong cái bài này thì chúng ta có thêm một phần là trực quan hóa cái kết quả của các giá trị loss trong quá trình train và validation. Thì đối với phần loss của tập train, lúc nào nó cũng có giá trị loss thấp hơn do đó có khi hiện tượng là overfitting. Và Validation thì thường là loss nó sẽ cao hơn so với tập train. Và để trực quan hóa cho mô hình thì chúng ta sẽ phải xác lập phương trình của đường thẳng là theta0 cộng theta1 x1",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=istYhrhklqs",
      "filename": "istYhrhklqs",
      "title": "[CS431 - Chương 2] Part 3b_2: Cài đặt mô hình logistic regression",
      "chunk_id": 9,
      "start_timestamp": "0:09:57",
      "end_timestamp": "0:10:46"
    }
  },
  {
    "page_content": "trình của đường thẳng là theta0 cộng theta1 x1 cộng theta2 x2 bằng 0. và chúng ta sẽ chuyển đổi nó về dạng x2 là bằng trừ theta1 chia cho theta2 nhân x1 trừ cho theta0 chia cho theta2. để đưa về cái dạng quen thuộc giống thời xưa, y bằng ax cộng b. thì một thành phần x1 là chúng ta sẽ pick ra giá trị là 1. chúng ta sẽ pick ra giá trị là trừ 1. rồi sau đó chúng ta sẽ pick ra giá trị là 6. và thế nó vào thì chúng ta sẽ có tọa độ tương ứng của x2. Đây là cách thức để trực quan cho bài Logistic",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=istYhrhklqs",
      "filename": "istYhrhklqs",
      "title": "[CS431 - Chương 2] Part 3b_2: Cài đặt mô hình logistic regression",
      "chunk_id": 10,
      "start_timestamp": "0:10:41",
      "end_timestamp": "0:10:46"
    }
  },
  {
    "page_content": "Đây là cách thức để trực quan cho bài Logistic Regression.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=istYhrhklqs",
      "filename": "istYhrhklqs",
      "title": "[CS431 - Chương 2] Part 3b_2: Cài đặt mô hình logistic regression",
      "chunk_id": 11,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Về lý thuyết thì decoder cũng sẽ tương tự như là encoder. Tuy nhiên, output của decoder sẽ có một vấn đề như thế này. Đó là decode là một quá trình mà chúng ta giải mã tuần tự. Giải mã tuần tự. Chúng ta không thể nào thực hiện song song được. Tại vì việc song song tương đương với việc chúng ta có thể nhìn thấy đáp án ở phía sau. Tức là tại một quá trình decode, chúng ta đưa ra cái output tại đây. Rồi sau đó chúng ta mới đưa ra cái output tại đây. Đây là lần thứ 1, đây là lần thứ 2. Và chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jKnjyvvXzXI",
      "filename": "jKnjyvvXzXI",
      "title": "[CS431 - Chương 10] Part 5: Kiến trúc Transformer: Bộ Decoder",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:52"
    }
  },
  {
    "page_content": "Đây là lần thứ 1, đây là lần thứ 2. Và chúng ta không được phép thấy từ thứ 3, thứ 4. Nếu chúng ta sử dụng self-attention trên decode, tương tự như self-attention của encode, thì những đường màu đỏ sẽ vi phạm, đó là chúng ta đã nhìn thấy đáp án phía sau. Tại vì cái thông tin tại vị trí số 1, nó nhận được cái thông tin tại cái layer, tại cái vị trí này. Tức là từ thứ 2, rồi từ thứ 3 này. Như vậy là đã thấy trước đáp án, như vậy là không có được phép như vậy. Vậy thì, chúng ta phải bỏ đi các cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jKnjyvvXzXI",
      "filename": "jKnjyvvXzXI",
      "title": "[CS431 - Chương 10] Part 5: Kiến trúc Transformer: Bộ Decoder",
      "chunk_id": 1,
      "start_timestamp": "0:00:46",
      "end_timestamp": "0:01:43"
    }
  },
  {
    "page_content": "như vậy. Vậy thì, chúng ta phải bỏ đi các cái cạnh nối màu đỏ này đi. Chúng ta phải đảm bảo như vậy, thì khi đó quá trình decode nó mới thực sự là đúng như quy tắc của mình. Đó là chúng ta lần lượt đưa ra các dự đoán cho từng từ của mình, chứ không được phép tổng hợp thông tin của những từ trong tương lai, tức là chúng ta thấy trước những từ trong tương lai. Vậy thì giải pháp đó là gì? Tại mỗi bước của decoder, chúng ta sẽ dần dần mở rộng tập key và value của mình. Cứ trong quá trình mà chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jKnjyvvXzXI",
      "filename": "jKnjyvvXzXI",
      "title": "[CS431 - Chương 10] Part 5: Kiến trúc Transformer: Bộ Decoder",
      "chunk_id": 2,
      "start_timestamp": "0:01:36",
      "end_timestamp": "0:02:27"
    }
  },
  {
    "page_content": "và value của mình. Cứ trong quá trình mà chúng ta decode là chúng ta có query rồi. Ví dụ tại đây, chúng ta sẽ có query của mình. thì chúng ta sẽ phải dần dần mở rộng ra tập key và value, tại vì chúng ta xử lý đến đâu, decode đến đâu thì chúng ta sẽ thấy đến đó, chứ chúng ta không được phép thấy những từ tiếp theo. Ví dụ, tại đây, ở vị trí gốc thì chúng ta chỉ được thấy những từ hiện tại và quá khứ là cái quá trình, cái từ chúng ta đã suy đoán trước đó. Chứ không được phép những từ của tương lai",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jKnjyvvXzXI",
      "filename": "jKnjyvvXzXI",
      "title": "[CS431 - Chương 10] Part 5: Kiến trúc Transformer: Bộ Decoder",
      "chunk_id": 3,
      "start_timestamp": "0:02:22",
      "end_timestamp": "0:03:07"
    }
  },
  {
    "page_content": "đó. Chứ không được phép những từ của tương lai là chúng ta không được phép. Và ở đây thì chúng ta cần phải che các trạng thái sau, tại vị trí thứ hai, chúng ta sẽ phải che các trạng thái sau, chúng ta không được phép thấy. Tại vị trí số hai, chúng ta có thể truyền thông tin ra sau nhưng không nhận được thông tin từ phía sau về, thì đó là nguyên tắc. Và đây, chúng ta sẽ có một cái hiệu ứng để minh họa cho cái việc này. Đầu tiên, đó là kết thúc cái quá trình encode, chúng ta sẽ bắt đầu đưa ra cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jKnjyvvXzXI",
      "filename": "jKnjyvvXzXI",
      "title": "[CS431 - Chương 10] Part 5: Kiến trúc Transformer: Bộ Decoder",
      "chunk_id": 4,
      "start_timestamp": "0:02:56",
      "end_timestamp": "0:03:42"
    }
  },
  {
    "page_content": "quá trình encode, chúng ta sẽ bắt đầu đưa ra cái dự đoán cho cái từ tiếp theo. Rồi sau đó chúng ta mở rộng ra và chúng ta sẽ lan truyền cái thông tin đến cái... sau khi chúng ta đã dự đoán xong, chúng ta sẽ lan truyền thông tin đến cái query tiếp theo. Cứ như vậy, là lan truyền và mở rộng dần ra. Với việc dần dần mở rộng ra thì nó sẽ bị cái gì? Đó chính là tính tuần tự. Tính tuần tự thì nó vi phạm cái nguyên lý hoặc mong muốn của transformer là chúng ta đang muốn song song hóa càng nhiều càng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jKnjyvvXzXI",
      "filename": "jKnjyvvXzXI",
      "title": "[CS431 - Chương 10] Part 5: Kiến trúc Transformer: Bộ Decoder",
      "chunk_id": 5,
      "start_timestamp": "0:03:31",
      "end_timestamp": "0:04:30"
    }
  },
  {
    "page_content": "chúng ta đang muốn song song hóa càng nhiều càng tốt. Cái tính tuần tự này dẫn đến là không có song song được. Không thể song song hóa được. Không thể tính toán song song được. Vậy thì giải pháp là như thế nào? Chúng ta biết rằng là self-attention Sở dĩ có thể song song hóa được Đó là vì cái từ tại thời điểm hiện tại Có thể nhìn được những cái từ của tương lai Những cái từ ở đằng sau đó Vậy thì nó mới có thể tính toán song song được. Vậy thì bây giờ làm sao để việc tính toán của các layer, các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jKnjyvvXzXI",
      "filename": "jKnjyvvXzXI",
      "title": "[CS431 - Chương 10] Part 5: Kiến trúc Transformer: Bộ Decoder",
      "chunk_id": 6,
      "start_timestamp": "0:04:24",
      "end_timestamp": "0:05:09"
    }
  },
  {
    "page_content": "giờ làm sao để việc tính toán của các layer, các trạng thái ẩn của các tầng của mình, nó vẫn thực hiện được một cách song song. Nó vẫn thực hiện được một cách song song. Đó chính là chúng ta sẽ sử dụng cơ chế Masked Multi-head Self-attention. Chúng ta sẽ che các attention từ phía sau bằng cách gán attention score của nó bằng trừ vô cùng. Vậy thì chúng ta sẽ có công thức ở đây. Chúng ta sẽ có ví dụ ở đây. Start, tức là bắt đầu quá trình decode. Do you understand? Thế thì tại vị trí Start, đây là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jKnjyvvXzXI",
      "filename": "jKnjyvvXzXI",
      "title": "[CS431 - Chương 10] Part 5: Kiến trúc Transformer: Bộ Decoder",
      "chunk_id": 7,
      "start_timestamp": "0:05:00",
      "end_timestamp": "0:05:56"
    }
  },
  {
    "page_content": "you understand? Thế thì tại vị trí Start, đây là quá trình decode. thì tại vị trí Start, chúng ta sẽ không được phép thấy từ Do, từ You, từ Understand. tại vì chúng ta đang cần phải predict, chúng ta cần predict, cần đoán ra, cần predict, cần dự đoán cái từ này. thì chúng ta không được thấy cái từ đáp án của nó. Rồi, đến cái quá trình mà decode cho cái từ do, thì chúng ta sẽ... ở đây chỗ này là màu trắng. Chỗ này là màu trắng, tức là chúng ta được phép thấy cái từ Start, chúng ta được phép thấy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jKnjyvvXzXI",
      "filename": "jKnjyvvXzXI",
      "title": "[CS431 - Chương 10] Part 5: Kiến trúc Transformer: Bộ Decoder",
      "chunk_id": 8,
      "start_timestamp": "0:05:46",
      "end_timestamp": "0:06:28"
    }
  },
  {
    "page_content": "phép thấy cái từ Start, chúng ta được phép thấy từ Start, nhưng không được phép thấy cái từ do và không được thấy từ you, understand. Trong quá trình mà decode cái từ do, chúng ta sẽ được thấy cái từ Start, được thấy cái từ do nhưng không được thấy từ you, understand. và trong quá trình mà decode từ understand chúng ta sẽ được thấy hết các từ Start, do, you nhưng không được thấy từ understand. thì đây chính là cái masked multi-head self-attention. và về công thức tính toán thì cũng rất là đơn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jKnjyvvXzXI",
      "filename": "jKnjyvvXzXI",
      "title": "[CS431 - Chương 10] Part 5: Kiến trúc Transformer: Bộ Decoder",
      "chunk_id": 9,
      "start_timestamp": "0:06:23",
      "end_timestamp": "0:07:19"
    }
  },
  {
    "page_content": "và về công thức tính toán thì cũng rất là đơn giản. nếu như cái key, nếu như cái k_j của mình, j của mình mà bé hơn i, Tức là cái key của mình là những từ đã thấy, Đã thấy, Thì chúng ta sẽ giữ nguyên công thức attention score là q_i nhân k_j. Nhưng nếu j mà lớn hơn hoặc bằng i, Tức là tại vị trí thứ i trở đi, Thì chúng ta không được phép thấy, Chúng ta sẽ gán là trừ vô cùng. Và tại sao chúng ta lại gán với trừ vô cùng mà không phải cộng vô cùng? Đó là vì sau khi chúng ta chuẩn hóa nó, thì ở đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jKnjyvvXzXI",
      "filename": "jKnjyvvXzXI",
      "title": "[CS431 - Chương 10] Part 5: Kiến trúc Transformer: Bộ Decoder",
      "chunk_id": 10,
      "start_timestamp": "0:07:06",
      "end_timestamp": "0:07:58"
    }
  },
  {
    "page_content": "Đó là vì sau khi chúng ta chuẩn hóa nó, thì ở đây trừ vô cùng sẽ biến thành số 0 sau khi thực hiện hàm softmax. Trừ vô cùng của mình sẽ biến thành số 0. Tức là chúng ta sẽ không tổng hợp thông tin của từ thứ gì với gì nữa. Những từ nào mà từ nó trở về sau là không được tổng hợp thông tin. Vậy, giải pháp đó là sử dụng Multihead Self-Attention sẽ giúp chúng ta có thể song song hóa được quá trình tính toán một cách dễ dàng với GPU mà vẫn không vi phạm nguyên tắc là không được phép nhìn những cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jKnjyvvXzXI",
      "filename": "jKnjyvvXzXI",
      "title": "[CS431 - Chương 10] Part 5: Kiến trúc Transformer: Bộ Decoder",
      "chunk_id": 11,
      "start_timestamp": "0:07:46",
      "end_timestamp": "0:08:27"
    }
  },
  {
    "page_content": "phạm nguyên tắc là không được phép nhìn những cái từ của tương lai. Và tương tự như vậy, chúng ta cũng sẽ thực hiện cái decoder hoàn toàn giống với lại encoder. Đây chính là cái khác lớn nhất của mình. Nó còn một cái khác nữa trong slide tiếp theo. Sau khi chúng ta thực hiện cái masked multi-head attention, thì chúng ta sẽ thực hiện cái add và norm. Nó cũng giống như bên đây. Bên đây là multi-head attention. Ngay sau đó là add và norm. Ở đây cũng vậy. Add và norm. Bây giờ chúng ta sẽ có một cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jKnjyvvXzXI",
      "filename": "jKnjyvvXzXI",
      "title": "[CS431 - Chương 10] Part 5: Kiến trúc Transformer: Bộ Decoder",
      "chunk_id": 12,
      "start_timestamp": "0:08:25",
      "end_timestamp": "0:09:21"
    }
  },
  {
    "page_content": "vậy. Add và norm. Bây giờ chúng ta sẽ có một cái khác nữa cũng khá là lớn trong cái bước gọi là decoder, đó chính là encoder-decoder attention. Và tên gọi tắt của nó là Cross-Attention. Cross là sự chuyển đổi giữa encoder và decoder. Ánh xạ giữa encode với decode. Thì nó gọi là Cross-Attention. Trong cơ chế attention, Query sẽ đến từ decoder. Query của mình sẽ đến từ decoder. Còn Key và Value, đây chính là Key và Value của mình. Và Key và Value của mình thì nó đến từ encoder, tức là S_i.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jKnjyvvXzXI",
      "filename": "jKnjyvvXzXI",
      "title": "[CS431 - Chương 10] Part 5: Kiến trúc Transformer: Bộ Decoder",
      "chunk_id": 13,
      "start_timestamp": "0:09:19",
      "end_timestamp": "0:10:26"
    }
  },
  {
    "page_content": "Value của mình thì nó đến từ encoder, tức là S_i. Transformer cũng vậy, giả sử như chúng ta có S1, S2,..., S_T thuộc R^D, tức là cái output của encoder. Đây là output của encoder. Đây là encoder. Và h1, h2,..., h_T là input decoder. Tức là chúng ta sẽ có cái h_i ở đây. Đây là input cho quá trình decoder. input cho quá trình decoder. Khi đó thì các bộ Key, Value và Query của mình thì nó sẽ có công thức như sau. Query thì nó sẽ lấy từ h_i. đây là Query. xin lỗi, ở đây là chúng ta nhầm, đây là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jKnjyvvXzXI",
      "filename": "jKnjyvvXzXI",
      "title": "[CS431 - Chương 10] Part 5: Kiến trúc Transformer: Bộ Decoder",
      "chunk_id": 14,
      "start_timestamp": "0:10:16",
      "end_timestamp": "0:11:29"
    }
  },
  {
    "page_content": "là Query. xin lỗi, ở đây là chúng ta nhầm, đây là Query. Query của mình chứ không phải là Key. rồi, ở đây chính là Query. đây chính là Query, còn đây chính là Key và Value. Query này chúng ta sẽ đi truy vấn trong tập Key ở đây để từ đó chúng ta sẽ tổng hợp thông tin. Công thức ở đây là đúng rồi. Key của mình sẽ lấy từ S_e, S_e là đến từ encoder. Query là từ input decoder. Key và Value là từ encoder. Query là từ decoder. Bộ ba là Q, K, V. công thức cũng y chang là self-attention nhưng ở đây là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jKnjyvvXzXI",
      "filename": "jKnjyvvXzXI",
      "title": "[CS431 - Chương 10] Part 5: Kiến trúc Transformer: Bộ Decoder",
      "chunk_id": 15,
      "start_timestamp": "0:11:23",
      "end_timestamp": "0:12:24"
    }
  },
  {
    "page_content": "cũng y chang là self-attention nhưng ở đây là cross-attention tức là key và value lấy từ phần output của encoder, query thì lấy từ input đầu vào của decoder. Tương tự như vậy, chúng ta cũng sẽ thực hiện add norm. Đây là một trick để giúp cho huấn luyện không có hiện tượng overfitting, cũng như là tránh được hiện tượng Vanishing Gradients. Cho đến bây giờ, chúng ta đã gần như hoàn thành decoder rồi. Hai sự khác biệt lớn nhất của decoder đó chính là Masked Multi-head attention và Cross-attention.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jKnjyvvXzXI",
      "filename": "jKnjyvvXzXI",
      "title": "[CS431 - Chương 10] Part 5: Kiến trúc Transformer: Bộ Decoder",
      "chunk_id": 16,
      "start_timestamp": "0:12:15",
      "end_timestamp": "0:13:02"
    }
  },
  {
    "page_content": "Masked Multi-head attention và Cross-attention. Đó là hai cái lớn nhất. Ngoài ra trong output của decoder Chúng ta vẫn sử dụng embedding như bình thường, Và đây là position Positional embedding Như bình thường. Đó là chúng ta sẽ thêm Feedforward và add norm. Tất cả các bước self-attention ở đây bản chất chỉ là sự tổng hợp thông tin, một cách có trọng số thôi. Chưa thật sự biến đổi sang một thông tin mới, do đó chúng ta sẽ thêm feedforward và add norm ở đây. Do đó chúng ta sẽ thêm linear để",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jKnjyvvXzXI",
      "filename": "jKnjyvvXzXI",
      "title": "[CS431 - Chương 10] Part 5: Kiến trúc Transformer: Bộ Decoder",
      "chunk_id": 17,
      "start_timestamp": "0:12:56",
      "end_timestamp": "0:13:30"
    }
  },
  {
    "page_content": "add norm ở đây. Do đó chúng ta sẽ thêm linear để chiếu từ không gian đặc trưng. Toàn bộ encoder và decoder này là không gian đặc trưng. Nó chưa phải là cái không gian Output của mình. Sang cái không gian Output của mình thì trong trường hợp này nó có thể là không gian từ điển. Nó có thể là không gian từ điển. Hoặc là cái keyword mà chúng ta cần trả về. Tại vì trong một số bài toán đó không phải là trả về một từ điển mà nó có thể là trả về cái nhãn từ loại. Tóm lại đó là nó chuyển từ không gian",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jKnjyvvXzXI",
      "filename": "jKnjyvvXzXI",
      "title": "[CS431 - Chương 10] Part 5: Kiến trúc Transformer: Bộ Decoder",
      "chunk_id": 18,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "từ loại. Tóm lại đó là nó chuyển từ không gian đặc trưng sang cái không gian Output. Cái không gian mà chúng ta cần phải trả kết quả về. Và cuối cùng, đó là chúng ta sẽ qua hàm Softmax để tính xác suất của từ tiếp theo mà mình dự đoán là gì. Chúng ta sẽ tính ra xác suất của từ tiếp theo. Rồi, thì đây chính là những bước cuối cùng của Decoder.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jKnjyvvXzXI",
      "filename": "jKnjyvvXzXI",
      "title": "[CS431 - Chương 10] Part 5: Kiến trúc Transformer: Bộ Decoder",
      "chunk_id": 19,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chúng ta sẽ tiến hành cài đặt mô hình logistic regression và sử dụng thư viện Keras. Với thư viện Keras, nó sẽ giúp chúng ta không cần phải tính đạo hàm của hàm loss một cách tường minh. Tức là ngầm bên trong Keras thì nó vẫn tính đạo hàm, nhưng nó sẽ giúp chúng ta không phải ngồi tính toán lại các công thức. để Keras tự tính đạo hàm, tự update tham số cho mình. Để minh họa và mô phỏng cho mô hình Logistic Regression, chúng ta sẽ sử dụng hai tập điểm xanh và một cam tự tạo, giống như trên đây.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jl9v7IDMTsk",
      "filename": "jl9v7IDMTsk",
      "title": "[CS431 - Chương 2] Part 3b_1: Cài đặt mô hình logistic regression",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:44"
    }
  },
  {
    "page_content": "điểm xanh và một cam tự tạo, giống như trên đây. Nếu chúng ta có thể tách hai tập điểm này ra bằng một đường thẳng, cách thức để tạo ra tập dữ liệu này là chúng ta sử dụng hai tâm. Và với hai tâm này thì chúng ta sẽ random dao động xung quanh này với một cái hàm nhiễu. Và lưu ý đó là input feature cho tập data này sẽ là 2 chiều. Đó là 2 tọa độ x1 và x2 trong không gian. Còn y của mình đó chính là thể hiện cho màu sắc của các cái data point ở đây, các cái điểm dữ liệu ở đây. Bên dưới là mô hình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jl9v7IDMTsk",
      "filename": "jl9v7IDMTsk",
      "title": "[CS431 - Chương 2] Part 3b_1: Cài đặt mô hình logistic regression",
      "chunk_id": 1,
      "start_timestamp": "0:00:43",
      "end_timestamp": "0:01:29"
    }
  },
  {
    "page_content": "các cái điểm dữ liệu ở đây. Bên dưới là mô hình Logistic Regression, ở trong đó sẽ là input layer Và trong trường hợp này thì m của mình là bằng 2, tương ứng là 2 tọa độ trong không gian của mình Và ở đây thì chúng ta sẽ có một cái hàm kích hoạt, Activation, một cái hàm kích hoạt là hàm Sigmoid Và cả hai cái thằng này thì nó sẽ được đặt tên đó là Dense, kết nối đầy đủ Rồi, bây giờ chúng ta sẽ cùng tiến hành cài đặt cho cái mô hình Logistic Regression thì cũng tương tự như Linear Regression Đầu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jl9v7IDMTsk",
      "filename": "jl9v7IDMTsk",
      "title": "[CS431 - Chương 2] Part 3b_1: Cài đặt mô hình logistic regression",
      "chunk_id": 2,
      "start_timestamp": "0:01:18",
      "end_timestamp": "0:02:12"
    }
  },
  {
    "page_content": "thì cũng tương tự như Linear Regression Đầu tiên, chúng ta sẽ tạo ra các dữ liệu mẫu. Chúng ta sẽ có n_samples, chính là số mẫu cho một loại điểm. Chúng ta sẽ sinh ra dữ liệu train và dữ liệu test, xin lỗi, và dữ liệu validation. Ví dụ lần này, chúng ta sẽ có thêm sự tham gia của tập dữ liệu validation. Điểm đỏ sẽ xoay xung quanh một tâm, thì chúng ta sẽ cùng theo dõi hình minh họa cho các điểm. Đối với các điểm màu đỏ, nó sẽ dao động xung quanh điểm có tọa độ 1,5. Vì vậy, chúng ta có tâm và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jl9v7IDMTsk",
      "filename": "jl9v7IDMTsk",
      "title": "[CS431 - Chương 2] Part 3b_1: Cài đặt mô hình logistic regression",
      "chunk_id": 3,
      "start_timestamp": "0:02:05",
      "end_timestamp": "0:03:12"
    }
  },
  {
    "page_content": "điểm có tọa độ 1,5. Vì vậy, chúng ta có tâm và chúng ta sẽ random xung quanh điểm 1,5 này. Đối với điểm màu xanh, chúng ta sẽ random xung quanh điểm có tọa độ 5,1. Tâm ở đây, chúng ta sẽ random noise xung quanh cái này Vậy như vậy thì hai tập điểm màu đỏ và màu xanh này thì đều có thể tách ra được bởi một cái đường thẳng Tương tự như vậy cho tập dữ liệu validation thì chúng ta cũng sẽ sử dụng công thức y chang Công thức y chang nhưng chúng ta sẽ để thêm cái hậu tố đó là validation Còn Y thì nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jl9v7IDMTsk",
      "filename": "jl9v7IDMTsk",
      "title": "[CS431 - Chương 2] Part 3b_1: Cài đặt mô hình logistic regression",
      "chunk_id": 4,
      "start_timestamp": "0:03:07",
      "end_timestamp": "0:03:51"
    }
  },
  {
    "page_content": "để thêm cái hậu tố đó là validation Còn Y thì nó sẽ bao gồm, đối với tập dữ liệu y_train thì phần đầu Red Point nó sẽ có Y tương ứng nhãn là 1 và phần Blue Point thì phần nhãn của mình tương ứng là 0 Rồi, bước tiếp theo thì chúng ta sẽ chạy lại Rồi, nó sẽ ra các tập điểm như thế này cũng có thể chia tách được ra bởi một cái đường thẳng. Đối với phần thuật toán huấn luyện, thì như đã đề cập, tất cả chúng ta sẽ sử dụng thư viện Keras. Và trong cái bài Linear Regression, chúng ta có một cái bộ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jl9v7IDMTsk",
      "filename": "jl9v7IDMTsk",
      "title": "[CS431 - Chương 2] Part 3b_1: Cài đặt mô hình logistic regression",
      "chunk_id": 5,
      "start_timestamp": "0:03:45",
      "end_timestamp": "0:04:28"
    }
  },
  {
    "page_content": "cái bài Linear Regression, chúng ta có một cái bộ khung chương trình. Ở đây chúng ta cũng sẽ sử dụng lại cái bộ khung đó. Tuy nhiên ở đây chúng ta tái sử dụng lại các phương thức là self.plot, self.summary, self.predict và self.get_weights. Chúng ta sẽ phải viết lại phương thức build và train Đối với phương thức build, chúng ta cũng phải có lớp đầu tiên là lớp Input Lớp Input, rồi Input, chúng ta phải truyền cho nó shape của đầu vào Shape này cũng tương tự như Linear Regression, nó sẽ có tham",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jl9v7IDMTsk",
      "filename": "jl9v7IDMTsk",
      "title": "[CS431 - Chương 2] Part 3b_1: Cài đặt mô hình logistic regression",
      "chunk_id": 6,
      "start_timestamp": "0:04:21",
      "end_timestamp": "0:05:22"
    }
  },
  {
    "page_content": "tương tự như Linear Regression, nó sẽ có tham số là input_dim Và có thêm như ý để hàm ý, cái shape này sẽ cho những dữ liệu đầu vào là vector, chứ không phải là một ma trận Vector này cũng có input_dim chiều, nó sẽ trả về một cái biến đó là input Kết hợp là output, thì output của mình sẽ là một lớp biến đổi là kết nối đầy đủ, Dense trong đó nó chỉ có duy nhất một node, chúng ta sẽ có duy nhất một node đầu ra và cái hàm activation của mình sẽ là hàm Sigmoid Chúng ta có thành phần bias, output là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jl9v7IDMTsk",
      "filename": "jl9v7IDMTsk",
      "title": "[CS431 - Chương 2] Part 3b_1: Cài đặt mô hình logistic regression",
      "chunk_id": 7,
      "start_timestamp": "0:05:08",
      "end_timestamp": "0:06:41"
    }
  },
  {
    "page_content": "Sigmoid Chúng ta có thành phần bias, output là bằng Dense Đầu ra là một node Activation Chúng ta sẽ để là bằng sigmoid, use_bias sẽ để là bằng True Lưu ý là chúng ta mới chỉ tạo cho lớp output, chúng ta phải truyền lớp đầu vào cho nó là input Tiếp theo, chúng ta sẽ đóng gói input và output này vào một biến tên là model và biến model này sẽ trả cho một phương thức đó là self.model Rồi, thì ở đây chúng ta sẽ không cần phải trả về gì hết Phương thức build này chúng ta sẽ không cần phải trả về gì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jl9v7IDMTsk",
      "filename": "jl9v7IDMTsk",
      "title": "[CS431 - Chương 2] Part 3b_1: Cài đặt mô hình logistic regression",
      "chunk_id": 8,
      "start_timestamp": "0:06:39",
      "end_timestamp": "0:07:33"
    }
  },
  {
    "page_content": "build này chúng ta sẽ không cần phải trả về gì hết Phương thức Train, chúng ta cần phải khởi tạo optimizer tf.keras.optimizers. Tương tự chúng ta vẫn sử dụng Stochastic Gradient Descent và chúng ta phải truyền tham số đầu vào là learning rate là bằng 0.01. Trong tương lai thì learning rate này chúng ta cũng hoàn toàn có thể tham số hóa nó nhưng mà thôi ở đây chúng ta sẽ tạm thời là cố định Tiếp theo, đó là self.model.compile Chúng ta sẽ truyền vào optimizer là bằng optimizer Đồng thời loss",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jl9v7IDMTsk",
      "filename": "jl9v7IDMTsk",
      "title": "[CS431 - Chương 2] Part 3b_1: Cài đặt mô hình logistic regression",
      "chunk_id": 9,
      "start_timestamp": "0:07:25",
      "end_timestamp": "0:08:36"
    }
  },
  {
    "page_content": "vào optimizer là bằng optimizer Đồng thời loss function thì chúng ta sẽ sử dụng là tf. Lúc trước thì chúng ta sử dụng là Mean Squared Error Chúng ta có thể sử dụng Binary Cross Entropy. Chúng ta sẽ khai báo như sau tf.keras.losses.BinaryCrossentropy Và lưu ý là nó phải khởi tạo như vậy là một cái đối tượng. Rồi đây cũng phải để thêm dùng đối tượng. Rồi, và bây giờ thì mình sẽ tiến hành train là self.model.fit dữ liệu X_train, y_train, X_validation, y_validation. và X_validation và y_validation",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jl9v7IDMTsk",
      "filename": "jl9v7IDMTsk",
      "title": "[CS431 - Chương 2] Part 3b_1: Cài đặt mô hình logistic regression",
      "chunk_id": 10,
      "start_timestamp": "0:08:19",
      "end_timestamp": "0:09:27"
    }
  },
  {
    "page_content": "y_validation. và X_validation và y_validation thì chúng ta sẽ đóng gói trong tham số validation_data Vậy chúng ta phải đóng gói nó lại, chứ không phải là truyền rời như thằng X_train và y_train được validation_data và ở đây chúng ta sẽ có thêm tham số là số lượng epoch thì ở đây sẽ để là num_epochs num_epochs của mình sẽ là bằng 500 epochs Rồi, như vậy thì chúng ta đã cài đặt xong mô hình Logistic Regression và hai phương thức như là self.plot, self.summary, self.predict, self.get_weights là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jl9v7IDMTsk",
      "filename": "jl9v7IDMTsk",
      "title": "[CS431 - Chương 2] Part 3b_1: Cài đặt mô hình logistic regression",
      "chunk_id": 11,
      "start_timestamp": "0:09:25",
      "end_timestamp": "0:10:16"
    }
  },
  {
    "page_content": "self.summary, self.predict, self.get_weights là chúng ta sẽ tái sử dụng lại các mô hình đa số nó cũng sẽ tái sử dụng lại như vậy Chủ yếu chúng ta sẽ tiến hành cài đặt phương thức build và phương thức train Bây giờ chúng ta sẽ chạy thử xem có lỗi gì không Bây giờ chúng ta sẽ tiến hành khởi tạo build mô hình và xem kiến trúc mô hình của mình như thế nào Đây sẽ là Logistic Regression Rồi chúng ta sẽ khởi tạo là Logistic Regression Rồi, build Thì ở đây chúng ta sẽ phải truyền vào tham số là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jl9v7IDMTsk",
      "filename": "jl9v7IDMTsk",
      "title": "[CS431 - Chương 2] Part 3b_1: Cài đặt mô hình logistic regression",
      "chunk_id": 12,
      "start_timestamp": "0:10:09",
      "end_timestamp": "0:11:04"
    }
  },
  {
    "page_content": "Thì ở đây chúng ta sẽ phải truyền vào tham số là input_dimension Thì như đã đề cập hồi nãy, tức là ở đây dimension đầu vào chúng ta sẽ có 2 thành phần là x1 và x2 Do đó ở đây thì chúng ta sẽ để ở đây tham số sẽ là 2 Và tóm tắt LogisticRegression.summary() Rồi, thì ở đây chúng ta sẽ thấy là nó sẽ có input nè Đầu vào của mình là 2 và đương nhiên không có thêm tham số nào Lớp tiếp theo là lớp Dense và số tham số của mình là 3 Tại sao lại là 3? Tại vì nó sẽ có 2 thành phần đầu vào nó sẽ có hai",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jl9v7IDMTsk",
      "filename": "jl9v7IDMTsk",
      "title": "[CS431 - Chương 2] Part 3b_1: Cài đặt mô hình logistic regression",
      "chunk_id": 13,
      "start_timestamp": "0:10:58",
      "end_timestamp": "0:11:40"
    }
  },
  {
    "page_content": "Tại vì nó sẽ có 2 thành phần đầu vào nó sẽ có hai thành phần đầu vào và đồng thời có thêm một thành phần bias nên số tham số của mình sẽ là 3 và output của mình sẽ là 1 node Vì vậy tổng số tham số sẽ là 3 và số tham số có thể train được trong trường hợp này là 3 Trong một số mô hình phức tạp hơn như CNN thì nó sẽ có tình huống trainable parameters sẽ ít hơn so với total parameters tổng số tham số là vì nó sẽ đóng băng một số phần và nó sẽ train một số phần thì chúng ta sẽ đến cái bài đó chi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jl9v7IDMTsk",
      "filename": "jl9v7IDMTsk",
      "title": "[CS431 - Chương 2] Part 3b_1: Cài đặt mô hình logistic regression",
      "chunk_id": 14,
      "start_timestamp": "0:11:34",
      "end_timestamp": "0:13:43"
    }
  },
  {
    "page_content": "một số phần thì chúng ta sẽ đến cái bài đó chi tiết sau Tiếp theo thì chúng ta sẽ tiến hành train mô hình của mình thì lưu ý là trong trường hợp này mô hình này mình sẽ có trả về quá trình train, lịch sử huấn luyện đối tượng History object truyền vào X_train, y_train, X_val, y_val lưu ý là tham số num_epochs và 500 Rồi, nếu không hiểu X_train là gì thì chúng ta sẽ lên đây xem đặt tên biến đã đúng hay chưa X_train là phải viết hoa chữ X và chữ Y y_train, X_val, y_val num_epochs Chúng ta sẽ xem",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jl9v7IDMTsk",
      "filename": "jl9v7IDMTsk",
      "title": "[CS431 - Chương 2] Part 3b_1: Cài đặt mô hình logistic regression",
      "chunk_id": 15,
      "start_timestamp": "0:13:31",
      "end_timestamp": "0:14:14"
    }
  },
  {
    "page_content": "y_train, X_val, y_val num_epochs Chúng ta sẽ xem trong hàm Train, num_epochs Ở đây thì hàm Fit, có thể là tham số epochs mình để viết dạng lỗi chính tả Chút nữa chúng ta sẽ quan sát, chúng ta sẽ thấy có hai đại lượng là loss của tập train và validation loss thì loss của tập train sẽ thấp hơn, trong trường hợp này nó thấp hơn so với validation loss Thì cũng đúng thôi, tại vì nó sẽ có hiện tượng gọi là overfitting và loss cho tập train thì thường nó sẽ thấp hơn so với validate Validate thường cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jl9v7IDMTsk",
      "filename": "jl9v7IDMTsk",
      "title": "[CS431 - Chương 2] Part 3b_1: Cài đặt mô hình logistic regression",
      "chunk_id": 16,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "sẽ thấp hơn so với validate Validate thường cái dữ liệu của mình nó sẽ mới hơn nó sẽ không có lặp lại trên tập train Vì vậy loss của validation sẽ cao hơn so với loss của tập train",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jl9v7IDMTsk",
      "filename": "jl9v7IDMTsk",
      "title": "[CS431 - Chương 2] Part 3b_1: Cài đặt mô hình logistic regression",
      "chunk_id": 17,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Một nền tảng toán học tiếp theo cũng rất quan trọng trong phạm vi của môn học này, đó chính là giải tích. Tại sao chúng ta cần phải học giải tích? Thì đầu tiên, đó là giải tích sẽ cung cấp cho chúng ta một cái công cụ để giúp chúng ta biểu diễn các cái biến đổi Ví dụ như chúng ta sẽ biến đổi từ dữ liệu gốc như là hình ảnh, văn bản, âm thanh sang các vector đặc trưng Và mỗi một cái phép biến đổi trong các mô hình máy học của chúng ta nó tương ứng sẽ là một hàm Như vậy thì các mô hình máy học sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jwfLIgOeneE",
      "filename": "jwfLIgOeneE",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.1: Ôn tập nền tảng giải tích (Part 1)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:56"
    }
  },
  {
    "page_content": "sẽ là một hàm Như vậy thì các mô hình máy học sẽ là một chuỗi các biến đổi của các hàm số. Như vậy thì công dụng đầu tiên dùng để biểu diễn chuỗi các biến đổi. Công dụng thứ hai của giải tích trong phạm vi môn học này, có lẽ đây cũng là một trong những công dụng quan trọng nhất. Đó chính là giúp tìm được mô hình tối ưu trong quá trình huấn luyện, sử dụng những công cụ của giải tích cụ thể là như hồi xưa chúng ta đã được học là một cái hàm số fx thì những cái điểm cực trị địa phương những cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jwfLIgOeneE",
      "filename": "jwfLIgOeneE",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.1: Ôn tập nền tảng giải tích (Part 1)",
      "chunk_id": 1,
      "start_timestamp": "0:00:47",
      "end_timestamp": "0:01:30"
    }
  },
  {
    "page_content": "thì những cái điểm cực trị địa phương những cái điểm cực tiểu hoặc cực đại địa phương thì tại những cái vị trí này đó chính là đạo hàm của mình đạo hàm bằng 0 và đây chính là ý then chốt mà để giúp cho mình tìm ra được cái mô hình tối ưu tại vì mô hình tối ưu chính là cái mô hình mà tại đó chúng ta đạt được những cái tiêu chí nào đó là nhỏ nhất hoặc là cao nhất hoặc là lớn nhất. Ví dụ như trong quá trình huấn luyện một cái mô hình chúng ta luôn mong muốn cái lỗi của cái việc mà nhận diện đối",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jwfLIgOeneE",
      "filename": "jwfLIgOeneE",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.1: Ôn tập nền tảng giải tích (Part 1)",
      "chunk_id": 2,
      "start_timestamp": "0:01:27",
      "end_timestamp": "0:02:11"
    }
  },
  {
    "page_content": "mong muốn cái lỗi của cái việc mà nhận diện đối tượng là thấp nhất thì lúc đó là cái hàm mô hình của mình sẽ phải tìm giá trị nhỏ nhất. Và như vậy thì một trong những cái công cụ mà chúng ta cần phải tìm hiểu kỹ trong cái phần giải tích này chính là đạo hàm. Và như đã đề cập ở trước thì mô hình của mình sẽ là một chuỗi các hàm số Chuỗi các biến đổi hay là chuỗi các hàm số Do đó thì một công cụ khác cũng rất quan trọng Đó chính là đạo hàm của hàm hợp Tiếp theo thì chúng ta sẽ tìm hiểu về các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jwfLIgOeneE",
      "filename": "jwfLIgOeneE",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.1: Ôn tập nền tảng giải tích (Part 1)",
      "chunk_id": 3,
      "start_timestamp": "0:02:03",
      "end_timestamp": "0:02:47"
    }
  },
  {
    "page_content": "hàm hợp Tiếp theo thì chúng ta sẽ tìm hiểu về các dạng hàm toán học Nếu như biểu diễn một cách đơn giản thì chúng ta có thể ghi là y là một hàm số bằng fx Với x là cái biến số đầu vào và y chính là cái giá trị của cái hàm số này Một cách biểu diễn khác, đó là chúng ta biểu diễn dưới dạng là sơ đồ Chúng ta sẽ có một input x và qua cái hàm f thì chúng ta sẽ tính ra được cái giá trị output là fx Đây là kết quả của một phép biến đổi của hàm F với thông tin đầu vào là x Thì đây chúng ta chỉ ôn lại",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jwfLIgOeneE",
      "filename": "jwfLIgOeneE",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.1: Ôn tập nền tảng giải tích (Part 1)",
      "chunk_id": 4,
      "start_timestamp": "0:02:41",
      "end_timestamp": "0:03:27"
    }
  },
  {
    "page_content": "tin đầu vào là x Thì đây chúng ta chỉ ôn lại khái niệm Khái niệm này thì chúng ta đã được học từ trong các chương trình phổ thông và chúng ta sẽ có rất nhiều những dạng hàm khác nhau Ví dụ như chúng ta sẽ có hàm đơn biến, hàm đa biến và hàm đơn biến là gì? là cái hàm số này nó chỉ bị phụ thuộc bởi duy nhất cái hàm này nó chỉ có phụ thuộc bởi duy nhất một biến số đầu vào còn trong cái ví dụ này thì cái hàm y sẽ bị phụ thuộc bởi 2 biến số đầu vào đó là x và z y thì sẽ được tính bởi 2 cái giá trị",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jwfLIgOeneE",
      "filename": "jwfLIgOeneE",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.1: Ôn tập nền tảng giải tích (Part 1)",
      "chunk_id": 5,
      "start_timestamp": "0:03:24",
      "end_timestamp": "0:04:09"
    }
  },
  {
    "page_content": "đó là x và z y thì sẽ được tính bởi 2 cái giá trị biến đầu vào là x và z do đó thì hàm số sẽ được gọi là hàm đa biến Khái niệm tiếp theo đó chính là hàm hợp Hàm hợp sẽ là một chuỗi các hàm liên tiếp Thì ở trong ví dụ này chúng ta thấy y là một cái hàm hợp là một phép biến đổi là một cái hàm được tạo bởi hai cái hàm thành phần là f và g trong đó f thì sẽ được thực hiện trước và g sẽ được thực hiện sau thì nếu xét về mặt ký hiệu thì lúc này ta sẽ ký hiệu là y và bằng g của fx trong đó f chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jwfLIgOeneE",
      "filename": "jwfLIgOeneE",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.1: Ôn tập nền tảng giải tích (Part 1)",
      "chunk_id": 6,
      "start_timestamp": "0:04:06",
      "end_timestamp": "0:04:46"
    }
  },
  {
    "page_content": "ký hiệu là y và bằng g của fx trong đó f chúng ta sẽ thực hiện trước và tính ra cái giá trị chúng ta sẽ thế vào để đưa vào hàm g kết quả của hàm f sẽ được đưa vào như là cái input đầu vào của hàm g còn về mặt ký hiệu khác đó là ký hiệu hình tròn ở đây thì nó sẽ ghi là g tròn f hay là y sẽ là một cái hàm hợp của g và f với f sẽ là cái thao tác thực hiện đầu tiên sau đó mới tính ra được cái kết quả và đưa vào cái đầu vào cho hàm g. Thì đây là một cái ví dụ. Ở đây chúng ta sẽ có f của mình sẽ là,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jwfLIgOeneE",
      "filename": "jwfLIgOeneE",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.1: Ôn tập nền tảng giải tích (Part 1)",
      "chunk_id": 7,
      "start_timestamp": "0:04:44",
      "end_timestamp": "0:05:27"
    }
  },
  {
    "page_content": "cái ví dụ. Ở đây chúng ta sẽ có f của mình sẽ là, fx sẽ là bằng x bình phương, cộng 1. Và gx sẽ là sin x. Thì khi đó cái hàm hợp của 2 cái hàm trên sẽ ký hiệu là y bằng g của fx sẽ bằng. Chúng ta sẽ thực hiện cái thao tác f trước. F trước, tức là chúng ta sẽ tính cái x bình cộng 1 trước, sau đó sẽ đưa vào cho hàm sin thực hiện sau. Thì ở đây chúng ta sẽ có một số bài tập các bạn có thể tự làm một cách dễ dàng. Tiếp theo thì chúng ta sẽ tìm hiểu về đạo hàm, như đã đề cập trước đây. Đây là một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jwfLIgOeneE",
      "filename": "jwfLIgOeneE",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.1: Ôn tập nền tảng giải tích (Part 1)",
      "chunk_id": 8,
      "start_timestamp": "0:05:20",
      "end_timestamp": "0:06:03"
    }
  },
  {
    "page_content": "về đạo hàm, như đã đề cập trước đây. Đây là một trong những công cụ quan trọng để giúp chúng ta tìm được mô hình tối ưu. Và để tìm được mô hình tối ưu thì nó cũng tương ứng trong lĩnh vực giải tích, đó chính là tìm cái giá trị nhỏ nhất và lớn nhất của hàm số. Thì khái niệm đạo hàm trong giải tích toán học thì đạo hàm của một hàm số là một cái đại lượng mô tả sự biến thiên của hàm số tại một cái điểm nào đó. Thì chúng ta lấy ví dụ ở đây, chúng ta sẽ có một cái hàm y bằng fx và tại vị trí này thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jwfLIgOeneE",
      "filename": "jwfLIgOeneE",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.1: Ôn tập nền tảng giải tích (Part 1)",
      "chunk_id": 9,
      "start_timestamp": "0:05:59",
      "end_timestamp": "0:06:52"
    }
  },
  {
    "page_content": "sẽ có một cái hàm y bằng fx và tại vị trí này thì chúng ta thấy là hệ số góc của điểm tiếp xúc tại vị trí x0 này thì hệ số góc của tiếp tuyến này sẽ thể hiện sự biến thiên và giá trị của hệ số góc này chính là đạo hàm thì đây là mình ôn lại kiến thức đạo hàm hồi xưa và cái đạo hàm này nó sẽ có các điểm thì đây chính là cái điểm cực tiểu và đây sẽ là cái điểm cực đại và chúng ta sẽ sử dụng đạo hàm như là cái công cụ để giúp cho mình khảo sát cũng như là tìm các điểm cực tiểu cực đại này thì nhắc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jwfLIgOeneE",
      "filename": "jwfLIgOeneE",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.1: Ôn tập nền tảng giải tích (Part 1)",
      "chunk_id": 10,
      "start_timestamp": "0:06:47",
      "end_timestamp": "0:07:52"
    }
  },
  {
    "page_content": "như là tìm các điểm cực tiểu cực đại này thì nhắc lại thôi, thì chúng ta thấy là ở đây tại đạo hàm f'x bằng 0 thì nó tương ứng sẽ là vị trí điểm cực tiểu trong cái hàm này ví dụ chúng ta có một cái hàm parabol thì điểm cực tiểu x sao ở đây nó sẽ thỏa mãn tính chất đó là f' của x sao sẽ là bằng 0 Và khi đó thì việc tìm điểm cực tiểu này, chúng ta sẽ đưa về việc giải phương trình của đạo hàm Đó là f'x, trong trường hợp này f'x chính là bằng x trừ 1 Thì chúng ta sẽ nhớ lại kiến thức hồi cấp 3 là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jwfLIgOeneE",
      "filename": "jwfLIgOeneE",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.1: Ôn tập nền tảng giải tích (Part 1)",
      "chunk_id": 11,
      "start_timestamp": "0:07:44",
      "end_timestamp": "0:08:07"
    }
  },
  {
    "page_content": "1 Thì chúng ta sẽ nhớ lại kiến thức hồi cấp 3 là đạo hàm của hàm này là bằng x trừ 1 dựa trên một số hàm cơ bản cũng như tính chất đạo hàm của các hàm Giải phương trình này xong thì chúng ta cho hàm này bằng 0 lúc đó x sao của mình chính là x sao bằng 1 tức là vị trí của x để cho hàm này đạt cực tiểu là x = 1",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=jwfLIgOeneE",
      "filename": "jwfLIgOeneE",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.1: Ôn tập nền tảng giải tích (Part 1)",
      "chunk_id": 12,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Biến thể DeepStack RNN khi cái văn bản của mình rất là dài, có thể lên đến hàng chục, hàng trăm, thậm chí là hàng ngàn chữ thì rõ ràng là cái số thao tác xử lý này sẽ được lập đi lập lại, lập đi lập lại và nó sẽ tiến tới sâu theo chiều ngang nhưng với mỗi một cái đặc trưng, với mỗi một cái đặc trưng tại một cái thời điểm tính toán thì nó đã thực sự sâu hay chưa? thì câu trả lời là chưa và nó đang thiếu một độ sâu theo chiều dọc. Nó mới chỉ sâu theo chiều ngang thôi, còn sâu theo chiều dọc là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:06"
    }
  },
  {
    "page_content": "theo chiều ngang thôi, còn sâu theo chiều dọc là chưa có. Do đó, ta có thể làm cho mô hình sâu hơn theo chiều dọc và chiều này được hiểu theo chiều của từng đặc trưng. Với mỗi đặc trưng ST, đây là một đặc trưng cấp thấp. Chúng ta sẽ làm cho nó nâng lên thành một đặc trưng cấp trung, mid-level. sau đó chúng ta lại nâng lên thành một đặc trưng cấp cao hơn thì đó là sâu theo chiều dọc và nó sẽ cho phép mô hình của mình nó có biểu diễn được các đặc trưng ở nhiều cấp độ nó sẽ biểu diễn được đặc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 1,
      "start_timestamp": "0:01:02",
      "end_timestamp": "0:01:45"
    }
  },
  {
    "page_content": "đặc trưng ở nhiều cấp độ nó sẽ biểu diễn được đặc trưng ở nhiều cấp độ và giống như trong mạng CNN chúng ta thấy ở trong mạng CNN thì ở những cái layer đầu tiên những cái layer đầu tiên thì cái feature map của mình là những cái đặc trưng cấp thấp sau đó chúng ta biến đổi thành các feature map càng về lớp cuối thì chúng ta thấy là feature map của mình tính đặc trưng, tính ngữ nghĩa của nó càng lúc càng cao thì ở đây sẽ là 2 level feature trong khi đó đối với mạng ANN thì truyền thống Với đặc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 2,
      "start_timestamp": "0:01:42",
      "end_timestamp": "0:02:33"
    }
  },
  {
    "page_content": "khi đó đối với mạng ANN thì truyền thống Với đặc trưng đầu vào XT, chúng ta chỉ mới thực hiện biến đổi trên 1 tầng, thì đặc trưng này vẫn còn mang tính chất là cấp thấp. Nó sẽ không thể nào giúp chúng ta giải quyết được các bài toán phức tạp, khó hơn. Và như vậy thì chúng ta sẽ có một phiên bản đó chính là DeepStack RNN. Từ stack có nghĩa là chồng Thì chữ stack này có nghĩa là chồng Và một cái tên gọi khác Đó là multi-layer ANN Tức là layer có Mạng ANN có nhiều tầng Rồi, thì đây là sơ đồ cho",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 3,
      "start_timestamp": "0:02:29",
      "end_timestamp": "0:03:23"
    }
  },
  {
    "page_content": "Mạng ANN có nhiều tầng Rồi, thì đây là sơ đồ cho mạng ANN mà với một layer Chúng ta sử dụng lại ví dụ là cũ là The Movie was terribly exciting và lưu ý là đây là chúng ta đang làm gọn chứ hàm ý từ đầu vào ở đây nó phải là embedding của từ Movie và nếu như chỉ có 1 layer thì cái ST tại đây là chỉ chứa được đặc trưng cấp thấp của từ này do đó chúng ta cần phải tổng hợp thêm các thông tin của mid-level và high-level feature muốn vậy thì chúng ta cần phải chồng các layer biến đổi Mỗi đường màu cam",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 4,
      "start_timestamp": "0:03:17",
      "end_timestamp": "0:04:12"
    }
  },
  {
    "page_content": "phải chồng các layer biến đổi Mỗi đường màu cam là một layer biến đổi Chúng ta sẽ chồng lên layer thứ 2, rồi chồng lên layer số 3 Đường đi di chuyển của thông tin sẽ được biểu diễn bởi các vector bằng mũi tên Thể hiện hướng đi của dữ liệu của mình sau đó trạng thái ẩn ở layer thứ y là S_yt là đầu vào layer thứ y+1 như vậy là kết quả của trạng thái ẩn layer thứ y sẽ là đầu vào layer thứ y+1 S_(y+1)t Và cách chúng ta luân chuyển thông tin giữa các layer đó là chúng ta sẽ tính toán trên layer số 1",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 5,
      "start_timestamp": "0:04:10",
      "end_timestamp": "0:05:08"
    }
  },
  {
    "page_content": "layer đó là chúng ta sẽ tính toán trên layer số 1 trước Rồi sau đó chúng ta truyền thông tin lên cho layer số 2 Rồi sau đó chúng ta truyền thông tin lên cho layer số 3 Thì đây là animation để minh họa cho cách thức chúng ta chuyển dữ liệu giữa các tầng với nhau Để dùng dạng công thức, chúng ta sẽ có các công thức như sau Đầu tiên là S_t^1, tức là layer số 1, đây sẽ là layer số 2, đây là layer số 3 và đây là layer số 1 Tại thời điểm thứ T là layer số 1, nó sẽ được tính thông tin, nó sẽ được tổng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 6,
      "start_timestamp": "0:04:58",
      "end_timestamp": "0:06:08"
    }
  },
  {
    "page_content": "số 1, nó sẽ được tính thông tin, nó sẽ được tổng hợp thông tin từ cái x_t, tức là cái này x_t cộng với lại thông tin của quá khứ nhưng ở cùng tầng Đây là layer số 1, đây là cùng tầng Thông tin của quá khứ ở cùng tầng tức là S^1_(T-1) Rồi, sau đó lên cái tầng thứ 2 thì S_t^2 Nó sẽ được tổng hợp thông tin từ cái tầng trước đó Nếu như trước đây cái S_t^1 nó tổng hợp thông tin từ x_t thì ở đây S_t^2 sẽ được tổng hợp thông tin từ S_t^1, tức là từ tầng thấp hơn chuyển lên thì đây chính là thông tin",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 7,
      "start_timestamp": "0:06:04",
      "end_timestamp": "0:07:20"
    }
  },
  {
    "page_content": "thấp hơn chuyển lên thì đây chính là thông tin của tầng thấp hơn chuyển lên đây chính là thông tin của tầng hoặc layer trước đó và nó sẽ không quên là tổng hợp thông tin với cái quá khứ của cái tầng hiện tại tức là cái thông tin quá khứ ở trên cái tầng hiện tại chính là S^2_(t-1) S^2_(t-1) thì đây chính là cái thông tin quá khứ Thông tin quá khứ như ở cùng tầng Và tương tự như vậy cho S_t^3, chúng ta cũng sẽ tổng hợp thông tin từ S_t^2 kết hợp với thông tin quá khứ cùng tầng đó là S^3_(t-1).",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 8,
      "start_timestamp": "0:07:09",
      "end_timestamp": "0:07:58"
    }
  },
  {
    "page_content": "với thông tin quá khứ cùng tầng đó là S^3_(t-1). Thì đây chính là dạng công thức biến đổi của DeepStack RNN. Và cũng không thể nào quên, không nhắc đến biến thể có sự kết hợp của DeepStack và Bidirectional. Bidirectional nhắc lại, đó chính là một biến thể giúp cho chúng ta chúng ta tổng hợp được thông tin ngữ cảnh theo chiều từ trái sang phải và từ phải sang trái đó sẽ giúp cho chúng ta hoàn thiện hơn cái thông tin về mặt ngữ cảnh Còn DeepStack là nó sẽ giúp cho mình cho các đặc trưng tại từng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 9,
      "start_timestamp": "0:07:54",
      "end_timestamp": "0:08:51"
    }
  },
  {
    "page_content": "là nó sẽ giúp cho mình cho các đặc trưng tại từng tầng nó sẽ học được các cấp của đặc trưng từ cấp thấp cho đến cấp giữa đến cấp cao như vậy hai cái DeepStack và Bidirectional nó thực hiện hai cái nhiệm vụ độc lập nhau Nếu chúng ta bổ trợ cho nhau thì kiến trúc sẽ càng hoàn thiện hơn và hiệu quả hơn. DeepStack Bidirectional nếu mà vẽ gọn lại thì chúng ta sẽ dùng sơ đồ này. Ở đây chúng ta sẽ thấy có những cái nét liền chính là cho chiều thuận Forward. Còn nét đứt là để thể hiện cho các đường",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 10,
      "start_timestamp": "0:08:37",
      "end_timestamp": "0:09:36"
    }
  },
  {
    "page_content": "Forward. Còn nét đứt là để thể hiện cho các đường theo chiều Backward Và ở đây chúng ta sẽ tổng hợp thông tin cho một tầng và với cái tầng này thì chúng ta lại đẩy lên tiếp chúng ta sẽ chồng thêm, chúng ta sẽ chồng thêm một cái tầng mới rồi chúng ta lại chồng lên một cái tầng mới Ví dụ này là chúng ta đang minh họa cho DeepStack Bidirectional là vừa có sự kết hợp của sự đi theo 2 chiều xử lý thông tin ngữ cảnh 2 chiều mà vừa có sự chồng tầng giữa tầng này với tầng nọ tầng thấp lên tầng cao Rồi,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 11,
      "start_timestamp": "0:09:29",
      "end_timestamp": "0:10:19"
    }
  },
  {
    "page_content": "tầng này với tầng nọ tầng thấp lên tầng cao Rồi, cuối cùng là mẹo thực hành cho bài học ngày hôm nay Đầu tiên là mẹo số 1, M1 là nên sử dụng Bidirectional khi có thể Tại sao chúng ta không dùng từ là luôn luôn mà dùng từ là có thể Tại vì có một số bài toán, ví dụ như Language Model, chúng ta không được phép thấy những thông tin của những từ phía sau Do đó Language Model là không có sử dụng Bidirectional được Nên ở đây chúng ta chỉ nói là nên sử dụng Bidirectional RNN khi có thể thôi Và thứ hai,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 12,
      "start_timestamp": "0:10:16",
      "end_timestamp": "0:11:09"
    }
  },
  {
    "page_content": "Bidirectional RNN khi có thể thôi Và thứ hai, đó là mẹo thứ 2 M2 là DeepStack RNN cho kết quả tốt hơn Đó cũng tương tự như mạng CNN Nó sẽ giúp kiến trúc của mình có thể học được các đặc trưng theo nhiều lớp khác nhau Theo nhiều mức độ khác nhau từ cấp thấp lên cấp cao Và ở đây chúng ta sẽ có thêm một số kinh nghiệm khác Đối với quá trình encoder trong mạng RNN Chúng ta biết rồi nó sẽ có một số biến thể là encoder Và decoder Encoder và Decoder Encoder là giúp chúng ta đọc hết toàn bộ nội dung",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 13,
      "start_timestamp": "0:11:07",
      "end_timestamp": "0:11:50"
    }
  },
  {
    "page_content": "Encoder là giúp chúng ta đọc hết toàn bộ nội dung đầu vào, đọc hết input Và Decoder là giúp chúng ta tạo sinh ra kết quả Ví dụ như biến thể Many-to-Many dạng 2 Đó là một cái ví dụ như vậy Many-to-Many dạng 2 chính là một cái kiểu là encoder-decoder Thì encoder mà từ 2 cho đến 4 lớp Thì cái lớp thứ 2, cái lớp thứ 2 hay cái tầng thứ 2 Nó giúp cho chúng ta cải thiện nhiều Nhưng mà theo kinh nghiệm của những cái người đi trước Thì đến cái lớp thứ 3, thứ 4 thì sự hiệu quả của nó ít hơn Tức là nó có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 14,
      "start_timestamp": "0:11:48",
      "end_timestamp": "0:12:28"
    }
  },
  {
    "page_content": "thứ 4 thì sự hiệu quả của nó ít hơn Tức là nó có hiệu quả hơn nhưng mà nó hiệu quả ít. Như vậy thì ở đây chúng ta cần phải có sự đánh đổi. Nếu như chúng ta thêm tầng thứ 3, thứ 4 thì điều gì xảy ra? Nó sẽ phát sinh thêm điều gì? Nó sẽ phát sinh thêm chi phí tính toán. Đó là điều chắc chắn. Và đồng thời nó sẽ làm cho mô hình của mình phức tạp hơn. Nếu chúng ta không quan tâm lắm về yếu tố chi phí tính toán, hoặc thời gian tính toán, thì chúng ta có thể thêm lớp số 3, số 4. Nhưng mà do là cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 15,
      "start_timestamp": "0:12:22",
      "end_timestamp": "0:13:04"
    }
  },
  {
    "page_content": "ta có thể thêm lớp số 3, số 4. Nhưng mà do là cái tình hình hiện tại của tính toán, chúng ta không quan tâm lắm về yếu tố chi phí tính toán, hoặc thời gian tính toán, thì chúng ta có thể thêm lớp số 3, số 4. Chi phí tính toán hoặc thời gian tính toán thì chúng ta có thể thêm lớp 3, 4. Nhưng mà do là tầng thứ 3 và thứ 4 này được thực hiện tuần tự, nó cũng không thể giúp chúng ta thực hiện song song được, nên chi phí tính toán và thời gian nó sẽ lâu. Đối với decoder thì theo kinh nghiệm đó là 4",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 16,
      "start_timestamp": "0:13:00",
      "end_timestamp": "0:13:38"
    }
  },
  {
    "page_content": "lâu. Đối với decoder thì theo kinh nghiệm đó là 4 lớp là cho kết quả tốt nhất. Nhưng mà lưu ý, đây là những kinh nghiệm cá nhân của những bài báo khoa học họ tổng hợp Còn thực tế nó cũng rất phụ thuộc vào khối lượng dữ liệu, nó phụ thuộc vào bài toán của mình Nếu dữ liệu của mình ít thì có khi càng thêm lớp, nó lại càng tệ hơn Tại vì nó phát sinh thêm trọng số hoặc là phát sinh thêm chi phí tính toán Làm cho phức tạp mô hình hơn, dẫn đến hiện tượng vanishing gradient hoặc là overfitting Do đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 17,
      "start_timestamp": "0:13:34",
      "end_timestamp": "0:14:38"
    }
  },
  {
    "page_content": "vanishing gradient hoặc là overfitting Do đó thì thêm không chắc là tốt Nhưng mà đối với trường hợp dữ liệu của mình đủ nhiều và bài toán đủ đơn giản thì chúng ta hoàn toàn có thể áp dụng là đối với decoder thì chúng ta sẽ có 4 lớp Rồi, và một trong những cái mẹo cuối nhưng mà nó không có được nhắc đến trong cái phần này trong cái bài này, đó chính là skip connection Các bạn quay lại bài về CNN và cụ thể đó là biến thể ResNet chúng ta thấy là skip connection giúp chúng ta giải quyết được hiện",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 18,
      "start_timestamp": "0:14:36",
      "end_timestamp": "0:15:24"
    }
  },
  {
    "page_content": "connection giúp chúng ta giải quyết được hiện tượng vanishing gradient nó sẽ giúp chúng ta giải quyết được hiện tượng vanishing gradient và điều đó đã giúp chúng ta có thể tăng độ sâu của mạng của mình lên có thể lên đến 8 lớp thì như hồi nãy chúng ta nói nếu như bình thường chúng ta không phải chịu sự ảnh hưởng của vấn đề về chi phí tính toán thì chúng ta có thể thêm 3 đến 4 lớp nhưng khi thêm vô mà không có cơ chế nào khác thì nó sẽ rất dễ xảy ra hiện tượng vanishing gradient và để khắc chế",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 19,
      "start_timestamp": "0:15:16",
      "end_timestamp": "0:16:13"
    }
  },
  {
    "page_content": "ra hiện tượng vanishing gradient và để khắc chế được chuyện này khắc chế được vấn đề vanishing gradient thì chúng ta sẽ sử dụng skip connection đó sẽ giúp chúng ta giải quyết vấn đề vanishing công thức của biến thể ResNet chính là F(x) là bằng Hàm biến đổi của mình, ví dụ như là RNN, đây là RNN Cộng cho x Đây chính là mẹo để giúp chúng ta giải quyết vấn đề về vanishing gradient Như vậy, trong bài học ngày hôm nay, chúng ta đã lần lượt đi qua các module các biến thể của RNN Và các biến thể này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 20,
      "start_timestamp": "0:16:06",
      "end_timestamp": "0:16:52"
    }
  },
  {
    "page_content": "module các biến thể của RNN Và các biến thể này là những biến thể kinh điển, đó là LSTM Cơ chế của LSTM đó là nhớ cái cần nhớ và quên cái cần quên Thông qua các cổng là Forget, cổng Input, cổng Output Và đồng thời nó sẽ kết hợp với một cái Cell, một cái Cell State Để lưu truyền cái thông tin từ quá khứ cho đến hiện tại Đây chính là ý tưởng của LSTM. LSTM sẽ giúp cho mình giải quyết được vấn đề vanishing gradient do sự điều phối thông tin dẫn đến cho gradient của mình tính toán một cách hiệu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 21,
      "start_timestamp": "0:16:46",
      "end_timestamp": "0:17:00"
    }
  },
  {
    "page_content": "đến cho gradient của mình tính toán một cách hiệu quả. Biến thể thứ 2 là Bidirectional RNN. Bidirectional RNN sẽ giúp chúng ta tổng hợp thông tin từ 2 chiều, theo chiều từ Forward từ trái sang phải và theo chiều từ phải qua trái. thì sẽ giúp chúng ta có được thông tin đầy đủ và toàn diện hơn. Và cuối cùng đó chính là biến thể DeepStack DeepStack RNN DeepStack RNN giúp chúng ta tăng độ sâu của mô hình thay vì chúng ta đi theo chiều ngang thì sẽ giúp chúng ta tăng theo chiều sâu và giúp cho các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 22,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "giúp chúng ta tăng theo chiều sâu và giúp cho các đặc trưng có thể học được từ cấp thấp, cấp giữa và cấp cao. Và đương nhiên là kết hợp Bidirectional DeepStack thì chúng ta sẽ có là DeepStack Bidirectional RNN. Đây là một biến thể phổ biến.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KjPEqyGCtUs",
      "filename": "KjPEqyGCtUs",
      "title": "[CS431 - Chương 8] Part 3: Một số biến thể của RNN: Deep Stacked RNN",
      "chunk_id": 23,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Rồi, đến năm 2012, AlexNet gây ra một tiếng vang lớn trong cộng đồng nghiên cứu khi kiến trúc mạng AlexNet giành được độ chính xác cao nhất, tỷ lệ lỗi thấp nhất và độ chính xác cao nhất cho cuộc thi trên tập ImageNet và nó chiến thắng tất cả những phương pháp sử dụng các đặc trưng và do các nhà khoa học họ thiết kế bằng tay Còn mạng CNN và AlexNet nó được thiết kế để cho tự động học các bộ lọc trích xuất đặc trưng thông qua các phép biến đổi convolution Thì những cái cải tiến chính của AlexNet",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KoBIBuqGb9A",
      "filename": "KoBIBuqGb9A",
      "title": "[CS431 - Chương 4] Part 1_2: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:49"
    }
  },
  {
    "page_content": "Thì những cái cải tiến chính của AlexNet đó chính là thay SIGMOID hàm activation SIGMOID bằng ReLU thì cái việc này nó sẽ giúp cho chúng ta tránh được cái hiện tượng, giảm được cái hiện tượng là vanishing. Gradient Vanishing Gradient là gì? Trong cái quá trình mà chúng ta cập nhật cái tham số theta bằng theta trừ cho alpha nhân cho đạo hàm của hàm loss theo theta, đúng không? Thì cái đạo hàm này nè Từng thành phần đạo hàm này sẽ được phân rã ra thành các hàm thành phần Nếu chúng ta viết dưới",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KoBIBuqGb9A",
      "filename": "KoBIBuqGb9A",
      "title": "[CS431 - Chương 4] Part 1_2: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 1,
      "start_timestamp": "0:00:44",
      "end_timestamp": "0:01:32"
    }
  },
  {
    "page_content": "thành các hàm thành phần Nếu chúng ta viết dưới dạng là Chain Rule, tức là đạo hàm của hàm hợp thì nó sẽ là đạo hàm của hàm loss theo một hàm trung gian, ví dụ như là đạo hàm của hàm thứ nhất theo hàm thứ hai Rồi vân vân cho đến cái hàm thứ n theo biến theta Thì một loạt các cái đạo hàm này, từng cái đạo hàm thành phần này nếu như nó là những cái con số rất là nhỏ Ví dụ như con số mà bé hơn một Thì khi chúng ta nhân các cái con số bé hơn một thì nó sẽ có xu hướng tiến về không Trong cái quá",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KoBIBuqGb9A",
      "filename": "KoBIBuqGb9A",
      "title": "[CS431 - Chương 4] Part 1_2: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 2,
      "start_timestamp": "0:01:26",
      "end_timestamp": "0:02:17"
    }
  },
  {
    "page_content": "thì nó sẽ có xu hướng tiến về không Trong cái quá trình cập nhật cái tham số của mình mà mục tiêu của cái việc cập nhật cái tham số này là để cho đạo hàm của mình càng lúc càng nhỏ, và gradient descent gradient descent tức là đạo hàm càng lúc càng giảm thì khi đạo hàm càng giảm thì các thành phần này càng lúc càng giảm các thành phần này càng lúc càng giảm thì dẫn đến đó là các con số nhỏ mà nhân với nhau sẽ tiến về 0 và khi đạo hàm xấp xỉ bằng 0 tức là bước nhảy theta này gần như không cập",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KoBIBuqGb9A",
      "filename": "KoBIBuqGb9A",
      "title": "[CS431 - Chương 4] Part 1_2: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 3,
      "start_timestamp": "0:02:08",
      "end_timestamp": "0:02:57"
    }
  },
  {
    "page_content": "0 tức là bước nhảy theta này gần như không cập nhật nó gần như không cập nhật thì đó chính là hiện tượng vanishing gradient nó sẽ làm cho quá trình huấn luyện chậm Rồi, thì tại sao sigmoid nó lại khiến cho hiện tượng vanishing gradient nó diễn ra gọi là phổ biến còn ReLU thì nó sẽ giúp cho mình giảm hiện tượng này, đó là thì chúng ta quan sát hàm sigmoid Rồi, với hàm sigmoid này thì chúng ta thấy nó rất dễ bị bão hòa Bão hòa theo nghĩa là gì? Khi giá trị đầu vào x của mình sigmoid Khi giá trị",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KoBIBuqGb9A",
      "filename": "KoBIBuqGb9A",
      "title": "[CS431 - Chương 4] Part 1_2: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 4,
      "start_timestamp": "0:02:55",
      "end_timestamp": "0:03:36"
    }
  },
  {
    "page_content": "giá trị đầu vào x của mình sigmoid Khi giá trị đầu vào x của mình nó chỉ mới đạt được những giá trị rất là bé thôi thì nó đã đạt được trạng thái đó là đạo hàm Độ dốc của đạo hàm gần như là đi ngang Độ dốc đạo hàm gần như đi ngang tức là đạo hàm của mình rất là nhỏ, nó tiến về 0 mà đạo hàm tiến về 0 thì tức là khi chúng ta nhân những cái giá trị này vô thì nó sẽ triệt tiêu thế thì tại sao ReLU lại chống được cái việc này? ReLU nó lại chống được cái việc này, đó là vì cái hàm ReLU của mình nó sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KoBIBuqGb9A",
      "filename": "KoBIBuqGb9A",
      "title": "[CS431 - Chương 4] Part 1_2: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 5,
      "start_timestamp": "0:03:34",
      "end_timestamp": "0:04:08"
    }
  },
  {
    "page_content": "việc này, đó là vì cái hàm ReLU của mình nó sẽ có tính chất đó là với những cái giá trị x mà lớn đúng không? lớn hơn 0, thì nó sẽ giữ nguyên cái giá trị hay nói cách khác Đó là đạo hàm của mình trong trường hợp này, độ dốc của mình trong trường hợp này luôn luôn là hằng số cố định. Và độ dốc của mình trong trường hợp này đó là bằng một. Thì việc đạo hàm bằng một này khiến cho các thành phần này, đâu đó các giá trị của mình sẽ cố định là bằng một. Nó không có tiến về số 0, nó sẽ không tiến về số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KoBIBuqGb9A",
      "filename": "KoBIBuqGb9A",
      "title": "[CS431 - Chương 4] Part 1_2: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 6,
      "start_timestamp": "0:04:02",
      "end_timestamp": "0:04:45"
    }
  },
  {
    "page_content": "Nó không có tiến về số 0, nó sẽ không tiến về số 0 mà nó sẽ để các giá trị là bằng một. Các giá trị bằng một khi nhân vô sẽ không giảm bớt hiện tượng kéo giá trị của mình về không. Đó là lý giải một cách hơi ngắn gọn cho việc tại sao dùng ReLU sẽ hiệu quả hơn Tăng tốc độ huấn luyện của mình hơn và giảm hiện tượng vanishing Bây giờ chúng ta sẽ nói thêm các cải tiến tiếp theo của AlexNet đó chính là tăng độ sâu của kiến trúc mạng Bình thường các mạng trước đó chỉ có 2 phép convolution và thêm 2",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KoBIBuqGb9A",
      "filename": "KoBIBuqGb9A",
      "title": "[CS431 - Chương 4] Part 1_2: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 7,
      "start_timestamp": "0:04:39",
      "end_timestamp": "0:05:35"
    }
  },
  {
    "page_content": "mạng trước đó chỉ có 2 phép convolution và thêm 2 phép biến đổi pooling, tức là 4 hoặc 5 phép biến đổi gì đó thôi Còn AlexNet thì nó sẽ có nhiều hơn số lượng phép biến đổi convolution và pooling Và khi mô hình học sâu này tăng lên thì đồng nghĩa là số lượng tham số cũng tăng lên Do đó, để tránh hiện tượng overfitting, AlexNet đã tăng cường dữ liệu nhiều hơn nó dùng phương pháp data augmentation Bằng cách đó là với mỗi ảnh Với mỗi ảnh, chúng ta sẽ thực hiện các phép tỷ lệ Rồi, chúng ta sẽ thực",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KoBIBuqGb9A",
      "filename": "KoBIBuqGb9A",
      "title": "[CS431 - Chương 4] Part 1_2: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 8,
      "start_timestamp": "0:05:31",
      "end_timestamp": "0:06:30"
    }
  },
  {
    "page_content": "sẽ thực hiện các phép tỷ lệ Rồi, chúng ta sẽ thực hiện phép xoay Rồi chúng ta thực hiện phép thêm nhiễu Rồi thay đổi độ sáng Với một ảnh chúng ta sẽ làm rất nhiều phép biến đổi khác nhau để tạo ra mẫu dữ liệu mới với cùng một nhãn Giống như là cái ảnh gốc đầu vào, thì nó sẽ giúp cho mình tăng data lên Và tăng data này lên thì nó sẽ giúp cho mình giảm hiện tượng overfitting Và một cái cải tiến cuối cùng so với những phiên bản trước đây không sử dụng GPU thì AlexNet đã cài đặt thuật toán của mình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KoBIBuqGb9A",
      "filename": "KoBIBuqGb9A",
      "title": "[CS431 - Chương 4] Part 1_2: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 9,
      "start_timestamp": "0:06:18",
      "end_timestamp": "0:07:15"
    }
  },
  {
    "page_content": "GPU thì AlexNet đã cài đặt thuật toán của mình để cho có thể chạy được trên GPU và tốc độ huấn luyện nó nhanh hơn gấp 50 lần thì đây chính là những cái cải tiến chính của mạng AlexNet Và khi chúng ta Google cái bài báo, khi chúng ta Google cái tên bài báo ở đây thì chúng ta thấy rằng AlexNet có số lượt trích dẫn là khoảng 128 ngàn trích dẫn tức là gì? khi mỗi bài báo được xuất bản thì họ sẽ trích dẫn đến những bài báo mà họ tham khảo thì bài AlexNet này được cộng đồng tham khảo đến 128 ngàn lần",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KoBIBuqGb9A",
      "filename": "KoBIBuqGb9A",
      "title": "[CS431 - Chương 4] Part 1_2: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 10,
      "start_timestamp": "0:07:08",
      "end_timestamp": "0:08:05"
    }
  },
  {
    "page_content": "này được cộng đồng tham khảo đến 128 ngàn lần thì đây là một trong những con số vô cùng khủng khiếp Tiếp theo chúng ta sẽ tìm hiểu đến kiến trúc mạng của VGG Đâu đó là vào 2014 đến 2015 Các cải tiến của VGG so với AlexNet rất là đơn giản đó chỉ là chúng ta thay các filter kích thước 5x5, 7x7 bằng các filter hay gọi là các bộ lọc đây là filter bằng các filter có kích thước là 3x3 và thực hiện liên tiếp nhau thì việc này sẽ giúp chúng ta giải quyết vấn đề gì thì chúng ta sẽ làm trên một filter",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KoBIBuqGb9A",
      "filename": "KoBIBuqGb9A",
      "title": "[CS431 - Chương 4] Part 1_2: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 11,
      "start_timestamp": "0:08:04",
      "end_timestamp": "0:09:01"
    }
  },
  {
    "page_content": "vấn đề gì thì chúng ta sẽ làm trên một filter 5x5 trước thì nếu như bình thường chúng ta sẽ có một ảnh đầu vào sau đó chúng ta nhân convolution với một filter kích thước là 5x5 chúng ta sẽ tạo ra một feature map và một điểm đặc trưng trên feature map output này nó được tạo bởi một vùng có kích thước 5x5 ở trên ảnh input Trên ảnh input thì điểm này sẽ bị phụ thuộc bởi vùng có kích thước là 5x5 Đây là cách bình thường Còn cái cải tiến của VGG đó là thay vì sử dụng kernel 5x5 thì nhóm tác giả",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KoBIBuqGb9A",
      "filename": "KoBIBuqGb9A",
      "title": "[CS431 - Chương 4] Part 1_2: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 12,
      "start_timestamp": "0:08:57",
      "end_timestamp": "0:10:07"
    }
  },
  {
    "page_content": "đó là thay vì sử dụng kernel 5x5 thì nhóm tác giả không sử dụng kernel 5x5 nữa hoặc 7x7 nữa mà thay hết bằng kernel filter 3x3 và thực hiện liên tiếp nhau, ví dụ đây chúng ta có một ảnh thực hiện convolution với một kernel kích thước là 3x3 rồi sau đó chúng ta sẽ tạo ra một tấm ảnh và cái ảnh này lại tiếp tục thực hiện convolution với lại kernel kích thước là 3x3 để tạo ra một tấm ảnh khác Bây giờ chúng ta sẽ xem điểm đặc trưng trên feature map cuối cùng ở đây Điểm đặc trưng này được tạo ra ở",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KoBIBuqGb9A",
      "filename": "KoBIBuqGb9A",
      "title": "[CS431 - Chương 4] Part 1_2: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 13,
      "start_timestamp": "0:09:51",
      "end_timestamp": "0:10:45"
    }
  },
  {
    "page_content": "cuối cùng ở đây Điểm đặc trưng này được tạo ra ở vùng có kích thước là 3x3 của feature map này Vùng 3x3 được tạo ra bởi vùng 5x5 Vùng 3x3 này thì tạo ra bởi vùng này, cái điểm này thì tạo ra bởi vùng này, cái điểm này thì tạo ra bởi vùng này Vì vậy, Feature Map của phép biến đổi cuối cùng sẽ được tạo bởi các điểm trong Feature Map của lớp trung gian. Và vùng kích thước sẽ là 3x3. Và vùng 3x3 sẽ được tạo bởi vùng 5x5 ở trên ảnh đầu vào. Vì vậy, xét về bản chất và tổng hợp thông tin. Nếu sử dụng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KoBIBuqGb9A",
      "filename": "KoBIBuqGb9A",
      "title": "[CS431 - Chương 4] Part 1_2: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 14,
      "start_timestamp": "0:10:41",
      "end_timestamp": "0:11:53"
    }
  },
  {
    "page_content": "về bản chất và tổng hợp thông tin. Nếu sử dụng phép biến đổi 5x5, thì một feature map output sẽ bị ảnh hưởng bởi 1 vùng 5x5 input đầu vào và dùng 2 phép convolution liên tiếp nhau thì nó cũng tương đương như vậy tức là 1 điểm ở đây sẽ được tổng hợp thông tin bởi 1 vùng 5x5 Vùng ảnh hưởng này người ta gọi là Receptive Field Cách làm của VGG có cái gì hơn? Để tìm số lượng tham số, nếu dùng kernel 5x5 thì tổng số lượng tham số của mình là 25 tham số sau này mình sẽ viết tắt bằng chữ P đi còn nếu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KoBIBuqGb9A",
      "filename": "KoBIBuqGb9A",
      "title": "[CS431 - Chương 4] Part 1_2: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 15,
      "start_timestamp": "0:11:44",
      "end_timestamp": "0:12:39"
    }
  },
  {
    "page_content": "số sau này mình sẽ viết tắt bằng chữ P đi còn nếu thực hiện hai phép convolution liên tiếp nó sẽ là 2 nhân cho 3 nhân 3 tức là 18 tham số vậy thì rõ ràng 18 sẽ bé hơn so với 25 và nếu chúng ta chia tỷ lệ thì 18 chia cho 25 thì đâu đó nó cỡ khoảng 70% Chúng ta đã tiết giảm được khoảng 30% số tham số Đây là giải thích tại sao việc bỏ các filter kích thước 5x5, 7x7 và thay bằng 3x3 sẽ giúp cho mình giảm số tham số Và giảm số tham số thì chúng ta biết rồi nó sẽ giúp cho mình giảm hiện tượng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KoBIBuqGb9A",
      "filename": "KoBIBuqGb9A",
      "title": "[CS431 - Chương 4] Part 1_2: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 16,
      "start_timestamp": "0:12:37",
      "end_timestamp": "0:13:22"
    }
  },
  {
    "page_content": "ta biết rồi nó sẽ giúp cho mình giảm hiện tượng overfitting overfitting Rồi, đồng thời, cái này thì không gọi là cải tiến nhưng mà VGG đã tăng cái độ sâu của mạng lên từ VGG 11 lên VGG 13, rồi lên VGG 16 và lên VGG 19 Thì cái này nó không được tính là cải tiến Và cải tiến lớn nhất của nó sẽ nằm ở cái chỗ này Đó là thay các cái filter lớn bằng những cái filter 3x3 liên tiếp Và trên đây thì chúng ta sẽ thấy là sơ đồ của một kiến trúc mạng là VGG16. Trong đó là VGG16 thì 16 thể hiện là các phép",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KoBIBuqGb9A",
      "filename": "KoBIBuqGb9A",
      "title": "[CS431 - Chương 4] Part 1_2: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 17,
      "start_timestamp": "0:13:18",
      "end_timestamp": "0:13:36"
    }
  },
  {
    "page_content": "Trong đó là VGG16 thì 16 thể hiện là các phép biến đổi bao gồm Convolution và Fully Connected. Tuy nhiên đây là 1, 2, 3, 4. Ở cái bước pooling này thì nó không có tạo ra đặc trưng mà nó chỉ là giảm chiều thôi nên nó không được tính. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16. thì ý nghĩa của con số 16 là như vậy Rồi, và bây giờ nếu như chúng ta nhìn vô số lượt trích dẫn của kiến trúc mạng VGG thì thấy là VGG có số lượt trích dẫn là 121.000 121.000 trích dẫn thì đây cũng là số lượt vô",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KoBIBuqGb9A",
      "filename": "KoBIBuqGb9A",
      "title": "[CS431 - Chương 4] Part 1_2: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 18,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "121.000 trích dẫn thì đây cũng là số lượt vô cùng khủng khiếp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KoBIBuqGb9A",
      "filename": "KoBIBuqGb9A",
      "title": "[CS431 - Chương 4] Part 1_2: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 19,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Mô hình tiếp theo là mô hình hồi quy tuyến tính, linear regression. Chúng ta sẽ nhắc lại mô hình máy học tổng quát. Với dữ liệu đầu vào x, giá trị dự đoán y, và mong muốn xấp xỉ gần với giá trị thực, chúng ta có 3 công việc cần phải làm khi thiết kế 1 mô hình. Đầu tiên là thiết kế hàm dự đoán, 2 là thiết kế hàm lỗi, 3 là đi tìm tham số theta sao cho hàm lỗi nhỏ nhất. và công việc này được giải quyết bằng thuật toán Gradient Descent Thế thì ở đây chúng ta có một cái nhấn mạnh đó là tùy vào cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:40"
    }
  },
  {
    "page_content": "chúng ta có một cái nhấn mạnh đó là tùy vào cái tính chất của cái cặp dữ liệu x, y để chúng ta thiết kế hai cái hàm này Thế thì chúng ta sẽ xem xét đến cái tình huống đầu tiên đó là giá trị đầu ra y có một cái mối quan hệ tuyến tính có một cái mối quan hệ tuyến tính với cái giá trị đầu vào x thì thế nào gọi là tuyến tính Tuyến tính có nghĩa là khi x tăng hoặc là khi x thay đổi x tăng thì y nó sẽ tăng hoặc là y sẽ giảm Ví dụ như đây là cái quan hệ đồng biến, chúng ta sẽ có cái mối quan hệ gọi là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 1,
      "start_timestamp": "0:00:35",
      "end_timestamp": "0:01:20"
    }
  },
  {
    "page_content": "đồng biến, chúng ta sẽ có cái mối quan hệ gọi là nghịch biến Khi x tăng và y nó lại có xu hướng là đi giảm xuống thì trong trường hợp này nó gọi là mối quan hệ tuyến tính Bước 1, chúng ta sẽ thiết kế hàm dự đoán Trong trường hợp y có mối quan hệ tuyến tính với x chúng ta có một đường thẳng qua như thế này Phương trình đường thẳng mà chúng ta đã học hồi xưa là y bằng ax cộng b Đó là phương trình hồi xưa hồi cấp 2 chúng ta học Trong trường hợp này chúng ta sẽ sử dụng cách ký hiệu theta Thay vì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 2,
      "start_timestamp": "0:01:10",
      "end_timestamp": "0:01:57"
    }
  },
  {
    "page_content": "chúng ta sẽ sử dụng cách ký hiệu theta Thay vì chúng ta để là a thì chúng ta sẽ để là theta 1 và b thì chúng ta sẽ để là theta 0 Vì vậy chúng ta sẽ có công thức cho hàm dự đoán f_theta(x) Với mẫu dữ liệu thứ i, ở đây chúng ta sẽ có nhiều mẫu dữ liệu Ở đây chúng ta sẽ có x_i và y_i Rồi, thì chúng ta sẽ có công thức như thế này Ở đây đó chính là thành phần bias Thành phần BIAS này có tác dụng đó là để cho giá trị dự đoán không phải lúc nào nó cũng chỉ phụ thuộc vào biến x Nó sẽ có những trường",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 3,
      "start_timestamp": "0:01:53",
      "end_timestamp": "0:02:41"
    }
  },
  {
    "page_content": "chỉ phụ thuộc vào biến x Nó sẽ có những trường hợp mà nó sẽ độc lập với biến x thì nó sẽ biểu diễn bởi BIAS này BIAS sẽ thể hiện cho những thành phần không phụ thuộc vào biến đầu vào Và với cách biểu diễn này, đường thẳng của chúng ta cũng sẽ rất là linh động, không nhất thiết nó phải đi qua góc tọa độ. Nó có thể đi qua những đường thẳng bất kỳ, nó sẽ đi qua bất kỳ những vị trí nào không nhất thiết phải đi qua góc tọa độ. Tạo ra sự tự do cho đường thẳng của mình. Bước thứ 2, đó là chúng ta sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 4,
      "start_timestamp": "0:02:32",
      "end_timestamp": "0:03:14"
    }
  },
  {
    "page_content": "thẳng của mình. Bước thứ 2, đó là chúng ta sẽ thiết kế hàm lỗi. Thì công thức cho hàm lỗi trong trường hợp này chúng ta sẽ sử dụng công thức đó là 1 phần 2n nhân cho tổng của y chạy từ 1 đến n của giá trị dự đoán Trừ cho cái giá trị thực tế Rồi tất cả bình phương Thế thì ở đây chúng ta sẽ đặt ra một câu hỏi đó là tại sao nó có một cái công thức có vẻ phức tạp như thế này công thức này là trung bình sai số bình phương thì tại sao nó lại có công thức phức tạp như thế này thì đầu tiên chúng ta sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 5,
      "start_timestamp": "0:03:09",
      "end_timestamp": "0:03:58"
    }
  },
  {
    "page_content": "phức tạp như thế này thì đầu tiên chúng ta sẽ xét đến những phiên bản ngây thơ nhất khi chúng ta thiết kế hàm lỗi này đó là nếu như chúng ta sử dụng y ngã_i trừ cho y_i thì sao tức là chúng ta sẽ không lấy bình phương mà chúng ta sẽ để là dấu trừ thôi Nếu chúng ta chọn giải pháp này, thì nó sẽ nảy sinh vấn đề đó là các sai số này khi chúng ta tính tổng lại có khả năng nó sẽ triệt tiêu lẫn nhau. Lấy ví dụ, với mẫu dữ liệu đầu tiên, sai số y ngã_i trừ y_i này là bằng 3. Với mẫu dữ liệu thứ 2, sai",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 6,
      "start_timestamp": "0:03:47",
      "end_timestamp": "0:04:45"
    }
  },
  {
    "page_content": "trừ y_i này là bằng 3. Với mẫu dữ liệu thứ 2, sai số đó là trừ 2. Với mẫu dữ liệu thứ 3, sai số là bằng 5. Và với mẫu dữ liệu thứ 4, sai số đó là, ví dụ như là trừ 6, thì khi chúng ta tính tổng các sai số này lại, chúng ta sẽ thấy như thế nào? 3 trừ 2 cộng 5 trừ 6 thì tổng sai số ra bằng 0. Với việc dùng công thức tổng, không có bình phương, thì các con số âm và dương sẽ triệt tiêu lẫn nhau Dẫn đến là, mặc dù hệ thống đang sai, nhưng mà cuối cùng tổng sai số là bằng không, đó là một điều vô lý",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 7,
      "start_timestamp": "0:04:32",
      "end_timestamp": "0:05:24"
    }
  },
  {
    "page_content": "tổng sai số là bằng không, đó là một điều vô lý Đây là một điều vô lý. Đây là cho phiên bản đầu tiên. Phiên bản thứ 2 là tại sao chúng ta không sử dụng công thức là tổng của trị tuyệt đối y ngã_i trừ cho y_i mà lại sử dụng hàm bình phương này. Trong trường hợp này thì nó vẫn thỏa mãn là nếu như chúng ta dùng trị tuyệt đối, Thì khi chúng ta tính tổng, tổng sai số của mình lúc này sẽ ra một con số rất lớn, như vậy là rất phù hợp về mặt ý nghĩa. Vì ở trong trường hợp này là 16, thì đây là một con",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 8,
      "start_timestamp": "0:05:12",
      "end_timestamp": "0:05:58"
    }
  },
  {
    "page_content": "ở trong trường hợp này là 16, thì đây là một con số rất lớn, thì đây là một con số rất phù hợp. Tuy nhiên nó sẽ bị một vấn đề là sang bước số 3, việc chúng ta tính đạo hàm cho một hàm trị tuyệt đối. đạo hàm cho 1 hàm trị tuyệt đối, đó là nhận 2 giá trị hoặc là 1 hoặc là trừ 1 tức là nó sẽ là bằng 1 nếu x dương và trừ 1 nếu x âm như vậy thì cái này sẽ tạo cho cái việc là cái hàm của chúng ta cái đạo hàm của chúng ta nó không liên tục và không liên tục thì dẫn đến là cái quá trình tính toán nó sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 9,
      "start_timestamp": "0:05:56",
      "end_timestamp": "0:06:36"
    }
  },
  {
    "page_content": "tục thì dẫn đến là cái quá trình tính toán nó sẽ phức tạp hơn do đó thì cái cách thiết kế này nó cũng không phù hợp Và như vậy thì từ 2 cái này thì chúng ta sẽ nảy ra, đó là chúng ta sẽ dùng công thức tính tổng của các sai số bình phương. Rồi, tuy nhiên khi tính tổng các sai số bình phương thì tại sao chúng ta lại phải chia trung bình? Thì nó sẽ nảy sinh 1 cái vấn đề như thế này. Nếu như chúng ta không chia trung bình và chúng ta có 1 cái giá trị lỗi, Ví dụ như chúng ta dự đoán giá nhà với tổng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 10,
      "start_timestamp": "0:06:28",
      "end_timestamp": "0:07:15"
    }
  },
  {
    "page_content": "lỗi, Ví dụ như chúng ta dự đoán giá nhà với tổng các sai số của mình đó chính là bằng 1.000 tỷ. Lấy ví dụ vậy. Thì hỏi đặt ra đó là cái sai số 1.000 tỷ này, liệu các bạn có dám mua một cái căn nhà mà được dự đoán bởi một cái hệ thống mà có sai số là 1.000 tỷ hay không? Thì câu trả lời đó chính là không chắc. Tại vì nếu như 1000 tỷ này mà chúng ta dự đoán trên 1 triệu căn nhà Trên 1 triệu căn nhà thì như vậy trung bình sai số trong 1 căn nhà đó là 0.001 tỷ Tức là đây là 1 con số rất là bé Đây là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 11,
      "start_timestamp": "0:07:12",
      "end_timestamp": "0:07:50"
    }
  },
  {
    "page_content": "0.001 tỷ Tức là đây là 1 con số rất là bé Đây là 1 con số rất là bé, nó chỉ khoảng là 1 triệu đồng thôi Như vậy thì chúng ta hoàn toàn có thể mua cái căn nhà này đúng không? Do cái sai số rất là bé Tuy nhiên nếu tổng số căn nhà trong trường hợp này mà dự đoán là sai trên 10 căn nhà thôi thì như vậy là sai số trung bình trong 1 căn nhà trong trường hợp này đó là 100 tỷ thì nếu đoán 1 căn nhà mà sai số 100 tỷ thì rõ ràng đây là 1 con số quá lớn như vậy thì đó là lý do tại sao chúng ta lại phải có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 12,
      "start_timestamp": "0:07:48",
      "end_timestamp": "0:08:25"
    }
  },
  {
    "page_content": "vậy thì đó là lý do tại sao chúng ta lại phải có chia trung bình để khi chúng ta ra được cái giá trị chia ra được cái giá trị lỗi Lỗi chúng ta biết được cái lỗi này đó là phù hợp hay không, có hợp lý hay không để sử dụng Ngoài ra ở đây chúng ta sẽ thấy nó có một cái con số 2, tại sao chúng ta lại có cái số 2 ở đây Để sau này khi chúng ta tiến hành tính đạo hàm cho cái hàm Loss này, thì nó sẽ có cái hàm mũ ở đây đúng không Thì chúng ta tính đạo hàm thì con số 2 này nó sẽ đem xuống và 2 chia 2 nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 13,
      "start_timestamp": "0:08:18",
      "end_timestamp": "0:09:09"
    }
  },
  {
    "page_content": "thì con số 2 này nó sẽ đem xuống và 2 chia 2 nó sẽ triệt tiêu đi Vì vậy, công thức của mình sau này sẽ đẹp hơn. Đó là lý do tại sao chúng ta có công thức hàm lỗi như trên. Hi vọng là qua các phiên bản này chúng ta sẽ hiểu hơn lý do động lực tại sao người ta chọn hàm lỗi này. Rồi, sang bước số 3, chúng ta sẽ đi tìm theta sao cho giá trị L, giá trị hàm loss là nhỏ nhất. Và khi này thì chúng ta có công thức là L(theta_0, theta_1), theta_0, theta_1 chính là hai tham số của theta. Với theta là một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 14,
      "start_timestamp": "0:09:05",
      "end_timestamp": "0:10:05"
    }
  },
  {
    "page_content": "chính là hai tham số của theta. Với theta là một vector bao gồm hai thành phần. Thì nó sẽ có công thức là bằng 1 phần 2n nhân cho tổng của cái công thức như sau. Và cái này chính là cái giá trị y ngã_i, y ngã_i là giá trị dự đoán. Còn y_i ở đây chính là giá trị thực tế Rồi, chúng ta sẽ tiến hành đi tính đạo hàm này Tính đạo hàm của L theo theta 0 thì chúng ta sẽ thấy là đạo hàm của cái này: 1 phần 2 N tổng của (theta 1 x_i cộng theta 0 trừ cho y_i) tất cả bình. Với i chạy từ 1 cho đến n. Thành",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 15,
      "start_timestamp": "0:10:00",
      "end_timestamp": "0:10:45"
    }
  },
  {
    "page_content": "tất cả bình. Với i chạy từ 1 cho đến n. Thành phần này là hằng số. Chúng ta sẽ tính đạo hàm của L theo theta 0. Đối với theta 0, đây là hằng số, do đó chúng ta sẽ giữ nguyên 1 phần 2n. Đạo hàm của tổng bằng tổng các đạo hàm. Và đây sẽ là một hàm hợp. Đạo hàm của này thì chúng ta sẽ có là 2 đưa cái này xuống, đó là theta 1 x_i Cộng cho theta 0 trừ cho y_i Sau đó chúng ta sẽ nhân đạo hàm của phần ruột bên trong Đạo hàm của phần ruột bên trong theo theta 0 Thì đây chính là hằng số đối với theta 0",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 16,
      "start_timestamp": "0:10:36",
      "end_timestamp": "0:11:15"
    }
  },
  {
    "page_content": "theta 0 Thì đây chính là hằng số đối với theta 0 Do đó chúng ta sẽ bỏ qua Và đạo hàm của theta 0 theo theta 0 chính là bằng 1 Vì vậy đem số 2 này ra ngoài triệt tiêu Vì vậy chúng ta sẽ có công thức như trên Và tương tự như vậy chúng ta hoàn toàn có thể tính đạo hàm của L theo theta 1 Bằng trung bình cộng của tổng của (theta 1 x_i cộng cho theta 0 trừ cho y_i) tất cả nhân với x_i Thì đây là cái công thức đạo hàm theo theta 1 Thì chúng ta yên tâm là với cái bài Linear Regression này thì chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 17,
      "start_timestamp": "0:11:08",
      "end_timestamp": "0:11:58"
    }
  },
  {
    "page_content": "là với cái bài Linear Regression này thì chúng ta sẽ còn ngồi tính toán đạo hàm Nhưng mà như chúng ta có đề cập trước đây, các Deep Learning Framework đã có công cụ để giúp chúng ta tự động tính các đạo hàm này và tự động tìm theta để cho hàm L là nhỏ nhất rồi Ở đây thì chúng ta tập luyện tính đạo hàm thôi để sau này chúng ta có thể tiến hành cài đặt và thử nghiệm Còn bước số 3 từ đây trở về sau chúng ta hoàn toàn có thể sử dụng thuật toán Adam để đi tìm giá trị nhỏ nhất Rồi, thì ở đây chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 18,
      "start_timestamp": "0:11:56",
      "end_timestamp": "0:12:35"
    }
  },
  {
    "page_content": "đi tìm giá trị nhỏ nhất Rồi, thì ở đây chúng ta sẽ sử dụng thuật toán Gradient Descent đã được học ở trong phần về mô hình tổng quát chúng ta sẽ có bước khởi tạo theta 0 và theta 1 là ngẫu nhiên đồng thời là 2 siêu tham số learning rate (alpha) và tham số dừng epsilon là 2 con số nhỏ thì alpha ở đây chúng ta có thể cho là 0.01 Còn epsilon ở đây thì chúng ta có thể cho đó là 0.001 Đó là những con số rất là bé Và chúng ta sẽ tiến hành lặp Và lưu ý đó là ở đây chúng ta có 2 tham số theta 0 và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 19,
      "start_timestamp": "0:12:33",
      "end_timestamp": "0:12:41"
    }
  },
  {
    "page_content": "ý đó là ở đây chúng ta có 2 tham số theta 0 và theta 1 Do đó thì cái bước cập nhật chúng ta sẽ cập nhật trên cả 2 tham số này Theta 0 sẽ bằng theta 0 trừ cho alpha nhân cho đạo hàm của L theo theta 0 Còn đối với theta 1, chúng ta sẽ phải tính đạo hàm của hàm loss theo theta 1 Và điều kiện dừng, chúng ta sẽ xét 2 điều kiện dừng Đó là khi đạo hàm của hàm loss theo theta 0 Và đạo hàm của hàm loss theo theta 1 đủ nhỏ Thì chúng ta sẽ kết thúc vòng lặp Thì đây chính là 3 bước cho thuật toán linear",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 20,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "lặp Thì đây chính là 3 bước cho thuật toán linear regression",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m8uqtMEg8-E",
      "filename": "m8uqtMEg8-E",
      "title": "[CS431 - Chương 2] Part 2a_1: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 21,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Mục tiêu của MobileNet là tăng độ chính xác, giảm khối lượng tính toán. Nhưng cái cải tiến của nó đồng thời đã giúp chúng ta giải quyết được hiện tượng overfitting luôn. Thì cái cải tiến của MobileNet đó là gì? MobileNet đã thay thế phép Convolution bình thường bằng phép Depthwise Separable Convolution, tức là DSC. Bản chất của phép DSC này thì nó thực hiện 2 bước. Nó cũng giống như bottleneck của GoogleNet, nó gồm 2 bước. Đầu tiên là Depthwise Convolution và sau đó sẽ thực hiện Pointwise",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MNHY9TA4fZs",
      "filename": "MNHY9TA4fZs",
      "title": "[CS431 - Chương 4] Part 1_4: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:50"
    }
  },
  {
    "page_content": "Convolution và sau đó sẽ thực hiện Pointwise Convolution Chi tiết chúng ta sẽ nói trong ví dụ sau Ở đây chúng ta sẽ có input Input của mình trong trường hợp này có 32 kênh Có độ sâu là 32 Và chúng ta sẽ lấy filter này Filter này sẽ chia sẻ với filter này, tức là dùng chung bộ filter Lấy filter này, chúng ta sẽ lần lượt thực hiện trên từng kênh độc lập nhau. Depthwise, tức là thực hiện một cách độc lập theo chiều độ sâu. Thực hiện độc lập. Lấy filter này, nhân với lại feature này để tạo ra",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MNHY9TA4fZs",
      "filename": "MNHY9TA4fZs",
      "title": "[CS431 - Chương 4] Part 1_4: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 1,
      "start_timestamp": "0:00:45",
      "end_timestamp": "0:01:29"
    }
  },
  {
    "page_content": "filter này, nhân với lại feature này để tạo ra feature map này. Lấy filter này, filter này, filter này giống nhau. Xin lỗi, nhân với feature map này để tạo ra feature map. lấy cái filter này, nhân với lại cái feature này để tạo ra feature map này và chúng ta concatenate, chúng ta nối tất cả cái kết quả này lại với nhau thì lúc này chúng ta sẽ tạo ra một cái feature map có kích thước bằng là 32 thì đây là cái bước đầu tiên sang cái bước thứ hai, bước thứ hai đó là chúng ta sẽ thực hiện Pointwise",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MNHY9TA4fZs",
      "filename": "MNHY9TA4fZs",
      "title": "[CS431 - Chương 4] Part 1_4: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 2,
      "start_timestamp": "0:01:26",
      "end_timestamp": "0:02:03"
    }
  },
  {
    "page_content": "thứ hai đó là chúng ta sẽ thực hiện Pointwise Convolution hay còn gọi là một nhân một Convolution thì ở đây chúng ta sẽ thực hiện cái phép Convolution giống như cái phép Convolution bình thường và cái kích thước đầu ra của mình trong trường hợp này đó là 64 kênh thì ở đây chúng ta sẽ thực hiện 64 Convolution 64 filter 1 nhân 1 nhân cho 32 tại vì cái depth ở đây là 32 như vậy là cái filter này là có kích thước là 1 nhân 1 nhân 32 1 nhân 1 nhân 32 và 64 kernel này 64 filter này Nếu chúng ta thực",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MNHY9TA4fZs",
      "filename": "MNHY9TA4fZs",
      "title": "[CS431 - Chương 4] Part 1_4: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 3,
      "start_timestamp": "0:01:59",
      "end_timestamp": "0:02:59"
    }
  },
  {
    "page_content": "và 64 kernel này 64 filter này Nếu chúng ta thực hiện phép Convolution bình thường thì số lượng tham số sẽ là 3 x 3 Và depth đầu vào của mình là 32 Đây là kích thước của filter Nhân với 64 filter như vậy thì nó sẽ ra là khoảng 18.000 tham số Còn nếu chúng ta thực hiện Depthwise Separable Convolution thì ở đây chúng ta sẽ có là kích thước của filter của mình sẽ là 32 x 3 Tức là sao? Ở đây kích thước của filter của mình là 3 x 3 và mình sẽ có độ sâu tương ứng là 32 Độ sâu là 32, với thằng này có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MNHY9TA4fZs",
      "filename": "MNHY9TA4fZs",
      "title": "[CS431 - Chương 4] Part 1_4: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 4,
      "start_timestamp": "0:02:51",
      "end_timestamp": "0:03:40"
    }
  },
  {
    "page_content": "tương ứng là 32 Độ sâu là 32, với thằng này có độ sâu là 32 Cái filter này có độ sâu là 32 và nó sẽ chia sẻ trọng số với các kênh này Vậy tổng số tham số của mình sẽ là 3 x 3 x 32 cho cái bước số 1 Đối với cái bước số 2, đây là bước 1 Đối với cái bước số 2 thì cái filter của mình nó sẽ có kích thước là 1 x 1 x 32 1 x 1 x 32 và có 64 filter như vậy Cộng lại 2 số lượng tham số của bước 1 và bước 2 thì số lượng tham số sẽ là 2000 Nếu chia ra 2000 cho 18000 thì đâu đó nó sẽ xấp xỉ là 1 phần 9 Như",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MNHY9TA4fZs",
      "filename": "MNHY9TA4fZs",
      "title": "[CS431 - Chương 4] Part 1_4: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 5,
      "start_timestamp": "0:03:37",
      "end_timestamp": "0:04:32"
    }
  },
  {
    "page_content": "cho 18000 thì đâu đó nó sẽ xấp xỉ là 1 phần 9 Như vậy, số lượng tham số của mình giảm xuống 1 phần 9 Như vậy, nó sẽ giúp cho mình giảm bộ nhớ RAM Giảm bộ nhớ RAM sẽ có 2 công dụng 1, giảm hiện tượng overfitting 2, tăng tốc độ tính toán lên Đây chính là mục tiêu chính của mạng MobileNet là tăng tốc độ tính toán Và như tên, MobileNet có thể triển khai trên các thiết bị di động Một trong những mạng CNN có khả năng triển khai trên thiết bị di động sử dụng những phần cứng không có quá đắt tiền và có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MNHY9TA4fZs",
      "filename": "MNHY9TA4fZs",
      "title": "[CS431 - Chương 4] Part 1_4: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 6,
      "start_timestamp": "0:04:29",
      "end_timestamp": "0:05:18"
    }
  },
  {
    "page_content": "dụng những phần cứng không có quá đắt tiền và có khối lượng xử lý lớn MobileNet có số lượng citation khá là lớn, đó là 24 ngàn citation. Đây cũng là một trong những kiến trúc mạng rất là nổi tiếng. Vì vậy, chúng ta sẽ tóm tắt lại một số thành tựu của các kiến trúc mạng. LeNet cải tiến lớn nhất của nó đó chính là phép Convolution và phép Pooling. Mục tiêu của Convolution đó là để giảm số lượng tham số và giảm tham số này để giúp chúng ta giảm hiện tượng overfitting. Pooling sau này thì nó cũng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MNHY9TA4fZs",
      "filename": "MNHY9TA4fZs",
      "title": "[CS431 - Chương 4] Part 1_4: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 7,
      "start_timestamp": "0:05:13",
      "end_timestamp": "0:06:13"
    }
  },
  {
    "page_content": "tượng overfitting. Pooling sau này thì nó cũng giúp chúng ta giảm số lượng tham số nhưng đồng thời nó cũng giúp chúng ta giảm khối lượng tính toán, giảm việc tính toán. việc giảm tham số này sẽ giúp chúng ta giảm hiện tượng overfitting AlexNet cải tiến lớn nhất của nó là nó sẽ thay thằng sigmoid bằng ReLU ReLU sẽ giảm hiện tượng Vanishing Gradient Đồng thời, nó tăng cường dữ liệu lên. Tăng dữ liệu này sẽ giúp chúng ta giảm hiện tượng overfitting. Đồng thời, nó sẽ là lần đầu tiên sử dụng GPU để",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MNHY9TA4fZs",
      "filename": "MNHY9TA4fZs",
      "title": "[CS431 - Chương 4] Part 1_4: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 8,
      "start_timestamp": "0:06:00",
      "end_timestamp": "0:06:59"
    }
  },
  {
    "page_content": "Đồng thời, nó sẽ là lần đầu tiên sử dụng GPU để tăng tốc độ tính toán lên. VGG đây là một trong những kiến trúc mạng có cải tiến rất đơn giản, đó là thay những thằng 5x5, 7x7 bỏ hết đi và thay bằng những 3x3 liên tiếp. Và việc cải tiến này đã giúp chúng ta giảm số lượng tham số với cùng một mục đích, với cùng một việc trích rút đặc trưng với receptive field giống nhau, thì nó đã giảm được số lượng tham số mà giảm số lượng tham số giúp chúng ta giảm được hiện tượng overfitting GoogleNet có hai",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MNHY9TA4fZs",
      "filename": "MNHY9TA4fZs",
      "title": "[CS431 - Chương 4] Part 1_4: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 9,
      "start_timestamp": "0:06:53",
      "end_timestamp": "0:07:48"
    }
  },
  {
    "page_content": "giảm được hiện tượng overfitting GoogleNet có hai cải tiến chính 1. Sử dụng bottleneck 1 nhân 1, Convolution Và hai, đó là Inception Module. Thì hai cái cải tiến này sẽ giúp cho chúng ta giảm số lượng tham số. Đồng thời, đó là do giúp cho chúng ta, Inception này sẽ giúp cho chúng ta tận dụng được các đặc trưng Từ nhiều loại, từ nhiều filter có kích thước khác nhau, ví dụ filter 3x3, filter 1x1, filter 5x5 Tại vì giả định của GoogleNet là họ không biết filter kích thước bao nhiêu là tối ưu thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MNHY9TA4fZs",
      "filename": "MNHY9TA4fZs",
      "title": "[CS431 - Chương 4] Part 1_4: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 10,
      "start_timestamp": "0:07:41",
      "end_timestamp": "0:08:31"
    }
  },
  {
    "page_content": "biết filter kích thước bao nhiêu là tối ưu thì họ sẽ sử dụng hết Đây chính là cái cải tiến của GoogleNet, giảm tham số này sẽ giúp chúng ta giảm hiện tượng overfitting ResNet, đơn giản nhất của ResNet là sử dụng skip connection Sử dụng skip connection và biểu diễn dưới dạng công thức thì chúng ta sẽ có h(x) sẽ là bằng Convolution của x cộng thêm với x thì đây chính là cái cải tiến lớn nhất của ResNet thì việc này sẽ giúp chúng ta tăng giá trị đạo hàm lên. Khi mình tính h' thì đạo hàm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MNHY9TA4fZs",
      "filename": "MNHY9TA4fZs",
      "title": "[CS431 - Chương 4] Part 1_4: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 11,
      "start_timestamp": "0:08:27",
      "end_timestamp": "0:09:23"
    }
  },
  {
    "page_content": "giá trị đạo hàm lên. Khi mình tính h' thì đạo hàm Convolution sẽ cộng thêm một. Nó sẽ giúp tăng giá trị đạo hàm lên. Và việc tăng đạo hàm từng thành phần lên sẽ giúp chúng ta giải quyết vấn đề Vanishing Gradient. Rồi, cuối cùng đó chính là MobileNet. Cải tiến lớn nhất của đó là thay vì chúng ta sử dụng 3x3 Convolution không sử dụng 3x3 Convolution nữa mà chúng ta sẽ kết hợp Depthwise Convolution cộng với lại phép 1x1 Convolution hay còn tên là Pointwise Convolution thì việc này sẽ giúp chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MNHY9TA4fZs",
      "filename": "MNHY9TA4fZs",
      "title": "[CS431 - Chương 4] Part 1_4: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 12,
      "start_timestamp": "0:09:18",
      "end_timestamp": "0:10:03"
    }
  },
  {
    "page_content": "Convolution thì việc này sẽ giúp chúng ta giảm được số lượng tham số và cụ thể ở đây là giảm xuống còn 1 phần 9, tức là đã giảm 8 phần 9 tại vì từ ban đầu giảm xuống 1 phần 9 thì nó đã giảm 8 phần 9 số lượng tham số và việc giảm tham số này sẽ có hai tác dụng đó là chống được overfitting và đồng thời nó sẽ tăng speed tốc độ tính toán của mình lên Như vậy thì ở trên đây, chúng ta đã tóm tắt qua các kiến trúc mạng và những cái cải tiến chính thì chúng ta thấy là hai cái vấn đề lớn nhất mà các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MNHY9TA4fZs",
      "filename": "MNHY9TA4fZs",
      "title": "[CS431 - Chương 4] Part 1_4: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 13,
      "start_timestamp": "0:09:57",
      "end_timestamp": "0:10:51"
    }
  },
  {
    "page_content": "chúng ta thấy là hai cái vấn đề lớn nhất mà các kiến trúc mạng tập trung giải quyết Chúng ta nhìn xuyên xuống đây, chỉ có hai vấn đề lớn nhất thôi Hai vấn đề, vấn đề đầu tiên đó chính là vấn đề Overfitting Và vấn đề thứ hai đó là Vanishing Gradient. Cái vấn đề về overfitting là xảy ra khi các kiến trúc mạng càng lúc càng sâu thì số lượng tham số càng tăng hoặc là số tham số càng tăng thì mô hình càng phức tạp, nó sẽ dễ dẫn đến hiện tượng overfitting Và để giải quyết vấn đề này thì chúng ta sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MNHY9TA4fZs",
      "filename": "MNHY9TA4fZs",
      "title": "[CS431 - Chương 4] Part 1_4: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 14,
      "start_timestamp": "0:10:43",
      "end_timestamp": "0:11:31"
    }
  },
  {
    "page_content": "Và để giải quyết vấn đề này thì chúng ta sẽ phải thiết kế để làm giảm số lượng tham số tham số, giảm số lượng tham số, hoặc tăng cường dữ liệu lên. Còn đối với vấn đề về vanishing gradient, nó sẽ gây ra việc tham số theta cập nhật, nó sẽ chậm. Tham số theta sẽ cập nhật, do giá trị đạo hàm này nó bé. Để chống hiện tượng vanishing gradient này, người ta sẽ có những giải pháp liên quan đến việc tăng giá trị đạo hàm từng thành phần trong hàm loss này lên. ResNet chỉ với một cái cải tiến rất tí ti,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MNHY9TA4fZs",
      "filename": "MNHY9TA4fZs",
      "title": "[CS431 - Chương 4] Part 1_4: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 15,
      "start_timestamp": "0:11:24",
      "end_timestamp": "0:12:01"
    }
  },
  {
    "page_content": "lên. ResNet chỉ với một cái cải tiến rất tí ti, rất là nhỏ, đó là cộng thêm cái x đầu vào, cộng thêm cái dữ kiện đầu vào, thì nó đã giúp cho chúng ta tăng giá trị đạo hàm và tăng giá trị đạo hàm giảm được hiện tượng vanishing. Đối với ResNet thì chúng ta có một cách giải thích khác cho việc cộng x này. Convolution này là tạo ra một feature. Nhưng feature này sẽ không còn giữ được thông tin của dữ kiện đầu vào nữa. Do đó chúng ta cộng thêm x. Đây chính là dữ kiện đặc trưng gốc. Việc cộng này sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MNHY9TA4fZs",
      "filename": "MNHY9TA4fZs",
      "title": "[CS431 - Chương 4] Part 1_4: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 16,
      "start_timestamp": "0:11:59",
      "end_timestamp": "0:12:21"
    }
  },
  {
    "page_content": "chính là dữ kiện đặc trưng gốc. Việc cộng này sẽ giúp chúng ta kết hợp những đặc trưng mới và những cái đặc trưng gốc để làm cho đầy đủ thông tin hơn cho cái quá trình nhận diện. Thì đây là một cái cách giải thích khác theo cái lý thuyết về mặt thông tin của ResNet. Đó là giải thích cái tính hiệu quả của ResNet. Như vậy thì qua những cái kiến trúc mạng này thì chúng ta đã học được rất nhiều những cái kỹ thuật khác nhau trong cái việc là cải tiến các cái mô hình học sâu. Hy vọng là các bạn có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MNHY9TA4fZs",
      "filename": "MNHY9TA4fZs",
      "title": "[CS431 - Chương 4] Part 1_4: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 17,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "các cái mô hình học sâu. Hy vọng là các bạn có thể vận dụng được những cái mẹo này, những cái kỹ thuật này để cải tiến cho những cái mô hình học sâu tiếp theo của mình. Cảm ơn các bạn đã xem video.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MNHY9TA4fZs",
      "filename": "MNHY9TA4fZs",
      "title": "[CS431 - Chương 4] Part 1_4: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 18,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chúng ta sẽ dần làm quen với việc tổng quát hóa và vector hóa mô hình máy học. Để tổng quát hóa và vector hóa, chúng ta sẽ đưa đến các khái niệm sử dụng các vector và ma trận. thì ở đây chúng ta sẽ có hai cái vector Đối với dữ liệu là một mẫu tức là gồm nhiều biến x1, x2, xm thì ở đây chúng ta sẽ cho một cái ví dụ đây là một cái mẫu dữ liệu được ký hiệu bởi một cái vector và vector này chúng ta lưu ý là ký hiệu bởi một cái ký tự viết in đậm và viết thường Với x0, thành phần đầu tiên là bias, đại",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MtJDVr5xHB4",
      "filename": "MtJDVr5xHB4",
      "title": "[CS431 - Chương 2] Part 2a_2: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:00"
    }
  },
  {
    "page_content": "thường Với x0, thành phần đầu tiên là bias, đại diện cho tất cả mô hình độc lập với các biến đầu vào Các biến đầu vào không còn là một biến đầu vào nữa mà nó có thể gồm nhiều biến đầu vào Lấy ví dụ bài toán dự đoán giá nhà thì x1 này của mình nó có thể sẽ là diện tích x2 này của mình có thể là số phòng và xm này có thể là khoảng cách đến trung tâm thành phố Thì đây chính là các cái biến số để giúp cho chúng ta đưa ra được cái dự đoán Cái nhãn của một mẫu dữ liệu sẽ là cái giá trị Y. Và các biến",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MtJDVr5xHB4",
      "filename": "MtJDVr5xHB4",
      "title": "[CS431 - Chương 2] Part 2a_2: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 1,
      "start_timestamp": "0:00:45",
      "end_timestamp": "0:01:45"
    }
  },
  {
    "page_content": "một mẫu dữ liệu sẽ là cái giá trị Y. Và các biến số để giúp mình dự đoán cái nhãn này là x1, x2, xm. Như vậy, chúng ta đã tổng quát hóa cho trường hợp nhiều biến, nhưng lưu ý là mới chỉ có một mẫu dữ liệu thôi. Với một mẫu dữ liệu thôi. Trong phần tiếp theo, chúng ta sẽ tổng quát hóa cho nhiều mẫu dữ liệu. Và tham số của mô hình dự đoán của mình trong trường hợp này đó chính là một vector theta bao gồm nhiều thành phần, thì theta 0, theta 1 và theta m thì tương ứng theta 0 sẽ được nhân với",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MtJDVr5xHB4",
      "filename": "MtJDVr5xHB4",
      "title": "[CS431 - Chương 2] Part 2a_2: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 2,
      "start_timestamp": "0:01:39",
      "end_timestamp": "0:02:23"
    }
  },
  {
    "page_content": "và theta m thì tương ứng theta 0 sẽ được nhân với bias, theta 1 sẽ được nhân với x1 và theta m sẽ nhân với xm Như vậy thì lúc này hàm dự đoán của mình f theta x nó sẽ được viết bằng tích vô hướng của theta và x và khi nhân tích vô hướng thì nó sẽ lấy từng thành phần nhân với nhau xong rồi cộng lại và hàm lỗi trong trường hợp này nó sẽ là lấy theta x trừ y tất cả bình đây chính là cái giá trị dự đoán hay còn gọi là y ngã và giá trị dự đoán này mình mong muốn xấp xỉ Y thì chúng ta sẽ lấy cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MtJDVr5xHB4",
      "filename": "MtJDVr5xHB4",
      "title": "[CS431 - Chương 2] Part 2a_2: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 3,
      "start_timestamp": "0:02:19",
      "end_timestamp": "0:03:02"
    }
  },
  {
    "page_content": "mình mong muốn xấp xỉ Y thì chúng ta sẽ lấy cái thằng này trừ nhau và bình phương đó chính là cho trường hợp một mẫu dữ liệu một mẫu dữ liệu còn trong trường hợp mà dữ liệu của mình là toàn bộ n mẫu thì mình sẽ vector hóa nó như thế này chúng ta sẽ ký hiệu bằng một cái ma trận chúng ta sẽ ký hiệu bằng một cái ma trận trong đó các cái cột của cái ma trận này nó tương ứng là một cái vector biểu diễn cho một mẫu vector của một mẫu Vector này sẽ thể hiện như vậy là dạng cột Các vector này thể hiện",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MtJDVr5xHB4",
      "filename": "MtJDVr5xHB4",
      "title": "[CS431 - Chương 2] Part 2a_2: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 4,
      "start_timestamp": "0:02:58",
      "end_timestamp": "0:03:48"
    }
  },
  {
    "page_content": "hiện như vậy là dạng cột Các vector này thể hiện như dạng cột Các cột này ráp lại với nhau, thì nó sẽ tạo ra thành một cái ma trận Và cái chỉ số ở phía trên tương ứng là chỉ số thứ tự của mẫu Nếu như chúng ta có n mẫu và từng cái x1, x2, cho đến xn là bao gồm m biến Cái ma trận X này sẽ có kích thước m cộng 1 nhân n Tại sao lại có cái m cộng 1 này? Đó chính là do thành phần bias, m biến, thêm một cái thành phần bias nữa là m cộng 1 Đối với nhãn, nhãn của toàn bộ n mẫu sẽ ký hiệu bằng một ma",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MtJDVr5xHB4",
      "filename": "MtJDVr5xHB4",
      "title": "[CS431 - Chương 2] Part 2a_2: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 5,
      "start_timestamp": "0:03:37",
      "end_timestamp": "0:04:43"
    }
  },
  {
    "page_content": "nhãn của toàn bộ n mẫu sẽ ký hiệu bằng một ma trận Y trong đó mỗi giá trị Y1, Y2, Yn chính là nhãn của mình và chúng ta sẽ có Y thuộc một ma trận kích thước đó là 1 nhân cho n hay còn gọi là một vector dạng nằm ngang X trong trường hợp này là một cái ma trận số dòng của mình sẽ là m cộng 1 và số cột của mình trong trường hợp này là n và như vậy thì cái hàm dự đoán của mình là bằng, chúng ta sẽ lấy tham số theta nhân cho từng mẫu dữ liệu theta nhân với x1, theta x2, theta xn Và khi này thì chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MtJDVr5xHB4",
      "filename": "MtJDVr5xHB4",
      "title": "[CS431 - Chương 2] Part 2a_2: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 6,
      "start_timestamp": "0:04:39",
      "end_timestamp": "0:05:30"
    }
  },
  {
    "page_content": "với x1, theta x2, theta xn Và khi này thì chúng ta sẽ giống như là rút thừa số chung như vậy đó Chúng ta sẽ rút thừa số chung theta ra Rồi sau đó đưa các cái x1, x2 vào bên trong cái ngoặc Và toàn bộ cái x1, x2 cho đến xn này nó chính là ma trận X Như vậy thì chúng ta sẽ có cái công thức cho cái hàm dự đoán Đó là theta chuyển vị nhân với X Đối với hàm lỗi thì chúng ta cũng hoàn toàn làm tương tự như vậy Hàm lỗi thì chúng ta cần phải lấy giá trị dự đoán trừ cho giá trị thực tế Giá trị dự đoán",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MtJDVr5xHB4",
      "filename": "MtJDVr5xHB4",
      "title": "[CS431 - Chương 2] Part 2a_2: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 7,
      "start_timestamp": "0:05:28",
      "end_timestamp": "0:06:22"
    }
  },
  {
    "page_content": "dự đoán trừ cho giá trị thực tế Giá trị dự đoán trong trường hợp này là y ngã Theta chuyển vị nhân với X này sẽ là một dạng vector dạng nằm ngang Theta chuyển vị nhân với X là một vector dạng nằm ngang Y cũng là vector dạng nằm ngang, là 1 nhân n Tại vì có n mẫu, nên có n giá trị dự đoán Đây là y ngã, đây là y Lấy 2 cái này trừ cho nhau, thì sẽ có theta chuyển vị nhân X trừ cho Y Kết quả của mình cũng sẽ ra vector dạng nằm ngang như thế này Và để tính tổng trung bình cộng của các cái sai số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MtJDVr5xHB4",
      "filename": "MtJDVr5xHB4",
      "title": "[CS431 - Chương 2] Part 2a_2: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 8,
      "start_timestamp": "0:06:15",
      "end_timestamp": "0:07:13"
    }
  },
  {
    "page_content": "để tính tổng trung bình cộng của các cái sai số bình phương thì chúng ta sẽ lấy các cái vector nằm ngang này nhân với nhau Rồi sau đó cộng lại thì đây chính là hai cái vector nhân với nhau thì đây chính là (theta chuyển vị nhân với X trừ Y) và cái vector dạng dọc như thế này (Theta chuyển vị nhân với X trừ Y) Tất cả chuyển vị sẽ tạo ra một vector dạng nằm dọc Khi nhân 2 cái này lại với nhau, chúng ta sẽ nhân từng phần tử Sau đó cộng lại, bản chất đây chính là sai số bình phương Đây chính là sai",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MtJDVr5xHB4",
      "filename": "MtJDVr5xHB4",
      "title": "[CS431 - Chương 2] Part 2a_2: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 9,
      "start_timestamp": "0:06:59",
      "end_timestamp": "0:07:46"
    }
  },
  {
    "page_content": "đây chính là sai số bình phương Đây chính là sai số trong một mẫu, mẫu thứ 2, mẫu thứ 3, mẫu thứ n Đây chính là các cái sai số và khi chúng ta nhân theo cái kiểu là hai cái vector nằm ngang, nhân với vector nằm dọc thì nó sẽ tạo ra tổng các sai số bình phương sau đó chúng ta sẽ chia trung bình cộng Và để tính đạo hàm cho hàm lỗi thì ở trong trường hợp này chúng ta lưu ý theta của mình nó không còn là một cái tham số dạng scalar mà nó là một vector do đó chúng ta phải dùng một cái ký hiệu nabla",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MtJDVr5xHB4",
      "filename": "MtJDVr5xHB4",
      "title": "[CS431 - Chương 2] Part 2a_2: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 10,
      "start_timestamp": "0:07:43",
      "end_timestamp": "0:08:29"
    }
  },
  {
    "page_content": "do đó chúng ta phải dùng một cái ký hiệu nabla Bản chất đây là một vector gradient, vector đạo hàm Cho một vector, chúng ta tính đạo hàm cho cái hàm ở trên 1 phần 2n chúng ta kéo xuống Và số 2 này, ở đây nó sẽ tương đương là (theta chuyển vị nhân X trừ Y) tất cả bình phương Bình phương chúng ta tính đạo hàm thì nó sẽ triệt tiêu đi con số 2 Thành phần này đem xuống dưới và đạo hàm bên trong ruột này theo theta Đạo hàm của (theta chuyển vị nhân X) theo theta, còn Y Đối với theta nó chính là hằng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MtJDVr5xHB4",
      "filename": "MtJDVr5xHB4",
      "title": "[CS431 - Chương 2] Part 2a_2: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 11,
      "start_timestamp": "0:08:25",
      "end_timestamp": "0:09:09"
    }
  },
  {
    "page_content": "theo theta, còn Y Đối với theta nó chính là hằng số Do đó chúng ta sẽ xem như thằng này đạo hàm là bằng 0 Đạo hàm của theta chuyển vị nhân X theo biến theta thì chúng ta sẽ có Chúng ta sẽ còn X Do đó đạo hàm của theta chuyển vị nhân X nó chính là X Nếu như chúng ta nhìn ở dưới dạng góc độ là đại số, đương nhiên cách này không chính thống, không đúng về mặt ký hiệu Nhưng nếu chúng ta nhìn theo cách này, đó là (theta chuyển vị nhân X trừ Y) tất cả bình phương Khi lấy đạo hàm của nó ra, chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MtJDVr5xHB4",
      "filename": "MtJDVr5xHB4",
      "title": "[CS431 - Chương 2] Part 2a_2: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 12,
      "start_timestamp": "0:09:06",
      "end_timestamp": "0:09:43"
    }
  },
  {
    "page_content": "bình phương Khi lấy đạo hàm của nó ra, chúng ta sẽ đem số 2 xuống, rồi thành phần này chúng ta sẽ kéo xuống Đó là (theta chuyển vị nhân X trừ Y) sau đó chúng ta sẽ nhân đạo hàm của phần ruột đạo hàm của phần ruột theo theta thì thành phần Y là 0 thành phần (theta chuyển vị nhân X) nó sẽ còn là X số 2 này nó sẽ triệt tiêu với này, nó sẽ bị mất như vậy nó sẽ còn là X nhân cho (theta chuyển vị nhân X trừ Y) thì đây là cái chứng minh cho công thức đạo hàm này và khi chúng ta đã tính được đạo hàm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MtJDVr5xHB4",
      "filename": "MtJDVr5xHB4",
      "title": "[CS431 - Chương 2] Part 2a_2: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 13,
      "start_timestamp": "0:09:40",
      "end_timestamp": "0:01:31"
    }
  },
  {
    "page_content": "đạo hàm này và khi chúng ta đã tính được đạo hàm rồi thì thuật toán gradient descent rất là đơn giản Đó là chúng ta sẽ khởi tạo ngẫu nhiên cái vector Lúc này theta của mình là vector rồi Tại vì cái x của mình bao gồm m biến Theta của mình thì nó sẽ là bao gồm m cộng 1 thành phần Theta sẽ là bao gồm theta 0, theta 1 cho đến theta m m cộng 1 thành phần Và các cái thành phần này chúng ta sẽ khởi tạo ngẫu nhiên Hai siêu tham số alpha và epsilon cũng khởi tạo các con số nhỏ chúng ta sẽ lặp và theta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MtJDVr5xHB4",
      "filename": "MtJDVr5xHB4",
      "title": "[CS431 - Chương 2] Part 2a_2: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 14,
      "start_timestamp": "0:01:26",
      "end_timestamp": "0:02:09"
    }
  },
  {
    "page_content": "khởi tạo các con số nhỏ chúng ta sẽ lặp và theta sẽ được cập nhật bằng theta trừ cho alpha nhân đạo hàm gradient của L theo theta nó sẽ có công thức là 1 phần n nhân X nhân (theta chuyển vị nhân X trừ Y) do đó chúng ta chép nó qua đây và chúng ta sẽ có công thức cập nhật Điều kiện dừng là nếu giá trị sai số đủ nhỏ thì chúng ta sẽ dừng lặp Chúng ta lưu ý là cái nabla của L theo theta nó là cái vector gradient hay nói cách khác nó là bằng các đạo hàm thành phần theo theta 0, đạo hàm của L theo",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MtJDVr5xHB4",
      "filename": "MtJDVr5xHB4",
      "title": "[CS431 - Chương 2] Part 2a_2: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 15,
      "start_timestamp": "0:02:02",
      "end_timestamp": "0:02:48"
    }
  },
  {
    "page_content": "hàm thành phần theo theta 0, đạo hàm của L theo theta 1, theo theta m. Thì đây là một cái vector, do đó chúng ta hoàn toàn có thể sử dụng cái giá trị độ lớn của cái vector này để làm cái điều kiện dừng. khi giá trị độ lớn của vector đạo hàm này, vector gradient này đủ nhỏ, chúng ta sẽ kết thúc vòng lặp. Đây chính là tổng quát hóa và vector hóa cho mô hình Linear Regression. Trong phần tiếp theo, chúng ta sẽ tiến hành cài đặt bằng hai phương pháp vector hóa và phương pháp không vector hóa. Và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MtJDVr5xHB4",
      "filename": "MtJDVr5xHB4",
      "title": "[CS431 - Chương 2] Part 2a_2: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 16,
      "start_timestamp": "0:02:40",
      "end_timestamp": "0:03:34"
    }
  },
  {
    "page_content": "vector hóa và phương pháp không vector hóa. Và cuối cùng cho phần Linear Regression này, chúng ta sẽ biểu diễn mô hình dưới dạng là đồ thị Đầu vào chúng ta sẽ có thành phần là bias, rồi các cái biến x1, x2, cho đến xm Và tương ứng từng đầu vào này chúng ta sẽ có các tham số theta 0, theta 1, theta 2 và theta m Và khi từng thành phần này nhân vô, chúng ta sẽ qua một hàm tính tổng Tại sao lại tổng? Tại vì tổng của từng các tích này 1 x theta 0, x1 x theta 1, x2 x theta 2, xm x theta m sau đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MtJDVr5xHB4",
      "filename": "MtJDVr5xHB4",
      "title": "[CS431 - Chương 2] Part 2a_2: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 17,
      "start_timestamp": "0:03:28",
      "end_timestamp": "0:03:52"
    }
  },
  {
    "page_content": "x1 x theta 1, x2 x theta 2, xm x theta m sau đó chúng ta cộng lại, chúng ta sẽ ra được giá trị dự đoán và đây là cái dạng viết với dạng vector hóa Với một đồ thị này chúng ta có thể hiểu được cách mà chúng ta lan truyền thông tin Và cái độ dài của các cạnh này tương ứng cho trọng số của thông tin. Thông tin này đưa vào có trọng số theta 0, thông tin này đưa vào có trọng số theta 1. Mỗi cái này sẽ có một cái trọng số. Rồi, tổng hợp thông tin để đưa ra giá trị dự đoán. thì đó là toàn bộ nội dung",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MtJDVr5xHB4",
      "filename": "MtJDVr5xHB4",
      "title": "[CS431 - Chương 2] Part 2a_2: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 18,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "ra giá trị dự đoán. thì đó là toàn bộ nội dung của phần Linear Regression.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=MtJDVr5xHB4",
      "filename": "MtJDVr5xHB4",
      "title": "[CS431 - Chương 2] Part 2a_2: Mô hình hồi quy tuyến tính (Linear Regression)",
      "chunk_id": 19,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề, học sâu, Machine Learning, Logistic Regression, CNN, ImageNet. Tiếp theo, đó chính là tensor 3D. Về mặt ký hiệu, chúng ta sẽ biểu diễn bởi một ký tự viết hoa và tô đậm. Và ở đây chúng ta thấy là tensor 3D nên chúng ta sẽ có 3 thành phần Và nói một cách khác thì đây là một tensor với 3 chiều không gian Và có kích thước từng chiều lần lượt sẽ là m, n và p Thì hiểu một cách nôm na, tensor 3D nó chính là một tập hợp các ma trận có cùng kích thước và chúng ta lấy một ví dụ là một tensor 3",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=N-9Uq1_lIzI",
      "filename": "N-9Uq1_lIzI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.2: Ôn tập nền tảng đại số tuyến tính (Part 2)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:55"
    }
  },
  {
    "page_content": "thước và chúng ta lấy một ví dụ là một tensor 3 chiều như sau thì để tạo ra tensor 3 chiều này chúng ta sẽ sử dụng đoạn code Python như sau thì chúng ta thấy các giá trị của vector đầu tiên đó là 123 tập hợp các vector cùng chiều thì nó sẽ tạo ra một ma trận và chúng ta sẽ có ma trận là 123456 tương ứng nó sẽ là lớp cắt ở đây và tập hợp các lớp cắt này, chúng ta sẽ có 10, 20, 30, 40, 50, 60 là một lớp cắt rồi 100, 200, 300, 400, 500, 600 là một lớp cắt thì tập hợp tất cả các lớp cắt ma trận",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=N-9Uq1_lIzI",
      "filename": "N-9Uq1_lIzI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.2: Ôn tập nền tảng đại số tuyến tính (Part 2)",
      "chunk_id": 1,
      "start_timestamp": "0:00:50",
      "end_timestamp": "0:01:52"
    }
  },
  {
    "page_content": "lớp cắt thì tập hợp tất cả các lớp cắt ma trận này, nó sẽ tạo thành một tensor và tensor 3D. và tương tự như vector và scalar thì tensor 3D thì chúng ta sẽ sử dụng ndim và shape để kiểm tra xem số chiều của nó có phải là 3 và kích thước theo từng chiều nó có phải là kích thước mà mình đang mong muốn khởi tạo hay không ví dụ như trong trường hợp này chúng ta sẽ thấy là kích thước của chiều đầu tiên sẽ là 4 chiều Kích thước của chiều thứ 2 sẽ là 2 chiều. Kích thước của chiều thứ 3 là 3 chiều. Và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=N-9Uq1_lIzI",
      "filename": "N-9Uq1_lIzI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.2: Ôn tập nền tảng đại số tuyến tính (Part 2)",
      "chunk_id": 2,
      "start_timestamp": "0:01:38",
      "end_timestamp": "0:02:31"
    }
  },
  {
    "page_content": "chiều. Kích thước của chiều thứ 3 là 3 chiều. Và 4 chiều chính là kích thước ở đây. Và ứng dụng của tensor 3D đó chính là dùng để biểu diễn ảnh màu. Trong số những các ảnh màu thì chúng ta thấy ảnh Red, Green, Blue (RGB) là một trong những các ảnh màu nổi tiếng và phổ biến thì chúng ta thấy có một hình mẫu ở đây Bản chất tấm hình mà chúng ta nhìn thấy ở đây đó chính là được tạo bởi 3 lớp cắt tương ứng là 3 kênh màu Red, Green và Blue và các giá trị trên từng kênh Red, Green, Blue này thì nó sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=N-9Uq1_lIzI",
      "filename": "N-9Uq1_lIzI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.2: Ôn tập nền tảng đại số tuyến tính (Part 2)",
      "chunk_id": 3,
      "start_timestamp": "0:02:25",
      "end_timestamp": "0:03:20"
    }
  },
  {
    "page_content": "trị trên từng kênh Red, Green, Blue này thì nó sẽ tạo thành một tensor và tensor này có độ sâu là 3 có chiều cao là 5 và có chiều ngang là 5 luôn thì đây là một tensor, một tensor 3D để biểu diễn cho tấm ảnh và lưu ý ở đây là đây là một ví dụ minh họa tượng trưng tại vì chúng ta thấy tấm ảnh này nó sẽ có rất nhiều hàng, rất nhiều cột chúng ta sẽ biểu diễn một tensor có kích thước đó là 3 nhân 5 nhân 5 và các tensor khác hay còn gọi là tensor nhiều chiều thì chúng ta sẽ ghép các tensor 3D có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=N-9Uq1_lIzI",
      "filename": "N-9Uq1_lIzI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.2: Ôn tập nền tảng đại số tuyến tính (Part 2)",
      "chunk_id": 4,
      "start_timestamp": "0:03:18",
      "end_timestamp": "0:04:01"
    }
  },
  {
    "page_content": "nhiều chiều thì chúng ta sẽ ghép các tensor 3D có cùng kích thước thì chúng ta sẽ tạo được tensor 4D rồi tương tự như vậy chúng ta ghép các tensor 4D sẽ tạo thành các tensor 5D là cứ như vậy. thì ở đây là hình ảnh minh họa cho các tensor 1D, 2D, 3D, 4D, 5D Mình rất hay lấy ví dụ này để minh họa cho ý nghĩa của các tensor nhiều chiều Đầu tiên, nếu như chúng ta xem thông tin của một bạn sinh viên bao gồm tuổi, lớp, điểm số trung bình của từng môn thì nó là một tensor 1D tập hợp các vector để biểu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=N-9Uq1_lIzI",
      "filename": "N-9Uq1_lIzI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.2: Ôn tập nền tảng đại số tuyến tính (Part 2)",
      "chunk_id": 5,
      "start_timestamp": "0:03:53",
      "end_timestamp": "0:05:03"
    }
  },
  {
    "page_content": "nó là một tensor 1D tập hợp các vector để biểu diễn các sinh viên thì nó sẽ tạo ra thành một ma trận 2 chiều và chúng ta sẽ xem ma trận này để biểu diễn cho một lớp học Rồi tập hợp các tensor 2D hay tập hợp các lớp học thì chúng ta sẽ tạo thành tensor 3D và nó tương ứng ý nghĩa trong thực tế đó chính là một trường học Rồi, tập hợp các tensor 3D tạo ra thành tensor 4D thì khái niệm này tương ứng trong cuộc sống của mình đó chính là tập hợp các trường trong một quốc gia Và tương tự như vậy,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=N-9Uq1_lIzI",
      "filename": "N-9Uq1_lIzI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.2: Ôn tập nền tảng đại số tuyến tính (Part 2)",
      "chunk_id": 6,
      "start_timestamp": "0:04:55",
      "end_timestamp": "0:06:08"
    }
  },
  {
    "page_content": "trường trong một quốc gia Và tương tự như vậy, tensor 5D thì nó sẽ là tập hợp các trường trong châu Á, ví dụ vậy, hoặc châu Âu. tập hợp các trường trong các châu lục thì nó sẽ tạo thành các trường trên thế giới Thì đây là một ví dụ dễ hiểu để giúp cho các bạn hiểu các khái niệm và tưởng tượng ra được khái niệm về tensor 5D, 6D và chúng ta có thể lấy minh họa cho tensor 4D là thường được sử dụng để biểu diễn cho các video tại sao? tại vì nếu như chúng ta xem một frame ảnh trong video là một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=N-9Uq1_lIzI",
      "filename": "N-9Uq1_lIzI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.2: Ôn tập nền tảng đại số tuyến tính (Part 2)",
      "chunk_id": 7,
      "start_timestamp": "0:06:01",
      "end_timestamp": "0:06:35"
    }
  },
  {
    "page_content": "như chúng ta xem một frame ảnh trong video là một tensor 3D thì tập hợp các tensor 3D này có cùng kích thước này thì có phải tạo thành một video không? Và như vậy thì video nó sẽ tương đương với việc chúng ta sẽ dùng tensor 4D để biểu diễn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=N-9Uq1_lIzI",
      "filename": "N-9Uq1_lIzI",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.2: Ôn tập nền tảng đại số tuyến tính (Part 2)",
      "chunk_id": 8,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Trong Transformer, chúng ta sẽ bắt gặp một cái khái niệm dùng rất là thường xuyên, đó chính là self-attention, tức là kỹ thuật tự chú ý. Thì ở trong bài trước, chúng ta đã nói về khái niệm là attention, tức là một từ truy vấn query của mình ở bước decoder nó sẽ truy xuất và tổng hợp thông tin từ các tập giá trị value của encoder. Thì như chúng ta thấy là đây là giai đoạn encoder và đây là giai đoạn decoder. trong giai đoạn decoder này, chúng ta sẽ phải lookup, chúng ta sẽ phải tra vào tất cả",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NsWX_5oV8bY",
      "filename": "NsWX_5oV8bY",
      "title": "[CS431 - Chương 10] Part 3: Cơ chế Self-Attention",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:48"
    }
  },
  {
    "page_content": "sẽ phải lookup, chúng ta sẽ phải tra vào tất cả những từ trong giai đoạn encoder. Còn đây là encoder, còn đây là quá trình là decoder. Khi đó, nó gọi là attention, tức là sự truy vấn của một query ở bước decoder, Truy xuất vào và tổng hợp từ thông tin của các giá trị ở encoder. Từ decoder mình sẽ truy xuất vào encoder. Ở đây chúng ta thấy là điểm mạnh của self-attention chính là khả năng song song. Trong sơ đồ trước đây chúng ta thấy là tại cái vị trí này chúng ta sẽ bị phụ thuộc vào phép tính",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NsWX_5oV8bY",
      "filename": "NsWX_5oV8bY",
      "title": "[CS431 - Chương 10] Part 3: Cơ chế Self-Attention",
      "chunk_id": 1,
      "start_timestamp": "0:00:42",
      "end_timestamp": "0:01:37"
    }
  },
  {
    "page_content": "vị trí này chúng ta sẽ bị phụ thuộc vào phép tính trước đó. Trong khi đó, tại đây thì chúng ta chỉ cần phụ thuộc vào 2 phép tính. Tại sao? Tại vì để tính ra được cái giá trị ở đây, chúng ta sẽ phải cần tính trước các giá trị ở layer trước đó. Ở đây là layer số 2. Ở đây là layer số 3. Đây là layer số 2. Và đây là layer số 1. thì để tính được layer số 3, chúng ta sẽ cần thông tin của layer số 1 và thông tin của layer số 1 thì lại cần thông tin của layer số 2 và ở layer số 2 thì lại cần thông tin",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NsWX_5oV8bY",
      "filename": "NsWX_5oV8bY",
      "title": "[CS431 - Chương 10] Part 3: Cơ chế Self-Attention",
      "chunk_id": 2,
      "start_timestamp": "0:01:32",
      "end_timestamp": "0:02:26"
    }
  },
  {
    "page_content": "layer số 2 và ở layer số 2 thì lại cần thông tin của layer số 1 thì tại đây chúng ta thấy là trạng thái ẩn này nó sẽ bị phụ thuộc bởi một phép tính trước đó, đó là đây Và các phép tính này thì thực hiện song song Chính vì nó thực hiện song song nên không có phép tính nào phụ thuộc với phép tính nào Rồi, và ở đây thì chúng ta sẽ có khái niệm self-attention thì thật ra nó cũng chính là attention nhưng mà nhưng mà thay vì chúng ta là mối quan hệ giữa encoder và decoder, đó là attention, và ở đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NsWX_5oV8bY",
      "filename": "NsWX_5oV8bY",
      "title": "[CS431 - Chương 10] Part 3: Cơ chế Self-Attention",
      "chunk_id": 3,
      "start_timestamp": "0:02:21",
      "end_timestamp": "0:03:16"
    }
  },
  {
    "page_content": "encoder và decoder, đó là attention, và ở đây đó là cơ chế attention trong giai đoạn encoder, hoặc là trong giai đoạn decoder. Tức là mỗi từ nó sẽ chú ý đến nhau trong cùng một cái input hoặc là trong output Thì giả sử như đây là nguyên giai đoạn encoder Thì các cái từ nó sẽ tự chú ý với nhau Trong đó nó cũng chú ý đến chính nó Chú ý đến chính nó ở đây Và chú ý đến những cái từ còn lại trong giai đoạn encoder của mình hoặc là trong giai đoạn decoder, tức là nó sẽ tự chú ý đến những từ trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NsWX_5oV8bY",
      "filename": "NsWX_5oV8bY",
      "title": "[CS431 - Chương 10] Part 3: Cơ chế Self-Attention",
      "chunk_id": 4,
      "start_timestamp": "0:03:09",
      "end_timestamp": "0:03:52"
    }
  },
  {
    "page_content": "decoder, tức là nó sẽ tự chú ý đến những từ trong giai đoạn decoder của mình. Đó là sự khác biệt giữa khái niệm attention và self-attention.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NsWX_5oV8bY",
      "filename": "NsWX_5oV8bY",
      "title": "[CS431 - Chương 10] Part 3: Cơ chế Self-Attention",
      "chunk_id": 5,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Biểu diễn từ bằng vector là vai trò cực kỳ quan trọng trong các mô hình mạng thần kinh. Tất cả các mô hình mạng thần kinh hiện nay đều phải xử lý tính toán dưới dạng các con số. và các con số này thì nó sẽ có thể tính toán như là vector nhân vector, vector nhân ma trận, hoặc là ma trận nhân ma trận và thậm chí là chúng ta có thể tính toán trên cái khối lượng lớn ví dụ như là tensor nhân với lại tensor như vậy thì cái nhu cầu là làm sao biểu diễn được một cái từ dưới dạng các cái vector đó là một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=O57P9YHZOE0",
      "filename": "O57P9YHZOE0",
      "title": "[CS431 - Chương 6] Part 3: Biểu diễn từ bằng Vector",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:43"
    }
  },
  {
    "page_content": "một cái từ dưới dạng các cái vector đó là một trong những cái nhu cầu rất là thiết yếu Và các kỹ thuật hiện nay thường được sử dụng là, đầu tiên, chúng ta có thể biểu diễn dưới dạng One-Hot Vector. Tức là một cái từ của mình trong tập từ điển nó xuất hiện ở vị trí nào, thì tương ứng là trong vector của mình, vị trí đó sẽ bật lên là 1 và những phần tử còn lại sẽ bật là 0. Chút nữa chúng ta sẽ nói rõ hơn cái ví dụ này. Thứ hai, đó là chúng ta có thể sử dụng mô hình Bag-of-Words. tức là cái vector",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=O57P9YHZOE0",
      "filename": "O57P9YHZOE0",
      "title": "[CS431 - Chương 6] Part 3: Biểu diễn từ bằng Vector",
      "chunk_id": 1,
      "start_timestamp": "0:00:34",
      "end_timestamp": "0:01:16"
    }
  },
  {
    "page_content": "sử dụng mô hình Bag-of-Words. tức là cái vector của mình nó có thể biểu diễn cho 1 câu hoặc là 1 đoạn văn và thứ 3, đó chính là chúng ta có thể biểu diễn từ bằng ngữ cảnh tức là 1 từ nếu như đứng một mình nó thì nó sẽ không có được ý nghĩa trọn vẹn mà chúng ta phải đối chiếu nó trong cái ngữ cảnh xung quanh Ví dụ như cái từ ông già hồi nãy, trong cái từ tiếng Việt, cái từ ông già này nếu mà mình để một nghĩa thông thường thì mình sẽ hiểu đó là một cái người lớn tuổi Nhưng mà nếu như mình đang",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=O57P9YHZOE0",
      "filename": "O57P9YHZOE0",
      "title": "[CS431 - Chương 6] Part 3: Biểu diễn từ bằng Vector",
      "chunk_id": 2,
      "start_timestamp": "0:01:09",
      "end_timestamp": "0:01:56"
    }
  },
  {
    "page_content": "một cái người lớn tuổi Nhưng mà nếu như mình đang nói về hai người bạn thân với nhau, đang trao đổi về chuyện gia đình thì có thể là chúng ta hiểu ông già này hàm ý là cái người cha của mình Như vậy thì muốn biết ý nghĩa của từ ông già này như thế nào thì chúng ta phải xem bối cảnh xung quanh để biết được ý nghĩa thực sự của từ này Rồi, chúng ta sẽ đến với cách biểu diễn đầu tiên đó là One-Hot Vector Trước đây chúng ta xem một từ được xem là một phần tử trong một từ điển, đúng không? Vì vậy,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=O57P9YHZOE0",
      "filename": "O57P9YHZOE0",
      "title": "[CS431 - Chương 6] Part 3: Biểu diễn từ bằng Vector",
      "chunk_id": 3,
      "start_timestamp": "0:01:44",
      "end_timestamp": "0:02:35"
    }
  },
  {
    "page_content": "phần tử trong một từ điển, đúng không? Vì vậy, bây giờ, chúng ta có cách biểu diễn One-Hot Vector như sau? Ý nghĩa của One-Hot là cái từ One-Hot Chính là có một con số 1 sẽ... Dịch sang tiếng Việt nó gọi là Vector Đơn Trội Tiếng Việt của mình là Vector Đơn Trội Tức là chỉ có một thằng trội lên thôi và còn lại là số 0 và ý nghĩa của số 1 này đó chính là vị trí của từ trong từ điển của mình là vị trí từ trong từ điển ví dụ như ở đây, chúng ta sẽ có hai từ là motel và hotel thì vị trí từ motel là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=O57P9YHZOE0",
      "filename": "O57P9YHZOE0",
      "title": "[CS431 - Chương 6] Part 3: Biểu diễn từ bằng Vector",
      "chunk_id": 4,
      "start_timestamp": "0:02:32",
      "end_timestamp": "0:03:18"
    }
  },
  {
    "page_content": "hai từ là motel và hotel thì vị trí từ motel là nó xuất hiện ở đây trong tập từ điển còn vị trí từ hotel thì nó xuất hiện ở đây nên ở đây nó sẽ bật lên 1 tất cả những vị trí còn lại sẽ để là số 0 thì đây là một cách biểu diễn rất là đơn giản và dễ hiểu. Tuy nhiên, với cách biểu diễn này thì nó vẫn chứa trong đó những cái vấn đề. Vấn đề đầu tiên đó là khi chúng ta tìm kiếm với từ khóa trên mạng internet, ví dụ vậy, và chúng ta tìm kiếm với từ là Saigon Hotel. Thì như vậy thì cái từ khóa Saigon",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=O57P9YHZOE0",
      "filename": "O57P9YHZOE0",
      "title": "[CS431 - Chương 6] Part 3: Biểu diễn từ bằng Vector",
      "chunk_id": 5,
      "start_timestamp": "0:03:14",
      "end_timestamp": "0:03:58"
    }
  },
  {
    "page_content": "Saigon Hotel. Thì như vậy thì cái từ khóa Saigon Hotel lẽ ra nó sẽ phải trả về các văn bản và các trang web mà có chứa cả hai từ khóa là Saigon Hotel và Saigon Motel Tại vì xét về mặt ý nghĩa là chúng ta đang tìm một nơi để mà mình dừng chân, mình nghỉ ngơi thì Saigon Hotel và Motel thì nó cũng sẽ giống nhau về mặt ý nghĩa đó thì lẽ ra cái kết quả nó sẽ phải trả về Nhưng nếu như chúng ta sử dụng hai cái từ đó dưới dạng là cái vector One-Hot Nếu như chúng ta biểu diễn hai cái từ Hotel và Motel",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=O57P9YHZOE0",
      "filename": "O57P9YHZOE0",
      "title": "[CS431 - Chương 6] Part 3: Biểu diễn từ bằng Vector",
      "chunk_id": 6,
      "start_timestamp": "0:03:54",
      "end_timestamp": "0:04:36"
    }
  },
  {
    "page_content": "như chúng ta biểu diễn hai cái từ Hotel và Motel dưới dạng là cái One-Hot vector thì cả hai cái vector này sẽ có tính chất trực giao với nhau nên sự tương đồng là bằng không Và hai cái từ khóa mà có độ tương đồng là không tức là khi chúng ta tìm kiếm thì rõ ràng là cái kết quả trả về những cái từ Motel nó sẽ không được trả về Nếu như chúng ta search với từ khóa là hotel Với những điểm yếu của cách biểu diễn One-Hot Embedding Các từ được xem như độc lập nhau Hotel và Motel nó xem như độc lập",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=O57P9YHZOE0",
      "filename": "O57P9YHZOE0",
      "title": "[CS431 - Chương 6] Part 3: Biểu diễn từ bằng Vector",
      "chunk_id": 7,
      "start_timestamp": "0:04:31",
      "end_timestamp": "0:05:21"
    }
  },
  {
    "page_content": "độc lập nhau Hotel và Motel nó xem như độc lập nhau Nó không xem xét đến ý nghĩa của từ đó là gì Chúng ta sẽ nảy sinh ra cách biểu diễn thứ hai Đó là biểu diễn từ bằng ngữ cảnh Và chúng ta có một câu nói rất là nổi tiếng của ông Firth vào năm 1957 đó là bạn sẽ biết được một cái từ bằng cách nhìn vào những từ xung quanh nó Bằng những cái từ xung quanh nó Lấy ví dụ ha Chúng ta sẽ lấy ví dụ là có một câu tiếng Anh là I love ... so much I love ... so much Thì cái từ mà chúng ta có thể điền vô ở đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=O57P9YHZOE0",
      "filename": "O57P9YHZOE0",
      "title": "[CS431 - Chương 6] Part 3: Biểu diễn từ bằng Vector",
      "chunk_id": 8,
      "start_timestamp": "0:05:15",
      "end_timestamp": "0:06:02"
    }
  },
  {
    "page_content": "much Thì cái từ mà chúng ta có thể điền vô ở đây được á thì nó sẽ có cùng cái ngữ cảnh với nhau. Ví dụ, I love you so much, I love him so much, và I love her so much. Các cái từ you, him, her đều có thể thay thế để điền vô vị trí ở giữa này. Do đó chúng ta có thể nói rằng từ you, him, her nó sẽ có cùng mối quan hệ về mặt ngữ cảnh. Tất cả từ này có thể thay thế được cho từ kia. Và như vậy thì một cái vector khi biểu diễn thì nó sẽ gọi là Word Vector Vector biểu diễn trong một từ thì nó gọi là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=O57P9YHZOE0",
      "filename": "O57P9YHZOE0",
      "title": "[CS431 - Chương 6] Part 3: Biểu diễn từ bằng Vector",
      "chunk_id": 9,
      "start_timestamp": "0:05:53",
      "end_timestamp": "0:06:34"
    }
  },
  {
    "page_content": "Vector biểu diễn trong một từ thì nó gọi là Word Vector Và đó là cái vector biểu diễn trong từ Trong đó các cái từ có cùng ngữ cảnh thì được biểu diễn tương đồng nhau nghĩa là sao những cái từ nào mà có thể thay thế cho nhau trong cùng một ngữ cảnh thì nó có khả năng là phải có cái tính tương đồng và để đo được cái sự tương đồng của hai vector thì chúng ta sẽ sử dụng cái công thức đó là tích vô hướng hay còn tên tiếng Anh đó là Dot Product và chúng ta sẽ có một cái ví dụ như sau hai vector",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=O57P9YHZOE0",
      "filename": "O57P9YHZOE0",
      "title": "[CS431 - Chương 6] Part 3: Biểu diễn từ bằng Vector",
      "chunk_id": 10,
      "start_timestamp": "0:06:32",
      "end_timestamp": "0:07:08"
    }
  },
  {
    "page_content": "chúng ta sẽ có một cái ví dụ như sau hai vector hotel và motel thì hai vector này chúng ta sẽ thấy nó có cái sự tương đồng tại sao? Tại vì ở cái phần tử đầu tiên chúng ta thấy đó là một cái giá trị dương là khoảng 0.28 thì ở đây cũng là một cái giá trị dương 0.4 ở đây là một cái giá trị dương là 0.79 thì ở đây là một cái giá trị dương 0.58 như vậy là có sự tương đồng và khi đến cái giá trị thứ 3 nó rớt xuống còn là giá trị âm đúng không? thì ở đây nó cũng ra giá trị âm rồi riêng cái phần tử thứ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=O57P9YHZOE0",
      "filename": "O57P9YHZOE0",
      "title": "[CS431 - Chương 6] Part 3: Biểu diễn từ bằng Vector",
      "chunk_id": 11,
      "start_timestamp": "0:07:03",
      "end_timestamp": "0:07:40"
    }
  },
  {
    "page_content": "nó cũng ra giá trị âm rồi riêng cái phần tử thứ 4 thì nó sẽ khác nhau đây là dương còn đây là âm thì điều này cũng dễ hiểu thôi Tại vì bản chất là hai từ hotel và motel nó cũng mặc dù nó giống nhau, nó có một số chỗ giống nhau về mặt ý nghĩa, nhưng nó cũng sẽ có những cái điểm khác biệt. Chứ không thể nào mà hai vector này nó tương đồng nhau hoàn toàn. Thành phần thứ 5 cũng là các con số dương, 0.1, và đây là 0.2, và đây là con số 0, thì đây cũng là con số 0. Vì vậy hai vector này có tính tương",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=O57P9YHZOE0",
      "filename": "O57P9YHZOE0",
      "title": "[CS431 - Chương 6] Part 3: Biểu diễn từ bằng Vector",
      "chunk_id": 12,
      "start_timestamp": "0:07:37",
      "end_timestamp": "0:08:17"
    }
  },
  {
    "page_content": "là con số 0. Vì vậy hai vector này có tính tương đồng, tương đối là cao. Xét về cách biểu diễn theo ngữ cảnh này. Và Word Vector còn một tên gọi khác trong các thuật ngữ tiếng Anh, đó chính là Word Embedding hoặc Word Representation. Nếu như sau này chúng ta xem các tài liệu, thì chúng ta thấy là khi nói về Word Vector, hoặc là khi nói về Word Embedding, hoặc là khi nói về Word Representation, thì tất cả đều có chung một ý nghĩa, đó chính là làm sao có thể biểu diễn một từ dưới dạng một vector.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=O57P9YHZOE0",
      "filename": "O57P9YHZOE0",
      "title": "[CS431 - Chương 6] Part 3: Biểu diễn từ bằng Vector",
      "chunk_id": 13,
      "start_timestamp": "0:08:07",
      "end_timestamp": "0:08:54"
    }
  },
  {
    "page_content": "sao có thể biểu diễn một từ dưới dạng một vector. Và để trực quan hóa không gian embedding, chúng ta đang sử dụng cái khái niệm ở đây, sử dụng cái cách dùng từ ở đây để trực quan hóa một cái từ trong một cái không gian embedding, thì chúng ta sẽ có cái ví dụ như sau Ví dụ như các từ pink, white, blue, thì đây là những cái từ mà đều có cái vai trò ngữ cảnh giống nhau Ví dụ như là mình thích màu vàng, I like pink, và mình thích màu vàng, I like yellow, I like pink, I like white, I like blue thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=O57P9YHZOE0",
      "filename": "O57P9YHZOE0",
      "title": "[CS431 - Chương 6] Part 3: Biểu diễn từ bằng Vector",
      "chunk_id": 14,
      "start_timestamp": "0:08:49",
      "end_timestamp": "0:09:32"
    }
  },
  {
    "page_content": "I like pink, I like white, I like blue thì tất cả những từ pink, white, blue đều có thể thay thế được cho nhau trong cái ngữ cảnh đó do đó thì từ pink, white và blue sẽ nằm gần nhau trong cái không gian embedding Tương tự như vậy các động từ is, am, are và was, were thì nằm gần nhau Rồi từ hotel và motel sẽ nằm gần nhau Và những cái từ nào mà nó khác biệt nhau về mặt ngữ cảnh thì chúng ta sẽ thấy là cái vị trí của nó trong không gian cũng cách xa nhau Ví dụ như pink, white, blue thì nó sẽ rất",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=O57P9YHZOE0",
      "filename": "O57P9YHZOE0",
      "title": "[CS431 - Chương 6] Part 3: Biểu diễn từ bằng Vector",
      "chunk_id": 15,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "xa nhau Ví dụ như pink, white, blue thì nó sẽ rất là xa so với lại các cái động từ to be ở đây Vì chúng nó có cái mối quan hệ về mặt ngữ cảnh rất là khác nhau Cảm ơn.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=O57P9YHZOE0",
      "filename": "O57P9YHZOE0",
      "title": "[CS431 - Chương 6] Part 3: Biểu diễn từ bằng Vector",
      "chunk_id": 16,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Bước thứ hai của quá trình xây dựng một mô hình máy học, đó chính là chúng ta thiết kế hàm mất mát. Thì ở đây chúng ta sẽ có các giá trị dự đoán. Và ở phía trên chúng ta sẽ có các giá trị thực, thực tế. Là chúng ta ký hiệu là y, y_t, y_hat_t. và chúng ta luôn mong muốn là hai cái giá trị này nó xấp xỉ với nhau, đúng không? thì chúng ta sẽ có cái hàm loss tại cái thời điểm thứ T, tức là chúng ta sẽ tính tại đây trước và chi phí, cái hàm chi phí hàm loss của mình sẽ được tính bằng công thức như",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ptwSPTt2XnM",
      "filename": "ptwSPTt2XnM",
      "title": "[CS431 - Chương 7] Part 2_2: Kiến trúc mạng RNN",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:42"
    }
  },
  {
    "page_content": "hàm loss của mình sẽ được tính bằng công thức như sau đó là hàm loss khi tại thời điểm thứ T theo theta thì nó sẽ là bằng công thức giống như công thức cross-entropy mà chúng ta đã học trước đây Và công thức của nó sẽ là tổng với j chạy từ 1 cho đến V trong đó V Lưu ý là trong cái bài này, trong cái ví dụ này thì V của mình là tập từ điển của mình nha Nó bị trùng một chút xíu, đây chính là tập từ điển của mình Tại vì sao? Tại vì cái y này nó sẽ là một cái vector Để cho biết là cái giá trị",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ptwSPTt2XnM",
      "filename": "ptwSPTt2XnM",
      "title": "[CS431 - Chương 7] Part 2_2: Kiến trúc mạng RNN",
      "chunk_id": 1,
      "start_timestamp": "0:00:36",
      "end_timestamp": "0:01:34"
    }
  },
  {
    "page_content": "sẽ là một cái vector Để cho biết là cái giá trị output của mình Nếu như đây là một cái bài toán đoán từ Thì đây sẽ là một cái vector có độ dài là số từ trong tập từ điển và chúng ta ký hiệu là trị tuyệt đối của V lưu ý cái V này là V từ điển nó không phải là cái ma trận V ở đây nó không phải là ma trận V ở đây và ở đây sẽ là số từ trong tập từ điển của mình rồi, nó sẽ tính trên từng cái phần tử j tại thời điểm thứ T rồi, nó sẽ là y_{t,j} nhân log của y_hat_{t,j} Và như vậy thì chúng ta có cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ptwSPTt2XnM",
      "filename": "ptwSPTt2XnM",
      "title": "[CS431 - Chương 7] Part 2_2: Kiến trúc mạng RNN",
      "chunk_id": 2,
      "start_timestamp": "0:01:30",
      "end_timestamp": "0:02:24"
    }
  },
  {
    "page_content": "của y_hat_{t,j} Và như vậy thì chúng ta có cái chuỗi với tất cả là T bước, đúng không? Cái chuỗi của chúng ta là x1 cho đến x_T Thì chúng ta sẽ phải tính tổng tất cả các cái sai số cho các cái time step t Vậy thì chúng ta sẽ có là loss tổng thể sẽ là bằng trung bình cộng Trung bình cộng của các cái loss thành phần trong đó cái loss thành phần thì nó sẽ có công thức là trừ của tổng y_{t,j} nhân log của y_hat_{t,j} và chúng ta sẽ tính trên tất cả các cái timestamp tính với t chạy từ 1 cho đến T",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ptwSPTt2XnM",
      "filename": "ptwSPTt2XnM",
      "title": "[CS431 - Chương 7] Part 2_2: Kiến trúc mạng RNN",
      "chunk_id": 3,
      "start_timestamp": "0:02:18",
      "end_timestamp": "0:03:12"
    }
  },
  {
    "page_content": "các cái timestamp tính với t chạy từ 1 cho đến T lớn Rồi, như vậy thì chúng ta đã thiết kế được cái hàm loss. Cách thức thiết kế hàm loss này cũng rất là đơn giản. Chúng ta sẽ sử dụng độ đo cross-entropy cho từng loss thành phần để tính ra được L_t. Và tổng tất cả L_t tính trung bình cộng lại, chúng ta sẽ có cái hàm loss trung bình. Thì đó là thiết kế cho cái hàm mất mát của việc dự đoán. Và sau đây thì chúng ta sẽ thể hiện một số tình huống sử dụng của mạng RNN Tình huống sử dụng nghĩa là sao?",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ptwSPTt2XnM",
      "filename": "ptwSPTt2XnM",
      "title": "[CS431 - Chương 7] Part 2_2: Kiến trúc mạng RNN",
      "chunk_id": 4,
      "start_timestamp": "0:03:04",
      "end_timestamp": "0:03:54"
    }
  },
  {
    "page_content": "của mạng RNN Tình huống sử dụng nghĩa là sao? Mạng RNN có thể áp dụng cho rất nhiều bài toán của NLP Ví dụ như trong tình huống đầu tiên đó là 1-to-1 Tức là đầu vào của mình sẽ có x1 và đầu ra của mình sẽ có y_1 thì ở đây chúng ta chỉ dự đoán trên một phần tử thôi thì ý nghĩa của nó có thể là cho cái bài toán là dịch dịch một cái từ nào đó chúng ta có thể là dịch từ đầu vào là một cái từ tiếng Anh và đầu ra sẽ là một cái từ tiếng Việt đối với bài toán one to many thì đầu vào của mình sẽ là một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ptwSPTt2XnM",
      "filename": "ptwSPTt2XnM",
      "title": "[CS431 - Chương 7] Part 2_2: Kiến trúc mạng RNN",
      "chunk_id": 5,
      "start_timestamp": "0:03:50",
      "end_timestamp": "0:04:37"
    }
  },
  {
    "page_content": "toán one to many thì đầu vào của mình sẽ là một từ và đầu ra của mình sẽ là nhiều từ thì ở đây, cái ngữ cảnh của mình nó có thể là mình cho đầu vào là một cái từ của một cái chủ đề ví dụ như mình có chủ đề là về biển và đầu ra của mình sẽ là một cái bài thơ một cái bài thơ về biển thì đây là một cái ngữ cảnh, một cái tình huống sử dụng của RNN cho cái dạng là OneToMany đối với cái dạng ManyToOne thì đầu vào của mình sẽ là rất nhiều từ Và đầu ra thì chúng ta chỉ có duy nhất một cái đầu ra thôi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ptwSPTt2XnM",
      "filename": "ptwSPTt2XnM",
      "title": "[CS431 - Chương 7] Part 2_2: Kiến trúc mạng RNN",
      "chunk_id": 6,
      "start_timestamp": "0:04:29",
      "end_timestamp": "0:05:14"
    }
  },
  {
    "page_content": "thì chúng ta chỉ có duy nhất một cái đầu ra thôi Và ngữ cảnh cho tình huống này đó là chúng ta có thể có một cái đoạn comment trên một cái mạng xã hội Và đầu ra của mình có thể là cho biết là đó là thể loại của mình là thể loại gì hoặc là cái cảm xúc của mình đó là positive, negative hay là neutral hoặc là có thể là cho cái bài toán spam detection đầu vào của mình sẽ là email, nội dung của một cái đoạn email và đầu ra thì cho biết đó là spam hay không phải là non spam thì đó là cho cái dạng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ptwSPTt2XnM",
      "filename": "ptwSPTt2XnM",
      "title": "[CS431 - Chương 7] Part 2_2: Kiến trúc mạng RNN",
      "chunk_id": 7,
      "start_timestamp": "0:05:09",
      "end_timestamp": "0:06:04"
    }
  },
  {
    "page_content": "hay không phải là non spam thì đó là cho cái dạng Many to Many, à xin lỗi cho Many to One Rồi, đối với cái Many to Many thì chúng ta sẽ có hai dạng. Dạng đầu tiên là Many to Many dạng 1 và Many to Many bên đây là Many to Many dạng 2. Thì Many to Many dạng 1, nó khác gì so với lại Many to Many dạng 2? Many to Many dạng 1 là chúng ta sẽ phải đọc xong hết toàn bộ cái nội dung này, rồi sau đó chúng ta mới đi ra, mới đưa ra cái phán đoán. Còn Many to Many dạng 2 là chúng ta đưa cái từ nào đến đâu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ptwSPTt2XnM",
      "filename": "ptwSPTt2XnM",
      "title": "[CS431 - Chương 7] Part 2_2: Kiến trúc mạng RNN",
      "chunk_id": 8,
      "start_timestamp": "0:05:52",
      "end_timestamp": "0:06:42"
    }
  },
  {
    "page_content": "to Many dạng 2 là chúng ta đưa cái từ nào đến đâu thì chúng ta sẽ tính ra cái output đến đó, đưa đến đâu, ra đến đó, đưa đến đâu, ra đến đó. Do đó, thì ở đây chúng ta sẽ có cái ngữ cảnh cho cái bài toán, cho cái dạng là Many to Many dạng 1, đó là bài toán dịch máy. Rõ ràng là chúng ta sẽ phải đọc hết toàn bộ cái nội dung của một cái đoạn văn, của một câu, xong rồi chúng ta mới có thể bắt đầu dịch được, đúng không? hoặc là bài toán tóm tắt văn bản đầu vào là chúng ta sẽ nhận một cái văn bản rất",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ptwSPTt2XnM",
      "filename": "ptwSPTt2XnM",
      "title": "[CS431 - Chương 7] Part 2_2: Kiến trúc mạng RNN",
      "chunk_id": 9,
      "start_timestamp": "0:06:40",
      "end_timestamp": "0:07:21"
    }
  },
  {
    "page_content": "đầu vào là chúng ta sẽ nhận một cái văn bản rất là dài và sau khi đọc xong hết thì chúng ta mới đưa ra cái bản tóm tắt thì đó là cho ứng dụng, ngữ cảnh ứng dụng cho cái Many to Many dạng 1 đối với cái Many to Many dạng 2 thì chúng ta sẽ đưa đến đâu? chúng ta đưa ra cái phán đoán đến đó thì ở đây nó có thể là cho cái bài toán là POS Tagging tức là đưa vô một cái từ Chúng ta sẽ cho biết từ đó là chủ ngữ Đưa vô một từ tiếp theo, đó sẽ là động từ Đưa vô cái từ tiếp theo, đó sẽ là vị ngữ Thì đây là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ptwSPTt2XnM",
      "filename": "ptwSPTt2XnM",
      "title": "[CS431 - Chương 7] Part 2_2: Kiến trúc mạng RNN",
      "chunk_id": 10,
      "start_timestamp": "0:07:15",
      "end_timestamp": "0:07:26"
    }
  },
  {
    "page_content": "vô cái từ tiếp theo, đó sẽ là vị ngữ Thì đây là một ngữ cảnh ứng dụng cho bài toán Many to Many v.2",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ptwSPTt2XnM",
      "filename": "ptwSPTt2XnM",
      "title": "[CS431 - Chương 7] Part 2_2: Kiến trúc mạng RNN",
      "chunk_id": 11,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Ôn tập - Phần 1. Bài 4. Các kiến trúc mạng CNN phổ biến Bài 4. Các kiến trúc mạng CNN phổ biến Thì phần đầu tiên chúng ta sẽ ôn tập lại kiến trúc mạng CNN Sau đó chúng ta sẽ đề cập đến một số kiến trúc mạng CNN phổ biến hiện nay Và các kiến trúc mạng này luôn là nền tảng Để cho các thuật toán cũng như là các bài toán trong thị giác máy tính Về sau họ sử dụng để phát triển tiếp Cuối cùng đó là chúng ta sẽ tìm hiểu các cách thức Để sử dụng một cái mạng huấn luyện sẵn (Pretrained Model) như thế nào",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PyC3pl_r8jw",
      "filename": "PyC3pl_r8jw",
      "title": "[CS431 - Chương 4] Part 1_1: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:42"
    }
  },
  {
    "page_content": "huấn luyện sẵn (Pretrained Model) như thế nào Về ôn tập mạng CNN thì chúng ta biết rằng Đầu vào của mạng CNN sẽ là tấm ảnh và ảnh này có thể là ảnh màu hoặc là ảnh xám Trong ví dụ này thì ảnh này có 3 kênh màu đó là Red, Green và Blue Và qua phép biến đổi convolution và ReLU ngay sau đó thì chúng ta sẽ tạo ra 1 tensor Và cái tensor này thì nó còn gọi là feature map Và sau đó thì chúng ta sẽ tiến hành thực hiện phép pooling để giảm chiều của dữ liệu này. Giả sử như tấm ảnh này, feature map này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PyC3pl_r8jw",
      "filename": "PyC3pl_r8jw",
      "title": "[CS431 - Chương 4] Part 1_1: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 1,
      "start_timestamp": "0:00:39",
      "end_timestamp": "0:01:32"
    }
  },
  {
    "page_content": "liệu này. Giả sử như tấm ảnh này, feature map này sẽ có kích thước đó là bề ngang là w, bề cao là h, Độ sâu, tức là số đặc trưng của mình là d Khi thực hiện phép pooling mặc định s của mình cho là bằng 2 Thì cái kích thước theo chiều ngang và chiều cao sẽ giảm một nửa Tức là bề ngang của mình lúc này sẽ còn là w chia 2 Và bề cao của mình trong trường hợp này sẽ giảm xuống còn là h chia 2 Nếu s bằng 3 thì cái này sẽ giảm 3 lần và chúng ta sẽ thực hiện phép pooling trên các kênh độc lập nhau, các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PyC3pl_r8jw",
      "filename": "PyC3pl_r8jw",
      "title": "[CS431 - Chương 4] Part 1_1: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 2,
      "start_timestamp": "0:01:21",
      "end_timestamp": "0:02:14"
    }
  },
  {
    "page_content": "hiện phép pooling trên các kênh độc lập nhau, các feature độc lập nhau do đó thì cái D này sẽ giữ nguyên chúng ta chỉ giảm cái bề ngang và bề cao của feature map thôi rồi tương tự như vậy cũng thực hiện với các phép convolution, ReLU và pooling thì đến cái bước cuối cùng thì chúng ta cũng sẽ ra được một cái tensor đó là feature Và feature này sẽ được duỗi ra để tạo ra thành một dạng vector Tại vì các phép biến đổi fully connected sau hay còn gọi là mạng Neural Network của mình thì nó chỉ có thể",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PyC3pl_r8jw",
      "filename": "PyC3pl_r8jw",
      "title": "[CS431 - Chương 4] Part 1_1: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 3,
      "start_timestamp": "0:02:09",
      "end_timestamp": "0:02:55"
    }
  },
  {
    "page_content": "là mạng Neural Network của mình thì nó chỉ có thể thực hiện được trên vector mà thôi Rồi, thì cái vector này khi chúng ta đi qua lớp biến đổi kết nối đầy đủ Cho đến lớp cuối cùng, chúng ta sẽ gặp lớp Softmax Mục tiêu của lớp Softmax này là chuyển đổi các vector về dạng phân bố xác suất Tức là với mỗi phần tử trong vector cuối cùng này, thì miền giá trị của nó là từ 0 cho đến 1 Và tổng tất cả các xác suất này, xác suất thuộc về lớp car, truck, van, bicycle tổng của nó sẽ là bằng một thì đây là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PyC3pl_r8jw",
      "filename": "PyC3pl_r8jw",
      "title": "[CS431 - Chương 4] Part 1_1: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 4,
      "start_timestamp": "0:02:49",
      "end_timestamp": "0:03:38"
    }
  },
  {
    "page_content": "bicycle tổng của nó sẽ là bằng một thì đây là công dụng của hàm Softmax thì chúng ta ôn lại về kiến trúc mạng CNN trong mạng CNN thì có một phép biến đổi nó có một phép biến đổi đó là phép convolution hay còn gọi là tích chập thì bản chất của phép convolution này đó là một phép biến đổi tuyến tính và nhiệm vụ của nó là để rút trích đặc trưng hình ảnh Ví dụ như chúng ta có 1 ảnh đầu vào ở đây và chúng ta nhân với lại 1 filter chúng ta nhân với 1 filter rồi filter này sẽ tạo ra 1 feature và cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PyC3pl_r8jw",
      "filename": "PyC3pl_r8jw",
      "title": "[CS431 - Chương 4] Part 1_1: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 5,
      "start_timestamp": "0:03:32",
      "end_timestamp": "0:04:29"
    }
  },
  {
    "page_content": "filter rồi filter này sẽ tạo ra 1 feature và cái feature này có cái ý nghĩa, có cái concept đó là các biên cạnh theo chiều dọc Các filter này giá trị của nó ban đầu là được khởi tạo ngẫu nhiên nhưng mà sau quá trình mà mạng CNN huấn luyện thì các giá trị tham số của filter này sẽ được mạng huấn luyện trên dữ liệu thật, được mạng CNN huấn luyện trên dữ liệu thật và nó sẽ tự động cập nhật và các trọng số của filter này sẽ cập nhật như thế nào để cho kết quả của việc nhận diện cuối cùng của mình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PyC3pl_r8jw",
      "filename": "PyC3pl_r8jw",
      "title": "[CS431 - Chương 4] Part 1_1: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 6,
      "start_timestamp": "0:04:19",
      "end_timestamp": "0:05:18"
    }
  },
  {
    "page_content": "cho kết quả của việc nhận diện cuối cùng của mình đạt được độ chính xác cao nhất Rồi, vừa rồi thì là phép biến đổi convolution, bây giờ chúng ta sẽ qua cái khái niệm gọi là các phép convolution Thì ở đây chúng ta sẽ có 1 animation đó là với input đầu vào qua nhiều filter thì chúng ta sẽ có nhiều feature và mỗi cái này sẽ gọi là 1 feature và tập hợp của các feature thì gọi là Feature Map là tập hợp của các feature thì ở đây chúng ta sẽ có một công thức để nhớ về kích thước của filter cũng như",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PyC3pl_r8jw",
      "filename": "PyC3pl_r8jw",
      "title": "[CS431 - Chương 4] Part 1_1: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 7,
      "start_timestamp": "0:05:05",
      "end_timestamp": "0:06:11"
    }
  },
  {
    "page_content": "thức để nhớ về kích thước của filter cũng như kích thước của tensor output nếu như đầu vào của mình, độ sâu này là có độ sâu là D thì filter của mình sẽ phải có độ sâu tương ứng D luôn để khi chúng ta lấy filter này nó trượt thì nó phải vừa khớp với input của mình và ở đây chúng ta sẽ có K filter và khi chúng ta thực hiện K filter này thì output của mình cũng sẽ có độ sâu tương ứng là K Chúng ta sẽ nhớ các cách thức để setup cho độ sâu của filter và output trong quá trình lập trình Vậy chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PyC3pl_r8jw",
      "filename": "PyC3pl_r8jw",
      "title": "[CS431 - Chương 4] Part 1_1: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 8,
      "start_timestamp": "0:06:03",
      "end_timestamp": "0:07:04"
    }
  },
  {
    "page_content": "và output trong quá trình lập trình Vậy chúng ta đã ôn qua một số kiến thức về những kiến trúc phổ biến Cấu tạo chung của một cái mạng CNN Bây giờ chúng ta sẽ đến với một số kiến trúc mạng CNN phổ biến Rồi, trên đây là sơ đồ về kết quả, về độ lỗi khi nhận diện hình ảnh. Thì ở đây là càng thấp là càng tốt. Và ở đây sẽ là các cột mốc về mặt thời gian. Thì ở đây chúng ta sẽ nói đến đầu tiên, đó là tập dữ liệu ImageNet. Đây là một trong những tập dữ liệu vô cùng lớn. ImageNet là viết tắt của chữ là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PyC3pl_r8jw",
      "filename": "PyC3pl_r8jw",
      "title": "[CS431 - Chương 4] Part 1_1: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 9,
      "start_timestamp": "0:07:00",
      "end_timestamp": "0:07:42"
    }
  },
  {
    "page_content": "liệu vô cùng lớn. ImageNet là viết tắt của chữ là ImageNet là Large Scale Visual Recognition Challenge tức là ImageNet được sử dụng cho cuộc thi là Large Scale Visual Recognition Challenge và cái scale cái kích thước của tập ImageNet này nó rất là rất là lớn đó bao gồm là 14 triệu bao gồm 14 triệu ảnh và tổng số lớp mà nó phải nhận diện đó là 20.000 lớp 20.000 lớp và cái cuộc thi này được tổ chức hàng năm từ năm 2010 trở về sau và hai bài toán chính mà nó thực hiện đó chính là bài toán phân",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PyC3pl_r8jw",
      "filename": "PyC3pl_r8jw",
      "title": "[CS431 - Chương 4] Part 1_1: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 10,
      "start_timestamp": "0:07:36",
      "end_timestamp": "0:08:18"
    }
  },
  {
    "page_content": "chính mà nó thực hiện đó chính là bài toán phân lớp, phân loại và bài toán phát hiện đối tượng thì ở đây chúng ta sẽ cùng điểm qua một số cái cột mốc của mạng CNN đầu tiên đó là cái cột mốc vào những năm 1990 tức là mạng CNN không phải có trong những năm 2010 trở lại đây mạng CNN nó có từ những năm 1990 tức là khoảng gần 30 năm rồi Và với những phiên bản đời đầu thì cho độ chính xác cũng chưa có được đủ tốt Mà cho đến khi năm 2012, với một số những cải tiến của AlexNet thì chúng ta sẽ thấy ra",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PyC3pl_r8jw",
      "filename": "PyC3pl_r8jw",
      "title": "[CS431 - Chương 4] Part 1_1: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 11,
      "start_timestamp": "0:08:12",
      "end_timestamp": "0:09:03"
    }
  },
  {
    "page_content": "cải tiến của AlexNet thì chúng ta sẽ thấy ra là có một sự bùng nổ của mạng học sâu Sau đây chúng ta sẽ lần lượt tìm hiểu qua một số kiến trúc mạng phổ biến, nổi tiếng Đầu tiên chúng ta cũng không nên quên nhắc lại về kiến trúc mạng LeNet Kiến trúc mạng LeNet thì một trong những phát kiến lớn nhất của nó chính là lớp tích chập tức là phép biến đổi convolution và lớp convolution là sự cải tiến của phép biến đổi fully connected Tức là cái phép kết nối đầy đủ Mạng LeNet thì nó sẽ cải tiến, nó không",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PyC3pl_r8jw",
      "filename": "PyC3pl_r8jw",
      "title": "[CS431 - Chương 4] Part 1_1: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 12,
      "start_timestamp": "0:08:58",
      "end_timestamp": "0:09:55"
    }
  },
  {
    "page_content": "đầy đủ Mạng LeNet thì nó sẽ cải tiến, nó không sử dụng cái Fully Connected nữa Mà nó sẽ sử dụng cái cơ chế đó là Locally Connected Và đồng thời là nó sẽ chia sẻ trọng số Nó sẽ chia sẻ trọng số Để chi, mục tiêu của nó đó chính là để làm giảm số lượng tham số của phép biến đổi của mình Và khi giảm số lượng tham số thì nó sẽ dẫn đến giúp chúng ta giảm được hiện tượng Overfitting Hiện tượng Overfitting có nghĩa là gì? Khi mô hình huấn luyện trên tập dữ liệu train, độ chính xác rất cao hoặc độ lỗi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PyC3pl_r8jw",
      "filename": "PyC3pl_r8jw",
      "title": "[CS431 - Chương 4] Part 1_1: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 13,
      "start_timestamp": "0:09:53",
      "end_timestamp": "0:10:37"
    }
  },
  {
    "page_content": "dữ liệu train, độ chính xác rất cao hoặc độ lỗi rất thấp nhưng khi chúng ta áp dụng trong thực tế, trên tập dữ liệu test, độ chính xác giảm rất đáng kể Đó là hiện tượng Overfitting Một trong những điểm nhấn khác của mạng CNN đó chính là lớp pooling, sử dụng average pooling Nhiệm vụ của average pooling này, như hồi nãy chúng ta có đề cập, đó là để giảm kích thước feature map của mình Ví dụ input feature map của mình là như thế này, sau khi thực hiện phép pooling, nó sẽ giảm xuống, còn khoảng một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PyC3pl_r8jw",
      "filename": "PyC3pl_r8jw",
      "title": "[CS431 - Chương 4] Part 1_1: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 14,
      "start_timestamp": "0:10:34",
      "end_timestamp": "0:11:25"
    }
  },
  {
    "page_content": "phép pooling, nó sẽ giảm xuống, còn khoảng một nửa Và lưu ý là giảm 1 nửa cho kích thước theo bề ngang và bề cao nhưng tổng số lượng các phần tử trong tensor này sẽ giảm 4 lần tại vì bề ngang mà giảm 2 lần, bề cao mà giảm 2 lần thì lúc đó là nhân lên, chúng ta sẽ ra là giảm đến 4 lần Và khi phép pooling này thực hiện cho đến bước cuối cùng Thực hiện cho đến bước cuối cùng, chúng ta sẽ có bước gọi là flatten để đưa vào mạng fully-connected ở phía sau Rõ ràng là khi kích thước của mình giảm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PyC3pl_r8jw",
      "filename": "PyC3pl_r8jw",
      "title": "[CS431 - Chương 4] Part 1_1: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 15,
      "start_timestamp": "0:11:11",
      "end_timestamp": "0:12:09"
    }
  },
  {
    "page_content": "phía sau Rõ ràng là khi kích thước của mình giảm xuống, khi kích thước của tensor, feature map giảm xuống thì khi chúng ta flatten ra, kích thước của vector này cũng sẽ giảm xuống Nếu màu đỏ tạo ra cái vector này, thì khi chúng ta dùng pooling, thì feature map sẽ giảm xuống một phần tư. Và khi giảm xuống một phần tư, thì các bạn sẽ thấy phép kết nối đầy đủ này, số lượng trọng số cũng sẽ giảm đi. Phép Pooling này sẽ có thêm một công dụng ngoài giảm kích thước của tensor, nó sẽ còn giảm số lượng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PyC3pl_r8jw",
      "filename": "PyC3pl_r8jw",
      "title": "[CS431 - Chương 4] Part 1_1: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 16,
      "start_timestamp": "0:12:06",
      "end_timestamp": "0:12:32"
    }
  },
  {
    "page_content": "kích thước của tensor, nó sẽ còn giảm số lượng tham số ở bước fully connected phía sau. Và đồng thời, việc này sẽ có hai công dụng. Công dụng đầu tiên là giảm hiện tượng overfitting. Công dụng thứ 2 là tăng tốc độ của quá trình và tính toán của mình lên. Rồi, nó sẽ có một cái thành phần nữa đó là Activation. Chúng ta trong phiên bản LeNet đời đầu vào những năm 1998, chúng ta sử dụng những hàm Activation kinh điển, đó là sigmoid và hàm tanh. Và đây là hình vẽ cho kiến trúc của LeNet thời điểm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PyC3pl_r8jw",
      "filename": "PyC3pl_r8jw",
      "title": "[CS431 - Chương 4] Part 1_1: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 17,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "đây là hình vẽ cho kiến trúc của LeNet thời điểm đó. Lưu ý là thời điểm đó người ta dùng từ khóa là Subsampling chúng ta hiểu đó chính là Pooling Đây chính là phép Pooling của mình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PyC3pl_r8jw",
      "filename": "PyC3pl_r8jw",
      "title": "[CS431 - Chương 4] Part 1_1: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 18,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề 3. Chúng ta sẽ bắt đầu vào một kiến trúc mạng rất nổi tiếng trong lĩnh vực học sâu, đó chính là mạng Convolutional Neural Network hay là mạng CNN. Thì ở phần đầu tiên, chúng ta sẽ giới thiệu qua về cái bài toán phân loại ảnh với cái mạng Neural Network. Tức là trong bài 2, chúng ta đã học và học đến cái bài về mạng học sâu đầu tiên, đó là mạng Neural Network. Tuy nhiên khi chúng ta áp dụng cái mạng này đối với một cái loại dữ liệu ảnh và cho một cái bài toán nó tương đối là phức tạp, thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:51"
    }
  },
  {
    "page_content": "một cái bài toán nó tương đối là phức tạp, thì điều gì sẽ xảy ra? Đầu tiên, chúng ta sẽ giới thiệu qua bài toán phân loại ảnh. Và ảnh ở đây sẽ có hai dạng. Loại đầu tiên là ảnh mức xám. Mỗi một pixel này sẽ biểu diễn bởi một giá trị màu. Và cái giá trị này thì thông thường sẽ biểu diễn bởi một con số. Từ 0 cho đến 255. Ví dụ như với ảnh Lena ở bên tay trái, thì cái dạng biểu diễn ở đây là mang tính chất minh hoạ thôi, thì nó sẽ mô tả bởi một cái ma trận, trong đó từng cái phần tử của ma trận nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 1,
      "start_timestamp": "0:00:49",
      "end_timestamp": "0:01:35"
    }
  },
  {
    "page_content": "ma trận, trong đó từng cái phần tử của ma trận nó sẽ nhận các cái giá trị từ 0 cho đến 255. Và ở đây chúng ta sẽ có các cái thông tin về bề ngang và bề cao, tương ứng là hai chiều không gian tấm ảnh. Đối với cái loại ảnh thứ hai đó là ảnh màu, và ảnh màu thì thông thường sẽ được biểu diễn bởi 3 kênh màu là red, green và blue, tương ứng là đỏ, xanh lá và xanh dương. Thì để tạo ra tấm ảnh màu này, chúng ta sẽ có 3 cái kênh Red, Green và Blue, và tương ứng từng cái kênh này, chúng ta sẽ có các cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 2,
      "start_timestamp": "0:01:31",
      "end_timestamp": "0:02:06"
    }
  },
  {
    "page_content": "ứng từng cái kênh này, chúng ta sẽ có các cái ma trận. Đây là ma trận biểu diễn cho kênh Red, đây là ma trận biểu diễn cho kênh Green, rồi là kênh màu xanh lá, và đây sẽ là ma trận biểu diễn cho kênh Blue, tức là màu xanh dương. Và 3 kênh màu này nó sẽ tương ứng với một cái thông số, đó gọi là độ sâu. Và toàn bộ ma trận này khi chúng ta ghép lại với nhau, thì nó sẽ được gọi là một cái tensor. Rồi bây giờ chúng ta sẽ tiến hành sử dụng cái mạng Neural Network để đi giải quyết cái bài toán đó là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 3,
      "start_timestamp": "0:02:04",
      "end_timestamp": "0:02:49"
    }
  },
  {
    "page_content": "Network để đi giải quyết cái bài toán đó là bài toán phân loại ảnh. Bài toán phân loại ảnh thì đầu vào của mình sẽ là một cái tấm ảnh. Và đầu ra mình sẽ có các cái nhãn tương ứng để cho biết cái loại đối tượng ở bên trong tấm ảnh này là gì. Thì cái loại đối tượng này nó sẽ có thể là cái nhãn xe cộ, nhà cửa và con người. Thế thì nếu như cái mạng Neural Network này mà nhận diện đúng thì nó sẽ phải trả ra cái nhãn đó là con người. Và điều gì sẽ xảy ra nếu như chúng ta sẽ thiết kế một cái mạng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 4,
      "start_timestamp": "0:02:42",
      "end_timestamp": "0:03:29"
    }
  },
  {
    "page_content": "xảy ra nếu như chúng ta sẽ thiết kế một cái mạng Neural Network với một cái kích thước gọi là tối thiểu? Tối thiểu này thể hiện ở tấm ảnh đầu vào của mình, kích thước rất lớn. Với những chuẩn ảnh hiện tại, chúng ta thấy là Full HD có thể lên trên 800-1000 pixel trong 1 chiều ngang hoặc chiều dọc. Nhưng ở đây chúng ta đang xét 1 tấm ảnh tối thiểu có kích thước 200x200. Và cái mạng này chỉ bao gồm duy nhất là 1 layer. Và với cái mạng này thì số node của cái mạng Neural Network này, chúng ta sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 5,
      "start_timestamp": "0:03:19",
      "end_timestamp": "0:04:20"
    }
  },
  {
    "page_content": "node của cái mạng Neural Network này, chúng ta sẽ cho đúng bằng số phần tử của ảnh đầu vào, thì 200 x 200 tương ứng là 40.000. Như vậy là layer duy nhất này sẽ có chứa 40 nghìn node. Và điều gì sẽ xảy ra với kiến trúc mạng tối thiểu này? Chúng ta sẽ xem xét tổng số trọng số của mạng tối thiểu này. Mỗi trọng số tương ứng là một cạnh nối từ điểm ảnh đầu vào đến một node đầu ra. Đầu ra, thì ở đây chúng ta sẽ có cái khái niệm gọi là fully connected, tức là kết nối đầy đủ. Mỗi một cái node đầu ra sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 6,
      "start_timestamp": "0:04:13",
      "end_timestamp": "0:05:02"
    }
  },
  {
    "page_content": "tức là kết nối đầy đủ. Mỗi một cái node đầu ra sẽ được kết nối đầy đủ với tất cả các điểm ảnh đầu vào. Thì số tham số trong trường hợp này sẽ là bao nhiêu? Do là kết nối đầy đủ nên chúng ta sẽ có số lượng tham số của cái tầng này là (200 x 200), tức là số điểm ảnh đầu vào, nhân với 40.000 đó chính là số node đầu ra. Và 40.000 đó chính là số node đầu ra. Như vậy (200 x 200) x 40.000 thì chúng ta có thể dùng máy tính để tính, nó sẽ ra là khoảng 1,6 tỷ tham số. Và với 1,6 tỷ tham số này thì chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 7,
      "start_timestamp": "0:04:56",
      "end_timestamp": "0:05:45"
    }
  },
  {
    "page_content": "tỷ tham số. Và với 1,6 tỷ tham số này thì chúng ta có kết luận là gì? Nó quá nhiều tham số. Thế thì khi số lượng tham số quá nhiều thì điều gì sẽ xảy ra? Khi số tham số của mình nhiều thì chúng ta sẽ bị hiện tượng nó gọi là overfitting. Nó sẽ bị hiện tượng overfitting. Overfitting nghĩa là sao? Khi mô hình của mình nó học, nó sẽ cố gắng bắt chước trên những mẫu dữ liệu mình đang có, nhưng mà không có tổng quát khi áp dụng lên trên những tập dữ liệu test thì độ chính xác cực kỳ thấp. Overfitting",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 8,
      "start_timestamp": "0:05:40",
      "end_timestamp": "0:06:46"
    }
  },
  {
    "page_content": "test thì độ chính xác cực kỳ thấp. Overfitting là tốt trên tập Train, nhưng rất là tệ trên tập Test. Thì điều này có thể minh họa, nó có thể lấy một ví dụ giống như trong giải hệ phương trình hồi xưa mình học. Nếu hệ phương trình của mình có 3 ẩn X, Y, Z, chúng ta cần bao nhiêu phương trình để giải được 3 cái ẩn này? Rõ ràng là nếu chúng ta chỉ có 2 hệ phương trình, (Ví dụ) 3x cộng cho 4y cộng cho 6z trừ 5 bằng 0, 7x trừ cho 6y cộng cho 3z cộng 1. Nếu như chỉ có 2 mẫu dữ liệu này, thì nó sẽ có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 9,
      "start_timestamp": "0:06:38",
      "end_timestamp": "0:07:28"
    }
  },
  {
    "page_content": "1. Nếu như chỉ có 2 mẫu dữ liệu này, thì nó sẽ có vô số nghiệm X, Y, Z. Và xác suất để tìm ra được một nghiệm cuối của kiến trúc mạng này là xác suất của nó sẽ là bằng một phần vô cùng. Tại vì ở đây chúng ta có vô số nghiệm, tức là xác suất là bằng 0. Do đó muốn mà tìm ra được các nghiệm X, Y, Z, tức là tìm ra cái bộ trọng số đúng cho cái kiến trúc mạng này thì chúng ta sẽ phải cần thêm ít nhất một phương trình nữa. Một phương trình nữa, thì cứ mỗi một phương trình thì tương ứng nó sẽ là một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 10,
      "start_timestamp": "0:07:19",
      "end_timestamp": "0:08:02"
    }
  },
  {
    "page_content": "mỗi một phương trình thì tương ứng nó sẽ là một cái mẫu dữ liệu. Một cái mẫu dữ liệu. Như vậy thì ở bên đây chúng ta có 1,6 tỷ tham số, tức là chúng ta sẽ cần đâu đó khoảng 1,6 tỷ mẫu dữ liệu. Cái mức độ nó tương đối là như vậy. Và các bạn tưởng tượng cái con số 1,6 tỷ này nó tương đương là dân số của Trung Quốc. Tức là với mỗi người Trung Quốc chúng ta sẽ phải yêu cầu họ đi tạo cho chúng ta một cái mẫu dữ liệu. Vì vậy đây là một con số vô cùng kinh khủng. Vì vậy với việc áp dụng mạng Neural",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 11,
      "start_timestamp": "0:07:56",
      "end_timestamp": "0:08:43"
    }
  },
  {
    "page_content": "kinh khủng. Vì vậy với việc áp dụng mạng Neural Network cho loại dữ liệu ảnh, với kiến trúc rất tối thiểu, thì chúng ta sẽ bị ngay vấn đề đó là quá nhiều tham số, và gây ra hiện tượng overfitting. Vì vậy bây giờ làm sao để có thể giảm được số lượng tham số này? Thì chúng ta sẽ có một cơ chế đầu tiên, đó là thay vì chúng ta fully connected, thì chúng ta sẽ chuyển sang là Locally Connected, nghĩa là sao? Mỗi một cái node của mạng Neural, thay vì chúng ta kết nối với tất cả các điểm ảnh của ảnh",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 12,
      "start_timestamp": "0:08:36",
      "end_timestamp": "0:09:18"
    }
  },
  {
    "page_content": "chúng ta kết nối với tất cả các điểm ảnh của ảnh đầu vào, thì bây giờ nó sẽ kết nối với một cái vùng cục bộ. Và cái vùng cục bộ này nó sẽ có cái kích thước mình lấy ví dụ như là 10 x 10, tức là bề ngang là 10 và bề cao là 10. Còn những cái điểm ảnh khác nó sẽ không kết nối, nó sẽ không có kết nối tới. Thì điểm ảnh sẽ tổng hợp thông tin trên một vùng cục bộ như thế này thôi. Vậy thì trong trường hợp này, khi mỗi 1 cái node sẽ được kết nối với 1 vùng có kích thước là 10 x 10, vậy thì hỏi tổng số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 13,
      "start_timestamp": "0:09:14",
      "end_timestamp": "0:09:55"
    }
  },
  {
    "page_content": "có kích thước là 10 x 10, vậy thì hỏi tổng số tham số trong trường hợp này sẽ là bao nhiêu? Và đáp số đó chính là chúng ta có 40.000 node đúng không? Chúng ta có 40.000 node và mỗi node kết nối vào vùng 10 x 10. Thế như vậy tổng số tham số của mình sẽ là nhân vô các con số này sẽ ra là 4 triệu tham số. Vậy thì từ 1,6 tỷ nó đã giảm xuống còn 4 triệu. Tức là chúng ta cảm nhận được sự sụt giảm rất là đáng kể. Nhưng mà 4 triệu tham số này thì liệu là nhiều hay ít? Thì chúng ta cũng hiểu là 4 triệu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 14,
      "start_timestamp": "0:09:53",
      "end_timestamp": "0:10:25"
    }
  },
  {
    "page_content": "nhiều hay ít? Thì chúng ta cũng hiểu là 4 triệu tham số, thì chúng ta sẽ cần đâu đó xấp xỉ khoảng 4 triệu mẫu đi. Thì cái 4 triệu này nó tương đương quy mô của một dân số của một thành phố. Ví dụ dân số thành phố Hồ Chí Minh, có thể là khoảng 4 triệu. Thì tính ra ra ngoài đường, chúng ta sẽ cứ mỗi người đi ngang qua, chúng ta sẽ nhờ họ làm một mẫu dữ liệu. Thì rõ ràng là 4 triệu nó vẫn còn là một con số rất là lớn. Nhưng tuy nhiên nó cũng đã giảm một cách đáng kể, so với lại cái phiên bản là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 15,
      "start_timestamp": "0:10:24",
      "end_timestamp": "0:11:08"
    }
  },
  {
    "page_content": "một cách đáng kể, so với lại cái phiên bản là fully connected rồi. Vậy thì bây giờ làm thế nào để có thể giảm thêm được số lượng tham số này? 4 triệu còn bao nhiêu? Chúng ta làm sao có thể giảm được? Cơ chế đó chính là chia sẻ tham số giữa các node. Nghĩa là sao? Cái node này và node này được biểu diễn bởi hai màu đen và màu đỏ. Node này biểu diễn bởi màu xanh lá, xanh dương, thì nó đang sử dụng các bộ trọng số khác nhau. Và bây giờ mình sẽ tạo một cơ chế đó là dùng chung. Chúng ta sẽ chia sẻ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 16,
      "start_timestamp": "0:11:05",
      "end_timestamp": "0:11:45"
    }
  },
  {
    "page_content": "một cơ chế đó là dùng chung. Chúng ta sẽ chia sẻ bộ trọng số này, nghĩa là bộ trọng số dùng cho cái node này cũng chính là bộ trọng số dùng cho cái node này. Nó gọi là Weight-sharing Locally Connected. Tham số được chia sẻ trên toàn bộ các vùng của ảnh cần biến đổi, nghĩa là trên cái vị trí này, nó sẽ dùng cái bộ tham số giống như tại đây, dùng với cùng một cái bộ tham số trên cái vùng tại đây, tức là đó sẽ có một cái bộ tham số trượt qua hết toàn bộ tấm hình. Và cứ mỗi một cái lần mà chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 17,
      "start_timestamp": "0:11:37",
      "end_timestamp": "0:12:21"
    }
  },
  {
    "page_content": "bộ tấm hình. Và cứ mỗi một cái lần mà chúng ta sẽ dừng ở đây, chúng ta sẽ trích rút thông tin và tạo ra giá trị cho cái node này. Và như vậy thì nhìn cái hình này chúng ta sẽ liên tưởng đến cái việc đó là, khi chúng ta thực hiện phép tổng hợp thông tin, thì nó sẽ tạo ra một tấm ảnh. Nó sẽ tạo ra một tấm ảnh khi chúng ta trượt một bộ tham số lên trên toàn bộ các vị trí ảnh. Trên đây chúng ta cũng sẽ trượt và điền các giá trị lên trên vùng ảnh output này. Đây chính là phép biến đổi convolution,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 18,
      "start_timestamp": "0:12:15",
      "end_timestamp": "0:13:09"
    }
  },
  {
    "page_content": "này. Đây chính là phép biến đổi convolution, một trong những phép biến đổi rất nổi tiếng trong lĩnh vực xử lý tín hiệu. Và phép biến đổi convolution này bản chất là một phép biến đổi tuyến tính. Chỉ là các thao tác nhân, sau đó cộng tổng hợp này thôi. Và ý nghĩa của phép biến đổi convolution này là nó trích rút đặc trưng hình ảnh. Ở đây chúng ta lấy ví dụ là một bộ lọc tên là Sobel, tên của một nhà khoa học. Thì các trọng số cho filter 1, 2, 1, 0, 0, 0, trừ 1, trừ 2, trừ 1. Ý nghĩa của tham số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 19,
      "start_timestamp": "0:13:05",
      "end_timestamp": "0:13:59"
    }
  },
  {
    "page_content": "0, 0, 0, trừ 1, trừ 2, trừ 1. Ý nghĩa của tham số là nó sẽ lấy tổng các pixel ở bên tay trái trừ tổng các pixel bên tay phải. Khi chúng ta đem filter này trượt trên toàn bộ tấm hình này, khi trượt đến đâu thì tương ứng chúng ta sẽ điền giá trị màu, và giá trị kết quả sau khi thực hiện điền lên đây. Và chúng ta quan sát kết quả thì chúng ta thấy là cái feature này, cái đặc trưng này nó có tính chất gì? Đặc trưng này nó có tính chất đó là nó thể hiện được những cái biên cạnh theo chiều dọc, những",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 20,
      "start_timestamp": "0:13:54",
      "end_timestamp": "0:13:59"
    }
  },
  {
    "page_content": "được những cái biên cạnh theo chiều dọc, những cái biên theo chiều dọc. Và các nhà khoa học Sobel họ nghĩ ra trọng số cho filter này, đúng không? Tuy nhiên thì mạng CNN sau này nó sẽ tự học và tự điền các giá trị trọng số cho các kernel này dựa trên, và được huấn luyện. Như vậy, trọng số này thay vì được gắn nhãn bởi kinh nghiệm của các nhà khoa học, thì trọng số này sẽ được tự động điền bằng cách huấn luyện với thuật toán Gradient Descent, thuật toán Backpropagation, nó dựa trên ý tưởng của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 21,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "toán Backpropagation, nó dựa trên ý tưởng của Gradient Descent.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=q3oZyk3l8EU",
      "filename": "q3oZyk3l8EU",
      "title": "[CS431 - Chương 3] Part 1: Giới thiệu mạng CNN",
      "chunk_id": 22,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề, học sâu, Machine Learning, ImageNet. Chủ đề, học sâu, Machine Learning, ImageNet. đó chính là vấn đề về Long Term Dependency Tức là một cái từ ở một vị trí cuối câu thì có khả năng phụ thuộc vào một cái từ ở vị trí đầu câu như vậy là nó có sự phụ thuộc rất là xa Và cái thứ hai, đó chính là vấn đề về Vanishing Gradient Đây là vấn đề kinh điển của lĩnh vực học sâu tại vì các mô hình học sâu như là RNN nói riêng cũng như là các mạng RNN khác nói chung thì các kiến trúc của mình sẽ bao gồm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qJj_LY1r91U",
      "filename": "qJj_LY1r91U",
      "title": "[CS431 - Chương 8] Part 1_1: Một số biến thể của RNN: LSTM",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:17"
    }
  },
  {
    "page_content": "nói chung thì các kiến trúc của mình sẽ bao gồm rất nhiều các thao tác biến đổi và do có rất nhiều các thao tác biến đổi như vậy sẽ dẫn đến là cái hàm của mình khi tính đạo hàm theo hàm hợp thì sẽ là bao gồm tích của các hàm hợp thành phần. Với mỗi hàm hợp thành phần, nếu như nhận các giá trị gradient nhỏ dần và có giá trị từ 0 cho đến 1, thì nó sẽ làm cho giá trị gradient của mình có xu hướng là thu hẹp lại và tiến về 0. Thì đây là cái vấn đề cốt yếu của Deep Learning. Nếu như không có những",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qJj_LY1r91U",
      "filename": "qJj_LY1r91U",
      "title": "[CS431 - Chương 8] Part 1_1: Một số biến thể của RNN: LSTM",
      "chunk_id": 1,
      "start_timestamp": "0:01:12",
      "end_timestamp": "0:01:59"
    }
  },
  {
    "page_content": "cốt yếu của Deep Learning. Nếu như không có những giải pháp để giải quyết, Thế thì các biến thể hôm nay mà chúng ta cùng tìm hiểu thì để giúp cho giải quyết cái vấn đề này Đầu tiên, đó là chúng ta sẽ ôn lại một số kiến thức cơ bản về mạng RNN Trong mạng RNN thì chúng ta sẽ tính toán 2 bước tại một thời điểm t Tại một thời điểm t thì chúng ta sẽ tính S_t đầu tiên S_t là trạng thái ẩn Và trạng thái ẩn này thì được tính từ cái giá trị quá khứ Và kết hợp với lại cái thông tin của hiện tại Sau khi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qJj_LY1r91U",
      "filename": "qJj_LY1r91U",
      "title": "[CS431 - Chương 8] Part 1_1: Một số biến thể của RNN: LSTM",
      "chunk_id": 2,
      "start_timestamp": "0:01:53",
      "end_timestamp": "0:02:45"
    }
  },
  {
    "page_content": "hợp với lại cái thông tin của hiện tại Sau khi đã tổng hợp được thông tin rồi, thì chúng ta sẽ tiến hành đưa ra cái giá trị dự đoán là y_t dựa trên cái công thức đó là softmax của W_o nhân với S_t Và một số tình huống sử dụng của mạng RNN bao gồm là tình huống One-to-one, tức là biến từ đầu vào và tạo ra một giá trị output One-to-many, tức là từ một đầu vào, chúng ta sẽ tạo ra một chuỗi output Lấy ví dụ như bài toán, tạo ra một bài thơ từ một chủ đề cho trước Many to One là đầu vào sẽ là một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qJj_LY1r91U",
      "filename": "qJj_LY1r91U",
      "title": "[CS431 - Chương 8] Part 1_1: Một số biến thể của RNN: LSTM",
      "chunk_id": 3,
      "start_timestamp": "0:02:33",
      "end_timestamp": "0:03:24"
    }
  },
  {
    "page_content": "chủ đề cho trước Many to One là đầu vào sẽ là một chuỗi và đầu ra sẽ là một giá trị. Ví dụ cho cái tình huống sử dụng này, đó là bài toán Sentiment Analysis hoặc là bài toán phân loại văn bản. Dạng Many to Many dạng 1, chúng ta phải đọc hết toàn bộ chuỗi rồi sau đó mới tính toán ra cái giá trị chuỗi Output. thì ví dụ minh họa cho Many to Many dạng 1 chính là bài toán dịch máy hoặc là bài toán tóm tắt văn bản. Many to Many dạng 2, thì đầu vào là chúng ta sẽ nhận vào từng từ và chúng ta sẽ đưa ra",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qJj_LY1r91U",
      "filename": "qJj_LY1r91U",
      "title": "[CS431 - Chương 8] Part 1_1: Một số biến thể của RNN: LSTM",
      "chunk_id": 4,
      "start_timestamp": "0:03:16",
      "end_timestamp": "0:04:10"
    }
  },
  {
    "page_content": "ta sẽ nhận vào từng từ và chúng ta sẽ đưa ra giá trị dự đoán ngay tại thời điểm đó. Ví dụ cho dạng Many to Many dạng 2 này chính là bài toán Part-of-Speech Tagging, tức là gán nhãn từ loại. Và nội dung của ngày hôm nay chúng ta sẽ bao gồm 3 phần chính. Phần đầu tiên đó chính là Long Short Term Memory, tức là đây là một trong những kiến trúc được sử dụng rất phổ biến cho đến vào giai đoạn là những năm 2016. LSTM có từ năm 1990 rồi, tức là nó có những năm 90 nhưng mà nó đã được sử dụng cho đến",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qJj_LY1r91U",
      "filename": "qJj_LY1r91U",
      "title": "[CS431 - Chương 8] Part 1_1: Một số biến thể của RNN: LSTM",
      "chunk_id": 5,
      "start_timestamp": "0:04:04",
      "end_timestamp": "0:05:01"
    }
  },
  {
    "page_content": "những năm 90 nhưng mà nó đã được sử dụng cho đến tận năm 2015-2016 cho đến khi có sự ra đời của Transformer và Attention. Trong phần thứ 2 thì chúng ta sẽ tìm hiểu về biến thể Bidirectional RNN, tức là RNN 2 chiều Và ở phần số 3, phần cuối cùng, chúng ta sẽ tìm hiểu về Stacked RNN Giới thiệu về LSTM, LSTM là một trong những biến thể của RNN bao gồm 4 thành phần chính Đầu tiên đó là thành phần về Context Cell Đúng như cái tên gọi của Context Cell, tức là cell để chứa thông tin về mặt ngữ cảnh Để",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qJj_LY1r91U",
      "filename": "qJj_LY1r91U",
      "title": "[CS431 - Chương 8] Part 1_1: Một số biến thể của RNN: LSTM",
      "chunk_id": 6,
      "start_timestamp": "0:04:52",
      "end_timestamp": "0:05:51"
    }
  },
  {
    "page_content": "tức là cell để chứa thông tin về mặt ngữ cảnh Để chứa thông tin về mặt ngữ cảnh của toàn bộ nội dung văn bản mà chúng ta đọc được Input Gate, tức là cổng input là nơi để cho chúng ta biết chúng ta sẽ nhận thông tin đó hay không chúng ta sẽ xử lý cái thông tin đó, đưa vào bên trong cái Context Cell này hay không Output Gate là để cho biết chúng ta có lấy cái thông tin đó và lấy cái thông tin từ cái Context Cell ra ngoài hay không và Forget Gate thì là cái cổng thông tin để cho chúng ta biết là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qJj_LY1r91U",
      "filename": "qJj_LY1r91U",
      "title": "[CS431 - Chương 8] Part 1_1: Một số biến thể của RNN: LSTM",
      "chunk_id": 7,
      "start_timestamp": "0:05:37",
      "end_timestamp": "0:06:34"
    }
  },
  {
    "page_content": "thì là cái cổng thông tin để cho chúng ta biết là có nên quên hết cái thông tin ở bên trong cái Context Cell này hay không Nếu chúng ta đưa hết thông tin vào bên trong và truyền đến cuối của văn bản, thì nó dẫn đến có rất nhiều thông tin thừa. Forget là nó sẽ giúp cho mình quên đi những thông tin không còn quan trọng nữa. Ba cổng này còn có một cách gọi khác. Đó chính là nó giúp cho chúng ta điều hướng luồng thông tin ra vào và ra khỏi Context Cell này Rồi, và mỗi cell trong mạng LSTM sẽ được",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qJj_LY1r91U",
      "filename": "qJj_LY1r91U",
      "title": "[CS431 - Chương 8] Part 1_1: Một số biến thể của RNN: LSTM",
      "chunk_id": 8,
      "start_timestamp": "0:06:26",
      "end_timestamp": "0:07:20"
    }
  },
  {
    "page_content": "Cell này Rồi, và mỗi cell trong mạng LSTM sẽ được xử lý tuần tự Nó cũng tương tự như cell của RNN, nó sẽ phải xử lý tuần tự Ở đây chúng ta sẽ ký hiệu là LSTM cell Bên trong LSTM cell này thì nó sẽ bao gồm 4 cái thành phần như đã nói LSTM rất thích hợp cho nhiệm vụ phân loại với dữ liệu tuần tự. LSTM chỉ là biến thể của RNN và phù hợp cho những dữ liệu giá trị sau phụ thuộc vào giá trị trước. LSTM cũng góp phần giải quyết vấn đề bộ nhớ ngắn hạn và Vanishing Gradient của RNN, chính nhờ cơ chế là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qJj_LY1r91U",
      "filename": "qJj_LY1r91U",
      "title": "[CS431 - Chương 8] Part 1_1: Một số biến thể của RNN: LSTM",
      "chunk_id": 9,
      "start_timestamp": "0:07:13",
      "end_timestamp": "0:08:01"
    }
  },
  {
    "page_content": "Vanishing Gradient của RNN, chính nhờ cơ chế là nhớ cái cần nhớ và quên cái cần quên nó sẽ giúp cho chúng ta tạo ra các gradient hiệu quả hơn Rồi, thì đối với cái mạng RNN truyền thống thì chúng ta sẽ thấy là cái hiện tượng mà rất hay mắc phải đó chính là hiện tượng Vanishing Gradient và thứ hai đó là chúng ta không nhớ được những cái thông tin đủ dài Tức là có những cái từ ở đầu câu nhưng mà đến cuối câu thì nó quên mất Thì cái việc mà một cái từ ở đầu câu mà đến cuối câu nó quên mất đó là vì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qJj_LY1r91U",
      "filename": "qJj_LY1r91U",
      "title": "[CS431 - Chương 8] Part 1_1: Một số biến thể của RNN: LSTM",
      "chunk_id": 10,
      "start_timestamp": "0:07:52",
      "end_timestamp": "0:08:35"
    }
  },
  {
    "page_content": "từ ở đầu câu mà đến cuối câu nó quên mất đó là vì trong cái quá trình mà thông tin nó truyền xuyên suốt Cái trạng thái ẩn (Hidden State), nó truyền xuyên suốt trạng thái S_t Thì thông tin nào nó cũng nạp vào, thông tin nào nó cũng nạp vào cái trạng thái ẩn này dẫn đến là những từ đầu sẽ bị pha loãng thông tin đi còn những từ ở giữa hoặc là từ gần cuối thì thông tin rất dày đặc và đầy đủ và đó là vì cái module hàm tanh này nè gặp bất cứ cái thông tin nào của x_t khi chúng ta đưa vào thì cũng đẩy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qJj_LY1r91U",
      "filename": "qJj_LY1r91U",
      "title": "[CS431 - Chương 8] Part 1_1: Một số biến thể của RNN: LSTM",
      "chunk_id": 11,
      "start_timestamp": "0:08:27",
      "end_timestamp": "0:08:42"
    }
  },
  {
    "page_content": "tin nào của x_t khi chúng ta đưa vào thì cũng đẩy vào bên trong trạng thái ẩn Tức là thông tin nào nó cũng sẽ sử dụng cái x_t này hết Nó không có cái tính chất gọi là chắt lọc thông tin",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qJj_LY1r91U",
      "filename": "qJj_LY1r91U",
      "title": "[CS431 - Chương 8] Part 1_1: Một số biến thể của RNN: LSTM",
      "chunk_id": 12,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề, học sâu, Machine Learning, ImageNet. số cột của cái ma trận A này nó phải đúng bằng với số phần tử của vector x được nhân khi đó thì tích Y bằng Ax là nó sẽ là một vector gồm có m chiều thì tại sao lại như vậy? thế là chúng ta sẽ giải thích kỹ hơn A nhân với x thì chúng ta sẽ thực hiện quy tắc đó là nhân theo lần lượt là hàng và cột giống như trong phần nhân vector với vector, thì nếu như A của mình là gồm tập hợp m hàng và như vậy thì mỗi một cái phần tử trên mỗi hàng nó sẽ gồm có n cột",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qPQT04u0kCs",
      "filename": "qPQT04u0kCs",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.4: Ôn tập nền tảng đại số tuyến tính (Part 4)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:13"
    }
  },
  {
    "page_content": "một cái phần tử trên mỗi hàng nó sẽ gồm có n cột ví dụ như ở đây a_1, a_2 và a_m thì đều có là n cột và chúng ta sẽ thực hiện phép nhân theo hàng với lại x là một vector theo cột thì a_2 nhân với x thì chúng ta sẽ ra được giá trị scalar là a_2x đây sẽ là giá trị scalar và chúng ta thực hiện m phép nhân này thì kết quả của mình sẽ có m giá trị scalar Và như vậy tập hợp tất cả các giá trị scalar này thì nó sẽ là 1 vector Và vector này gồm có m phần tử Như vậy thì ở đây lưu ý này của chúng ta chỉ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qPQT04u0kCs",
      "filename": "qPQT04u0kCs",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.4: Ôn tập nền tảng đại số tuyến tính (Part 4)",
      "chunk_id": 1,
      "start_timestamp": "0:01:06",
      "end_timestamp": "0:02:11"
    }
  },
  {
    "page_content": "tử Như vậy thì ở đây lưu ý này của chúng ta chỉ là nhắc lại những ý mình đã nói trên Kết quả của một phép tính nhân giữa ma trận và vector thì nó sẽ là 1 vector Và cái kích thước của phép nhân này chính là 1 vector m chiều ở đây thì chúng ta thấy, cái vector mình có thể xem nó như là một cái ma trận, tức là n nhân 1 thì nhân một cái ma trận là m nhân n với một cái vector n nhân 1 thì nó sẽ ra một cái vector m nhân 1 tức là một cái vector gồm có m phần tử Và đây là một ví dụ để minh họa cho phép",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qPQT04u0kCs",
      "filename": "qPQT04u0kCs",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.4: Ôn tập nền tảng đại số tuyến tính (Part 4)",
      "chunk_id": 2,
      "start_timestamp": "0:02:04",
      "end_timestamp": "0:02:56"
    }
  },
  {
    "page_content": "phần tử Và đây là một ví dụ để minh họa cho phép nhân ma trận và vector này Nếu như chúng ta xem các thành phần dinh dưỡng vitamin A, C và chất xơ của các loại rau củ đó là cà rốt, bắp cải, dưa leo là cái ma trận được đánh khung ở đây thì khi đó giả sử x, y, z là số kg cà rốt bắp cải dưa leo mà mình mua được trong lần đi chợ này thì khi đó lượng vitamin A, C và chất xơ của bữa ăn ngày hôm đó nó chính là kết quả của phép nhân ma trận thì ở đây chúng ta thấy cũng giữ nguyên tắc nhân theo hàng và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qPQT04u0kCs",
      "filename": "qPQT04u0kCs",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.4: Ôn tập nền tảng đại số tuyến tính (Part 4)",
      "chunk_id": 3,
      "start_timestamp": "0:02:46",
      "end_timestamp": "0:03:45"
    }
  },
  {
    "page_content": "ta thấy cũng giữ nguyên tắc nhân theo hàng và cột thì cái hàng này tương ứng nó sẽ ra một cái phần tử đó là lượng vitamin A vitamin A rồi cái hàng thứ hai nhân với lại cái vector x, y, z thì nó sẽ ra cái lượng là vitamin C và cái hàng thứ ba nhân với lại cái x, y, z thì mình sẽ ra cái lượng là chất xơ Giá trị của mình sẽ là hàm lượng vitamin A, C và chất xơ khi mình đi chợ trong ngày hôm đó Một ví dụ khác là chúng ta nhớ lại giải hệ phương trình tuyến tính hồi cấp 3 thì ở đây chúng ta sẽ có hệ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qPQT04u0kCs",
      "filename": "qPQT04u0kCs",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.4: Ôn tập nền tảng đại số tuyến tính (Part 4)",
      "chunk_id": 4,
      "start_timestamp": "0:03:37",
      "end_timestamp": "0:04:30"
    }
  },
  {
    "page_content": "tuyến tính hồi cấp 3 thì ở đây chúng ta sẽ có hệ phương trình ví dụ như là a_1x, b_1y, c_1z bằng d_1 thì chúng ta có 3 cái hệ phương trình như thế này thì hệ phương trình này có thể viết gọn lại thành, thực ra cũng không phải là gọn nhưng mà có thể biểu diễn nó dưới dạng là tích của ma trận các hệ số với một cái vector này, là vector này chính là biến số mà cần giải Và đây chính là cái vector kết quả Như vậy thì cái hệ phương trình có thể biểu diễn dưới dạng một cái phép nhân ma trận với vector",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qPQT04u0kCs",
      "filename": "qPQT04u0kCs",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.4: Ôn tập nền tảng đại số tuyến tính (Part 4)",
      "chunk_id": 5,
      "start_timestamp": "0:04:22",
      "end_timestamp": "0:05:13"
    }
  },
  {
    "page_content": "dưới dạng một cái phép nhân ma trận với vector Tiếp theo thì chúng ta sẽ tìm hiểu về phép nhân ma trận với ma trận Mở rộng của ma trận với vector thì chúng ta sẽ có phép nhân ma trận với ma trận Và ở đây thì chúng ta cũng sẽ thực hiện cái nguyên tắc tương tự như vậy đó là chúng ta sẽ nhân theo hàng và theo cột, hàng và cột thì khi đó chúng ta sẽ có cái kết quả là a_2 b_1 thì nó sẽ được tạo bởi cái hàng thứ 2 và cột thứ 1 của cái ma trận thì chúng ta lưu ý ở đây nó sẽ có cái khái niệm gọi là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qPQT04u0kCs",
      "filename": "qPQT04u0kCs",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.4: Ôn tập nền tảng đại số tuyến tính (Part 4)",
      "chunk_id": 6,
      "start_timestamp": "0:05:07",
      "end_timestamp": "0:05:59"
    }
  },
  {
    "page_content": "ta lưu ý ở đây nó sẽ có cái khái niệm gọi là chuyển vị chuyển vị thì mục tiêu đó là gì? là để biểu diễn dưới dạng hàng Và đây là một số cái thao tác, một số cái tính chất của phép nhân, ví dụ như có tính chất giao hoán, tính chất kết hợp Rồi, thì ứng dụng của phép nhân ma trận này đó chính là dùng trong các biến đổi hình học Lấy ví dụ như để thực hiện cái phép Translate, tức là cái phép tịnh tiến, thì chúng ta sẽ lấy một cái điểm gốc, chúng ta nhân với lại cái ma trận Translate này, thì cái ma",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qPQT04u0kCs",
      "filename": "qPQT04u0kCs",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.4: Ôn tập nền tảng đại số tuyến tính (Part 4)",
      "chunk_id": 7,
      "start_timestamp": "0:05:49",
      "end_timestamp": "0:06:39"
    }
  },
  {
    "page_content": "với lại cái ma trận Translate này, thì cái ma trận Translate nó sẽ là một cái ma trận kích thước là 3x3 và nếu như tập hợp các cái điểm đầu vào thì chúng ta sẽ tạo thành một cái hình ảnh, ví dụ ở đây có cái hình là chữ F, Chữ F này, khi nhân với cái ma trận tịnh tiến, thì nó sẽ tạo thành một hình ảnh kết quả. Thì các hệ số cho cái ma trận tịnh tiến này sẽ là 1 0 x, 0 1 y, 0 0 1. x, y này chính là cái vector tịnh tiến của mình. Tương tự như vậy, cho cái thao tác là Scale, tức là tỷ lệ xung quanh",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qPQT04u0kCs",
      "filename": "qPQT04u0kCs",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.4: Ôn tập nền tảng đại số tuyến tính (Part 4)",
      "chunk_id": 8,
      "start_timestamp": "0:06:32",
      "end_timestamp": "0:07:18"
    }
  },
  {
    "page_content": "cái thao tác là Scale, tức là tỷ lệ xung quanh gốc tọa độ Origin và phép xoay. Nếu như thực hiện phép Scale, thì chúng ta muốn biến đổi cái ảnh của mình Thay đổi cái kích thước theo cái trục bề ngang và bề cao Giả sử như cái hệ số tỷ lệ của mình là W và H Thì khi đó đây chính là cái ma trận mà được sử dụng để biến đổi Và cái ảnh gốc của mình, ảnh gốc của mình nhân với cái ma trận này thì sẽ ra cái ảnh đã được Scale Tương tự như vậy thì chúng ta sẽ có ảnh gốc nhân với ma trận Rotate, tức là ma",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qPQT04u0kCs",
      "filename": "qPQT04u0kCs",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.4: Ôn tập nền tảng đại số tuyến tính (Part 4)",
      "chunk_id": 9,
      "start_timestamp": "0:07:12",
      "end_timestamp": "0:08:08"
    }
  },
  {
    "page_content": "sẽ có ảnh gốc nhân với ma trận Rotate, tức là ma trận xoay với một góc là theta độ thì hệ số của ma trận này sẽ là cos theta trừ sin theta 0, sin theta cos theta 0, 0 0 1 thì nhân cái ma trận này với cái ảnh gốc thì nó sẽ tạo ra một cái hình ảnh được xoay một cái góc là theta độ Sau đây sẽ là một số cái ma trận đặc biệt trong đại số tuyến tính Đầu tiên đó là ma trận đơn vị thì ma trận đơn vị nó sẽ là một cái ma trận vuông ma trận vuông là gì? là ma trận có số hàng và số cột bằng nhau và tất cả",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qPQT04u0kCs",
      "filename": "qPQT04u0kCs",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.4: Ôn tập nền tảng đại số tuyến tính (Part 4)",
      "chunk_id": 10,
      "start_timestamp": "0:08:05",
      "end_timestamp": "0:08:50"
    }
  },
  {
    "page_content": "ma trận có số hàng và số cột bằng nhau và tất cả các phần tử trên đường chéo chính này nè nó đều là bằng một và tất cả những phần tử nằm ngoài đường chéo chính này bằng 0 thì đây chính là ma trận đơn vị và ma trận đơn vị thì nó có tính chất đó là ma trận A mà nhân với I thì I là ma trận đơn vị thì nó sẽ bằng chính ma trận A thì trong Python nó sẽ có một cái hàm của NumPy đó là hàm `eye`, là để khởi tạo ra một cái ma trận đơn vị Ma trận tiếp theo, đó chính là ma trận nghịch đảo thì ma trận",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qPQT04u0kCs",
      "filename": "qPQT04u0kCs",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.4: Ôn tập nền tảng đại số tuyến tính (Part 4)",
      "chunk_id": 11,
      "start_timestamp": "00:08:46",
      "end_timestamp": "0:09:31"
    }
  },
  {
    "page_content": "theo, đó chính là ma trận nghịch đảo thì ma trận nghịch đảo là một cái ma trận vuông, ký hiệu là n nhân n thì ký hiệu là một cái ma trận A mũ trừ 1, đây là ma trận nghịch đảo Và ma trận này cũng sẽ là một ma trận vuông và ma trận nghịch đảo có tính chất đó là A mũ trừ 1 Nhân với lại cái ma trận gốc là A thì nó sẽ là bằng ma trận đơn vị Thì đây là một cái loại ma trận đặc biệt và tuy nhiên cái ma trận nghịch đảo này không phải lúc nào nó cũng tồn tại Hãy subscribe cho kênh Ghiền Mì Gõ để không",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qPQT04u0kCs",
      "filename": "qPQT04u0kCs",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.4: Ôn tập nền tảng đại số tuyến tính (Part 4)",
      "chunk_id": 12,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "tại Hãy subscribe cho kênh Ghiền Mì Gõ để không bỏ lỡ những video hấp dẫn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qPQT04u0kCs",
      "filename": "qPQT04u0kCs",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.3.4: Ôn tập nền tảng đại số tuyến tính (Part 4)",
      "chunk_id": 13,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Trong phần tiếp theo, chúng ta sẽ cùng tìm hiểu về cơ chế attention để giúp cho chúng ta giải quyết một số vấn đề của mạng RNN trong bài toán dịch máy, nói riêng, và trong các bài toán của NLP nói chung. Thì đầu tiên chúng ta sẽ cùng xem lại kiến trúc sequence to sequence và chúng ta xem coi vấn đề của nó đang mắc phải hiện giờ đó là gì. Tại cái node cuối cùng của quá trình encoder, chúng ta thấy là toàn bộ nội dung của câu văn nguồn đã dồn vào cái vector này Và như vậy thì nó sẽ gây ra cái điểm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ROIgZ5tyDFo",
      "filename": "ROIgZ5tyDFo",
      "title": "[CS431 - Chương 9] Part 2_1: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:47"
    }
  },
  {
    "page_content": "vector này Và như vậy thì nó sẽ gây ra cái điểm nghẽn Nó giống như là chúng ta hình dung cái phễu của mình vậy đó Nó hình dung nó giống như là cái phễu thông tin thì toàn bộ nội dung chúng ta đưa vào đây và ở đây thì nó sẽ bị dồn vào cái miệng phễu nó gọi là bottleneck nó sẽ bị dồn vào thế thì ở đây cũng vậy toàn bộ thông tin của từ I, từ Am, từ Not, từ Sure và các từ Am, Not, Sure dồn hết vào đây thì nó sẽ gây ra hiện tượng điểm nghẽn thế thì đó là về mặt hình tượng còn về mặt ý nghĩa thực sự",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ROIgZ5tyDFo",
      "filename": "ROIgZ5tyDFo",
      "title": "[CS431 - Chương 9] Part 2_1: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 1,
      "start_timestamp": "0:00:45",
      "end_timestamp": "0:01:22"
    }
  },
  {
    "page_content": "là về mặt hình tượng còn về mặt ý nghĩa thực sự của cái điểm nghẽn đó là gì đó là khi chúng ta xử lý đến cái từ Sure Cho dù chúng ta có sử dụng kiến trúc, chúng ta có sử dụng các biến thể như LSTM, GRU, Bidirectional RNNs, thì nó đều không thể giải quyết được vấn đề cố hữu, đó chính là vấn đề về thông tin bị mất, bị phai, hoặc khi lan truyền theo chiều tuần tự này. khi chúng ta lan truyền tuần tự thì cái thông tin của những cái từ đầu tiên nó đã bị mất thông tin, nó bị phai thông tin nhiều và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ROIgZ5tyDFo",
      "filename": "ROIgZ5tyDFo",
      "title": "[CS431 - Chương 9] Part 2_1: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 2,
      "start_timestamp": "0:01:19",
      "end_timestamp": "0:02:07"
    }
  },
  {
    "page_content": "bị mất thông tin, nó bị phai thông tin nhiều và dẫn đến đó là khi chúng ta lan truyền được đến cái từ cuối cùng ở đây, đến cái từ cuối cùng ở đây để tính ra được cái giá trị ở đây thì cái thông tin của cái từ Sure, ví dụ ở đây là cái thông tin của từ Sure là cần thiết để mà đưa ra được cái dự đoán đúng không? nó đã bị quên do nó đã bị biến đổi quá nhiều từ Sure đến đây là bị biến đổi 1 lần, 2 lần, 3 lần, 4 lần, 5 lần đến đây thì nó đã bị biến đổi hết 5 lần qua 5 lần biến đổi đó thì hàm lượng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ROIgZ5tyDFo",
      "filename": "ROIgZ5tyDFo",
      "title": "[CS431 - Chương 9] Part 2_1: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 3,
      "start_timestamp": "0:01:57",
      "end_timestamp": "0:02:45"
    }
  },
  {
    "page_content": "đổi hết 5 lần qua 5 lần biến đổi đó thì hàm lượng thông tin nó bị loãng đi đó chính là vấn đề thực sự của sequence to sequence và giải pháp làm sao có thể giải quyết vấn đề này thì chúng ta sẽ sử dụng cơ chế là Attention với cơ chế Attention thì cách thức làm của chúng ta sẽ là như sau Đầu tiên, khi chúng ta bắt đầu quá trình decode thì chúng ta sẽ đi tính score của trạng thái ở đây và đi tính với lại tất cả các cái score trạng thái ẩn của câu input của mình thì ở đây nó gọi là Attention Score",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ROIgZ5tyDFo",
      "filename": "ROIgZ5tyDFo",
      "title": "[CS431 - Chương 9] Part 2_1: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 4,
      "start_timestamp": "0:02:42",
      "end_timestamp": "0:03:40"
    }
  },
  {
    "page_content": "của mình thì ở đây nó gọi là Attention Score Mục tiêu của nó là gì? Mục tiêu của việc tính Attention Score này là tại thời điểm tôi bắt đầu quá trình decode ra đây thì tôi sẽ để tâm từ Attention tiếng Anh Khi dịch ra tiếng Việt, mình có thể dùng từ nôm na đó là để tâm Tôi sẽ để tâm đến từ nào trong 4 từ ở đây khi tôi bắt đầu dịch tại vị trí này Để tính được sự để tâm đó, chúng ta sẽ dùng công thức tính độ tương đồng có thể dùng độ đo tích vô hướng và các giá trị Scalar, các giá trị ở đây, nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ROIgZ5tyDFo",
      "filename": "ROIgZ5tyDFo",
      "title": "[CS431 - Chương 9] Part 2_1: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 5,
      "start_timestamp": "0:03:36",
      "end_timestamp": "0:04:24"
    }
  },
  {
    "page_content": "và các giá trị Scalar, các giá trị ở đây, nó thể hiện cho sự tương đồng đó tuy nhiên các giá trị tương đồng này nếu như chúng ta sử dụng độ đo tích vô hướng thì nó sẽ chưa có được chuẩn hóa về một cái không gian xác suất do đó thì chúng ta sẽ tiến hành cái bước tiếp theo đó là tính Attention Distribution Attention Distribution là nó sẽ quy chiếu về một cái không gian có cái giá trị là từ 0 cho đến 1 để normalize để chuẩn hóa nó lại và đưa về cái không gian phân bố xác suất Với cái Distribution",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ROIgZ5tyDFo",
      "filename": "ROIgZ5tyDFo",
      "title": "[CS431 - Chương 9] Part 2_1: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 6,
      "start_timestamp": "0:04:20",
      "end_timestamp": "0:05:09"
    }
  },
  {
    "page_content": "không gian phân bố xác suất Với cái Distribution này, chúng ta thấy rằng cái cột này sẽ cao hơn hẳn so với các cái cột này Thì điều đó có nghĩa là gì? khi chúng ta bắt đầu quá trình decode thì tại thời điểm này nó sẽ bắt đầu để ý từ này chúng ta sẽ chuyển sang các ví dụ khác đó là để ý đến từ I nhiều hơn sau những từ còn lại để ý đến từ I nhiều hơn và khi đó thì chúng ta sẽ biết rằng là toàn bộ thông tin của cái S1 này nè Nó nên được tổng hợp nhiều nhất để mà đưa ra cái phán đoán đưa ra cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ROIgZ5tyDFo",
      "filename": "ROIgZ5tyDFo",
      "title": "[CS431 - Chương 9] Part 2_1: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 7,
      "start_timestamp": "0:04:59",
      "end_timestamp": "0:05:51"
    }
  },
  {
    "page_content": "nhiều nhất để mà đưa ra cái phán đoán đưa ra cái phán đoán tiếp theo đưa ra cái phán đoán của cái quá trình mình dịch Thay vì là chúng ta đưa thông tin của cái từ Sure Thay vì chúng ta đưa thông tin của từ Sure thì chúng ta nên đưa thông tin của cái từ I Nó sẽ giúp chúng ta dịch ở chỗ này chính xác hơn Trong khi đó, với cái phiên bản cũ thì cái thông tin của từ Sure ở đây là nhiều nhất Đúng không? Thông tin của từ Sure nhiều nhất và đưa ra đến đây Thì cái việc dự đoán tiếp theo nó sẽ bị ảnh",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ROIgZ5tyDFo",
      "filename": "ROIgZ5tyDFo",
      "title": "[CS431 - Chương 9] Part 2_1: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 8,
      "start_timestamp": "0:05:45",
      "end_timestamp": "0:06:28"
    }
  },
  {
    "page_content": "đây Thì cái việc dự đoán tiếp theo nó sẽ bị ảnh hưởng bởi từ Sure nhiều hơn là cái từ I Và khi chúng ta đã tính được cái Attention Distribution này rồi Chúng ta biết là chúng ta cần phải quan tâm chúng ta phải để ý đến cái từ I nhiều hơn rồi thì chúng ta sẽ đến giai đoạn đó là tổng hợp thông tin tổng hợp thông tin cái vector này là tổng có trọng số của các s1, s2, s3 cho đến s4 này theo trọng số, theo tỷ trọng đã được tính toán ở Attention Distribution và Attention tổng hợp các thông tin đó nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ROIgZ5tyDFo",
      "filename": "ROIgZ5tyDFo",
      "title": "[CS431 - Chương 9] Part 2_1: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 9,
      "start_timestamp": "0:06:22",
      "end_timestamp": "0:07:10"
    }
  },
  {
    "page_content": "và Attention tổng hợp các thông tin đó nó gọi là Attention Output và Attention Output thì sử dụng Attention Distribution để cộng có trọng số các vector đầu vào này, cộng có trọng số các vector trạng thái ẩn này và Attention Output sẽ liên quan đến từ mà chúng ta cần phải để ý tại quá trình dịch tại đây và nó sẽ loại bỏ được những thông tin thừa nó sẽ loại bỏ được những thông tin thừa Thì những thông tin dư thừa thì nó sẽ có Attention Score thấp hoặc Attention Distribution thấp Ví dụ đây là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ROIgZ5tyDFo",
      "filename": "ROIgZ5tyDFo",
      "title": "[CS431 - Chương 9] Part 2_1: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 10,
      "start_timestamp": "0:07:02",
      "end_timestamp": "0:07:55"
    }
  },
  {
    "page_content": "hoặc Attention Distribution thấp Ví dụ đây là thông tin thừa nên chiều cao của nó thấp Còn những thông tin của những từ nào có liên quan nhiều thì nó sẽ là cao Ví dụ đây là một cái minh họa cho chuyện đấy và khi chúng ta tổng hợp được cái thông tin của cái Attention Output này phối hợp với lại cái thông tin của cái trạng thái ẩn tại đây thì chúng ta sẽ có đầy đủ thông tin hơn chúng ta sẽ có đầy đủ thông tin quan trọng để giúp cho cái việc đưa ra cái dự đoán là y mũ 1 rồi tương tự như vậy chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ROIgZ5tyDFo",
      "filename": "ROIgZ5tyDFo",
      "title": "[CS431 - Chương 9] Part 2_1: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 11,
      "start_timestamp": "0:07:49",
      "end_timestamp": "0:08:38"
    }
  },
  {
    "page_content": "cái dự đoán là y mũ 1 rồi tương tự như vậy chúng ta sẽ đến cái từ thứ 2 và chúng ta cũng lấy cái vector ẩn trong quá trình decode ở đây đi tính dot product tích vô hướng với các vector ẩn của encoder rồi sau đó chúng ta sẽ ra được các score các score này chưa được chuẩn hóa do đó chúng ta sẽ dùng hàm chuẩn hóa và chút nữa chúng ta sẽ nói rõ hơn là công thức chuẩn hóa như thế nào chúng ta sẽ chuẩn hóa nó về cái không gian xác suất như thế này và ở đây thì đó cho thấy là cái từ Not cái từ Not này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ROIgZ5tyDFo",
      "filename": "ROIgZ5tyDFo",
      "title": "[CS431 - Chương 9] Part 2_1: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 12,
      "start_timestamp": "0:08:36",
      "end_timestamp": "0:09:16"
    }
  },
  {
    "page_content": "đây thì đó cho thấy là cái từ Not cái từ Not này là chúng ta sẽ để ý nhiều nhất cái từ Not này sẽ để ý nhiều nhất còn cái từ Am thì nó sẽ để ý ít hơn còn hai cái từ I và Sure tương ứng ở đây thì nó sẽ để ý ít nhất thì trong tiếng Pháp cái Not này là phủ định thì ở trong tiếng Pháp tương ứng nó sẽ là hai cái từ là ne... pas ne, cái gì đấy, là pas đó là Not trong tiếng Anh thì cái từ ne này là nó sẽ chú ý đến nó sẽ để ý đến hai cái từ Am và từ Not nó không có chú ý đến từ I và từ Sure rồi tương",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ROIgZ5tyDFo",
      "filename": "ROIgZ5tyDFo",
      "title": "[CS431 - Chương 9] Part 2_1: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 13,
      "start_timestamp": "0:09:12",
      "end_timestamp": "0:09:55"
    }
  },
  {
    "page_content": "nó không có chú ý đến từ I và từ Sure rồi tương tự như vậy, đến cái từ tiếp theo nó cần phải dự đoán thì sau khi chúng ta đưa vào cái từ ne thì cái từ tiếp theo chúng ta cần phải dự đoán nó để ý đến từ thứ 2 nhiều hơn tức là từ Am nhiều hơn thể hiện qua chiều cao này và trong tiếng Pháp suis tức là động từ to be Am của tiếng Anh suis này tức là Am của tiếng Anh như vậy với Attention Distribution này nó cũng thể hiện đúng là khi tôi bắt đầu đưa vào từ ne tôi sẽ bắt đầu để tâm để ý đến từ thứ 2",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ROIgZ5tyDFo",
      "filename": "ROIgZ5tyDFo",
      "title": "[CS431 - Chương 9] Part 2_1: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 14,
      "start_timestamp": "0:09:51",
      "end_timestamp": "0:10:18"
    }
  },
  {
    "page_content": "vào từ ne tôi sẽ bắt đầu để tâm để ý đến từ thứ 2 tức là từ Am nhiều hơn do đó hàm lượng thông tin của từ Am sẽ được truyền vào đây Attention Output này nhiều hơn dẫn đến việc đưa ra dự đoán y mũ 3 chính xác hơn rồi tương tự như vậy pas trong tiếng Pháp là Not nó sẽ để ý vào từ Not này nhiều hơn suis sûr thì nó trong tiếng Pháp tương ứng là từ Sure các bạn sẽ thấy là ở đây nó sẽ cho cái trọng số cao hơn Hãy subscribe cho kênh Ghiền Mì Gõ để không bỏ lỡ những video hấp dẫn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=ROIgZ5tyDFo",
      "filename": "ROIgZ5tyDFo",
      "title": "[CS431 - Chương 9] Part 2_1: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 15,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề, học sâu, machine learning, imageNet. Chủ đề, học sâu, imageNet. đối tượng. Đây có vẻ như là một trong những bài toán khởi nguồn cho mạng CNN Deep Learning. Đầu vào của chúng ta sẽ có ảnh của 1 cái object và đầu ra mình sẽ cần phải phân loại xem cái object trong cái ảnh đầu vào này nó thuộc cái phân lớp là gì. Thì ở đây với một cái giả định rằng là trong cái tống hình này nó chỉ chứa với một giả định trong tóm hình này chỉ chứa duy nhất đối tượng. Thì như vậy thì output của mình chỉ có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:13"
    }
  },
  {
    "page_content": "đối tượng. Thì như vậy thì output của mình chỉ có duy nhất một cái đáp án thôi, đó là cho bài toán phân loại đối tượng. Và kiến trúc của mình sẽ bao gồm hai thành phần. thành phần đầu tiên là học rút trích đặc trưng và thành phần thứ hai là mình sẽ phân lớp các đặc trưng đó vào các class thì đối với thành phần feature learning rút trích đặc trưng thì chúng ta sẽ có các module chính chính là module về convolution, relu và pooling Còn đối với mô đun về phân loại thì chúng ta sẽ có các biến đổi là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 1,
      "start_timestamp": "0:01:07",
      "end_timestamp": "0:01:55"
    }
  },
  {
    "page_content": "về phân loại thì chúng ta sẽ có các biến đổi là fully connected và shoppe map. Đây chính là kiến trúc cao mảng của mạng CNN. Ngoài bài toán phân loại đối tượng mà chúng ta được tìm hiểu đó là object classification. thì chúng ta sẽ có một chủ đề cũng là phân loại đối tượng nhưng mà nó ở cấp độ gọi là 5-grand 5-grand có nghĩa là miệng có nghĩa là miệng nghĩa là sao? ví dụ, trước đây chúng ta chỉ phân biệt là hoa, cây cối, chó mèo v.v. Nhưng mà bây giờ trong các loại hoa thì nó có rất nhiều những",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 2,
      "start_timestamp": "0:01:50",
      "end_timestamp": "0:02:43"
    }
  },
  {
    "page_content": "giờ trong các loại hoa thì nó có rất nhiều những loại hoa mà thuộc các chi, các nhánh trong giới sinh vật Bị dụng hoa lài, hoa hoẹ, hoa hồng Thậm chí là trong hoa hồng nó cũng có rất nhiều giống hoa hồng Hoa cút nó cũng có rất nhiều giống hoa cút Ở đây chính là Firebrand Object Classification và trong hình ở đây thì chúng ta có thù bộ dataset đó là PassportFlowerDataset Tương tự như vậy cho bài toán phân loại xe thì có rất nhiều loại xe khác nhau và thậm chí là trong cùng một hãng xe thì chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 3,
      "start_timestamp": "0:02:39",
      "end_timestamp": "0:03:21"
    }
  },
  {
    "page_content": "và thậm chí là trong cùng một hãng xe thì chúng ta sẽ có rất nhiều dòng tối như xe hạng A, B, C, V, V rồi ứng với từng hãng thì nó cũng sẽ có các đời xe thì ở đây chúng ta sẽ có một bộ data set đó là Stanford Car data set để thu thập và phân loại các dòng xe từng xưa đến nay. Rồi, ngoài ra thì nó cũng có một bài toán dạng phân loại đối tượng và dạng FireRain tức là MIN, đó chính là Fade Recognition. Trước đây thì chúng ta chỉ cần detect cái Fade, Tức là chúng ta sẽ đi so sánh cái gương mặt với",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 4,
      "start_timestamp": "0:03:16",
      "end_timestamp": "0:04:08"
    }
  },
  {
    "page_content": "Tức là chúng ta sẽ đi so sánh cái gương mặt với lại các bộ phần khác trong cơ thể Ví dụ như là tay, ví dụ như là chân, hoặc là với những đối tượng khác Ví dụ như là xe, ví dụ như là cây Trong nội bộ phay, tức là cái gương mặt này, chúng ta sẽ có rất nhiều những định danh Mục tiêu của mình là phân biệt được định danh số 1 với định danh số 2, đó chính là data set. thì ở đây chúng ta sẽ có một bộ data set, đó là Webfade 260M data set và cái bộ data set này thì có chứa rất nhiều những ảnh gương",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 5,
      "start_timestamp": "0:03:59",
      "end_timestamp": "0:04:52"
    }
  },
  {
    "page_content": "set này thì có chứa rất nhiều những ảnh gương mặt, rất nhiều những cái tư thế ví dụ như là chúng ta nhìn trực diện, nhìn về bên tay phải, nhìn xuống dưới, và nhìn về bên tay trái rồi có rất nhiều những cái chủng tộc Ví dụ có người da trắng, người da màu, rồi có người châu Á, người châu Âu, người châu Phi, v.v. Và có rất nhiều trạng thái. Ví dụ ở đây chúng ta sẽ có trạng thái đó là đeo khẩu trang, rồi sẽ có tình thú đó là ảnh này được chụp từ thời xưa, ảnh trắng đen. Rồi trạng thái ở đây thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 6,
      "start_timestamp": "0:04:44",
      "end_timestamp": "0:05:33"
    }
  },
  {
    "page_content": "thời xưa, ảnh trắng đen. Rồi trạng thái ở đây thì cũng bao gồm là trạng thái về cảm xúc, dù như đang là tươi cười hoặc là đang giận dữ, ví dụ vậy. Và từ đó thì chúng ta sẽ thấy là tập Webfile 260 triệu data set này là thể hiện được tính khó của bài tán phân loại đối tượng. Nó rất là khác so với bài tán phân loại đối tượng bình thường mà mình đã được tìm hiểu trước đây. ở đây là các đối tượng ở đây nó sẽ có cùng những đặc điểm tổng thể đó là đều có mắt, có mũi, có miệng, có tóc nhưng mà làm sao",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 7,
      "start_timestamp": "0:05:29",
      "end_timestamp": "0:06:18"
    }
  },
  {
    "page_content": "có mắt, có mũi, có miệng, có tóc nhưng mà làm sao có thể phân biệt được những chi tiết nhỏ để có thể giúp phân biệt được người này với người kia Thí dụ để phân biệt được người này với người kia thì người ta hay so sánh về màu da, so sánh về tỷ lệ phân bố, độ dài của các bộ phận trên mặt của mình thì tất cả những yếu tố đó tạo nên độ khó của bài tuán này. Và ứng dụng trong lĩnh vực về y tế thì chúng ta sẽ có bài tuán phân loại ung thư da. Ví dụ như ở đây là chúng ta sẽ có một vết. Nếu mà nhìn bề",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 8,
      "start_timestamp": "0:06:09",
      "end_timestamp": "0:06:58"
    }
  },
  {
    "page_content": "ở đây là chúng ta sẽ có một vết. Nếu mà nhìn bề ngoài mà người bình thường thì có thể chúng ta sẽ xem đây là... có thể hiểu đây là cái nốt ruồi nhưng thực tế thì nó có thể là một dấu hiệu của bệnh ung thư. thì ở đây chúng ta sẽ phải phân biệt, xem làm hai loại, đó là giấu hiệu của mình là lành tính hay là ác tính. Thì đây là cái bài toán uống dụng trong lĩnh vực về y tế. Và trong ảnh y tế thì một số loại ảnh có tính phức tạp cao hơn và có đồ men không giống với đồ men của lĩnh vực mà mình thay",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 9,
      "start_timestamp": "0:06:45",
      "end_timestamp": "0:07:28"
    }
  },
  {
    "page_content": "không giống với đồ men của lĩnh vực mà mình thay thú luyện trên tập dữ liệu MNS. Ví dụ như là chụp trên ảnh CT scan hoặc là chụp trên ảnh MRI thì tất cả những cái này đều là những cái định giảng ảnh và nó không phổ biến trong thế giới thực dẫn đến đó là khi mà chúng ta hú luyện các cái mạng CNN trên các cái domain này thì có khi chúng ta sẽ phải hú luyện lại từ đầu chúng ta cũng không có thể tái sử dụng được nhiều những cái đặc trưng trong ảnh màu ảnh thế giới thực của mình. Và một số cái kỹ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 10,
      "start_timestamp": "0:07:20",
      "end_timestamp": "0:08:11"
    }
  },
  {
    "page_content": "màu ảnh thế giới thực của mình. Và một số cái kỹ thuật mà ứng dụng của cái mạng CNN cho cái bài toán phân lớp ở những cái bài toán như vừa đề cập thì nó có rất nhiều những kỹ thuật khác nhau. Ví dụ, đối với cái bài toán nhận nhiễm ngươi mặt thì ở đây người ta sẽ tập trung vào cái việc là cải tiến chắc chắn hàm loss. Như chúng ta đã biết là trong một mô hình máy học thì nó sẽ có hàm mô hình và chúng ta sẽ có hàm loss Hàm loss Đối với hàm mô hình thì họ vẫn sử dụng kiến trúc mạng hoặc là các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 11,
      "start_timestamp": "0:08:03",
      "end_timestamp": "0:08:43"
    }
  },
  {
    "page_content": "thì họ vẫn sử dụng kiến trúc mạng hoặc là các thành phần như là conclusion, pooling, activation, re-loop Nhưng mà khi tính toán các kế độ sai lệch giữa mẫu dữ liệu của mình với lại những mẫu dữ liệu của gương mặt của mình với lại những gương mặt khác thì chúng ta sẽ phải sử dụng cái Hamlox đặc biệt tại vì gương mặt là một loại đối tượng đặc biệt. Nó có những bộ phận rất giống nhau nhưng mà đồng thời nó cũng khác nhau ở các yếu tố rất là nhỏ. Và giải pháp của các thương tiết tượng tiên tiến nhất",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 12,
      "start_timestamp": "0:08:41",
      "end_timestamp": "0:09:28"
    }
  },
  {
    "page_content": "pháp của các thương tiết tượng tiên tiến nhất hiện nay cho bài toán nhận diệu của bà Đều là đến từ các cải tiến cho HamLogs Ví dụ như chúng ta có các mô hình như là ArcFade, SphereFade, VoW Thì nó đều sử dụng các độ đo là Angular, MarginLogs Và ý tưởng của HamLogs này là nó sẽ ếp để cho mô hình cố gắng học được những đặc trưng phân biệt cao và những đặc trưng phân biệt cao để tách biệt giữa những gương mặt tương tự nhau Ví dụ như chúng ta thấy trên hình trò này, mỗi một cái chấm đại diện cho",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 13,
      "start_timestamp": "0:09:25",
      "end_timestamp": "0:10:06"
    }
  },
  {
    "page_content": "trên hình trò này, mỗi một cái chấm đại diện cho đặc trưng của một face, một gương mặt Mình sẽ viết bằng tiếng Anh để cho nó rõ Mỗi cái này là một gương mặt và những gương mặt nào gần nhớm nhau Thực tế chúng ta thấy là có những gương mặt 2 người khác nhau nhưng mà có cái nét gương mặt nó ná ná giống nhau thì những cái phay đó nó sẽ đặt nằm ở gần nhau trên cái cung hình tròn này và nhiệm vụ của các cái HamLog này là cố gắng tắt các cái phay tương tự nhau tắt ra xa cái phay tương tự nhau tắt ra",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 14,
      "start_timestamp": "0:10:01",
      "end_timestamp": "0:10:45"
    }
  },
  {
    "page_content": "tự nhau tắt ra xa cái phay tương tự nhau tắt ra xa ví dụ chúng ta thấy là ở đây trên cái cung góc này, đúng không? thì nó sẽ maximize, tức là cực đại hóa cái góc này để tắt các cái điểm ở trên cái hình tròn này Đối với những gương mặt khác nhau thì nó sẽ tắt càng xa nhau ra Và đó chính là ý tưởng của việc tài tiến hàm loss Mặt khác trong một số lĩnh vực, ví dụ trong lĩnh vực y khoa Thì kết quả của mình khi chúng ta phân loại mà ra được độ chính xác là 99% Thì đôi khi các bác sĩ họ sẽ không sử",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 15,
      "start_timestamp": "0:10:34",
      "end_timestamp": "0:11:20"
    }
  },
  {
    "page_content": "xác là 99% Thì đôi khi các bác sĩ họ sẽ không sử dụng kết quả của mình kể cả khi mình chứng minh với họ là phương pháp, mô hình của mình có thể nhận diện được chính xác các bệnh đến 99% mà họ chỉ có thể sử dụng hệ thống của mình khi mình chỉ ra được những khu vực có khả năng là xác định được đó là bệnh. Tức là bên cạnh việc đưa ra kết quả đầu cuối là có bệnh hay không bệnh, thì ở đây mình phải thuyết phục người sử dụng rằng tôi đưa ra nhận diện được người này bị bệnh khi nó có những dấu hiệu rõ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 16,
      "start_timestamp": "0:11:11",
      "end_timestamp": "0:11:58"
    }
  },
  {
    "page_content": "người này bị bệnh khi nó có những dấu hiệu rõ ràng và những dấu hiệu đó thì nó thể hiện ở những khu vực nhất định. Tức là chúng ta sẽ có tính giải thích được. Tính giải thích được trong AI hiện nay rất là quan trọng, tại vì nó không chỉ chứng minh được rằng AI có khả năng tiếp cận được những tri thức của con người, nhưng mà đồng thời nó có khả năng suy luận logic và đưa ra được những cái cao cứ, cách khách quan để giúp cho chúng ta đưa ra được cái kết quả phán xét cuối cùng. Như vậy thì trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 17,
      "start_timestamp": "0:11:54",
      "end_timestamp": "0:12:41"
    }
  },
  {
    "page_content": "cái kết quả phán xét cuối cùng. Như vậy thì trong phần trước chúng ta đã nghiên cứu về các bài toán phân loại đối tượng ở các cấp độ khác nhau. Đó là bài toán về FireRain. Đó là phân loại đối tượng ở mức độ miệng. Ví dụ như bài toán về phân loại các loài hoa, phân loại, biệt các loài xe, các chuẩn loại xe, gương mặt, v.v. Và có những hướng tiếp cận như là thay đổi các giá trị hàm loss hoặc là hướng tiếp cận RadCam, hoặc là Cam trong việc giải thích được các mô hình của mình. Trong phần tiếp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 18,
      "start_timestamp": "0:12:33",
      "end_timestamp": "0:13:11"
    }
  },
  {
    "page_content": "thích được các mô hình của mình. Trong phần tiếp theo, chúng ta sẽ cùng tìm hiểu về ứng dụng của mạng Deep Neural Network trong việc truy vấn hình ảnh. Y tưởng bài toán truy vấn hình ảnh đó là cho trước một cơ sở dưới điều ảnh, chúng ta sẽ có một ảnh truy vấn hay query và nhiệm vụ của mình đó là sẽ xác định độ tương đồng giữa ảnh truy vấn với tất cả những ảnh trong dataset và chúng ta sẽ trả về danh sách đã được sắp xét dảm dần theo mức độ tương đồng dảm dần thì ở đây chúng ta sẽ có một cái ví",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 19,
      "start_timestamp": "0:13:09",
      "end_timestamp": "0:13:53"
    }
  },
  {
    "page_content": "đồng dảm dần thì ở đây chúng ta sẽ có một cái ví dụ ở đây chúng ta sẽ có một cái ảnh query query số 1, đây là ảnh query thứ 2 vâng vâng, đây là ảnh truy vấn và ứng với một ảnh truy vấn thì chúng ta sẽ lục tìm trong cái database để trả về mỗi một cái hàng này sẽ là một cái kết quả ra về, trong đó cái nhóm đầu tiên được tô bởi màu xanh đậm chính là những cái ảnh có cái mức độ tương đồng rất là cao đối với cái ảnh truy vấn Chúng ta sẽ xét cái kết quả theo từng hàng. Và ở cột ở giữa thì chúng ta sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 20,
      "start_timestamp": "0:13:45",
      "end_timestamp": "0:14:29"
    }
  },
  {
    "page_content": "theo từng hàng. Và ở cột ở giữa thì chúng ta sẽ thấy là đây là những cái kết quả trí vấn có ký độ tương đồng cao. Bên đây là rất cao, còn đây là cao. Thì chúng ta có thể thấy là cái object trong ảnh trí vấn ở đây thì nó xuất hiện đâu đó ở một cái khu vực tương đối là nhỏ trong cái phạm vi của tấm hình. Và nó sẽ bị ảnh hưởng bởi những cái yếu tố như là bị che khuất Ví dụ như là chúng ta sẽ bị ảnh hưởng bởi yếu tố về ánh sáng và góc nhìn Thì nó sẽ làm cho kết quả so sánh tương đồng sẽ bị giảm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 21,
      "start_timestamp": "0:14:23",
      "end_timestamp": "0:15:05"
    }
  },
  {
    "page_content": "sẽ làm cho kết quả so sánh tương đồng sẽ bị giảm xuống đáng kể Và đối với cái nhóm ảnh màu vàng thì đây là những cái ảnh có độ tương đồng rất là thấp Tại vì khu vực có sự xuất hiện của object của mình rất là nhỏ và nó đã bị biến đổi rất là nhiều thì đây là cái bài toán tri văn ảnh và ý tưởng chính để giải quyết cái bài toán tri văn ảnh này đó chính là chúng ta sẽ sử dụng các cái mạng CNN đã được huấn luyện sẵn như là những cái bộ rút trích trạch trưng thì ý tưởng nhanh nhất đó chính là chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 22,
      "start_timestamp": "0:14:56",
      "end_timestamp": "0:15:44"
    }
  },
  {
    "page_content": "trưng thì ý tưởng nhanh nhất đó chính là chúng ta sẽ fit tất cả những cái ảnh trong cái database vào bên trong cái mạng CNN và mỗi một cái ảnh này nó sẽ tương ứng là một cái điểm, một cái feature. Và khi có cái ảnh query thì chúng ta sẽ đưa vào cái mạng CNN một lần nữa và chúng ta sẽ ra được cái feature một cam này. Và chúng ta sẽ dùng các giải thuật về độ tương đồng hoặc là độ đo khuãng cách để xác định tốt những cái ảnh có cái sự tương đồng hoặc là có cái khuãng cách đến đặc trưng truy vấn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 23,
      "start_timestamp": "0:15:37",
      "end_timestamp": "0:16:20"
    }
  },
  {
    "page_content": "hoặc là có cái khuãng cách đến đặc trưng truy vấn này. và tương ứng từng đặc trưng này chính là kết quả trả về, là những tấm ảnh chúng ta trả về. Đây là ý tưởng cho bài toán tri bóng ảnh. Tuy nhiên cách tiếp cận này sẽ xem toàn bộ tấm hình này như là một đặc trưng và nó sẽ bị một số vấn đề đó là nó sẽ bỏ sót những đặc trưng chi tiết ở bên trong. đôi khi chúng ta nhìn một tấm hình thì chúng ta sẽ xác định xem hình đó tốc vư tương đồng có giống với ảnh trí vấn hay không dựa vào những bộ phận bên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 24,
      "start_timestamp": "0:16:17",
      "end_timestamp": "0:16:56"
    }
  },
  {
    "page_content": "ảnh trí vấn hay không dựa vào những bộ phận bên trong ví dụ như chúng ta nhìn tấm hình này thì đôi khi chúng ta sẽ chỉ cần một bộ phận này thôi để đi so sánh chứ không sử dụng những thông tin ở vùng màu đen, những vùng mà chiếm tỷ trọng rất nhiều diện tích Như vậy thì đòi hỏi là đặc trưng CNN, đặc trưng Deep Learning của mình sẽ phải đi sâu vô đến mức độ chi tiết từng khu vực nhỏ. Và đó chính là ý tưởng của nguyên tiếp cận là đặc trưng DELF, DELF. Và ý tưởng của mạng CNN cho việc rút trích đặc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 25,
      "start_timestamp": "0:16:52",
      "end_timestamp": "0:17:41"
    }
  },
  {
    "page_content": "Và ý tưởng của mạng CNN cho việc rút trích đặc trưng này, đó chính là chúng ta sẽ fit cái tâm ảnh này vào và chúng ta sẽ đúc trích ra đặc trưng dày đặt với mỗi một cái cột ở đây nó sẽ tương ứng là một cái đặc trưng trên một cái vùng cục bộ ở bên trong tấm hình tuy nhiên thì trong cái tấm hình của mình nó sẽ có những cái vùng không có chứa nhiều thông tin ví dụ như trong cái tấm hình vừa rồi chúng ta thấy là những cái vùng mà đồng màu khu vực này thì nó sẽ có ít thông tin để giúp cho mình phân",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 26,
      "start_timestamp": "0:17:32",
      "end_timestamp": "0:18:13"
    }
  },
  {
    "page_content": "thì nó sẽ có ít thông tin để giúp cho mình phân biệt do đó thì cái khu vực này nó sẽ có cái Attention Thấp hoặc là có trọng số thấp nó sẽ có cái Score Thấp. Và những cái khu vực nào mà có cái sự thay đổi về mặt hình ảnh nhiều thì chỗ đó nó sẽ có trọng số cao. Thế thì để huấn luyện và xác định được cái khu vực nào có trọng số thấp khu vực nào có trọng số cao thì chúng ta sẽ có một cái lớp gọi là tính Attention Score và dựa trên Attention Score này nó sẽ bỏ đi những feature nào mà không có vai",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 27,
      "start_timestamp": "0:18:06",
      "end_timestamp": "0:18:42"
    }
  },
  {
    "page_content": "này nó sẽ bỏ đi những feature nào mà không có vai trò quan trọng và chỉ trừ lại những feature tốt, những feature đặc trưng cho tấm ảnh và gom lại thành deal feature. Và chúng ta đại diện một cái tấm ảnh này bằng danh sách những feature có vai trò quan trọng trên những khu vực có sự biến động, những khu vực có những điểm đặc trưng. Và dựa trên đặc trưng đã rút trích được từ tấm ảnh này, chúng ta sẽ làm tương tự như vậy cho toàn bộ tấm ảnh. Rồi sau đó chúng ta sẽ đánh chỉ mục vào trong một kế cơ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 28,
      "start_timestamp": "0:18:39",
      "end_timestamp": "0:19:15"
    }
  },
  {
    "page_content": "đó chúng ta sẽ đánh chỉ mục vào trong một kế cơ sở du lịch chỉ mục. Và đối với ảnh query thì chúng ta cũng sẽ có các feature sau khi đã thực hiện với Attention Score. Chúng ta sẽ có những đặc trưng đại diện cho một tấm ảnh. và chúng ta sẽ đi so sánh, đi tra cứu trong cái last scale index này để từ đó xác định ra những cái feature nào gần với lại cái feature này nhất Rồi đồng thời chúng ta sẽ thực hiện cái theo tác là Geometric Verification tức là chúng ta sẽ tinh chỉnh lại cái ế vô về mặt hình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 29,
      "start_timestamp": "0:19:10",
      "end_timestamp": "0:19:25"
    }
  },
  {
    "page_content": "chúng ta sẽ tinh chỉnh lại cái ế vô về mặt hình học để cho cái tấm hình của mình xác định xem là vị trí thực sự của cái object nó nằm ở đâu Ví dụ trong hình hình này chúng ta thấy là qua cái phép Geometric Verification thì chúng ta xác định được cái object thực sự của mình là nằm trong cái khung màu vàng. Và chúng ta sẽ thực hiện một số thuật toán kinh điển ví dụ như thuật toán Ransack để loại bỏ những cái cặp điểm Outlayer những cặp điểm mà không thực sự tương đồng và trừa lại những cái cặp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 30,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "thực sự tương đồng và trừa lại những cái cặp điểm Inlayer và đếm cái số điểm đó rồi chúng ta sẽ... số cặp điểm tương đồng nào mà càng cao thì mức độ giống nhau giữa ảnh trí vấn và ảnh đó sẽ là càng tốt và chúng ta sẽ sắp thích hạn đó càng cao. ỉ thì ý tưởng chính của thuật toán Dell là như vậy và các mô hình trí vấn có sử dụng Deep Learning thì cũng sẽ dựa trên ý tưởng này để phát triển tiếp. Hiện nay cũng có rất nhiều những phương pháp đã cải tiến từ phương pháp của Dell và cho những kết quả",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 31,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "tiến từ phương pháp của Dell và cho những kết quả rất ấn tượng trong bài toán trí vấn hình ảnh. Hãy subscribe cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video hấp dẫn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RVFApjx4KKI",
      "filename": "RVFApjx4KKI",
      "title": "[CS431 - Chương 5] Part 1-2: Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
      "chunk_id": 32,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chúng ta sẽ cài cho biến thể cuối cùng. Khi bỏ relu, chúng ta thấy nó đã chạy xong rồi. Chúng ta sẽ quan sát thử. Chúng ta sẽ vẽ. thì nhìn vào sơ đồ này, ở đây chúng ta sẽ phải gom nó lại, gom hai cái legend này lại rồi, về lại rồi chúng ta sẽ thấy là cái relu phi mạng số 2 nó giảm rất là nhanh, đúng không, nó giảm rất là nhanh Nó nằm bên dưới đường màu xanh Thì điều đó có nghĩa là gì? Điều đó là, ví dụ tại Epoch số 5 Thì phương pháp V2, tức là khi sử dụng Relu Nó cho cái loss thấp hơn sau với",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rVpEwMijtvQ",
      "filename": "rVpEwMijtvQ",
      "title": "[CS431 - Chương 3] Part 3_3: Cài đặt mạng CNN",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:03"
    }
  },
  {
    "page_content": "khi sử dụng Relu Nó cho cái loss thấp hơn sau với phiên bản số 1, tức là dùng sigmoid Tức là nó đã giúp cho mình hội tụ nhanh hơn Nhưng mà đương nhiên khi số Epoch càng lớn thì cả hai thằng cũng sẽ tiện trọng về Nhưng mà nó sẽ tốn thời gian hơn Tập Amnest là tập tuyến tính, rất dễ, rất đơn giản. Nó sẽ không thể nào thể hiện được sự khuất đại tốc độ train của Relu nhanh hơn sau Vsigmoid như thế nào. Khi chúng ta train với tập dữ liệu lớn như E-Mainnet, chúng ta sẽ thấy Relu hữu quả hơn rất",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rVpEwMijtvQ",
      "filename": "rVpEwMijtvQ",
      "title": "[CS431 - Chương 3] Part 3_3: Cài đặt mạng CNN",
      "chunk_id": 1,
      "start_timestamp": "0:00:59",
      "end_timestamp": "0:01:42"
    }
  },
  {
    "page_content": "E-Mainnet, chúng ta sẽ thấy Relu hữu quả hơn rất nhiều. Nó sẽ giảm xuống, chúng ta sẽ thấy sự sụp giảm về loss rất nhanh. Đó chính là ý nghĩa của biến thể đầu tiên, đó là bỏ Sigloin Moist và thay thế bằng Relu thì tốc độ ngồi tụ của nó sẽ nhanh hơn. Còn về đủ chính xác, theo thời gian dài, đâu đó nó vẫn sẽ sắp xỉ với Sigmoid nhưng mà với thời gian mà mình có thể chờ đợi được để có thể huyết luyện thì việc dùng Sigmoid sẽ chậm hơn rất nhiều. Rồi tiếp theo, đó là chúng ta sẽ bỏ hết các lớp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rVpEwMijtvQ",
      "filename": "rVpEwMijtvQ",
      "title": "[CS431 - Chương 3] Part 3_3: Cài đặt mạng CNN",
      "chunk_id": 2,
      "start_timestamp": "0:01:35",
      "end_timestamp": "0:02:27"
    }
  },
  {
    "page_content": "Rồi tiếp theo, đó là chúng ta sẽ bỏ hết các lớp Pulling. Rồi, chúng ta đã cài đặt rồi. Bây giờ chúng ta sẽ sử dụng nó Ở đây chúng ta sẽ để là CNNv3 và History sẽ là History số 3 Ở đây chúng ta sẽ khô phục ngược trở lại là SIGMOID Rồi chạy Bây giờ chúng ta sẽ trở thành hầm loss khi có đồng thời cả 3 keyspare 1, 2, 3 V3 sẽ là width down, pull list Chúng ta sẽ cài luôn phiên bản thứ 4, đó chính là chúng ta bỏ hết các lớp comparison một điều rất là thú vị đó là chúng ta đặt sự nguyên ngờ là cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rVpEwMijtvQ",
      "filename": "rVpEwMijtvQ",
      "title": "[CS431 - Chương 3] Part 3_3: Cài đặt mạng CNN",
      "chunk_id": 3,
      "start_timestamp": "0:02:19",
      "end_timestamp": "0:03:49"
    }
  },
  {
    "page_content": "là thú vị đó là chúng ta đặt sự nguyên ngờ là cái mạng conclusion thì cái vai trò của conclusion rõ ràng rất là lớn nhưng bây giờ chúng ta sẽ làm một thí nghiệm đó là bỏ hết cái conclusion thì xem điều gì sẽ xảy ra thì đó chính là ý nghĩa của cái phiên bạng số 4 rồi bây giờ may quá cái phiên bạng số 3 nó đã chạy xong và chúng ta sẽ xem thử Rồi, chúng ta thấy là nếu như không có pool link, thì cái loss của mình gần như không giảm, nó cứ diễn nguyên. Loss gần như không giảm, nó cứ diễn nguyên. Rõ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rVpEwMijtvQ",
      "filename": "rVpEwMijtvQ",
      "title": "[CS431 - Chương 3] Part 3_3: Cài đặt mạng CNN",
      "chunk_id": 4,
      "start_timestamp": "0:03:46",
      "end_timestamp": "0:04:40"
    }
  },
  {
    "page_content": "Loss gần như không giảm, nó cứ diễn nguyên. Rõ ràng là cái vai trò của pool link cũng rất là quan trọng. Nếu không có pool link thì cái loss của mình gần như đi ngang, nó không giúp cho mình giảm xuống. Bây giờ chúng ta sẽ quay sang phiên bản tiếp theo, đó là không có lớp Conv ở đây chúng ta phải sử dụng biến thể đầu tiên để mình code, chứ nếu không là sẽ nhầm lẩm Rồi, không có Conv, chúng ta sẽ bỏ đi lớp này, bỏ đi lớp này và ở đây chúng ta sẽ truyền vào input, head2 sẽ truyền vào đây tới đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rVpEwMijtvQ",
      "filename": "rVpEwMijtvQ",
      "title": "[CS431 - Chương 3] Part 3_3: Cài đặt mạng CNN",
      "chunk_id": 5,
      "start_timestamp": "0:04:30",
      "end_timestamp": "0:06:15"
    }
  },
  {
    "page_content": "truyền vào input, head2 sẽ truyền vào đây tới đó chúng ta sẽ giãn cái thước liên tiếp 2 lần rồi, ở đây sẽ là cnn v4 rồi, bây giờ chúng ta sẽ gọi cái ham này khởi tạo v4, history là 4 cat run và tương tự như vậy, chúng ta sẽ vẽ sơ đồ ở đây rồi, chúng ta sẽ có history là 4 Rồi, chúng ta thấy là cái loss của mình cũng có giảm, tuy nhiên cái tốc độ giảm của nó khá là chậm, tốc độ giảm khá chậm thì điều này cũng minh chứng cho cái việc đó là cái convolution của mình, nó đã giúp cho cái việc hướng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rVpEwMijtvQ",
      "filename": "rVpEwMijtvQ",
      "title": "[CS431 - Chương 3] Part 3_3: Cài đặt mạng CNN",
      "chunk_id": 6,
      "start_timestamp": "0:06:05",
      "end_timestamp": "0:06:42"
    }
  },
  {
    "page_content": "của mình, nó đã giúp cho cái việc hướng luyện nó nhanh hơn mặc dù accuracy thì nó cũng có sửa vứt là nó càng lúc càng tăng, đúng không? nó có sửa vứt càng tăng nhưng với cùng cái số y tốc thì không có convolution, tốc độ nó sẽ chậm hơn rất là nhiều Cái đường màu đỏ là version 4 thì chúng ta thấy là nó nằm ở phía trên nếu không có Correlation nó sẽ nằm phía trên Vì vậy, cái phiên bản mà hoàn thiện nhất của chúng ta chính là cái phiên bản màu kem ở đây là đường nằm ở dưới cùng Là tương ứng phiên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rVpEwMijtvQ",
      "filename": "rVpEwMijtvQ",
      "title": "[CS431 - Chương 3] Part 3_3: Cài đặt mạng CNN",
      "chunk_id": 7,
      "start_timestamp": "0:06:33",
      "end_timestamp": "0:07:11"
    }
  },
  {
    "page_content": "ở đây là đường nằm ở dưới cùng Là tương ứng phiên bản số 2 là thay cái sigmoid bằng Renu trong đó vẫn phải giữ vừa có Pulling và vừa có Convolution Vì vậy thì đây chính là bài tập tutorial để giúp chúng ta hiểu được vai trò của từng phát biến đổi bên trong nạc CNN",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rVpEwMijtvQ",
      "filename": "rVpEwMijtvQ",
      "title": "[CS431 - Chương 3] Part 3_3: Cài đặt mạng CNN",
      "chunk_id": 8,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Bây giờ chúng ta sẽ đến với phần về công thức. Nãy giờ là chúng ta đang mô phẫu cách thức vận hành của 1 kick attention. Còn về công thức tính thì chúng ta sẽ tính như thế nào? Tại đây chúng ta sẽ có các hệ thống ký hiệu. Với n-coder chúng ta sẽ ký hiệu bằng chìa S. S1, S2, S3 cho đến SN Quá trình decode thì sẽ ký hiệu H Quá trình decode thì sẽ ký hiệu H và H ở đây sẽ là đi theo ký trục thời gian Và tại thời điểm đầu tiên thì T thời gian sẽ là bằng 1 Tiếp theo thì nó sẽ lấy HT này đi nhân tích",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=S8__bXkLSbM",
      "filename": "S8__bXkLSbM",
      "title": "[CS431 - Chương 9] Part 2_2: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:46"
    }
  },
  {
    "page_content": "1 Tiếp theo thì nó sẽ lấy HT này đi nhân tích vô hướng với các giá trị S ht sẽ nhân tích vô hướng với là s1 ht s2 ht s3 ht sn sau khi chúng ta tính xong chúng ta được các cái score chúng ta sẽ chuẩn hóa nó và để chuẩn hóa về không gian sát xuất thì không hàm nào khác chúng ta đã từng học đó chính là chúng ta sử dụng hàm sop x chúng ta sẽ sử dụng hàm sop x và ký hiệu cho toàn bộ nội dung của tính Attention Score là R để tính Attention Distribution thì chúng ta sẽ ký hiệu là chữ alpha alpha là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=S8__bXkLSbM",
      "filename": "S8__bXkLSbM",
      "title": "[CS431 - Chương 9] Part 2_2: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 1,
      "start_timestamp": "0:00:40",
      "end_timestamp": "0:01:30"
    }
  },
  {
    "page_content": "thì chúng ta sẽ ký hiệu là chữ alpha alpha là thể hiện trọng số đã được chuẩn hóa của R vậy thì alpha t sẽ là bằng shop mark của RT alpha chính là đại chuẩn hóa của RT Sau khi chúng ta đã có được bộ trọng số alpha này rồi, chúng ta sẽ bắt đầu tổng hợp thông tin cho AttentionOutput và ký hiệu nó bằng chữ C C này còn có một ý nghĩa khác, nó chỉ là Contacts và C này sẽ là tổng có trọng số của các trạng thái của các bếp tơ ẩn của n-dot coder của quá trình encoder, chính là S1, S2, S3, S4 còn trọng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=S8__bXkLSbM",
      "filename": "S8__bXkLSbM",
      "title": "[CS431 - Chương 9] Part 2_2: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 2,
      "start_timestamp": "0:01:27",
      "end_timestamp": "0:02:22"
    }
  },
  {
    "page_content": "trình encoder, chính là S1, S2, S3, S4 còn trọng số tương ứng của nó chính là alpha Ti trong đó T là đại diện cho thời điểm T T là đại diện cho thời điểm T mà mình bắt đầu quá trình decode Y là dượt từ 1 cho đến n Dượt từ đầu cho đến cuối, cuối đoạn của encoder Vector Output, Attention Output này, đó là CT nó sẽ được thực hiện con card hiệu là dấu chống phải, nó nối chuỗi, con card lại với nhau để tạo ra 1 vector tổng hợp và từ vector tổng hợp này, chúng ta sẽ đi tính giá trị y ngã t và đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=S8__bXkLSbM",
      "filename": "S8__bXkLSbM",
      "title": "[CS431 - Chương 9] Part 2_2: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 3,
      "start_timestamp": "0:02:15",
      "end_timestamp": "0:03:20"
    }
  },
  {
    "page_content": "này, chúng ta sẽ đi tính giá trị y ngã t và đây chính là cái cáp tính dựa hoàn toàn vào tình ủ không có attention tức là y ngã t thì nó sẽ là bằng softmax của v nhân với lại cái vector này nhân với cái vector ngã ct chấm phải ht Đây là cách tính khi không có attention, cũng giống như trường hợp apply attention Rồi, và ở đây thì chúng ta sẽ có một cái bài tập đó là nếu như chúng ta giả định cái vector ht này nó có kích thước là rd tức là h là một cái vector d chiều Rt là một cái vector có kích",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=S8__bXkLSbM",
      "filename": "S8__bXkLSbM",
      "title": "[CS431 - Chương 9] Part 2_2: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 4,
      "start_timestamp": "0:03:09",
      "end_timestamp": "0:04:30"
    }
  },
  {
    "page_content": "cái vector d chiều Rt là một cái vector có kích thước bao nhiêu, aT là một vector có kích thước bao nhiêu, cT và hT là kích thước bao nhiêu C, vector Attention Output sẽ có kích thước bao nhiêu Bây giờ chúng ta sẽ tính toán cái dấu chấm hỏi này sẽ là các giá trị gì Nếu như bạn nào nhanh chí thì có thể nhìn vô đây là R này là tập hợp của các dấu hình tròn này Ở đây có bao nhiêu? Có N, có N phần tử như vậy R này sẽ là R n, tương ứng là R Và mỗi phần tử ht nhân với st là một giá trị vô hướng Bộ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=S8__bXkLSbM",
      "filename": "S8__bXkLSbM",
      "title": "[CS431 - Chương 9] Part 2_2: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 5,
      "start_timestamp": "0:04:21",
      "end_timestamp": "0:05:27"
    }
  },
  {
    "page_content": "phần tử ht nhân với st là một giá trị vô hướng Bộ hợp của giá trị vô hướng sẽ tạo ra một vector và có n với giá trị vô hướng như vậy alpha là vector attention distribution của vector trưởng hóa của RT do đó số chiều của alpha t không thay đổi sau với RT nếu ở đây là Rn thì ở đây cũng sẽ là Rn bước tiếp theo là chúng ta sẽ thực hiện phép con card nhưng mà để con cat được, chúng ta phải có cái CT nhưng mà chúng ta chưa có CT, nhưng mà chúng ta phải tính cái này trước CT bản chất là tổng trọng số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=S8__bXkLSbM",
      "filename": "S8__bXkLSbM",
      "title": "[CS431 - Chương 9] Part 2_2: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 6,
      "start_timestamp": "0:05:24",
      "end_timestamp": "0:06:08"
    }
  },
  {
    "page_content": "tính cái này trước CT bản chất là tổng trọng số của các cái SE đây là giá trị Scaler đây là giá trị Scaler, còn đây là Vector mà Vector SE thì để mà có thể nhân được cái S với lại cái H, đúng không? để mà S và H có thể nhân được với nhau thì tụi nó phải có cùng số chiều do đó thì sI ở đây cũng là 1 cái rD, tức là vector d chiều như vậy thì ở đây là vector d chiều tổng trọng số của các vector d chiều thì nó sẽ là 1 cái vector d chiều đó là 1 cái vector d chiều rồi, và khi chúng ta con cap 2 cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=S8__bXkLSbM",
      "filename": "S8__bXkLSbM",
      "title": "[CS431 - Chương 9] Part 2_2: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 7,
      "start_timestamp": "0:05:59",
      "end_timestamp": "0:07:04"
    }
  },
  {
    "page_content": "vector d chiều rồi, và khi chúng ta con cap 2 cái này lại với nhau CT là một vector đ HT là một vector đ Vậy ở đây sẽ là 2D Đây là đáp án cho bài tập của mình Hi vọng khi bạn làm được bài tập này Các bạn có thể hiểu được cơ chế vận hành của Attention Tại sao Attention thì hiệu quả attention cho hiệu suất cao hơn hẳn so với lại các phương pháp trước đây thì hiệu suất cao hơn này được thể hiện qua việc mà chúng ta thực nghiệm nhưng mà nếu mà nói về mặt lý thuyết thì quá trình decoder nó sẽ cho",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=S8__bXkLSbM",
      "filename": "S8__bXkLSbM",
      "title": "[CS431 - Chương 9] Part 2_2: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 8,
      "start_timestamp": "0:06:58",
      "end_timestamp": "0:07:49"
    }
  },
  {
    "page_content": "về mặt lý thuyết thì quá trình decoder nó sẽ cho phép nhìn lại toàn bộ câu văn hồn của mình Như chúng ta đã thấy là khi chúng ta tính cái y ngã t, chúng ta sẽ phải dựa trên cái thông tin của cả ht mà kết hợp với lại cả thông tin ct Trong đó thông tin ct là có chứa thông tin của toàn bộ cái cobang ngu của mình Ngoài ra, decoder cho phép chúng ta tập trung hơn tại một số phần nhất định trong covalent. ở đây chúng ta quan sát để tính ra output IT chúng ta sẽ có sự tổng hợp thông tin của CT và CT",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=S8__bXkLSbM",
      "filename": "S8__bXkLSbM",
      "title": "[CS431 - Chương 9] Part 2_2: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 9,
      "start_timestamp": "0:07:46",
      "end_timestamp": "0:08:39"
    }
  },
  {
    "page_content": "chúng ta sẽ có sự tổng hợp thông tin của CT và CT là tổng trọng số tổng trọng số của attention distribution này với vector ẩn như vậy nó bừa cho phép chúng ta nhìn lại toàn bộ nội dung của Công văn nguồn, nhưng nó cũng không phải là tổng hợp Nó sẽ không đều cộng trung bình cộng tất cả các thông tin này Mà nó chỉ chú tâm đến những vị trí nào của encoder mà nó thực sự liên quan đến quá trình dạy mạ ở đây Ví dụ như ở đây nó sẽ chú tâm đến từ đầu tiên là từ ai nhiều hơn so với các từ khác Chỉ cần",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=S8__bXkLSbM",
      "filename": "S8__bXkLSbM",
      "title": "[CS431 - Chương 9] Part 2_2: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 10,
      "start_timestamp": "0:08:37",
      "end_timestamp": "0:09:27"
    }
  },
  {
    "page_content": "là từ ai nhiều hơn so với các từ khác Chỉ cần để tập trung vào một số phần chứ không phải là nó sẽ đi nhìn hết nội dung của câu băng uổn Attention giải quyết được bánh đề điểm ngãn như chúng ta đã đề cập ở những slide đầu Attention giúp chúng ta giải quyết được bánh đề Vanishing Radius khi nó tạo được các đường tắt Thì cái đường TAC này chính là cái skip connection Và cái cơ chế skip connection này lại một lần nữa chúng ta nhắc đến Nó là sự kế thừa của mạng CNN, một cái biến thể của CNN đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=S8__bXkLSbM",
      "filename": "S8__bXkLSbM",
      "title": "[CS431 - Chương 9] Part 2_2: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 11,
      "start_timestamp": "0:09:21",
      "end_timestamp": "0:10:21"
    }
  },
  {
    "page_content": "kế thừa của mạng CNN, một cái biến thể của CNN đó chính là Graysnet Và nó đã được chứng minh trong rất nhiều những cái bài báo khoa học Skip connection sẽ giúp chúng ta chống được hiện tượng vanishing rất là tốt Với công thức rất đơn giản, x là hàm biến đổi, x là hàm giảm dạm hình tượng bán x. và Attention cho phép chúng ta một khả năng nữa cũng rất là thú vị đó chính là khả năng diễn đạt hay còn gọi là khả năng giải thích kết quả và trực quan hóa thì ở đây các bạn là cái gì trong Attention",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=S8__bXkLSbM",
      "filename": "S8__bXkLSbM",
      "title": "[CS431 - Chương 9] Part 2_2: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 12,
      "start_timestamp": "0:10:15",
      "end_timestamp": "0:10:59"
    }
  },
  {
    "page_content": "hóa thì ở đây các bạn là cái gì trong Attention giúp chúng ta có khả năng diễn đạt được đó chính là extension distribution với extension distribution thì tại thời điểm t chúng ta biết là chúng ta phải để tâm đến từ nào thì họ sẽ tìm cách visualize ma trận alpha visualize alpha t theo thời gian để từ đó là có khả năng biết được tại một thời điểm nào đó thì mô hình của mình Cái quá trình decoder của mình nó đang chú ý đến cái từ nào Và bên đây chính là một cái minh họa cho cái matrix alpha đó Ví",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=S8__bXkLSbM",
      "filename": "S8__bXkLSbM",
      "title": "[CS431 - Chương 9] Part 2_2: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 13,
      "start_timestamp": "0:10:58",
      "end_timestamp": "0:11:52"
    }
  },
  {
    "page_content": "là một cái minh họa cho cái matrix alpha đó Ví dụ như chúng ta thấy ở đây Cái từ 1992 ở đây Ta lấy tình huống dễ nhất đi ha 1992 trong từ tiếng Anh Thì chiếu xuống đây Chiếu xuống đây Và nó phát sáng nhất Và nó chỉ phát sáng trên nguyên cái cọt này thì nó chỉ phát sáng tại nhiêu nhất vị trí này Thì ánh xạ sang đây Và chính là cái từ này của tiếng Pháp nhưng ở một số từ nó có sự phát sáng trên nhiều giá trị ví dụ, ở đây chúng ta thấy là cái từ size nó đã phát sáng trên 3 từ là a, s, size thì từ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=S8__bXkLSbM",
      "filename": "S8__bXkLSbM",
      "title": "[CS431 - Chương 9] Part 2_2: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 14,
      "start_timestamp": "0:11:44",
      "end_timestamp": "0:11:52"
    }
  },
  {
    "page_content": "nó đã phát sáng trên 3 từ là a, s, size thì từ size trong tiếng Anh này đã được chia theo thì bị động quá khứ và tương ứng trong tiếng Pháp, để chia được bị đọc 3 hướng, nó cũng sẽ cần có 1 3 chữ này x, x, y thì đây là dí lý giải tại sao từ tiếng Anh này có sự liên đối đến cộng các từ tiếng Pháp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=S8__bXkLSbM",
      "filename": "S8__bXkLSbM",
      "title": "[CS431 - Chương 9] Part 2_2: Cơ chế Attention trong Sequence-to-Sequence",
      "chunk_id": 15,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Về cấu tạo của một mạng Conversion Neural Network, thì nó sẽ có những thành phần chính sau đây. Nếu như chúng ta tra cứu trên mạng internet, chúng ta thấy là khi người ta vẽ một kiến trúc mạng CNN, thì nó hay sử dụng trái dạng là hình khối, ảnh đầu vào, ví dụ ở đây là ảnh một chiếc xe, rồi nó sẽ biến đổi thành một cái khối, thì cái khối này nó gọi là đặc trưng. và nó được thực hiện bởi phép convolution đây là phép convolution và ngay sau phép convolution nó sẽ thực hiện phép reload rồi khi tạo",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:38"
    }
  },
  {
    "page_content": "nó sẽ thực hiện phép reload rồi khi tạo ra feature này xong chúng ta sẽ thực hiện phép pooling để giảm kích thước của tấm hình này lại giảm kích thước của đặc trưng này lại và cứ như vậy tự nhiên khi chúng ta mới bắt đầu tìm hiểu kiến trúc này thì chúng ta sẽ hơi bị rối do đó ở đây chúng ta sẽ phân loại ra Bốn phép biến đổi chính mà mạng CNN được sử dụng xuyên suốt trong toàn bộ kiến trúc này. Bốn phép biến đổi đó chính là phép Combustion, phép Activation, tức là tư ấn là hàm kích hoạt, tầng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 1,
      "start_timestamp": "0:00:35",
      "end_timestamp": "0:01:14"
    }
  },
  {
    "page_content": "Activation, tức là tư ấn là hàm kích hoạt, tầng kích hoạt, tầng Pooling và tầng Fully Connected. Đây chính là 4 phép, 4 phép biến đổi chính và chúng ta sẽ phối hợp. Phối hợp như thế nào? Thông thường tất cả các tầng Convolution và Activation tầng kích hoạt nó sẽ đi chung với nhau thành một cặp tức là ngay sau Convolution nó sẽ là tầng Activation và tầng Activation này thì thường người ta sử dụng cái hàm đó là hàm đây luôn và như vậy nó sẽ đi theo cặp với nhau và cái cặp điến đổi Convolution",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 2,
      "start_timestamp": "0:01:11",
      "end_timestamp": "0:01:49"
    }
  },
  {
    "page_content": "theo cặp với nhau và cái cặp điến đổi Convolution Activation này nó sẽ được thực hiện ca lần thì lâu lâu nó sẽ chọt vào một cái tầng Pooling Mục tiêu của tầng pooling này là để giảm kích thước của feature Và khi giảm kích thước của feature thì sau này tầng pooling data sẽ giảm số lượng tham số Cái việc giảm số lượng tham số sẽ có tác dụng gì thì chúng ta sẽ bàn luận sau Phối hợp các cặp convolution, activation và pooling này, chúng ta sẽ làm n lần và cứ thực hiện đi thực hiện lại. Thì hết cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 3,
      "start_timestamp": "0:01:45",
      "end_timestamp": "0:02:36"
    }
  },
  {
    "page_content": "lần và cứ thực hiện đi thực hiện lại. Thì hết cái giai đoạn này, nó sẽ gọi là rút chích trạc trưng Nó sẽ gọi là rút chích trạc trưng Và khi kết thúc giai đoạn rút chích trạc trưng này, nó sẽ đến cái tầng fully connected thì ở đây sẽ là tầng thực hiện công việc đó là phân lớp đặc trưng các đặc trưng đã được thực hiện bởi 3 tầng trước thì qua đến tầng Fully Connected này thật ra nó chính là mạng Neural Network của mình và tầng Neural Network này, nhiệm vụ của nó sẽ là đi phân loại đặc trưng Chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 4,
      "start_timestamp": "0:02:31",
      "end_timestamp": "0:03:22"
    }
  },
  {
    "page_content": "vụ của nó sẽ là đi phân loại đặc trưng Chúng ta sẽ có một số đồ đó là một ảnh một chiếc xe nó thực hiện phép convolution, rồi lại relu, convolution relu, sau đó lại pooling rồi sau đó convolution relu, convolution relu, rồi lại pooling Tương ứng với tham số k và n ở đây, k của mình trong trường hợp này chính là 2 Chúng ta thực hiện 2 lần các trập biến đổi Conversion Relu, Conversion Relu N là bằng 3, có nghĩa là nguyên 1 bộ này chúng ta sẽ thực hiện 3 lần Conversion Relu, Conversion Relu và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 5,
      "start_timestamp": "0:03:16",
      "end_timestamp": "0:04:07"
    }
  },
  {
    "page_content": "hiện 3 lần Conversion Relu, Conversion Relu và Pulling Đây là 1 bộ, rồi 1 bộ nữa Và 1 bộ nữa Như vậy là n trong trường hợp này là bằng 3 và khi thực hiện xong thì nó sẽ đến tầng Fully Connected để thực hiện phần lớp và cái Output của mình đầu ra kỳ vọng nó sẽ ra một cái phân vố sát xúc trong đó cái phần Car, tức là chiếc xe đó, nó cho phân vố cao nhất thì đây chính là một cái kiến trúc mạng CNN phổ dụng thế thì bây giờ tiếp theo, chúng ta sẽ đến với công thức biến đổi của từng lớp biến đổi này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 6,
      "start_timestamp": "0:04:00",
      "end_timestamp": "0:04:50"
    }
  },
  {
    "page_content": "với công thức biến đổi của từng lớp biến đổi này Đầu tiên đó chính là tầng Convolution Tầng Convolution như đã giới thiệu trong phần 1 thì bản chất của phép biến ngộ rỗi Convolution tức là chúng ta sẽ có một cái filter và chúng ta sẽ trượt lên trên toàn bộ tấm ảnh này và ảnh này nối là ảnh xám ảnh ở đây là ảnh xám và khi chúng ta biến đổi xong thì chúng ta sẽ tạo ra một cái feature ở đây không phải feature map là feature nha feature tức là chúng ta mới có một cái đặc trưng thôi rồi kết quả của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 7,
      "start_timestamp": "0:04:48",
      "end_timestamp": "0:05:27"
    }
  },
  {
    "page_content": "ta mới có một cái đặc trưng thôi rồi kết quả của phép biến đổi x với phép biến đổi convolution trên filter w thì nó sẽ tạo ra một cái feature thì đây là phép biến đổi convolution và điều gì xảy ra nếu như chúng ta thực hiện phép biến đổi convolution nhưng mà trên 3 kênh màu là red, green, blue Vì vậy ở đây, một cách tổn quát có thể là có độ sâu Độ sâu trong trường hợp này sẽ là bằng 3 kênh màu Thì ở đây chúng ta sẽ sử dụng một cái filter Nó sẽ có độ sâu tương ứng màu với sổng độ sâu của input",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 8,
      "start_timestamp": "0:05:23",
      "end_timestamp": "0:06:11"
    }
  },
  {
    "page_content": "có độ sâu tương ứng màu với sổng độ sâu của input Thì đây chính là dữ liệu đầu vào Còn đây là filter Và filter này sẽ có độ sâu đúng bằng với độ sâu của input. Và khi chúng ta tưởng tượng filter này giống như là một cục Robic, chúng ta cũng sẽ trượt lên toàn bộ dữ liệu đồ vào này. thì chúng ta sẽ tính ra một cái vị trí này, chúng ta sẽ tính ra được một giá trị dịch chuyển tiếp thì chúng ta sẽ lại tính một giá trị tiếp theo dịch chuyển tiếp chúng ta sẽ dịch chuyển đến một cái vị trí mới, chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 9,
      "start_timestamp": "0:06:08",
      "end_timestamp": "0:06:58"
    }
  },
  {
    "page_content": "ta sẽ dịch chuyển đến một cái vị trí mới, chúng ta sẽ tính ra một cái giá trị cứ như vậy chúng ta sẽ tạo ra một cái feature như vậy đối với phép convolution nhưng mà trên cái dữ liệu đầu vào thay vì là ảnh xám mà là ảnh red and blue thì cái độ sâu của cái filter của mình nó phải đúng bằng cái độ sâu của cái ảnh đầu vào Và như vậy thì kết quả ở đây chúng ta sẽ có là kết quả cho một đặc trưng, tức là một filter Một filter thì chúng ta sẽ ra một đặc trưng Giống như hồi nãy trong slide minh hoại",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 10,
      "start_timestamp": "0:06:44",
      "end_timestamp": "0:07:26"
    }
  },
  {
    "page_content": "đặc trưng Giống như hồi nãy trong slide minh hoại cho lọc Sobeo, thì đặc trưng của lọc Sobeo tương ứng là nó sẽ tìm ra cạnh theo chiều thẳng đứng, theo chiều dọc Với mẫu filter, chúng ta sẽ có một đặc trưng và nhiều filter, chúng ta sẽ tạo ra nhiều đặc trưng Với cảnh đầu vàng, nhân với lại filter màu vàng, chúng ta sẽ tạo ra đặc trưng vàng Với filter màu xanh, chúng ta sẽ tạo ra đặc trưng xanh Cứ như vậy ở đây chúng ta có 4 filter, tương ứng chúng ta sẽ có 4 đặc trưng và chúng ta sẽ trồng lớp 4",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 11,
      "start_timestamp": "0:07:23",
      "end_timestamp": "0:08:20"
    }
  },
  {
    "page_content": "ta sẽ có 4 đặc trưng và chúng ta sẽ trồng lớp 4 đặc trưng này lại với nhau thì nó sẽ tạo ra thành 1 tensor output và trong cái feature này chúng ta sẽ tạo ra 1 khối 3D khối 3D này được gọi chính là tensor và tên của nó gọi là feature map trong slide trước thì nó gọi là Feature còn tập hợp các Feature thì người ta sẽ gọi nó là Feature Map nếu ảnh đầu vào của mình kích thước là 28 thì ảnh đầu ra kích thước sẽ còn 24 là tại vì sao? tại vì khi chúng ta app filter chúng ta trượt lên đây Khi mà chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 12,
      "start_timestamp": "0:08:11",
      "end_timestamp": "0:09:10"
    }
  },
  {
    "page_content": "ta app filter chúng ta trượt lên đây Khi mà chúng ta app lên cái bin của tấm ảnh, chúng ta trượt đến đây và chúng ta sẽ chạm đến cái bin này và nó sẽ không lố ra bên ngoài, do đó nó sẽ bị thất thoát, sẽ bị mất đi, giảm từ 28 xuống còn 24 Đó là lý do tại sao nó giảm xuống Và ở đây thì chúng ta chỉ cần nhớ đến một công thức liên quan đến việc kích thước của filter Nếu như ảnh đầu vào trong trường hợp này có độ sâu là D, thì filter của mình sẽ có độ sâu đúng bằng D luôn, đúng bằng D luôn, tức là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 13,
      "start_timestamp": "0:09:06",
      "end_timestamp": "0:09:48"
    }
  },
  {
    "page_content": "độ sâu đúng bằng D luôn, đúng bằng D luôn, tức là bằng 3. Và ở đây nếu như số lượng filter của mình là K, tổng quát là K, trong trường hợp này K là bằng 4, thì kế độ sâu của tensor output của mình nó cũng chính là bằng k. Có bao nhiêu filter thì ở đây nó sẽ có bấy nhiêu kế độ sâu thì đây là một cái của quy luật chúng ta ráng nhớ công thức của nó. Và ở đây chúng ta sẽ có cái ví dụ tính toán số học ở đây chúng ta sẽ có kế phép biến đổi là convolution với cái dữ liệu ảnh đầu vào là ảnh 5x5 với cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 14,
      "start_timestamp": "0:09:43",
      "end_timestamp": "0:10:38"
    }
  },
  {
    "page_content": "với cái dữ liệu ảnh đầu vào là ảnh 5x5 với cái giá trị như trên khi chúng ta app filter 3x3 lên đây, thì chúng ta sẽ lần lượt lấy các giá trị này nhân với lại các phần tử ở đây, thì không nhân với trường 1 nó sẽ ra là 0 và cứ như vậy, không nhân với 25 nó sẽ ra là 0 và cứ nhân vô thì chúng ta sẽ có được kết quả như thế này và chúng ta sẽ lưu ý là phải thực hiện thêm 1 cái thao tác nữa là tổng tất cả các phần tử trên cái mask này 75, 80, 0 sẽ là 35 Vì vậy, tại vị trí này, khi nhân với filter 3x3",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 15,
      "start_timestamp": "0:10:33",
      "end_timestamp": "0:11:29"
    }
  },
  {
    "page_content": "Vì vậy, tại vị trí này, khi nhân với filter 3x3 này thì nó sẽ tạo ra 1 giá trị đó là 235 và chúng ta sẽ lần lượt trượt từ trái sang phải tương ứng ở đây, chúng ta sẽ điền giá trị ra Và chúng ta sẽ có 1 animation để minh họa cho phép trượt này Ấn đầu vào sẽ là ảnh 5x5 và filter mình là 3x3 Thì chúng ta sẽ cho filter này trượt lên trên vị trí đầu tiên Và chúng ta sẽ thấy ý nghĩa của filter này chính là những con số 0 này khi nhân với các giá trị trên điểm ảnh góc thì nó sẽ trịt tiêu 0 này sẽ là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 16,
      "start_timestamp": "0:11:23",
      "end_timestamp": "0:12:09"
    }
  },
  {
    "page_content": "trên điểm ảnh góc thì nó sẽ trịt tiêu 0 này sẽ là trịt tiêu chỉ còn lại số 1 này ý nghĩa của filter này chính là tổn tất cả các giá trị màu của cảnh đầu vào theo trục dọc này ở đây chúng ta thấy là 0 là cộng 0 là cộng 1 tương ứng nó sẽ là 1 các bạn sẽ thử tự điền vào các giá trị này xem nó là bao nhiêu khi trượt qua đây, nó tương xác là 3 cộng 4 cộng 1 ý nghĩa của filter này là cộng các thành phần trên cột ở giữa 3 cộng 4 cộng 1 sẽ ra là 8 0 cộng 2 cộng 0 sẽ ra là 2 rồi trượt xuống dưới 0 cộng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 17,
      "start_timestamp": "0:12:01",
      "end_timestamp": "0:12:42"
    }
  },
  {
    "page_content": "2 cộng 0 sẽ ra là 2 rồi trượt xuống dưới 0 cộng 1 cộng 0 ra 1 cứ như vậy nó sẽ lấp đầy Và lưu ý là ở đây đầu vào của mình là 3 x 3 nhưng mà khi chúng ta tính với cái filter này xong thì nó chỉ còn lại cái feature là cái thước là 3 x 3 thôi Đầu vào là 5 x 5 thì output của mình chỉ là 3 x 3 thôi Rồi, ở đây chúng ta sẽ có thêm một cái tham số nữa đó là stride tức là cái độ dài của cái bức trượt filter thì ở đây nếu bình thường chúng ta sẽ trượt một đấu bị Ở đây chúng ta sẽ có cái Strike trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 18,
      "start_timestamp": "0:12:38",
      "end_timestamp": "0:13:27"
    }
  },
  {
    "page_content": "một đấu bị Ở đây chúng ta sẽ có cái Strike trong trường hợp này chúng ta sẽ cho Strike là bằng 2 tức là chúng ta sẽ nhảy khóc Rồi Ở trong trường hợp này, ví dụ chúng ta đã làm trong Slide trước thì Strike là bằng 1 nhưng mà bây giờ chúng ta sẽ làm thử với Strike bằng 2 với Strike bằng 2 thì các giá trị ở đây là bao nhiêu thì cái mẹo để chúng ta có thể tính nhanh đó đó chính là chúng ta sẽ lấy các giá trị này chúng ta đình xuống và nhảy khóc, bỏ qua cái này Chúng ta lấy giá trị này, chúng ta đi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 19,
      "start_timestamp": "0:13:24",
      "end_timestamp": "0:14:12"
    }
  },
  {
    "page_content": "qua cái này Chúng ta lấy giá trị này, chúng ta đi hình xuống Nhẹ cốc, bỏ qua và đi hình xuống Nhẹ cốc, bỏ cái năng này đi hình xuống Như vậy, khi chúng ta trượt Rồi, như vậy thì khi chúng ta trượt Thì cái giá trị khi mà với cái bước nhảy Strike bằng 2 đó sẽ là 1, 2, 2, 5 và giá trị này hiểu một cách nona đó là nó cộp p từ feature map ở phía dưới cộp xuống nhưng mà nó bỏ qua hàng và cột này tức là nó đang làm giảm nó đang làm giảm cái độ phân giải của feature map của đặc trưng với cái bút nhảy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 20,
      "start_timestamp": "0:14:09",
      "end_timestamp": "0:14:43"
    }
  },
  {
    "page_content": "của feature map của đặc trưng với cái bút nhảy là stride là 2 và chúng ta sẽ giảm khoảng một nửa Tiếp theo đó chính là padding thì hồi nãy chúng ta đã nói rồi với một ảnh 5x5 sau khi nhân với filter 3x3 thì nó sẽ giảm xuống là còn 3x3 nhưng mà chúng ta mong muốn là giữ nguyên cái thông tin của đặc trưng đồ vào giữ nguyên cái thông tin của đặc trưng đồ vào thì thay vì là giảm xuống còn 3x3 chúng ta mong muốn là không nó vẫn giữ nguyên cái kích thước góc đầu vào là 5x5 thì ở đây chúng ta lấy một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 21,
      "start_timestamp": "0:14:38",
      "end_timestamp": "0:15:21"
    }
  },
  {
    "page_content": "góc đầu vào là 5x5 thì ở đây chúng ta lấy một cái ví dụ nhỏ hơn chúng ta lấy một cái ví dụ nhỏ hơn để dễ tính ảnh đầu vào nếu như kích thước là 3x3 thì khi chúng ta lấy cái filter 3x3 chúng ta trồng lên đây chúng ta thực hiện cái phép tính tổng ở đây đúng không rồi tổng theo cái cọc ở giữa nè thì 4 cộng 1 cộng 4 nó sẽ ra là 9 và kết thúc cái quá trình nhân convolution và cái kích thước của mình nó giảm xuống chỉ còn là 1x1 thì mình không muốn cái điều này mình muốn là giữ nguyên cái kích thước",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 22,
      "start_timestamp": "0:15:19",
      "end_timestamp": "0:16:02"
    }
  },
  {
    "page_content": "điều này mình muốn là giữ nguyên cái kích thước đầu vào mình muốn giữ nguyên kích thước đầu vào thì nếu như ở đây ảnh của mình là 3x3 mình sẽ trèn thêm các cái giá trị ở bên ngoài vào thì nó sẽ tạo ra thành một cái ảnh là 5x5 với các cái giá trị padding ở đây và lấy cái ảnh 5x5 này nhân với filter 3x3 thì nó sẽ tạo ra cái feature map là 3x3 như vậy là 3x3 đầu vào và đầu ra của mình nó bỗng giữ nguyên là 3x3 thì giá trị ở đây chính là giá trị padding và mình có rất nhiều chiến thuật trong trường",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 23,
      "start_timestamp": "0:15:59",
      "end_timestamp": "0:16:41"
    }
  },
  {
    "page_content": "và mình có rất nhiều chiến thuật trong trường hợp này chúng ta đang chèn thêm số 0 vào viền của tấm ảnh sẽ có một số chiến thuật khác thì thật ra theo quan điểm của mình chiến thuật các bạn chọn padding kiểu nào thì nó cũng không ảnh hưởng nhiều lắm đến kết quả cuối cùng tại vì sao? Tại vì tấm ảnh của các bạn là một tấm ảnh rất lớn và object của các bạn trong tấm hình này cũng là object rất lớn. Và việc đưa ra quyết định phân loại nội dung của tấm ảnh này sẽ dựa trên phần rột của tấm ảnh. Chứ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 24,
      "start_timestamp": "0:16:38",
      "end_timestamp": "0:17:21"
    }
  },
  {
    "page_content": "tấm ảnh này sẽ dựa trên phần rột của tấm ảnh. Chứ còn phần biên của tấm ảnh này sẽ không đóng góp nhiều trong việc đưa ra thông tin để phân loại dữ liệu bên trong. do đó phần ngoại biên này không quá quan trọng nó sẽ không quan trọng do đó các bạn dùng chiến thuật padding nào cũng được zero padding, chèn số 0, hoặc là padding theo mỗi chiều nó sẽ có cái thước khác nhau, ví dụ như chèn bên chiều phía trên là 2 nhưng bên chiều dọc thì nó lại chỉ chèn 1 ở đây là chèn theo kiểu là lấy giá trị gần",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 25,
      "start_timestamp": "0:17:16",
      "end_timestamp": "0:18:00"
    }
  },
  {
    "page_content": "chèn 1 ở đây là chèn theo kiểu là lấy giá trị gần nhất để copy ra Ví dụ như đây, số 1 đúng không? thì nó sẽ copy ra đây Số 9 thì nó sẽ copy ra đây Các chiến thuật trẻ này thì theo mình đó là nó cũng không ảnh hưởng nhiều đến kết quả nhận viện cuối cùng Và đến tầng tiếp theo đó chính là tầng activation Tầng activation này thì đây là một cái tầng mà biến đổi phi tuyến Thì như chúng ta đã từng nhận xét trước đó, cái phép Conversion này đó là cái phép biến đổi tuyến tính Nếu như chúng ta thực hiện",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 26,
      "start_timestamp": "0:17:58",
      "end_timestamp": "0:18:43"
    }
  },
  {
    "page_content": "biến đổi tuyến tính Nếu như chúng ta thực hiện cái phép Conversion nối tiếp với một cái phép Conversion mà không có cái phép tuyến tính ở giữa thì có không có một cái phép vi tuyến ở giữa đó, thì đâu đó nó sẽ tạo ra thành một cái tổ hợp một cái tổ hợp tuyến tính mà thôi Tức là tuyến tính và biến đổi tuyến tính thì nó sẽ tạo ra một tổ hợp tuyến tính Mà tổ hợp tuyến tính thì nó sẽ không giải được Nó sẽ không giải quyết được các bài toán phi tuyến Do đó thì chúng ta sẽ phải ngay sau phép tiến đổi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 27,
      "start_timestamp": "0:18:40",
      "end_timestamp": "0:18:43"
    }
  },
  {
    "page_content": "Do đó thì chúng ta sẽ phải ngay sau phép tiến đổi collision chúng ta phải có một tầng activation phi tuyến Trước đây người ta sử dụng hàm sigmoid Nhưng mà gần đây, khi khối lượng dữ liệu lớn, khi kiến trúc mạng càng sâu hơn, thì người ta nhận thấy là đổi từ sigmoid sang relu sẽ giúp cho việc huấn luyện nhanh hơn. Và việc huấn luyện sẽ nhanh hơn. Và việc này là do chúng ta làm giảm hiện đựng vanishing. Radiant Đây là chủ đề thêm để cho các bạn tìm hiểu về sao. Với việc sử dụng tầng Activation là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 28,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "hiểu về sao. Với việc sử dụng tầng Activation là Relu, nó đã giúp cho mình giảm hiện tượng Vanishing Radiant, tức là tiêu biến với đạo hàm. Đạo hàm của mình trong quá trình cập nhật, nó càng lúc càng nhỏ, dẫn đến bước cập nhật của mình sẽ càng chậm. Activation dùng Hamrelo, thì đạo hàm của mình sẽ không bị hiện tượng này và không bị hiện tượng này thì nó sẽ hướng luyện và nhanh hơn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SKcHedTJIL0",
      "filename": "SKcHedTJIL0",
      "title": "[CS431 - Chương 3] Part 2_1: Một số thành phần của mạng CNN",
      "chunk_id": 29,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chúng ta sẽ cài đặt thục bán LiniRech. Chúng ta sẽ cài bằng 3 phiên bản. Đó là chúng ta sẽ dùng tham số theta như là những biến rời này. Theta0, theta1 ở đây. Trong các phiên bản dạng vector hóa, chúng ta sẽ gom tất cả tham số theta 0, theta 1 vào chung một cái biến, đó là theta. Vì việc này sẽ giúp cho chương trình của mình nhìn gọn hơn. Và phiên bản số 3, đó là chúng ta sẽ sử dụng thư viện Keras. Thì cái phiên bản kài đặt sử dụng thư viện Keras sẽ giúp cho chúng ta tiết kiệm được rất nhiều",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:56"
    }
  },
  {
    "page_content": "sẽ giúp cho chúng ta tiết kiệm được rất nhiều công sức trong việc tính đạo hàm. Chúng ta sẽ không cần phải ngồi tính toán các giá trị đạo hàm một cách tường minh mà Keras sẽ tự tính toán và tự tính đạo hàm này cho chúng ta luôn. Đối với phim bảng tham số rời rạt, chúng ta sẽ cài đặt với phim bảng tham số rời rạt. Đối với phim bảng tham số rời rạt, chúng ta sẽ khởi tạo một đoạn code để tạo sinh ra dữ liệu mẫu. Chúng ta sẽ chạy thử đoạn code ở đây. Và như chúng ta thấy, thì ở đây là phương trình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 1,
      "start_timestamp": "0:00:50",
      "end_timestamp": "0:01:39"
    }
  },
  {
    "page_content": "Và như chúng ta thấy, thì ở đây là phương trình đường thẳng mà mình cho trước, đó là trừ 6x cộng 10 Và để tăng thêm tính thật, tức là mình thêm một cái đại lượng nhiễu để cho các cái điểm của mình nó đừng có tri thẳng có tóc, mà nó sẽ giao đoạn xung quanh một cái đường thảo của mình thôi thì ở đây chúng ta sẽ có thêm là cái noise là bằng random theo cái phân bố là normal với tâm, với cái mean là 0 và độ lịch chuẩn của mình là 2 x của mình là cái giá trị từ 1 cho đến 10 với cái mức nhảy là 0.5",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 2,
      "start_timestamp": "0:01:29",
      "end_timestamp": "0:02:33"
    }
  },
  {
    "page_content": "giá trị từ 1 cho đến 10 với cái mức nhảy là 0.5 Vì vậy, để tạo ra các dữ liệu gần với mô hình ở đây, chúng ta sẽ tạo ra một phương trình đường thảo, dự dụng như y là 3x cộng cho 8, tức là một dạng hèn đồng ý. Và điểm ở đây chúng ta sẽ mô phỏng bằng các điểm màu xanh, Chúng ta sẽ sửa lại chương trình này một chút xíu để các điểm mình generate ra ở đây nhìn giống 1 chút Đây sẽ là 3x gọn cho 8 Các điểm ở đây là các điểm màu xanh rồi Chúng ta muốn cho giao động này nhìn có vẻ lớn hơn, thì chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 3,
      "start_timestamp": "0:02:22",
      "end_timestamp": "0:03:18"
    }
  },
  {
    "page_content": "giao động này nhìn có vẻ lớn hơn, thì chúng ta có thể để ở đây là mũ Standard deviation bằng mũ. Chúng ta thấy là kế độ rộng của nó và giao động của điểm x này nó lớn hơn sau với Standard deviation bằng mũ. Tiếp tục tài huấn luyện, tham số đời đạt, tham số theta 0 và theta 1 là ngộ nhiên Chúng ta sẽ hiện thực hóa chương trình theta 0 và ngộ nhiên cho giá trị trường 123 Theta 1 là 4,5,6 Chúng ta có thể sử dụng hàm random, nhưng ở đây chúng ta sẽ gắn trực tiếp giá trị luôn để cho nó đơn giản Tiếp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 4,
      "start_timestamp": "0:03:10",
      "end_timestamp": "0:04:36"
    }
  },
  {
    "page_content": "trực tiếp giá trị luôn để cho nó đơn giản Tiếp theo, chúng ta sẽ khởi tạo các giá trị alpha và epsilon. Thì giá trị alpha, thông thường là những giá trị bé. Chúng ta sẽ để alpha bằng 0.01 và epsilon là 0.001. Tiếp theo, chúng ta sẽ kệ đặt vòng lập và cập nhật tham số theta 0 dựa trên cái input này. Theta 0 là bằng theta 0 trừ alpha nhân cho đạo hàm của hàm loss theo theta 0. Theta 0 được tính bởi cái công thức ở đây. Còn đạo hàm của Theta, đạo hàm của hàm loss theo Theta 1 thì chúng ta sẽ có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 5,
      "start_timestamp": "0:04:23",
      "end_timestamp": "0:05:20"
    }
  },
  {
    "page_content": "hàm của hàm loss theo Theta 1 thì chúng ta sẽ có cái công thức ở đây. Ừ, rồi, ý nghĩa của cái công thức này đó là gì? Đạo hàm của hàm loss theo Theta 0 nó sẽ là bằng trung bình cọng 1,9, của tổng là trung bình cọng của, ở đây chính là, giá trị đoán trừ giá trị kỹ tế. Chúng mình cộng 1 củ giá trị chỉ đoán Trừ cho giá trị thực tế, đối với chúng ta sẽ có thêm 1 thành phần nhân vxe ở đây nữa Khác công thức ở bên đây 1 chút xíu, đó là có cái thành phần này Chúng ta sẽ tiến hành kể đặt y, trúc,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 6,
      "start_timestamp": "0:05:15",
      "end_timestamp": "0:06:38"
    }
  },
  {
    "page_content": "phần này Chúng ta sẽ tiến hành kể đặt y, trúc, theta, không vàng theta, không, trừ cho alpha, nhân với lại trung bình cọng Bên này chúng ta thấy là trung bình cọng, thì nó sẽ là np.min Và giá trị dự đoán, tức là x nhân bán lại theta 1 Cộng cho theta 0 Trừ cho y Rồi, tương tự như vậy Theta 1 là bằng Theta 1 trừ cho alpha và ở đây khi chúng ta thực hiện bên trong hàm NP.min thì chúng ta phải có chú ý là chúng ta phải nhuôn thêm cái thành phần nữa là x ở đây sau đó, điều kiện dừng là nếu trị tiết",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 7,
      "start_timestamp": "0:06:27",
      "end_timestamp": "0:07:31"
    }
  },
  {
    "page_content": "là x ở đây sau đó, điều kiện dừng là nếu trị tiết đối của đạo hàm HL theo theta 0 và trị tiết đối của đạo hàm HL theo theta 1 mà bé hơn 1 cái ngữ, thì chúng ta sẽ dùng gip abs Chị thiệt đối của đạo hềm thì chúng ta sẽ cộp y các giá trị mà chúng ta đã tính ở trên. Bé hơn epsilon, trị thiệt đối, chúng ta sẽ cộp y tương tự tại đây. Bé hơn epsilon thì chúng ta sẽ crack. Bây giờ mình sẽ tiến hành trải thử chương trình này May quá không có lỗi Thế thì ở đây chúng ta sẽ xem coi theta không là bao",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 8,
      "start_timestamp": "0:07:25",
      "end_timestamp": "0:08:39"
    }
  },
  {
    "page_content": "thì ở đây chúng ta sẽ xem coi theta không là bao nhiêu Chúng ta sẽ in ra là print theta không Theta 1 Theta 0 là 7,7 Nếu chúng ta so vào công thức gốc ở đây, chúng ta thấy là 7,7 gần với con số 8 Theta 1 sẽ ra là 2,97 Nó sẽ gần với con số 3 Rõ ràng chúng ta thấy là trong thuộc bán này, mình không thể sử dụng công thức tường minh của model y bằng 3x gọn 8 Mọi thứ chỉ được tính toán dựa trên các điểm lấy mẫu Chúng ta không hề biết trước công thức này Nhưng sau khi huấn luyện xong thì các giá trị",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 9,
      "start_timestamp": "0:08:37",
      "end_timestamp": "0:09:33"
    }
  },
  {
    "page_content": "này Nhưng sau khi huấn luyện xong thì các giá trị theta 0 và theta 1 đều sắp xỉ với lại công thức mà chúng ta đã chọn ban đầu Bây giờ chúng ta sẽ đến mức thứ 3 đó là chúng ta sẽ trực quan hóa môi này Để trực quan hóa môi này chúng ta chỉ việc tập vi kế đạn khố trên đây đồng thời chúng ta sẽ và vẽ thêm hàm vô hình dự đoán cho tương trình giá trị x đầu vào của mình sẽ là x giá trị dự đoán của mình sẽ là x dân với theta 1 cộng cho theta 0 Chúng ta sẽ không có thêm số là PO, tại vì ở dòng trên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 10,
      "start_timestamp": "0:09:23",
      "end_timestamp": "0:10:11"
    }
  },
  {
    "page_content": "ta sẽ không có thêm số là PO, tại vì ở dòng trên chúng ta đang vẽ dưới dạng điểm, còn ở bên dưới chúng ta đang muốn vẽ mô hình dưới dạng đường, chúng ta sẽ không có thêm số này. Sau khi vẽ xong, chúng ta thấy là đường thẳng mô hình của mình đi xuyên qua đám bay điểm của dự kiểu mẫu. Điều đó cho thấy là cái môn của mình rất là khớp Rồi, bây giờ chúng ta sẽ tiến hành qua cái bước kẻ đặt tiếp theo, cái phiên bản kẻ đặt tiếp theo, đó chính là phiên bản vector hóa Thì trong cái phiên bản vector hóa",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 11,
      "start_timestamp": "0:10:05",
      "end_timestamp": "0:10:59"
    }
  },
  {
    "page_content": "bản vector hóa Thì trong cái phiên bản vector hóa này thì cái tham số theta của mình nó sẽ được khởi tạo ngộ duyên Và theta này bản chất nó chính là một cái bộ các cái giá trị theta 0 và theta 1 Chúng ta sẽ có công thức trực tiếp cho tính đạo hềm của hềm loss theo theta ở đây. Chúng ta sẽ cộp bình luận chương trình ở phía trên lại. Thay vì chúng ta tính, chúng ta để hai giá trị theta 0 và theta 1 là hai giá trị rời nè, thì chúng ta sẽ gom nó lại thành một biến theta. Chúng ta lưu ý là, trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 12,
      "start_timestamp": "0:10:50",
      "end_timestamp": "0:11:52"
    }
  },
  {
    "page_content": "thành một biến theta. Chúng ta lưu ý là, trong trường hợp này, thê ta của mình sẽ là một khi ma trận kích thước là 2 như 1. Khi khởi tạo ở đây, chúng ta sẽ để là np.array và chúng ta phải để nó là ma trận 2 như 1. Trong đó giá trị đầu tiên là 123 và giá trị tiếp theo sẽ là 456. Rồi, chúng ta sẽ bỏ 2 giá trị thê ta không thê ta 1 ở đây. Trong công thức cập nhật này, chúng ta cũng sẽ sửa lại theta vào theta trừ cho alpha nhau Ở đây chúng ta sẽ là nhân với là x, nhân cho theta chuyển viện như x",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 13,
      "start_timestamp": "0:11:41",
      "end_timestamp": "0:12:45"
    }
  },
  {
    "page_content": "nhân với là x, nhân cho theta chuyển viện như x trừ y Để thực hiện được cái này, chúng ta sẽ phải có thêm cái biến x, cái ma trình x x của mình sẽ là hàng đầu tiên sẽ là các giá trị bias hàng ở dưới chính là các giá trị x1, x2 và khi có x này rồi thì chúng ta mới có thể thực hiện được công thức tính đạo hàm ở đây Chúng ta sẽ dùng đạo hàm ở trong biến rát, trương ứng là radian Tại vì ở đây chúng ta tính đạo hàm theo một vector, chúng ta phải dùng từ là radian Đạo hàm theo từng biến thì nó sẽ là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 14,
      "start_timestamp": "0:12:35",
      "end_timestamp": "0:13:40"
    }
  },
  {
    "page_content": "từ là radian Đạo hàm theo từng biến thì nó sẽ là derivative Còn đạo hàm theo vector thì chúng ta phải dùng là rad Rồi, ở đây sẽ là nhân cái rad và trong đó rad thì sẽ được tính... Chúng ta sẽ có 1 cái ma trận x x sẽ là hàng đầu tiên, con số 1 là np.once và số chiều của nó sẽ là 1 và số dòng sẽ là 1 và số hàng sẽ tương ứng là số phần tử của mình thì mình có thể để là lend khuôn x thì mình có thể để là len của y, tức là số phần tử Rồi đây là cái hàng đầu tiên sang cái hàng thứ 2 đó chính là x",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 15,
      "start_timestamp": "0:13:35",
      "end_timestamp": "0:14:37"
    }
  },
  {
    "page_content": "hàng đầu tiên sang cái hàng thứ 2 đó chính là x nhưng mà lưu ý, x ở đây á, ban đầu á, nó là một vector chúng ta phải convert cái x này về cái dạng là ma trộn có số hàng là 1 và số cột sẽ là số phần tử RESET số hàng là 1 Số dòng, nếu muốn khai báo thường minh, để là landA cũng được Nếu mà nó hơi dài, thì ở đây mình có thêm một cái mẹo, đó là chúng ta sẽ để là triều 1 Tức là chương trình sẽ tự tính số cọt mình là bao nhiêu dựa trên số cọt x3 đầu Chúng ta sẽ phải nối hai cái hàng này lại với nhau",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 16,
      "start_timestamp": "0:14:28",
      "end_timestamp": "0:16:01"
    }
  },
  {
    "page_content": "ta sẽ phải nối hai cái hàng này lại với nhau và chúng ta sẽ xử dụng thư biện vã hình nâng p.concat Rồi, ở đây chúng ta sẽ hiện thực khóa công thức cho đạo hàm đó là 1 phần n, chúng ta sẽ làm 1 chia cho len của i Rồi nhân với lại x, chúng ta sẽ nhân với lại thành phần đầu tiên đó chính là x Rồi thành phần thứ 2 sẽ là một cái kết quả theta chuyển bị x trừ y chuyển bị Rồi nhân với x là nhân với lại một cái thành phần chuyển bị Thì bên trong này đó sẽ là, là nó ý là x ở đây chúng ta sẽ là .doc Rồi,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 17,
      "start_timestamp": "0:15:58",
      "end_timestamp": "0:17:08"
    }
  },
  {
    "page_content": "là, là nó ý là x ở đây chúng ta sẽ là .doc Rồi, và chúng ta sẽ tính ở đây Sau khi tính xong thì chúng ta sẽ phải cập nhật lại đạo hàm này thêm một lần nữa Sẽ tính lại đạo hàm này thêm một lần nữa Và để lấy phần tử đầu tiên thì sẽ là RAT Không Không là phần tử đầu tiên Thịt phần thứ 2 sẽ là 1 không Ở đây thì có cái lỗi, ở đây chúng ta sẽ có cái lỗi đó là chỉ có lỗi Only integer scalar can be converted to scalar index Khi có lỗi xảy ra, mình sẽ thử in ra các giá trị để xem có đúng ý đồ của mình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 18,
      "start_timestamp": "0:17:03",
      "end_timestamp": "0:17:31"
    }
  },
  {
    "page_content": "in ra các giá trị để xem có đúng ý đồ của mình hay không Đầu tiên, toàn card này sẽ phải gói lại Chúng ta sẽ gói hàng số 1 với x và gói lại trong một kế bộ tuyệt vời. sau đó chúng ta mới truyền vào bên trong của bàn cat tế bay Ok, đã chạy được rồi Bây giờ chúng ta sẽ cùng kiểm tra xem cái giá trị theta của mình sau khi chạy xong, nó có giá trị là bao nhiêu? Nếu chúng ta so với lại giá trị đạt được trend trước đây thì chúng ta thấy giá trị nó giống nhau theta 0 là 7,7 và theta 1 là bằng 2,9 7,7,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 19,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "nhau theta 0 là 7,7 và theta 1 là bằng 2,9 7,7, 2,9 Vì vậy là nó rất hợp với cách kề đặt sử dụng tham số đời này Và tương tự như vậy thì chúng ta cũng sẽ trực quan hoái và không lượt tuyên nếu như kết quả nó ra giống như là cái mồm ở đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPoJ8VS7nLc",
      "filename": "sPoJ8VS7nLc",
      "title": "[CS431 - Chương 2] Part 2b_1: Cài đặt mô hình linear regression",
      "chunk_id": 20,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chúng ta sẽ qua phiên bản cài đặt. Tiếp theo, quan trọng nhất và nó sẽ được xử dụng xuyên suốt cho môn học này của mình. Đó là chúng ta sẽ xử dụng thư viện TensorFlow và Keras. Với cách cài đặt mà xử dụng Keras, nó sẽ giúp chúng ta đỡ phải đi tính đạo hàng. Tại vì nó đã hỗ trợ cho mình việc tính đạo hàng. và các thư viện liên quan đến tối ưu hóa Optimizer để hữu luyện Ở đây chúng ta sẽ có một bộ khung cài đặt Chúng ta sẽ phải tuân thủ để từ nay về sau việc cài đặt sẽ đi theo đúng frame như thế",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPqwytzfxqM",
      "filename": "sPqwytzfxqM",
      "title": "[CS431 - Chương 2] Part 2b_2: Cài đặt mô hình linear regression",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:42"
    }
  },
  {
    "page_content": "về sau việc cài đặt sẽ đi theo đúng frame như thế này Đối tượng của mình sẽ được cài đặt bằng một dạng Class Và My Model sau này chúng ta có thể đổi nó thành tên của keyboard của mình Init chính là cái constructor nếu như chúng ta muốn có những cái khởi tạo bò bào bằng đầu hoặc không thì chúng ta sẽ return quần đơn phương thức tiếp theo rất là quan trọng và bắt phù phải có đó chính là phương thức build phương thức build này để cho chúng ta biết kiến trúc của mô hình nó như thế nào phương thức",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPqwytzfxqM",
      "filename": "sPqwytzfxqM",
      "title": "[CS431 - Chương 2] Part 2b_2: Cài đặt mô hình linear regression",
      "chunk_id": 1,
      "start_timestamp": "0:00:36",
      "end_timestamp": "0:01:18"
    }
  },
  {
    "page_content": "kiến trúc của mô hình nó như thế nào phương thức tiếp theo cũng rất là quan trọng đó chính là phương thức train dùng để hướng luyện mô hình với các data train của mình Phương thức thứ 2 là lưu mô hình này xuống dưới file Nếu quá trình train mô hình tốn rất nhiều thời gian, có thể lên đến vài tiếng hoặc vài ngày thì việc lưu mô hình này sẽ giúp chúng ta tái sử dụng được mô hình này sau Và đi kèm với phương thức cell thì chắc chắn nó sẽ có phương thức gọi là phương thức load để load mô hình này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPqwytzfxqM",
      "filename": "sPqwytzfxqM",
      "title": "[CS431 - Chương 2] Part 2b_2: Cài đặt mô hình linear regression",
      "chunk_id": 2,
      "start_timestamp": "0:01:12",
      "end_timestamp": "0:01:56"
    }
  },
  {
    "page_content": "thức gọi là phương thức load để load mô hình này lên từ file Rồi, chúng ta sẽ tóm tác mô hình thông qua phương thức summary để cho biết kiến thúc mô hình này bao gồm các lớp nào và số thêm số của từng lớp ra sao. Và cuối cùng, đó chính là phương thức Redict. Chúng ta sẽ đưa ra dự đoán dựa trên input đầu vào. Rồi, bây giờ chúng ta sẽ copy toàn mộ nội dung ở đây và chúng ta sẽ hiện thực hóa nó cho mô hình Linear Regression. Đầu tiên là MyModel, chúng ta sẽ để là LinearRegression Để mà có thể hiện",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPqwytzfxqM",
      "filename": "sPqwytzfxqM",
      "title": "[CS431 - Chương 2] Part 2b_2: Cài đặt mô hình linear regression",
      "chunk_id": 3,
      "start_timestamp": "0:01:44",
      "end_timestamp": "0:03:09"
    }
  },
  {
    "page_content": "ta sẽ để là LinearRegression Để mà có thể hiện thực khóa được cho phương thức build thì chúng ta sẽ phải farm keras.tile Lớp input là lớp đầu tiên Tiếp theo là để tính ra đồ đồ đen Lớp dance là kết nối đầy đủ với đồ đen Đó là lý do tại sao nó đặt tên là dance Rồi Rồi bây giờ chúng ta sẽ tiến hành kể đặt phương thích build Và để cho tổng quát thì cái build này chúng ta sẽ phải cho biết số input dimension của mình là bao nhiêu trong trường hợp này, input dimension của mình là 1 nhưng một cách",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPqwytzfxqM",
      "filename": "sPqwytzfxqM",
      "title": "[CS431 - Chương 2] Part 2b_2: Cài đặt mô hình linear regression",
      "chunk_id": 4,
      "start_timestamp": "0:02:55",
      "end_timestamp": "0:04:12"
    }
  },
  {
    "page_content": "này, input dimension của mình là 1 nhưng một cách tổng quát, chúng ta nên để tham số máy cho m này là input dimension Chúng ta sẽ đi lần lực từ trái qua phải Đầu tiên sẽ là tạo nớp input Nó sẽ cho biết là cái shape, tức là cái kích thước của nó là bao nhiêu shape là quy định input tên là phải, tức là nó chỉ là 1 cái vector thôi rồi, chúng ta sẽ trả về đây sẽ là input trả về 1 cái biến tên là input tiếp theo, chúng ta sẽ tính cái output output của mình là kết quả của phép biến đổi là dense đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPqwytzfxqM",
      "filename": "sPqwytzfxqM",
      "title": "[CS431 - Chương 2] Part 2b_2: Cài đặt mô hình linear regression",
      "chunk_id": 5,
      "start_timestamp": "0:04:01",
      "end_timestamp": "0:05:07"
    }
  },
  {
    "page_content": "mình là kết quả của phép biến đổi là dense đây chính là output đây chính là giá trị dự đoán Output là kết quả của vết biến đổi đen kết nối đầy đủ Trong dance này, chúng ta phải chỉ cho phương thức này biết output sẽ có bao nhiêu nodes Trong trường hợp này, chúng ta chỉ có duy nhất 1 node, chúng ta sẽ để là 1 Activation là tính tán tính tính mà không có Activation nào Giờ thì sẽ để Activation là bài lên Và trong hàm logistic thì tham số Activation sẽ là hàm segue Chúng ta có sử dụng BIAS hay",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPqwytzfxqM",
      "filename": "sPqwytzfxqM",
      "title": "[CS431 - Chương 2] Part 2b_2: Cài đặt mô hình linear regression",
      "chunk_id": 6,
      "start_timestamp": "0:05:00",
      "end_timestamp": "0:06:01"
    }
  },
  {
    "page_content": "sẽ là hàm segue Chúng ta có sử dụng BIAS hay không? Chúng ta sẽ có sử dụng, chúng ta sẽ để là trục Đây là lớp biến đổi, lớp biến đổi này phải cho biết những truyện layer đầu vào là gì Vì vậy, cái cú pháp của mình sẽ ra để mở vào input Lưu ý đây là biến input này Biến input này là cái lớp biến input này Chứ nó không phải là module và class tên là input phí hoa Chứ ý ở đây, nó phải là ký niến đã tạo ra ở bữa trưa đó Rồi, cuối cùng chúng ta sẽ đóng gói toàn bộ input và output này để đóng gói,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPqwytzfxqM",
      "filename": "sPqwytzfxqM",
      "title": "[CS431 - Chương 2] Part 2b_2: Cài đặt mô hình linear regression",
      "chunk_id": 7,
      "start_timestamp": "0:05:56",
      "end_timestamp": "0:06:51"
    }
  },
  {
    "page_content": "đóng gói toàn bộ input và output này để đóng gói, chúng ta sẽ sử dụng một cái đối tượng tên là model.import from Keras.import model Một cái thụ tính là cell.module Rồi Và ở đây thì chúng ta cũng không có nhu cầu phải trả về cái model này mà mọi thứ nó sẽ được đóng gói hết vô bên trong Rồi ở đây return, thì ta sẽ không return gì hết Đối với phương thức trend, chúng ta sẽ phải trả về mô đồ này Một cái thụ tính là cell.module đối với phương thức Trend, chúng ta phải cung cấp cho mô đồ biết dự kiện",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPqwytzfxqM",
      "filename": "sPqwytzfxqM",
      "title": "[CS431 - Chương 2] Part 2b_2: Cài đặt mô hình linear regression",
      "chunk_id": 8,
      "start_timestamp": "0:06:47",
      "end_timestamp": "0:07:47"
    }
  },
  {
    "page_content": "chúng ta phải cung cấp cho mô đồ biết dự kiện đau vào là X, Xtrend và output mình mong muốn nó sắp xỉ là Etrend Vì vậy mình sẽ phải cung cấp một bộ là X, Y, Xtrain và Ytrain Vì vậy thì ở đây phải có thêm item số nữa là Xtrain Tại vì train nó phải đi theo một cặp Xe Và trong tương lai thì chúng ta hoàn toàn có thể đưa thêm các cặp biến nữa đó là validation để kiểm tra xem cái model của mình khi mình train thì loss cho validation có giảm theo hay không cell . để thực hiện được ham train này chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPqwytzfxqM",
      "filename": "sPqwytzfxqM",
      "title": "[CS431 - Chương 2] Part 2b_2: Cài đặt mô hình linear regression",
      "chunk_id": 9,
      "start_timestamp": "0:07:41",
      "end_timestamp": "0:08:50"
    }
  },
  {
    "page_content": "cell . để thực hiện được ham train này chúng ta phải cho model biết optimizer sẽ sử dụng là gì ở đây mình sẽ để là tensorflow.keras.optimizer. ở đây nó sẽ không biết tensorflow là gì, ở đây nó sẽ phải là import tensorflow.htf Chúng ta sẽ sử dụng Stochastic Gradient Descent Tuy nhiên ở đây nó sẽ có một số optimizer khác Ví dụ như là Adam Mặc định nếu chúng ta không biết gì hết về môn hình Chúng ta có thể sử dụng Adam Tuy nhiên môn Linear Reaction là một môn rất đơn giản sau đó chúng ta sẽ sử",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPqwytzfxqM",
      "filename": "sPqwytzfxqM",
      "title": "[CS431 - Chương 2] Part 2b_2: Cài đặt mô hình linear regression",
      "chunk_id": 10,
      "start_timestamp": "0:08:47",
      "end_timestamp": "0:09:38"
    }
  },
  {
    "page_content": "là một môn rất đơn giản sau đó chúng ta sẽ sử dụng không cụ vừa phải, đó là sgd và khi khai báo kế kế Stochastic Gradient Descent thì chúng ta phải cho nó biết là learning rate của mình là bằng bao nhiêu thì by default chúng ta có thể sử dụng là 0.01 Rồi, đồng thời là chúng ta sẽ phải cho cái model của mình nhận biết cho cái model của mình, nhận biết cái optimizer này Chúng ta sẽ truyền vào optimizer và phải cho nó biết hàm loss Ở đây trong RAS nó cũng hỗ trợ cho mình luôn các hàm loss phổ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPqwytzfxqM",
      "filename": "sPqwytzfxqM",
      "title": "[CS431 - Chương 2] Part 2b_2: Cài đặt mô hình linear regression",
      "chunk_id": 11,
      "start_timestamp": "0:09:31",
      "end_timestamp": "0:10:28"
    }
  },
  {
    "page_content": "RAS nó cũng hỗ trợ cho mình luôn các hàm loss phổ biến, ở đây chúng ta sẽ xử lý mse Sau khi model của mình đã được đóng gói, đã được compile với phương thức xử lời với tham số optimizer và hàm loss loss thì chúng ta sẽ sell.module.fit chúng ta sẽ fit isTrend và isTrend rồi thì chúng ta sẽ trả cái tham số các cái quá trình hướng luyện nó có những cái tham số nào ví dụ giá trị loss nó trả như thế nào nó gọi là history thì nó sẽ trả ra đây sau khi train xong, chúng ta sẽ lưu mô đồ này xuống chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPqwytzfxqM",
      "filename": "sPqwytzfxqM",
      "title": "[CS431 - Chương 2] Part 2b_2: Cài đặt mô hình linear regression",
      "chunk_id": 12,
      "start_timestamp": "0:10:26",
      "end_timestamp": "0:11:25"
    }
  },
  {
    "page_content": "train xong, chúng ta sẽ lưu mô đồ này xuống chúng ta sẽ phải có thêm một tham số đó là mô đồ thách là đường dẫn đến phai môn của mình và hạm này thì chúng ta cũng không có nhu cầu phải trả về gì hết chúng ta sẽ để cell.mô đồ.cell và chúng ta sẽ để mô đồ thách, đường dẫn đến nơi mình ngủ luôn Lát mode lên thì chúng ta cũng sẽ phải có đường dẫn đến nơi mình đã lưu Và đây sẽ là Sale.mode Ở đây chúng ta sẽ có thêm một cái phương thức nữa của Keras Đó là Keras.mode Chúng ta sẽ import phương thức là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPqwytzfxqM",
      "filename": "sPqwytzfxqM",
      "title": "[CS431 - Chương 2] Part 2b_2: Cài đặt mô hình linear regression",
      "chunk_id": 13,
      "start_timestamp": "0:11:20",
      "end_timestamp": "0:12:15"
    }
  },
  {
    "page_content": "là Keras.mode Chúng ta sẽ import phương thức là Load mode Lót model Rồi ở đây sẽ là lót model và chúng ta sẽ truyền cái đường dẫn vào bộ đồ bản Sau khi lóa xong, nó sẽ trả vào cái biến self.model này Rồi summary thì self.model.summary Rồi để dự đoán thì chúng ta sẽ phải có một cái biến đồ vào đó là isted Chúng ta sẽ không có edit, tại vì khi dự đoán mà chúng ta đâu có cái nhãn của kế hoạch trả về đâu, chúng ta chỉ có cái input đầu vào thôi để edit Và ở đây sẽ là cell.model.predit.exit Rồi, bây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPqwytzfxqM",
      "filename": "sPqwytzfxqM",
      "title": "[CS431 - Chương 2] Part 2b_2: Cài đặt mô hình linear regression",
      "chunk_id": 14,
      "start_timestamp": "0:12:10",
      "end_timestamp": "0:13:04"
    }
  },
  {
    "page_content": "Và ở đây sẽ là cell.model.predit.exit Rồi, bây giờ chúng ta sẽ kệ thử Maywax không có lỗi Bước tiếp theo chúng ta sẽ khởi tạo mô hình và phấn luyện với cái dự liệu mẫu Mô hình sẽ là linear impression và chúng ta sẽ đưa vào một cái biến đó là link-pressed Tiếp theo, chúng ta sẽ gọi trà hàm build và phải có một tham số đó là input-dimension linh freh.build tham số ở đây là 1 tại vì sao? tại vì ở trong dữ liệu này chúng ta chỉ có duy nhất 1 biến đồ vào ở đây chúng ta sẽ không tính thành phần Bay",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPqwytzfxqM",
      "filename": "sPqwytzfxqM",
      "title": "[CS431 - Chương 2] Part 2b_2: Cài đặt mô hình linear regression",
      "chunk_id": 15,
      "start_timestamp": "0:13:00",
      "end_timestamp": "0:14:35"
    }
  },
  {
    "page_content": "vào ở đây chúng ta sẽ không tính thành phần Bay As tại vì trong lớp dense ở trên đây nó đã có tham số cho biết có sử dụng Bay As hay không input điện vào đây chỉ là cái nhĩa liệt thô bàn đầu, không tính thành phần bài x, là phải chú ý cho đó ha Rồi ở đây build sẽ là bầu 1 và chúng ta sẽ tiến hành là trend và chúng ta sẽ truyền vào x và y, x và y chính là cái cặp, các cái cặp dữ liệu rồi Invalid argument compile optimizer Keras.optimizer giá trị loss của mình trả ra là đến 1000 Rõ ràng đây là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPqwytzfxqM",
      "filename": "sPqwytzfxqM",
      "title": "[CS431 - Chương 2] Part 2b_2: Cài đặt mô hình linear regression",
      "chunk_id": 16,
      "start_timestamp": "0:14:30",
      "end_timestamp": "0:15:37"
    }
  },
  {
    "page_content": "loss của mình trả ra là đến 1000 Rõ ràng đây là một giá trị rất lớn Thế thì chúng ta sẽ phải tham số hóa số lượng nâng ipop ipop nghĩa là dạ sử dụ liệu chúng ta có 100 mổ thì 1 ipop nghĩa là chúng ta train hết 100 mổ dụ liệu này và nếu chúng ta muốn train, train lại nhiều lần Chúng ta sẽ phải khai báo cho nó biết là số lần chúng ta muốn trainy train lại dự hiệu Ví dụ như chúng ta muốn trainy train lại là 500 lần Chúng ta sẽ để là epoch bằng 500 Và ở đây chúng ta mô đồ chân fit thì chúng ta sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPqwytzfxqM",
      "filename": "sPqwytzfxqM",
      "title": "[CS431 - Chương 2] Part 2b_2: Cài đặt mô hình linear regression",
      "chunk_id": 17,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Và ở đây chúng ta mô đồ chân fit thì chúng ta sẽ phải có Là epoch Là bài num epoch và khi kết thúc thuật toán chạy hết 500 epoch thì nó giảm xuống còn là khoảng 10 thôi từ 300 xuống còn 10",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=sPqwytzfxqM",
      "filename": "sPqwytzfxqM",
      "title": "[CS431 - Chương 2] Part 2b_2: Cài đặt mô hình linear regression",
      "chunk_id": 18,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Môn logistic regression cũng được phát triển từ môn học tông quát. Chúng ta nhắc lại, đầu vào là chúng ta sẽ có dữ liệu x và đầu ra chúng ta sẽ có dữ liệu y. Tùy vào tính chất của cặp dữ liệu xy này, mình sẽ thiết kế các hàm môn dự đoán s theta x và hàm độ lỗi dự đoán l theta xi. Còn công việc số 3 là tìm theta so cho hàm đổ lộ nhỏ nhất này, thì chúng ta cũng đã có công cụ đó là thuộc toán gradient descent. Đối với môi logistic regression, chúng ta sẽ phải đi giải quyết một bài toán, trong đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:47"
    }
  },
  {
    "page_content": "ta sẽ phải đi giải quyết một bài toán, trong đó chúng ta phải phân ra làm 2 lớp, xanh và cam ở đây. 2 x1 của x2 chính là đặc trưng đầu vào, trong trường hợp này chúng ta sẽ lấy mặt phẳng 2 chiều và chúng ta sẽ phải phân tách 2 tập điểm xanh và một cái em này ra làm 2 phần và trong trường hợp này thì dữ liệu của mình nó gọi là phân tách được một cách tiến tính hay còn gọi là linear separable thì ở đây chúng ta sẽ có được một cái đường thẳng tách ra làm 2 thì theo như kiến thức toán tấp 2, tấp 3",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 1,
      "start_timestamp": "0:00:35",
      "end_timestamp": "0:01:20"
    }
  },
  {
    "page_content": "ra làm 2 thì theo như kiến thức toán tấp 2, tấp 3 mà chúng ta đã học thì với phương trình đường thẳng này chúng ta có thể biết nó như dạng là ax1 cộng cho bx2 cộng cho c bằng 0 và tất cả những điểm nào mà nằm trên đường thẳng này thì khi thế vào các điểm x1, x2 nằm trên đường thẳng này thế vào thì chúng ta sẽ có giá trị bằng 0 Bây giờ chúng ta sẽ làm quen với bộ tham số Theta 1 x 1 cộng Theta 2 x 2 cộng Theta 0. Thì nếu những điểm nào nằm trên đường thẳng này thì nó sẽ ra bằng không. Còn những",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 2,
      "start_timestamp": "0:01:17",
      "end_timestamp": "0:02:14"
    }
  },
  {
    "page_content": "thẳng này thì nó sẽ ra bằng không. Còn những điểm nào nằm về phía bên trên, ví dụ như ở đây, Thế vô thì sẽ ra giá trị lớn hay không. Còn những điểm nằm dưới như vậy thì sẽ là nằm đỉnh nằm hay không. Như vậy dựa trên quan sát, kiến thức tóm tóm cấp 3, cấp 2, cấp 3 mà chúng ta đã học được thì chúng ta sẽ thiết kế hàm dự đoán bằng dạng như trên. Đó là fθx1 x2, x là dự kiện vô vào, hai đặc trưng vô vào. Nó sẽ làm bằng 1, tức là cái nhãn y này là bằng 1. Nếu theta0 cộng ra theta1 x1 cộng ra theta2",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 3,
      "start_timestamp": "0:02:08",
      "end_timestamp": "0:02:57"
    }
  },
  {
    "page_content": "1. Nếu theta0 cộng ra theta1 x1 cộng ra theta2 x2 theta0 x1 cộng ra theta1 x1 cộng ra theta x2 lứa 1 bằng 0, tức là nó thuộc về một nửa cái mặt phẳng này, thì nó sẽ đợt gán giá trị là 1. và nó sẽ bán bằng 0, cái nhãn dự đoán của mình sẽ bán bằng 0 nếu như theta 0 cộng cho theta 1 x 1 cộng theta 2 x 2 nó bẫy hơn 0, tức là nó nằm về một lửa phía bên này thì nếu như chúng ta thiết kế cái hàm dự đoán như thế này thì điều gì sẽ xảy ra? điều gì sẽ xảy ra? đó là hàm này là hàm không liên tục Hàm này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 4,
      "start_timestamp": "0:02:50",
      "end_timestamp": "0:03:41"
    }
  },
  {
    "page_content": "ra? đó là hàm này là hàm không liên tục Hàm này là hàm không liên tục. Hàm không liên tục thì sau này khi chúng ta đến cái bút số 3, chúng ta tính đạo hàm sẽ rất là khó. Do đó chúng ta phải cố gắng tế kế hàm fθx sao cho nó phải là một cái hàm liên tục. Chúng ta sẽ tìm cách thi kế bằng cách dựa trên quan sát đó là miền giá trị của theta 0, tức là miền giá trị của giá trị này, của ký phép tính này nè nó sẽ thuộc cái đoạn là từ trừ vô cụm cho đến cọng vô cụm trong khi đó trong khi đó cái giá trị",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 5,
      "start_timestamp": "0:03:39",
      "end_timestamp": "0:04:30"
    }
  },
  {
    "page_content": "cọng vô cụm trong khi đó trong khi đó cái giá trị mà mình mong muốn dự đoán nó sẽ nhận hai giá trị là 0 và 1 trong khi đó cái miền giá trị của theta 0 cộng cho theta 1 x1 cộng cho theta 2 x2 đó là trừ vô cụm cho đến cọng vô cụm thì để x, để x cho các cái giá trị này Tổng này về giá trị từ 0 cho đến 1, chúng ta sẽ sử dụng hàm sigmoid. Hàm sigmoid sẽ có công thức như sau. Sigmoid của x, chúng ta sẽ biết x thường là bằng 1 phần 1 cộng cho e mũ trừ x. Và dạng đồ thị hàm số của hàm sigmoid sẽ có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 6,
      "start_timestamp": "0:04:22",
      "end_timestamp": "0:05:24"
    }
  },
  {
    "page_content": "x. Và dạng đồ thị hàm số của hàm sigmoid sẽ có dạng như sau. Từ trừ vô cùng cho đến cọng vô cùng Với giá trị đầu vào của mình là từ trừ vô cùng cho đến cọng vô cùng thì qua hàm sigmoid Nó sẽ ép về miền giá trị là từ 0 cho đến 1 Đây là sơ đồ, đồ thị của hàm sigmoid Vì vậy, giá trị đầu vào theta 0, theta 1, theta x1, theta 2, theta x2 từ trừ vô cùng cho đến cọng vô cùng qua hàm sigmoid, thì nó đã đưa về miền giá trị từ 0 cho đến 1. Ta sẽ thuộc đoạn từ 0 đến 1. Đây chính là cách để chúng ta thiết",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 7,
      "start_timestamp": "0:05:15",
      "end_timestamp": "0:06:06"
    }
  },
  {
    "page_content": "từ 0 đến 1. Đây chính là cách để chúng ta thiết kế hàm dự đoán. Với hàm sigmoid này, đây là một hàm liên tục. Hàm sigmoid là một hàm liên tục và giá trị bên trong này cũng là một hàm liên tục Do đó thì f của mình sẽ là một hàm liên tục Và như vậy đó thì thỏa mãn được kiến tố đó là Hàm của mình liên tục để sau này đến chức bước số 3 chúng ta tính đạo hàm, quá dễ Rồi, ở cái dạng vector hóa Tức là nếu như dữ liệu của chúng ta là một mẫu Trong trường hợp này chúng ta sẽ có x1 và x2 Nhưng một cách",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 8,
      "start_timestamp": "0:06:02",
      "end_timestamp": "0:07:03"
    }
  },
  {
    "page_content": "hợp này chúng ta sẽ có x1 và x2 Nhưng một cách tổng quát, x của mình sẽ bao gồm m thành phần, x bao gồm m thành phần chứ không chỉ có hai thành phần, còn một chính là thành phần bias. Tham số của mình là theta, theta sẽ bao gồm theta 0, theta 1, theta 2, theta m tương ứng với x tàu bào. Vì vậy, hàm dị bán của mình sẽ là, viết gọn lại, s theta x sẽ là bằng sigmoid của theta, Đối với việc vector hóa cho dữ liệu toàn mẫu tập hợp tất cả mẫu dữ liệu của tập dữ liệu của bản liệu của mình, đây là 1",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 9,
      "start_timestamp": "0:06:40",
      "end_timestamp": "0:07:47"
    }
  },
  {
    "page_content": "của tập dữ liệu của bản liệu của mình, đây là 1 mẫu, đây là 1 mẫu thứ 1, mẫu thứ 2 và đây là mẫu thứ n. Mỗi cái mổ này sẽ dự diện như vậy là một cái cột và tập hợp tất cả cái cột này sẽ tạo thành một cái ma trận Và Theta sẽ là một cái vector Theta 0, Theta 1 cho đến Theta m Vì vậy cái hàm dự đoán của mình sẽ được viết gọn lại cũng cùng một cái công thức như trên Nếu như công thức ở trên đây, đó là theta chuyển vị nhân vụ x này là một mẫu Thì qua cái công thức bên đây, x này đó là n mẫu, tức là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 10,
      "start_timestamp": "0:07:39",
      "end_timestamp": "0:08:30"
    }
  },
  {
    "page_content": "cái công thức bên đây, x này đó là n mẫu, tức là toàn bộ, toàn bộ các mẫu dữ liệu của mình Và theta chuyển vị nhân vụ x trong trường hợp này nó chính là một vector dạng nằm ngang Các vẻ tươi sẽ là giá trị y ngã dự đoán. Hàm sigmoid không phải là tính cho một giá trị scalar, mà là tính cho một vẻ tươi. Và kết quả của phép sigmoid này, kết quả của phép biến đổi sigmoid, hàm sigmoid này trên cái vector này sẽ ra một cái vector. Nó sẽ ra một cái vector. Và tình phần tử trong đây tương ứng chính là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 11,
      "start_timestamp": "0:08:23",
      "end_timestamp": "0:09:11"
    }
  },
  {
    "page_content": "Và tình phần tử trong đây tương ứng chính là sigmoid của phần tử ở phía trên. Phần tử này qua hàm sigmoid nó sẽ tính ra cái giá trị ở đây. Vì vậy ở đây nó sẽ là tính 11 y, tức là tính trên từng phần tử và sigmoid của 1 vector nằm ngang, nó sẽ ra 1 vector nằm ngang. Và chúng ta sẽ qua cái bước thứ 2, đó là chúng ta sẽ thiết kế hàm lỗi. Và trong trường hợp này thì cái y, giá trị thực tế là nó sẽ nhận 2 giá trị là 1, y bằng 1 hoặc là y bằng 0, tương ứng là 2 cái phần lớp của mình. Đối với hàm nổi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 12,
      "start_timestamp": "0:09:07",
      "end_timestamp": "0:09:55"
    }
  },
  {
    "page_content": "ứng là 2 cái phần lớp của mình. Đối với hàm nổi cho trường học một ngổ dí liệu và không có vector hóa, không vector hóa nghĩa là chúng ta sẽ tính trên từng phần tử riêng việc, thay vì tính hàng hoàn. Và ở đây chúng ta sẽ có công thức hàm nổi nổi như trên. Ở đây chúng ta sẽ đặt một câu hỏi là, tại sao công thức của hàm nổi này có vẻ phức tạp quá? Tại sao công thức này có vẻ phức tạp? Nếu có hàm log của 1 trừ y, nhưng cho log của 1 trừ y nghĩa thì hàm này quá phức tạp. Tại sao chúng ta không sử",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 13,
      "start_timestamp": "0:09:43",
      "end_timestamp": "0:10:29"
    }
  },
  {
    "page_content": "hàm này quá phức tạp. Tại sao chúng ta không sử dụng chính hàm mean square mse của hàm chơi phần linear regression, đó là công thức L theta là bằng 1 phần 2n trung quình cọng của y nghĩa y trường cho y tất cả bình phương. Tại sao chúng ta không dùng công thức này mà lại sử dụng công thức ở trên? Rồi, thì bây giờ trước tiên chúng ta phải kiểm tra xem công thức ở trên có tính đúng đắn hay không. Thế thì, yêu cầu đặt ra của hàm lỗi đó là nếu chúng ta đáng đúng thì lỗi của mình phải bằng 0. Nếu mà",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 14,
      "start_timestamp": "0:10:22",
      "end_timestamp": "0:11:27"
    }
  },
  {
    "page_content": "ta đáng đúng thì lỗi của mình phải bằng 0. Nếu mà đúng thì lỗi của mình phải bằng 0. Và nếu chúng ta đáng sai thì lỗi của mình phải lớn hơn 0. Bây giờ chúng ta sẽ xét thử 1 trường hợp nếu y của mình là bằng 1 Đây là giá trị thực tế, nhưng giá trị giữ đoán của mình là y bằng 0 Bây giờ chúng ta sẽ xét trường hợp y của mình là đoán đúng rồi đi Y cũng bằng 1 đi, khi chúng ta thế vào công thức này Loss của mình trong trường hợp này sẽ là giá trị bằng bao nhiêu? Bằng trường, y bằng 1, dưỡng nguyên,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 15,
      "start_timestamp": "0:11:16",
      "end_timestamp": "0:12:07"
    }
  },
  {
    "page_content": "bao nhiêu? Bằng trường, y bằng 1, dưỡng nguyên, kéo 1 xuống Lock y là lock y ngã, y ngã là bằng 1, 1 trừ y trong trường hợp này, 1 trừ cho 1 là 0 Vì vậy, phần còn lại là không cần tính nữa. Vì vậy, nó sẽ là bằng trừ lóc của 1. Trừ lóc của 1, trong đồ thị của hàm lóc, đây là hàm lóc x. Giá trị tại đây là bằng 1. Lóc của 1 tại vị trí này tương ứng là bằng 0. Vì vậy, trong trường hợp đáng đúng, trong trường hợp đáng đúng, thì log của mình sẽ là bằng 0 và tương tự như vậy nếu cho trường hợp y bằng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 16,
      "start_timestamp": "0:12:05",
      "end_timestamp": "0:12:59"
    }
  },
  {
    "page_content": "0 và tương tự như vậy nếu cho trường hợp y bằng 0 và y ngã bằng 0 tức là đây cũng đáng đúng nhưng trong trường hợp y bằng 0 thì các bạn thấy chúng ta cũng sẽ ra được giá trị sai số là bằng 0 Bây giờ chúng ta sẽ xem trong cái trường hợp chúng ta sẽ xem trong cái trường hợp đó là nếu chúng ta đoán sai nếu chúng ta đoán sai y bằng một và y ngã dĩa đoán là bằng không thì thế vô cái công thức chúng ta sẽ thế vô cái công thức ở trên đây thì nó sẽ ra như thế nào y bằng một Loss của mình sẽ là bằng trừ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 17,
      "start_timestamp": "0:12:54",
      "end_timestamp": "0:13:45"
    }
  },
  {
    "page_content": "thế nào y bằng một Loss của mình sẽ là bằng trừ 1 dự quyên nhân cho lốc y ngã trong trường hợp này là bằng 0 1 trừ y sẽ là bằng 1 trừ 1, dễ nói là phần sau chúng ta bỏ qua Vậy nó sẽ là bằng trừ lốc 0 Chúng ta thấy là với cái độ thể hàm số này thì khi x của mình mà tiến về 0 Vì vậy, lốc giá trị của lốc sẽ tiến về trừ vô cồn Và đó sẽ là bằng trừ của trừ vô cồn, tức là bào cọng vô cồn Hay nói cách khác, nếu bán sai thì mất mát của chúng ta là chúng ta sẽ mất nguyên 1 căn nhà Tức là chính là 1 giá",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 18,
      "start_timestamp": "0:13:40",
      "end_timestamp": "0:14:36"
    }
  },
  {
    "page_content": "ta sẽ mất nguyên 1 căn nhà Tức là chính là 1 giá trị rất là lớn Bây giờ chúng ta sẽ thử nghiệm trên giá trị msi, trên công thức mean square Nếu như dự đáng sai, nếu như dự đáng đúng thì lỗi cũng sẽ bằng 0 Thế bộ chúng ta cũng sẽ không bằng 0 Nếu chúng ta đáng sai, tức là chúng ta sẽ có y trừ cho y ngã tất cả mình Y mà trừ y ngã mình tức là bằng 1, trừ không tất cả mình sẽ là bằng 1 Nếu dùng công thức MSE này thì sự trừng phạt này nó quá bé so với lại công thức của hàm Loss ở đây Cái này quá bé,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 19,
      "start_timestamp": "0:14:31",
      "end_timestamp": "0:15:31"
    }
  },
  {
    "page_content": "lại công thức của hàm Loss ở đây Cái này quá bé, còn cái này là rất vừa lấu Thì việc lớn bé này nó sẽ ảnh hưởng như thế nào Khi chúng ta có hàm Mất Mát mà lớn thì việc cập nhật đạo hàm, việc tính đạo hàm Thế ta, nếu như đạo hàm này đơn biến hoặc Napala của L theo Thế ta, thì khi giá trị này có độ giốc lớn, tức là độ giốc của hàm L này lớn, thì khi đó đạo hàm của mình sẽ lớn. Ngược lại, nếu như min square row này giá trị của mình nhỏ, Khi đó tính đạo hàm, độ giốc của đạo hàm sẽ nhỏ Dẫn đến bước",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 20,
      "start_timestamp": "0:15:22",
      "end_timestamp": "0:16:17"
    }
  },
  {
    "page_content": "đạo hàm, độ giốc của đạo hàm sẽ nhỏ Dẫn đến bước cập nhật sẽ chậm Chúng ta có công thức theta là theta trừng cho alpha nhân cho đạo hàm của loss theo theta Nếu như độ giốc của hàm L này, ví dụ có hai cái hàm Đây là hàm thứ nhất, hàm L1 và hàm thứ 2. Cả hai hàm này, trong đó hàm L1 chúng ta thấy có độ giốc rất là lớn, đúng không? Khi đó, cái đạo hàm, giá trị đạo hàm của mình sẽ lớn. Còn hàm L2, độ giốc của mình nó tì thoái thoải, do đó đạo hàm của nó bé. Nếu như độ giốc lớn, bước nhảy của mình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 21,
      "start_timestamp": "0:16:13",
      "end_timestamp": "0:17:19"
    }
  },
  {
    "page_content": "nó bé. Nếu như độ giốc lớn, bước nhảy của mình sẽ lớn, dẫn đến việc cập nhật thay ta sẽ nhanh. Do đó, chúng ta sử dụng công thức trừ của lốc Y ngã cộng cho một trừ Y ngã, thì nó sẽ giúp cho việc huấn luyện sẽ thực hiện rất là nhanh, nhanh hơn sau với việc dùng công thức mean square ở đây. Đó là lý do tại sao mình lại sử dụng công thức mean square. Bây giờ chúng ta sẽ qua công thức cho trường hợp nhiều mẫu và có vector hóa. Với từng mẫu dữ liệu, chúng ta gáp lại, thì chúng ta sẽ có một cái ma",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 22,
      "start_timestamp": "0:17:06",
      "end_timestamp": "0:17:54"
    }
  },
  {
    "page_content": "chúng ta gáp lại, thì chúng ta sẽ có một cái ma trận x và cái nhãn y của dữ liệu sẽ là một cái vector dạng làm ngô tham số của mình là theta 0, theta 1 và theta m thì khi đó, cái hàm lỗi của mình sẽ có cái công thức đó là 1 phần 2, 1 phần n Binary, cái chữ b-c-e này là viết tác của chữ binary cross entropy Thì đây chính là cái công thức mà hồi nãy mình đã luy kê, mình đã trình bày đó là bằng y trừ của y nhân cha lóc của y ngã cộng cho 1 trừ y nhân cha lóc của 1 trừ y y ngã Đây là công thức",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 23,
      "start_timestamp": "0:17:52",
      "end_timestamp": "0:18:36"
    }
  },
  {
    "page_content": "y nhân cha lóc của 1 trừ y y ngã Đây là công thức binary cross entropy và lưu ý là chúng ta sẽ tính trên từng phần tử. Gì là sao? Khi chúng ta tính sigmoid của theta x, chúng ta sẽ có chuỗi các phần tử dạng vector dạng nằm ngang. Y là y ngã, còn y của mình cũng sẽ có chuỗi các phần tử tạo thành một vector nằm ngang. và chúng ta sẽ tính toán đổ lỗi trên hai giá trị y,y ngã này bằng cách nó sẽ lấy từng phần tử ở đây ra từng phần tử của y ngã với từng phần tử của y thế vào cổng tích này để tính",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 24,
      "start_timestamp": "0:18:30",
      "end_timestamp": "0:18:44"
    }
  },
  {
    "page_content": "từng phần tử của y thế vào cổng tích này để tính rồi sau đó nó lại cộng trung bình lại nó sẽ cộng hết, cộng trung bình nó sẽ thực hiện trên từng phần tử của y ngã và y này để mà tính ra hàm lỗi Và dạng đồ thị của hàm logistic regression của mình thì nó sẽ xong cũng tương tự như hàm linear regression. Nếu như linear regression, chúng ta đến bức tổng này là xong, chúng ta sẽ qua tiếp một phép tiến độ nữa là hàm sigmoid. sau khi thực hiện phép tổng này, chúng ta sẽ có công thức y ngã là bằng Fθx",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 25,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "này, chúng ta sẽ có công thức y ngã là bằng Fθx là bằng sigmoid của theta chuyển vị những x. Theta chuyển vị những x chính là cái kết quả sau khi thực hiện cái này. Qua hàm sigmoid thì nó sẽ ra cái y ngã. Trong đó, công thức của sigmoid sẽ là bằng 1 phần 1 cộng cho E1,1 x. Đây là dạng đồ thị của mô gần Logistic Regression.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=T2xJmTiRM5o",
      "filename": "T2xJmTiRM5o",
      "title": "[CS431 - Chương 2] Part 3a: Mô hình hồi quy luận lý (Logistic Regression)",
      "chunk_id": 26,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Ưng dụng tiếp theo của mạng CNN trong các bài toán liên quan đến thị giác máy tính chính là phát hiện đối tượng. Và đây có thể nói là một trong những bài toán có sức ảnh hưởng rất lớn về mặt ứng dụng. Nó có ứng dụng trong xe tự hành, ví dụ như khi chiếc xe trên đường sẽ được trang bị các camera đặt ở tất cả các hướng nhìn của xe. và nó sẽ phát hiện xem xung quanh có những cái xe hoặc các cái phương tiện đi lại hoặc là những cái người bộ hành và những cái vật cảng nào để từ đó nó đưa ra quyết",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:39"
    }
  },
  {
    "page_content": "những cái vật cảng nào để từ đó nó đưa ra quyết định là xe nên đi theo vương nào. Và bài tán phát hiện tới tượng này thì mất vô từ một trực quan hóa của mạng CNN. Trước đây thì chúng ta đã từng thảo luận về bài Deep Visualization Tuned Box Và nó có một số tính chấp của feature map trong mạng CNN, đó chính là tính bất biến về trình tự không gian và tỉ lệ. Ví dụ trong tấm hình này, chúng ta thấy người đàn ông ngồi trước mạng hình, đây chính là feature map Và cái đốn sáng này có cái concept, có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 1,
      "start_timestamp": "0:00:35",
      "end_timestamp": "0:01:27"
    }
  },
  {
    "page_content": "map Và cái đốn sáng này có cái concept, có cái ý nghĩa đó chính là thể hiện cái concept là ngươi mặt. Chính cái tấm ảnh này là chính cái tấm ảnh mà làm cho cái feature map này là phát sáng nhất. Thì chúng ta thấy điểm chung của tất cả các cái ảnh này đó chính là có cái ngươi mặt. Và khi cái người này di chuyển thì chúng ta sẽ thấy là cái đốn sáng này cũng di chuyển theo. và sau đó sẽ có một người đàn ông khác mặc áo màu đen đi vào khung hình thì chúng ta thấy là cái người này nằm ở phía tay",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 2,
      "start_timestamp": "0:01:17",
      "end_timestamp": "0:02:01"
    }
  },
  {
    "page_content": "thì chúng ta thấy là cái người này nằm ở phía tay trái, đúng không? thì cái đống sáng tương ứng nó cũng nằm phía tay trái so với cái đống sáng lớn này tức là trình tự về không gian cái người mặc áo đen đứng về phía đằng xa và đứng sau thì cái đống sáng này cũng đứng về đằng xa và phía sau là tương ứng với lại cái vị trí của cái người áo đen. Đồng thời về mặt tỷ lệ, chúng ta thấy là cái gương mặt của cái người mặt áo đen đó bằng khoảng một nửa, theo một chiều nữa, hoặc là bằng một phần tư về mặt",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 3,
      "start_timestamp": "0:01:57",
      "end_timestamp": "0:02:36"
    }
  },
  {
    "page_content": "một chiều nữa, hoặc là bằng một phần tư về mặt diện tích thôi. Thôi bây giờ chúng ta sẽ nói về chiều ngang, chiều dọc đi. Thì cái mặt của cái người áo đen bằng một khoảng một nửa so với lại cái người mặt áo tím ở đây, thì cái đốn sáng tương ứng cũng sẽ bằng một nửa. Như vậy, đây chính là tính bất biến về mặt tỉa lại. Và dựa trên đặc điểm này, chúng ta ấn dụng các feature maps của mạng CNN để giải quyết các bài toán về phát hiện đối tượng. Ví dụ như đốm sáng này là đại diện cho concept về mặt",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 4,
      "start_timestamp": "0:02:33",
      "end_timestamp": "0:03:22"
    }
  },
  {
    "page_content": "như đốm sáng này là đại diện cho concept về mặt mưu mặt và chúng ta sẽ dùng các thuật ván để ước lượng bounding box để ước lượng bounding box xung quanh đống sáng này sau khi đã ước lượng xong, chúng ta sẽ nổi suy ra bounding box ở ảnh góc Tại vì chúng ta đã biết bề ngang và bề cao của tấm ảnh này rồi. Chúng ta biết bề ngang và bề cao của feature map rồi. Thì khi đó chúng ta hoàn toàn có thể thực hiện được thao tác tội suy. Tuy nhiên thì đây chỉ là ý tưởng xơ khởi và làm sao để cho mạng của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 5,
      "start_timestamp": "0:03:12",
      "end_timestamp": "0:04:06"
    }
  },
  {
    "page_content": "chỉ là ý tưởng xơ khởi và làm sao để cho mạng của mình có khả năng học và đoán được vị trí bounding box từ đầu tới cuối. thì chúng ta sẽ phải thiết kế lại mạng CNN chúng ta phải điều chỉnh lại mạng CNN một chút để đạt được nhiệm vụ đó là phát hiện đối tượng và lưu ý là bài toán phát hiện đối tượng sẽ có một tính chấp là cái object của mình nó sẽ không xuất hiện trọn vẹn bên trong khu hình mà ở đâu đó nó chỉ xuất hiện ở một khu vực nhỏ đâu đó thôi và nhiệm vụ của mình sẽ là phải tìm ra vị trí đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 6,
      "start_timestamp": "0:03:59",
      "end_timestamp": "0:04:48"
    }
  },
  {
    "page_content": "và nhiệm vụ của mình sẽ là phải tìm ra vị trí đó và trong hình này có thể có rất nhiều object khác hoặc có rất nhiều loại đối tượng khác và thậm chí không có đối tượng nào cả thì đó chính là bài toán phát hiện đối tượng và nó khác như thế nào so với bài toán phân loại đối tượng và đây là khái niệm thì chúng ta đã đề cập rồi Tức là trong tấm hình này có rất nhiều loại đối tượng khác nhau Và có đồng thời, cũng có khả năng là có hai đối tượng cùng loại Ví dụ ở đây là cái chai, thì ở đây cũng có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 7,
      "start_timestamp": "0:04:43",
      "end_timestamp": "0:05:19"
    }
  },
  {
    "page_content": "loại Ví dụ ở đây là cái chai, thì ở đây cũng có xuất hiện là cái chai Rồi, và ở đây là có cái tli Và cũng có những đối tượng mà một loại, ví dụ như là cái tô, cái laptop Thì đây là nhiệm vụ chính của bài tán phát hiện đối tượng Đó chính là chúng ta sẽ xác định vùng hình hột Vùng hình hột hay còn gọi là vùng bounding box Nếu có sự xuất hiện của một hoặc nhiều đối tượng trong tấm hình này Chúng ta sẽ phải tính đến cả tình huống trong tấm hình không có object nào Thì để ứng dụng mạng CNN cho bài",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 8,
      "start_timestamp": "0:05:15",
      "end_timestamp": "0:06:02"
    }
  },
  {
    "page_content": "có object nào Thì để ứng dụng mạng CNN cho bài toán phát hiện đối tượng này thì có rất nhiều hướng tiếp cận trong đó hướng tiếp cận về 2 giai đoạn Và nổi tiếng nhất đó chính là cái mô hình là R-CNN, Fast R-CNN và Faster R-CNN. Thì chúng ta sẽ không có thời gian để mà đi hết được toàn bộ các kiến trúc này. Chúng ta sẽ bàn về cái mô hình mà nổi tiếng và gần nhất trong 3 cái mô hình này. Và ý tưởng của nó cũng được sử dụng cho rất nhiều những cái thuốc tán phát hiện đối tượng về sau. và có sử dụng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 9,
      "start_timestamp": "0:05:57",
      "end_timestamp": "0:06:44"
    }
  },
  {
    "page_content": "tán phát hiện đối tượng về sau. và có sử dụng những mô hình tiên tiến nhất của Deep Learning như Vision Transformer. Đầu tiên, đó là giai đoạn số 1. Chúng ta sẽ phải xác định xem vùng có khả năng đối tượng, tức là trong tấm hình này. Mình sẽ chỉ ra những khu vực nào có khả năng có đối tượng. Nhưng đối tượng đó là đối tượng gì? Hạ vụ phân giải, chúng ta sẽ tính sau. Thì sang giai đoạn số 2, chúng ta sẽ phân loại xem ứng với từng cái bounding box đó thì ở đây nó sẽ là cái class của nó là cái gì,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 10,
      "start_timestamp": "0:06:36",
      "end_timestamp": "0:07:24"
    }
  },
  {
    "page_content": "đó thì ở đây nó sẽ là cái class của nó là cái gì, cái tên của cái đối tượng trong cái bounding box này là gì đồng thời chúng ta có thể sẽ phải tinh chỉnh lại cái bounding box sao cho nó khớp với đối tượng hơn Vùng màu đen này chỉ là khu vực tạm thời thôi để localize vị trí có hành hăng của đối tượng Sau đó chúng ta sẽ thực hiện bước lượng bounding box một cách chính xác nhất vào đối tượng của mình Đó chính là ý tưởng của phương pháp phát hiện đối tượng 2 giai đoạn Đối với thật quán Faster ACNN,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 11,
      "start_timestamp": "0:07:16",
      "end_timestamp": "0:07:57"
    }
  },
  {
    "page_content": "tượng 2 giai đoạn Đối với thật quán Faster ACNN, nó sẽ khai thác đặc trưng Deep Feature và ở hai bước. Bước đầu tiên đó chính là Reasoned Proposal Network, RPN. Nhìn vụ của bước này đó chính là xác định những khu vực có khả năng có đối tượng. Và cách thức để xác định những vùng có khả năng đối tượng đó là dựa trên quan sát. Khi chúng ta rút trích ra feature map, chúng ta sẽ thấy có những chỗ có respawn. Thì đây chính là những cái chỗ có khả năng phó đối tượng Và từ những cái đốn sáng này, những",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 12,
      "start_timestamp": "0:07:52",
      "end_timestamp": "0:08:36"
    }
  },
  {
    "page_content": "phó đối tượng Và từ những cái đốn sáng này, những cái chỗ Respo này Mình sẽ đưa qua một cái mạng Neural Network để chỉ ra những cái báo điện bóc Chỉ ra được những cái báo điện bóc là chỗ đó có khả năng phó đối tượng Sau đó với cái báo điện bóc này, chúng ta sẽ kết hợp với một cái feature map Và lưu ý là feature map này nó được chia sẻ, nó share feature, tức là feature map này và feature map này là một. Feature map này kết hợp với lại cái bounding box mà qua cái mạng region proposal network, nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 13,
      "start_timestamp": "0:08:32",
      "end_timestamp": "0:09:19"
    }
  },
  {
    "page_content": "box mà qua cái mạng region proposal network, nó sẽ khoanh vùng cái feature map này, nó sẽ trích cái feature map này ra. Và từ cái feature map này, đi đến đến thực hiện cái công đoạn nó gọi là detector, Chúng ta sẽ chỉ ra vị trí chính xác hơn Chúng ta sẽ chỉ ra vị trí chính xác hơn Cái BoundingBox chính xác Đồng thời là chúng ta sẽ phải có thêm cái class Cái class name tức là cái tên của cái đối tượng đó là gì Thì đây chính là cái ý tưởng của Faster ACNN và hướng tiếp cận Faster CNN thì nó sẽ có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 14,
      "start_timestamp": "0:09:12",
      "end_timestamp": "0:09:51"
    }
  },
  {
    "page_content": "ACNN và hướng tiếp cận Faster CNN thì nó sẽ có một cái điểm yếu là nó sẽ chậm và nó phải tách ra làm hai giai đoạn thì bây giờ người ta có ý tưởng là làm sao train từ đầu đến cuối, tức là chúng ta sẽ thực thi từ đầu đến cuối chỉ cần fit vào một tấm ảnh đầu ra nó sẽ ra được cái bounding box của các object luôn mà không cần phải chia ra làm hai bước tại vì chia ra làm hai bước thì nó sẽ có tình trạng là bước này phải chờ bước kia nó sẽ chậm Còn cứu kích cận 1 giai đoạn thì nó sẽ loại bỏ hoàn toàn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 15,
      "start_timestamp": "0:09:44",
      "end_timestamp": "0:10:44"
    }
  },
  {
    "page_content": "kích cận 1 giai đoạn thì nó sẽ loại bỏ hoàn toàn bước đổi xếp đối tượng hoặc là region proposal network mà nó sẽ thực thi từ đầu đến cuối hay là end to end 1 cái mạng CNN luôn. Rồi, và cái ý tưởng của cái hướng 1 giai đoạn này nổi tiếng nhất chính là YOLO. Và cái YOLO thì ở đây chúng ta đang nói là YOLO phiên bản đầu. Tuy nhiên, YOLO cho đến thời điểm hiện nay, năm 2024, là nó đã có YOLO phiên mạng 10. Tức là cứ cải tiến nhiều. Nhưng mà ý tưởng chính nhất của nó, mẫu chính là làm sao fit một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 16,
      "start_timestamp": "0:10:39",
      "end_timestamp": "0:11:23"
    }
  },
  {
    "page_content": "chính nhất của nó, mẫu chính là làm sao fit một cái tấm ảnh đầu vào. Đây chính là cái ảnh thô đầu vào. Và cái output đầu ra của mình nó sẽ là một cái tensor. Và cái tensor này nó có thể encode. Tức là nó có chứa đủ được cái thông tin về mặt ClassName Rồi về mặt BoundingBox Thì ở đây nó sẽ có cái trick là mỗi một cái ảnh của mình thì nó giả sử là nó chia ra một cái ô lưới Ví dụ như trong trường hợp này, nó nghĩ rằng là cái ô lưới của mình sẽ là kích thước là 7 x 7 Tức là object của mình đâu đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 17,
      "start_timestamp": "0:11:17",
      "end_timestamp": "0:12:06"
    }
  },
  {
    "page_content": "kích thước là 7 x 7 Tức là object của mình đâu đó chỉ xuất hiện trong những khu vực 7 x 7 này mà thôi Và nó sẽ có cái tình huống đó là với một cái ô này Thì nó có khả năng là có hiện tượng chồng đối tượng Tức là hiện tượng mẹ mùng con, một đối tượng ở đằng trước và một đối tượng ở đằng sau thì nó sẽ thiết kế cái TensorFlow này làm sao đó đủ để có thể encode được cả những cái tình huống đó tức là có những cái object này nó trồng lên cái object kia và tất cả mọi thứ nó sẽ encode trong cái 30",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 18,
      "start_timestamp": "0:11:57",
      "end_timestamp": "0:12:44"
    }
  },
  {
    "page_content": "kia và tất cả mọi thứ nó sẽ encode trong cái 30 triều độ sau này trong cái 30 triều độ sau này nó sẽ phải có đầy đủ là className đó sẽ phải có đầy đủ là tỏa độ x, tỏa độ y, quid và height của các object trong đó và với mỗi một cell ở đây, chúng ta sẽ có được thông tin vị trí của một object trong đó Thì như vậy là ý tưởng của YOLO là biến một tỉnh đầu vào, feed-through để tạo thành một tensor, tensor, tensor, tensor. Rồi, cuối cùng chúng ta sẽ ra được một cái tensor và cái tensor này có khả năng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 19,
      "start_timestamp": "0:12:41",
      "end_timestamp": "0:13:36"
    }
  },
  {
    "page_content": "được một cái tensor và cái tensor này có khả năng encode được thông tin, vã độ và vị trí cũng như là cái nhãn của cái object ở bên trong cái khu vực đó. thì các phiên bản sau của YOLO có rất nhiều cải tiến nó cũng kế thừa rất nhiều thành tựu của Deep Learning trong việc thay đổi kiến trúc và trong việc thiết kế output làm sao cho nó tiện nhất và có khả năng giải quyết được bài bản Object Detection trong tình huống đó là object của mình nó nhỏ tức là cái bấn đề về Scale rồi cái bấn đề về trồng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 20,
      "start_timestamp": "0:13:15",
      "end_timestamp": "0:14:27"
    }
  },
  {
    "page_content": "là cái bấn đề về Scale rồi cái bấn đề về trồng lấp occlusion Đi chông lắc Còn nguyên nhiên là cái tốc độ luôn luôn là điểm mạnh của các hướng tiếp cậu vào một giai đoạn Thì nó vẫn luôn luôn là làm sao cho cải tiến với tốc độ càng lúc càng nhanh Nhưng đồng thời là nó vẫn phải đảm bảo được cái độ chính xác ngang bằng hoặc là thậm chí là cố gắng để tốt hơn các hướng tiếp cậu 2 giai đoạn Nếu so phương tiếp cận YOLO V3, thì so với faster ACNN, YOLO V3 cho tốc độ nhanh hơn faster ACNN rất nhiều lần.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 21,
      "start_timestamp": "0:14:20",
      "end_timestamp": "0:14:58"
    }
  },
  {
    "page_content": "cho tốc độ nhanh hơn faster ACNN rất nhiều lần. Ví dụ YOLO V3 có 45 frames per second, tức là nó đã có thể thực thi được thời gian thực. trong khi đó Faster R-CNN là 7 frames per second nó dưới mức 24 fps để tạm gọi là có thể thực hiện được thời gian thực nhưng đồng thời nó sẽ đánh đổi độ chính xác Faster R-CNN cho độ chính xác cao hơn YOLO đến hơn 10% Thực sự trong bài toán Detection thì 10% là một con số rất là lớn và tùy vào cái nhu cầu cũng như là cái ngự cảng và mình sẽ quyết định xem chọn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 22,
      "start_timestamp": "0:14:53",
      "end_timestamp": "0:15:37"
    }
  },
  {
    "page_content": "là cái ngự cảng và mình sẽ quyết định xem chọn được cái môi nào nếu như chúng ta không cần phải thực hiện cái thực phán quá nhanh, real time và chúng ta cần độ chính xác thì chúng ta sẽ sử dụng luyện tiết cờ 2 gia đoạn và cụ thể là Faster AC Name cũng như là các biến thể của Faster AC Name PY còn nếu như chúng ta cần thực thi theo thời gian thực thì lúc đó và chúng ta cũng phải cân bằng được kiểu vô vẻ độ chính xác thì lúc đó YOLO, các phiên bản của YOLO cũng như SSD đây là một cái tên của một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 23,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "của YOLO cũng như SSD đây là một cái tên của một cái thức toán khác thì chúng ta sẽ chọn cái nguyên tiết cận là một giai đoạn để sử dụng và với cái sâu đồ này thì chúng ta thấy là sự tương quan giữa các nguyên tiết cận thì YOLO là cho MIP 50, tức là một độ đo, thể hiện độ chính xác Time, tức là thời gian để thực thi YOLO V3 cho tốc độ cao nhất trong số khuếng tiếp cận Right Về độ chính xác, YOLO V3 là 51% Thua sau với Feature Pyramid Network Cũng là một trong những khuếng tiếp cận của Object",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 24,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "là một trong những khuếng tiếp cận của Object Detection rất nổi tiếng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Til9AdPO7JE",
      "filename": "Til9AdPO7JE",
      "title": "[CS431 - Chương 5] Part 3: Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
      "chunk_id": 25,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Tiếp theo, chúng ta sẽ tìm hiểu về kiến trúc mạng Google Lnet. Chữ L này tương ứng là tượng chữ YAN LE KUN. Khu Google Lnet đã có những cái kể tiếng sau. Đầu tiên, nó sử dụng lớp Botonet, convolution 1v1 để giảm chiều đặc trưng trước khi thực hiện phép biến đổi. Trên kênh filter có kích thước lớn hơn là 5.5. Lưu ý là trong Google Lnet thì họ phát triển xong xong với lại VGG Nên ở đây họ vẫn sử dụng những filter có kích thức lớn Rồi, đồng thời cái cải tiến thứ 2 đó là họ sử dụng module Inception",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:42"
    }
  },
  {
    "page_content": "cải tiến thứ 2 đó là họ sử dụng module Inception Là các nhánh xong xong, ở đây chúng ta thấy là có các nhánh xong xong Rồi sau đó nó sẽ tổng hợp thông tin lại Với các filter với kích thước khác nhau Chúc nửa chúng ta sẽ bàn thêm 2 cái cải tiến này cụ thể đó là gì Cái kiến trúc mạng của Google Internet cho số được trích dẫn đó là 58 ngàn 58 ngàn trích dẫn, cũng rất là lớn Rồi đầu tiên đó là cái cải tiến đầu tiên đó là Bot.net Thì bình thường nếu như chúng ta không sử dụng Bot.net, tức là cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 1,
      "start_timestamp": "0:00:37",
      "end_timestamp": "0:01:20"
    }
  },
  {
    "page_content": "như chúng ta không sử dụng Bot.net, tức là cái cách làm bình thường Đây là chính là cái cách làm bình thường Thì chúng ta sẽ thực hiện phép convolution trên kernel, trên filter có kích thước là 5x5 Và số lượng filter ở đây sẽ là 48 filter Số lượng filter ở đây sẽ là 48 filter Thì khi nhân với kernel kích thước là 5x5, đương nhiên chúng ta sẽ ngầm hiểu là nó sẽ có 5x5x480 tại vì quy ước đó là độ sâu của filter nó phải bằng với độ sâu của input của mình thì nó sẽ tạo ra một cái feature map có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 2,
      "start_timestamp": "0:01:18",
      "end_timestamp": "0:02:07"
    }
  },
  {
    "page_content": "của mình thì nó sẽ tạo ra một cái feature map có kích thước là 14 như 14 đây là trục không gian, width, height, dĩa nguyên và độ sâu của mình nó sẽ bằng đúng số lượng filter của mình độ sâu của mình ở đây là 48 đúng bằng số lượng filter Thì nếu như không sử dụng Botox thì ở đây chúng ta sẽ xem tổng số tham số của mình nếu như chúng ta sử dụng Nếu như chúng ta sử dụng, đó chính là 14 x 14 x 480 x 5 x 5 x 400 x 48 thì đây chính là tổng số lượng phép tính Nếu như có sử dụng Botonac thì Botonac ở",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 3,
      "start_timestamp": "0:02:02",
      "end_timestamp": "0:03:06"
    }
  },
  {
    "page_content": "tính Nếu như có sử dụng Botonac thì Botonac ở đây là gì? Botonac đó là khi input đầu vào của mình thay vì chúng ta trực tiếp biến đổi thành output có kích thước đó là 14 x 14 x 48 thì chúng ta sẽ qua một cái bức trung gian và cái trung gian này nó sẽ sử dụng cái 1 nhân 1 convolution nó sử dụng 1 nhân 1 convolution và thay vì biến đổi trực tiếp sang 48 thì chúng ta sẽ qua một cái trung gian đó là sử dụng 16 cái filter 16 cái filter có kích thước là 1 nhân 1 nhân cho 480 Rồi, như vậy thì sau đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 4,
      "start_timestamp": "0:03:04",
      "end_timestamp": "0:03:54"
    }
  },
  {
    "page_content": "là 1 nhân 1 nhân cho 480 Rồi, như vậy thì sau đó chúng ta sẽ nhân với filter có kích thước là 5x5 x16 thì bình thường ở bên đây là 5x5 x480 thì nhờ cái bot tonac này thay vì chúng ta nhân với 480 thì chúng ta chỉ việc nhân với 16 thôi và đây chính là lý do để giúp cho kiến trúc mạng của mình giảm số lượng tham số và đồng thời nó cũng sẽ giảm số lượng tính toán rất là nhiều Đối với phép biến đổi confusion 1 x 1 và 16 phép biến đổi này thì chúng ta sẽ có là 14 x 14 x 480 x 1 x 1 x 16 Đây chính là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 5,
      "start_timestamp": "0:03:44",
      "end_timestamp": "0:04:33"
    }
  },
  {
    "page_content": "sẽ có là 14 x 14 x 480 x 1 x 1 x 16 Đây chính là số phép tính Tư tưởng như vậy thì 5 x 5 x 48 thì chúng ta sẽ có như đây phép tính Tổng lại thì nó chỉ có 5 triệu phép tính so với lại 112 triệu phép tính thì chúng ta thấy là số lượng phép tính nó giảm đi rất là nhiều Rồi, và khi số phép tính này giảm thì số tham số của mình đồng thời cũng sẽ giảm luôn số tham số của mình trong trường hợp này cũng sẽ giảm luôn thì bình thường ở bên đây, số tham số của mình đó là 5 x 5 x 48 và 5 x 5 x 480, đây là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 6,
      "start_timestamp": "0:04:31",
      "end_timestamp": "0:05:21"
    }
  },
  {
    "page_content": "của mình đó là 5 x 5 x 48 và 5 x 5 x 480, đây là kích thước của filter sau đó nó sẽ nhân với lại 48 filter nữa nó sẽ nhân với 48 filter nữa thì bên đây chúng ta chỉ có là 1 nhân 1 nhân cho 480 đây là cái kích thước của cái filter của mình và mình sẽ nhân với lại số lượng là 16 16 cái filter kích thước là 1 nhân 1 nhân 4580 rồi và sau đó chúng ta sẽ thực hiện FabConclusion với cái kernel cái filter có kích thước là 5 Nhân 5, nhân 16, đây là cái kích thước của cái filter của mình để thoải mạng là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 7,
      "start_timestamp": "0:05:17",
      "end_timestamp": "0:05:58"
    }
  },
  {
    "page_content": "thước của cái filter của mình để thoải mạng là cái độ sâu giống với bên này Và nhân với lại 48 Nhân với lại 48 thì ở đây sẽ ra là số lượng tham số Và chúng ta có thể tính toán là tổng của hai cái này nó sẽ nhỏ hơn rất là nhiều Sau với lại số lượng tham số ở đây Thì đây là cái lớp bottleneck Ngoài ra, một cái cải tiến khác của Google Lens S đó chính là cái lớp module gọi Inception Thì ý tưởng của cái module Inception đó là gì? Chúng ta sẽ không biết được là cái filter của mình kích thước 1 x 1,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 8,
      "start_timestamp": "0:05:52",
      "end_timestamp": "0:06:34"
    }
  },
  {
    "page_content": "được là cái filter của mình kích thước 1 x 1, hay là 3 x 3, hay là 5 x 5, hay là 7 x 7 v.v. Chúng ta không biết được cái kích thước của cái filter bao nhiêu là tốt nhất Do đó thì chúng ta cứ thực hiện hết thì ở đây chúng ta sẽ thực hiện với convolution 1.1 ở đây thực hiện convolution 3.3 đây thực hiện convolution 5.5 và max pooling 3.3 và sau đó chúng ta sẽ thực hiện phép biến đổi là concat tại vì sau mỗi phép biến đổi này sau mỗi phép biến đổi này chúng ta sẽ có 1 feature map và lưu ý, đó là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 9,
      "start_timestamp": "0:06:32",
      "end_timestamp": "0:07:37"
    }
  },
  {
    "page_content": "này chúng ta sẽ có 1 feature map và lưu ý, đó là feature map này có thể có các hít độ sâu khác nhau Nhưng cái kích thước bền ngang bền cao nó phải giống nhau Và khi chúng ta con cat lại thì chúng ta sẽ tạo ra một cái feature map Có kích thước rất là dài Đoạn này là cái này, đoạn này là cái này, đoạn này là cái này và đoạn này là cái này. Đây là minh hòa cho việc thực hiện phép Toàn Cát, Toàn Cát là nối đặc trưng lại. Và cái đặc trưng tổng này người ta hy vọng là nó chứa đầy đủ thông tin để giúp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 10,
      "start_timestamp": "0:07:28",
      "end_timestamp": "0:08:17"
    }
  },
  {
    "page_content": "ta hy vọng là nó chứa đầy đủ thông tin để giúp cho chúng ta thực hiện cái quá trình biến đổi và nhiệm vệ. Rồi, vậy thì sau khi chúng ta đã khảo sát qua các kiến trúc mạng LENET, AlexNet và Google LENET, thì chúng ta đang quan sát có một cái điều gì đang xảy ra. Đầu tiên là AlexNet. AlexNet nó chỉ có 8 layer và có độ dài như thế này. VGG có đến 19 layer và xét về mối tương quan thì chúng ta thấy nó dài hơn rất nhiều so với lại 8 layer Và Google L Lnet cho số layer là 22 layer, như vậy chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 11,
      "start_timestamp": "0:08:05",
      "end_timestamp": "0:08:57"
    }
  },
  {
    "page_content": "L Lnet cho số layer là 22 layer, như vậy chúng ta thấy là càng về sau, số layer của mình sẽ càng tăng Và kết quả của mình kể lúc độ chính xác càng tốt Thay hiện qua cái việc đó là AlexNet thì chiến thắng trong cuộc thi vào năm 2012 VGG thì năm 2014 Và GoogleNet cũng chiến thắng trong năm 2014, tức là cái độ chính xác của mình càng lúc càng tan Độ chính xác Và GoogleNet nếu mà chúng ta vẽ chúng ta sẽ thấy nó nhỏ chi chít như thế này Vậy thì theo một cách nội suy bình thường thì chúng ta sẽ suy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 12,
      "start_timestamp": "0:08:52",
      "end_timestamp": "0:09:33"
    }
  },
  {
    "page_content": "một cách nội suy bình thường thì chúng ta sẽ suy nghĩ rằng là Thôi, chúng ta cứ việc tăng layer lên thì tự nhiên độ chính xác của mình sẽ tăng lên, đúng không? Thì ở đây chúng ta sẽ có một biểu đồ để so sánh mối tương quan về kích thước của các kiến trúc mạng AlexNet 8 layer chỉ có nhiều đây, VGG 19 chỉ có kích thước này nhiều đây Và ResNet chiến thắng trong cuộc thi năm 2015 nó có cái kích thước nhiều đài Rất là dài so với những kiến trúc trước đây Thế thì khi kiến trúc mạng mà càng dài kiến",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 13,
      "start_timestamp": "0:09:29",
      "end_timestamp": "0:10:13"
    }
  },
  {
    "page_content": "đây Thế thì khi kiến trúc mạng mà càng dài kiến trúc mạng càng có nhiều các lớp biến đổi thì nó sẽ có những cái bấn đề gì Và ResNet đã giải quyết cái bấn đề đó như thế nào thì chúng ta sẽ cùng tìm hiểu trong phần tiếp theo đó là kiến trúc mạng ResNet Cái vấn đề mà ResNet họ phát hiện ra đó là khi tăng cái độ chính xác lên thì hình như có vẻ cái độ chính xác sẽ càng tăng. Đó là cái quan sát khi trên 3 cái kiến trúc mạng là CalixNet, VGG, rồi Inception. Tuy nhiên khi mà họ tiến hành càng tăng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 14,
      "start_timestamp": "0:10:10",
      "end_timestamp": "0:10:50"
    }
  },
  {
    "page_content": "Tuy nhiên khi mà họ tiến hành càng tăng nhiều hơn nữa, khi số lượng layer mà lớn hơn 20, thì điều này nó không còn đúng nữa. tăng độ sâu lên và nó không còn hiệu quả nữa Thay hiện qua cái việc ở đây là hạm độ lỗi Độ lỗi là càng thấp càng tốt thì các bạn thấy là 20 layer thì nó nằm ở dưới cồn tức là tốt nhất Đây là tốt nhất Còn cái 56 layer nhiều nhất thì nó lại nằm ở trên cồn tức là tệ nhất Nó không còn đúng như cái mà mình mong đợi nữa Tức là càng tăng số layer thì cái độ lỗi của mình càng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 15,
      "start_timestamp": "0:10:47",
      "end_timestamp": "0:11:27"
    }
  },
  {
    "page_content": "càng tăng số layer thì cái độ lỗi của mình càng giảm hay là độ đổi của mình càng nhỏ, độ chính xác càng cao Vậy thì, cái cải tiến của Restad nó rất là đơn giản đó là nó vẫn sẽ tăng cái độ sâu, tăng cái số lớp và cái độ sâu của khi khiến trúc mạng lên thì cái này nó không được tính là cải tiến ha nhưng cái cải tiến lớn nhất của nó, cái vấn đề hay nhất của nó đó chính là cái Skip Connection Một cái mạng bình thường Feature Map x đồ vào nó sẽ thực hiện cái phép Combination, Relu, Combination, Relu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 16,
      "start_timestamp": "0:11:23",
      "end_timestamp": "0:12:13"
    }
  },
  {
    "page_content": "cái phép Combination, Relu, Combination, Relu để tạo ra cái hát Đây là cái hàm miễn đổi theo cách bình thường thì Residual, ResNet đã có cái module gọi là Residual là nó đã thực hiện phép cộng với 9 đặc trưng x đầu vào Nếu như chúng ta nhìn vô cái hàm ở đây thì chúng ta thấy công thức rất là đơn giản x thực hiện conclusion thì đây là cách làm bình thường và nó sẽ lấy dự kiện x đầu vào cộng vào chính kết quả của hai phép convolution vừa rồi thì hx là mặt fx cộng x thì đây chính là cái cải tiến",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 17,
      "start_timestamp": "0:12:10",
      "end_timestamp": "0:13:12"
    }
  },
  {
    "page_content": "hx là mặt fx cộng x thì đây chính là cái cải tiến của nó bình thường, đây là bình thường và cải tiến của nó cực kỳ đơn giản, đúng không? Vậy thì chúng ta sẽ cùng tìm hiểu xem tại sao phép biến đổi này nó có thể cải thiện được mô hình thì chúng ta phải nhắc lại đến hiện tượng ra vợ lạp vanishing Hiện tượng vanishing gradient này gây ra khi đạo hàm của hàm hợp là đạo hàm của hàm loss theo hàm 1 đạo hàm của hàm 1 theo hàm thứ 2 đạo hàm thứ n theo biến theta Trong quá trình cập nhật, đạo hàm này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 18,
      "start_timestamp": "0:13:02",
      "end_timestamp": "0:13:59"
    }
  },
  {
    "page_content": "biến theta Trong quá trình cập nhật, đạo hàm này càng lúc càng bé Và các giá trị bé mà nhân với nhau sẽ dẫn đến cái thằng này tiến về 0 Như vậy để hảm lại việc tiến đạo hàm này, trên ruôi này, đạo hàm hàm hợp này tiến về 0 thì chúng ta sẽ tìm cách tăng giá trị này lên, tăng giá trị đạo hàm lên và cách tăng rất dễ là chúng ta sẽ cộng thêm một đại lực x thì bây giờ chúng ta sẽ xem thử nếu như bình thường hàm hx của mình là hàm conclusion của x khi chúng ta tính đạo hàm h phải x thì nó sẽ là đạo",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 19,
      "start_timestamp": "0:13:55",
      "end_timestamp": "0:14:47"
    }
  },
  {
    "page_content": "chúng ta tính đạo hàm h phải x thì nó sẽ là đạo hàm của phép biến đổi convolution này bây giờ nếu như hx của mình nó sẽ là bằng convolution của x cộng thêm x thì lúc này đạo hàm của h lúc này, lưu ý là cái h của mình bây giờ mình đang dùng cái hiệu ở trên đây Đạo hàm của H của mình lúc này chính là Convolution cộng thêm một Và chính cái thao tác cộng thêm một này đã giúp cho đạo hàm của mình tăng giá trị lên Tăng cái giá trị của tương đạo hàm thành phần này lên Và các đạo hàm thành phần này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 20,
      "start_timestamp": "0:14:42",
      "end_timestamp": "0:15:22"
    }
  },
  {
    "page_content": "thành phần này lên Và các đạo hàm thành phần này tăng lên thì đa giảm của hàng này sẽ giảm xuống Vậy là cái đà giảm sẽ bị giảm xuống Và dẫn đến đó là cái chuỗi đạo hàm này nó sẽ lâu tiếng về không hơn Thì việc sử dụng cái Skip Connection này Nó sẽ giúp cho chúng ta chống được cái hiện tượng vanishing radian Và chống bionic sync radia này thì nó sẽ giúp cho chúng ta phấn luyện nhanh hơn Tại vì sao? Khi đạo hàm này đủ lớn, phấn luyện nhanh hơn Phấn luyện nhanh hơn thì do là theo tác theta là bằng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 21,
      "start_timestamp": "0:15:19",
      "end_timestamp": "0:16:07"
    }
  },
  {
    "page_content": "luyện nhanh hơn thì do là theo tác theta là bằng theta trừ cho alpha Nhân cho đạo hàm của L theo theta thì giá trị này lớn, lâu giảm Vẫn đến là bước nhảy của mình sẽ nhanh Nó sẽ nhảy nhanh Động học nhảy nhanh hơn Thì đó chính là cái cải tiến của mạng ResNet Và với một cái cải tiến vô cùng bé như thế này thôi Thì chúng ta thấy là cái impact của nó cực kỳ cao Và cho đến thời điểm hiện giờ là năm 2024 Thì những cái mạng CNN mà khi người ta nhắc đến Để mà làm một cái backbone, để làm một cái nền",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 22,
      "start_timestamp": "0:16:03",
      "end_timestamp": "0:16:49"
    }
  },
  {
    "page_content": "Để mà làm một cái backbone, để làm một cái nền tảng Để cho huấn luyện để giải quyết các bài đoán bình thị giám mỹ tính Người ta vẫn nhắc đến ResNet rất là nhiều Và bên trái đó chính là cái hình hồi nãy Hồi nãy chúng ta show là càng tăng số lượng layer lên thì độ chính xác hoặc accuracy sẽ càng giảm Tức là càng tệ, càng tăng layer lên thì nó sẽ càng tệ Độ lỗi rất là cao, nhưng khi sử dụng với ResNet thì chúng ta sẽ thấy những cái thằng có độ lỗi thấp nhất Đúng không? Là 110 layer, 56 layer, 44",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 23,
      "start_timestamp": "0:16:42",
      "end_timestamp": "0:17:01"
    }
  },
  {
    "page_content": "thấp nhất Đúng không? Là 110 layer, 56 layer, 44 layer, 32 layer, 20 layer Tức là những cái thằng nằm ở dưới là những cái thằng có số lượng layer rất là lớn Lớn hơn sau với những thằng ở trên Như vậy, nhờ cái module Skip Connection này hay còn gọi là Residual Module Thì nó đã giúp cho cái mạng của mình có khả năng là càng look càng dài hơn Rồi, và với cái cải tiến rất là đơn giản như vậy thì cái bài ResNet đạt được số lượt trích dẫu là 214.000 Tức là các bài báo trước các bạn thấy là đều dưới",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 24,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Tức là các bài báo trước các bạn thấy là đều dưới 200.000 Riêng cái bài này với cải tiến rất là đơn giản đúng không và cái thời điểm mà nó ra cũng là ra sau những cái bài kia là 2016, những bài kia là 2014-2015 thằng này ra sau nhưng mà cái số lực trích dẫn còn nhiều hơn và nhiều hơn gần gấp đôi sau với lại các cái bài trước thì đủ cho thấy là ResNet nó có một cái sức ảnh hưởng kinh khủng khiếp như thế nào",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tMKUb4k5nZw",
      "filename": "tMKUb4k5nZw",
      "title": "[CS431 - Chương 4] Part 1_3: Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
      "chunk_id": 25,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Bước tiếp theo, chúng ta sẽ khởi tạo các mô in. Rồi, CNN.View. Và ở đây chúng ta sẽ copy xuống các thêm số để tránh bị sơ sớ. Đầu tiên input dimension thì ảnh này của mình, nếu thông thường chúng ta sẽ để là 28,28. Tuy nhiên, mô hình convolution chỉ có thể thực hiện được khi nó phải làm 1 tensor 3 chiều Giờ đó ở đây chúng ta sẽ để là 28,28,1 Activation thì chúng ta sẽ để là sigmoid Conv1 chúng ta sẽ để là 6 Conv2 chúng ta sẽ để là 16 FC chúng ta sẽ để là 1 FC Lức số 1 chúng ta sẽ để là 120 FC số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TNrJYPuDADM",
      "filename": "TNrJYPuDADM",
      "title": "[CS431 - Chương 3] Part 3_2: Cài đặt mạng CNN",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:57"
    }
  },
  {
    "page_content": "để là 1 FC Lức số 1 chúng ta sẽ để là 120 FC số 2 thì chúng ta sẽ để là 84 và hàm activation ở đây thì chúng ta sẽ để là hàm sigmoid rồi, bây giờ chúng ta sẽ chạy thử và chương trình chạy được rồi bây giờ chúng ta sẽ xem coi là cái mạng CNN này trong summary xem có thể thực hiện được hay không để xem cái kích thước, cái kiến trúc của cái mạng CNN này thì chúng ta có thể thấy là trong cái mạng CNN này nó thỏa mãn được đúng như kiến trúc mà chúng ta mong muốn là bao gồm thực hiện kếp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TNrJYPuDADM",
      "filename": "TNrJYPuDADM",
      "title": "[CS431 - Chương 3] Part 3_2: Cài đặt mạng CNN",
      "chunk_id": 1,
      "start_timestamp": "0:00:54",
      "end_timestamp": "0:01:30"
    }
  },
  {
    "page_content": "mà chúng ta mong muốn là bao gồm thực hiện kếp fed-conversion số 1 với 6 filter thực hiện conversion số 2 với 16 filter rồi và cái chích thước của các tensor cũng giảm dần đó là từ 28 xuống 14 xuống 7 giống như trong thiết kế ở đây và số neuron của mình Số tham số của mình sẽ là 100.000 tham số Bây giờ chúng ta sẽ tiến hành Trend Chúng ta sẽ truyền vào 2 tham số, đó là isTrend và isTrend Tuy nhiên isTrend phải ở dạo là One Hot Rồi, thì việc Trend này đâu đó nó có thể tốn Ồ, ở đây chúng ta quên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TNrJYPuDADM",
      "filename": "TNrJYPuDADM",
      "title": "[CS431 - Chương 3] Part 3_2: Cài đặt mạng CNN",
      "chunk_id": 2,
      "start_timestamp": "0:01:27",
      "end_timestamp": "0:02:22"
    }
  },
  {
    "page_content": "này đâu đó nó có thể tốn Ồ, ở đây chúng ta quên mất một cái việc Đó là sau đây để mà có thể vẽ được cái KamLoss, vẽ được cái giá trị loss theo số ipop, chúng ta sẽ phải gán vào một cái biến, đó là History Rồi sau đó thì ở đây ta mới có thể thực hiện được cái việc trực quan hóa này Rồi, để trực quan hóa cho cái mô hình thì chúng ta sẽ phải lấy ra các filter thì ở đây chúng ta sẽ lấy ra filter nốt đầu tiên đó chính là cnn.get-way get-way ở đây chúng ta sẽ để layer số 1 tại vì layer số 0 là input",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TNrJYPuDADM",
      "filename": "TNrJYPuDADM",
      "title": "[CS431 - Chương 3] Part 3_2: Cài đặt mạng CNN",
      "chunk_id": 3,
      "start_timestamp": "0:02:17",
      "end_timestamp": "0:03:10"
    }
  },
  {
    "page_content": "ta sẽ để layer số 1 tại vì layer số 0 là input rồi layer số 1 là kết thúc hình xanh rồi chúng ta sẽ cùng quan sát Nhưng mà đương nhiên là phải chờ cái mô hình này húi ra xong thì chúng ta mới có thể thấy được cái Amplos này chạy như thế nào Ở đây thì chúng ta quan sát thấy là cái Amplos của mình đã giảm từ 0.18 trong cái E-Pop đầu tiên Giảm xuống còn 0.13, giảm xuống còn 0.10 và đến cái E-Pop thứ 25, 26 thì giảm xuống còn 0.01 và hi vọng là đến hip hop số 30 thì loss của mình đã giảm xuống",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TNrJYPuDADM",
      "filename": "TNrJYPuDADM",
      "title": "[CS431 - Chương 3] Part 3_2: Cài đặt mạng CNN",
      "chunk_id": 4,
      "start_timestamp": "0:03:01",
      "end_timestamp": "0:03:55"
    }
  },
  {
    "page_content": "đến hip hop số 30 thì loss của mình đã giảm xuống 0.007 và accuracy cho tập dữ liệu trend đã lên đến 99.85% và chúng ta quan sát thì loss giảm rất tốt chúng ta quan sát trend loss này giảm rất tốt bây giờ chúng ta sẽ xem mytrnw có giá trị là bao nhiêu thì ở đây chúng ta sẽ thấy là cái W này sẽ là một Array, xin lỗi là một List bao gồm hai phần tử, thì phần tử đầu tiên chính là cái số, trọng số, số filter của phép biến mựa Comparison đầu tiên và thành phần thứ hai chính là BIAS, tại vì chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TNrJYPuDADM",
      "filename": "TNrJYPuDADM",
      "title": "[CS431 - Chương 3] Part 3_2: Cài đặt mạng CNN",
      "chunk_id": 5,
      "start_timestamp": "0:03:49",
      "end_timestamp": "0:04:41"
    }
  },
  {
    "page_content": "thành phần thứ hai chính là BIAS, tại vì chúng ta có sử dụng BIAS W không chính là trọng số của mình Để xem trọng số này có kích thước bao nhiêu Chúng ta là chấm shape, trong đó 3, 1, 6 3, 3 chính là kích thước của kênh kênh lộ và 1 chính là input, dimension của input của đầu vào chỉ có 1 kênh lộ, nó sẽ là 1 output của mình sẽ là 6, 6 filter để trực quan, chúng ta sẽ có số filter là 6 Rồi chúng ta sẽ duyệt qua A từ 0 đến 5 để truyền vô đây. Rồi đây là W0, W0.sh, 3316. Thì chúng ta sẽ lấy cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TNrJYPuDADM",
      "filename": "TNrJYPuDADM",
      "title": "[CS431 - Chương 3] Part 3_2: Cài đặt mạng CNN",
      "chunk_id": 6,
      "start_timestamp": "0:04:37",
      "end_timestamp": "0:05:20"
    }
  },
  {
    "page_content": "đây là W0, W0.sh, 3316. Thì chúng ta sẽ lấy cái chỉ số Y chạy ở đây trước, rồi sau đó lấy chỉ số Z chạy ở đây. Thì ở đây một cách tổng quát, trong lớp conversion số 2, thì số 1 này sẽ chóa, chuyển thành là số 16. Vì vậy ở đây chúng ta sẽ để là, Y là chạy cho 1 cái rank, rank này thì ở đây chúng ta để là 1, nhưng mà sắp tới có thể để là 16. Đây chính là 6 filter ở cái nấp đầu tiên. Thì chúng ta có thể hiểu ý nghĩa của filter này là chúng ta lấy sai số, sự chênh lệch của vùng phía bên phải, phía",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TNrJYPuDADM",
      "filename": "TNrJYPuDADM",
      "title": "[CS431 - Chương 3] Part 3_2: Cài đặt mạng CNN",
      "chunk_id": 7,
      "start_timestamp": "0:05:18",
      "end_timestamp": "0:06:04"
    }
  },
  {
    "page_content": "số, sự chênh lệch của vùng phía bên phải, phía dưới, so với lại vùng phía trái, bên trên. Ý nghĩa của filter này là lấy sự chênh lệch giữa hàng ở giữa sau với lại hai cái hàng ở phía trên và phía dưới thì mỗi một cái filter này sẽ thể hiện một đặc trưng mà đạo rồi tiếp theo là chúng ta sẽ tiến hành thử nghiệm với một số biến thể khác nhau nhưng trước khi qua thử nghiệm một số biến thể khác nhau thì chúng ta sẽ thử cái hàng Redic cái hàng Redic thì cln.redic Chúng ta sẽ truyền vào xstat và mẫu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TNrJYPuDADM",
      "filename": "TNrJYPuDADM",
      "title": "[CS431 - Chương 3] Part 3_2: Cài đặt mạng CNN",
      "chunk_id": 8,
      "start_timestamp": "0:06:00",
      "end_timestamp": "0:07:34"
    }
  },
  {
    "page_content": "thì cln.redic Chúng ta sẽ truyền vào xstat và mẫu dữ liệu thứ 300 Rồi Ở đây thì hàm predict, chúng ta sẽ xem lại hàm predict của mình truyền vào cell.module.xtest bây giờ chúng ta sẽ xem tiếp xtest đã được last rồi và đã được chuẩn hóa rồi đúng không? rồi bây giờ chúng ta sẽ thử truyền vào như thế này Thứ modest thì xét mifornia �� recovered và xét m pandemia Shinry Cooper thì xét m partnering và 28 x 1 rồi sau đó chúng ta mới đưa vào để cho cái mùi mình có thể predict được cnn.predict rồi ồ,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TNrJYPuDADM",
      "filename": "TNrJYPuDADM",
      "title": "[CS431 - Chương 3] Part 3_2: Cài đặt mạng CNN",
      "chunk_id": 9,
      "start_timestamp": "0:07:29",
      "end_timestamp": "0:08:41"
    }
  },
  {
    "page_content": "mùi mình có thể predict được cnn.predict rồi ồ, cũng chưa được nè Rồi, ở đây, cái số này sẽ phải để lên trước, cái này sẽ phải để lên trước là 1,28 Ok, được rồi. Tức là nó sẽ phải để cái chỉ số của cái thứ tự lên trước, nó sẽ hơi ngược bây giờ chúng ta sẽ thử xem cái nhãn này nó sẽ ra cái giá trị là bao nhiêu tại vì ở đây nó chỉ trả ra một cái vector one hot chúng ta sẽ phải có thêm một cái hàm nữa đó là argument max là np.argument max rồi nó sẽ là 4 Bây giờ chúng ta sẽ xem mổ thứ 300 này x y",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TNrJYPuDADM",
      "filename": "TNrJYPuDADM",
      "title": "[CS431 - Chương 3] Part 3_2: Cài đặt mạng CNN",
      "chunk_id": 10,
      "start_timestamp": "0:08:37",
      "end_timestamp": "0:09:49"
    }
  },
  {
    "page_content": "là 4 Bây giờ chúng ta sẽ xem mổ thứ 300 này x y tx của mình thứ 300 đó là bằng bao nhiêu? đó là mũ Rồi, bây giờ chúng ta sẽ thử những mổ khác chúng ta sẽ thử những mổ khác ở đây chúng ta sẽ để là Relic Dream nhẽ dự đoán là red còn ở đây sẽ là nhãn thực tế và ở đây chỉ số này chúng ta sẽ thêm số hóa nữa idx là bằng 100 dù vậy Và chúng ta sẽ để đây là idx Rồi, thì đại đa số chúng ta thấy là cái độ chính xác rất là cao Chúng ta thử rất nhiều những cái nhãn khác giao Thì nó đều ra là nhựa đoán và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TNrJYPuDADM",
      "filename": "TNrJYPuDADM",
      "title": "[CS431 - Chương 3] Part 3_2: Cài đặt mạng CNN",
      "chunk_id": 11,
      "start_timestamp": "0:09:45",
      "end_timestamp": "0:10:25"
    }
  },
  {
    "page_content": "cái nhãn khác giao Thì nó đều ra là nhựa đoán và thực tế không phải nhau Bây giờ, trong cái mạng CNN thì chúng ta thấy nó có rất nhiều những cái module khác nhau Và tại thời điểm hiện tại thì chúng ta sẽ chưa hiểu rõ cái vai trò của từng module này Do đó thì chúng ta sẽ làm một cái thuyết nghiệm, nó gọi là Application Study với các biến thể khác nhau bằng cách đó là chúng ta sẽ lần lượt thay đổi một số cái cấu hình của chương trình của mình chúng ta sẽ thay đổi một số cái cấu hình Thì cái phiên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TNrJYPuDADM",
      "filename": "TNrJYPuDADM",
      "title": "[CS431 - Chương 3] Part 3_2: Cài đặt mạng CNN",
      "chunk_id": 12,
      "start_timestamp": "0:10:19",
      "end_timestamp": "0:11:29"
    }
  },
  {
    "page_content": "ta sẽ thay đổi một số cái cấu hình Thì cái phiên bản, cái biến thể đầu tiên đó là chúng ta sẽ bỏ đi cái thay cái hàm sigmoid bằng relu chúng ta sẽ thay cái sigmoid bằng relu Vì vậy chúng ta sẽ cốp tập kód ở đây đem số Chúng ta sẽ đem nền hàm này thay sigmoid bằng relu Vì vậy bản chất là biến thể này chúng ta không cần phải kệ thạc lại Mà chúng ta chỉ sửa tham số của mình thôi Chúng ta chỉ sửa tham số khi gọi hầm build thôi Rồi, và đây là relu Rồi, sau đó chúng ta sẽ tiến hành là cnn.trend và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TNrJYPuDADM",
      "filename": "TNrJYPuDADM",
      "title": "[CS431 - Chương 3] Part 3_2: Cài đặt mạng CNN",
      "chunk_id": 13,
      "start_timestamp": "0:11:24",
      "end_timestamp": "0:12:32"
    }
  },
  {
    "page_content": "Rồi, sau đó chúng ta sẽ tiến hành là cnn.trend và ist trend, rồi i trend oh Và lưu ý ở đây chúng ta sẽ để cái History là History số 2 Rồi, bây giờ chúng ta sẽ tiến hành build cái này Và tranh thủ trong thời gian chờ đợi thì chúng ta sẽ thử viết code trước cho cái phần là vẽ cái giá trị loss Chúng ta sẽ thêm một cái đường nữa, đó là History số 2 Và ở đây sẽ là trainLogs v1 Đây sẽ là trainLogs v2 Trong đó v2 đó là dùng value Dùng value Rồi, tương tự như vậy, bây giờ chúng ta sẽ chờ đợi Chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TNrJYPuDADM",
      "filename": "TNrJYPuDADM",
      "title": "[CS431 - Chương 3] Part 3_2: Cài đặt mạng CNN",
      "chunk_id": 14,
      "start_timestamp": "0:12:23",
      "end_timestamp": "0:12:44"
    }
  },
  {
    "page_content": "tự như vậy, bây giờ chúng ta sẽ chờ đợi Chúng ta sẽ viết trước cái code cho các biến thể Tiếp theo, biến thể bỏ hết các lớp Cooling thì chúng ta làm cũng rất là nhanh Cooling chúng ta sẽ bỏ xóa đi và lưu ý là phải để gối đầu các cái biến Ví dụ như ở đây C1 thì sẽ được truyền trực tiếp sang đây rồi C3 thì sẽ truyền trực tiếp sang đây Vì vậy chúng ta đã xong biến thể số 3, chúng ta sẽ để là CNSv3",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TNrJYPuDADM",
      "filename": "TNrJYPuDADM",
      "title": "[CS431 - Chương 3] Part 3_2: Cài đặt mạng CNN",
      "chunk_id": 15,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Trong phần tiếp theo, chúng ta sẽ cùng tìm hiểu về kiến trúc mạng Recurrent Neural Network. Đầu tiên, chúng ta sẽ phải xem dạng triển khai của mạng Recurrent Network. Thứ nhất, chúng ta xem đối với mạng Neural Network, điểm yếu của nó là chúng ta không thể mả hóa được yếu tố về mặt trình tự của các từ. Còn các cái từ, ví dụ như cái từ thứ xt trừ 1, xt, rồi xt cộng 1 Chúng ta đưa nó về một cái dạng Better One Hot như thế này thì rõ ràng là nó sẽ không biết cái từ nào là từ xuất hiện trước, từ nào",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:48"
    }
  },
  {
    "page_content": "biết cái từ nào là từ xuất hiện trước, từ nào là xuất hiện sau Thế thì, đối với mạng Recurrent, Neural Network, thì cái yếu tố là Recurrent Vì tiếng Việt đó là hồi quy Thì hồi quy chính là cơ chế để giúp cho mình mở hóa yếu tố về mặt trình tự Nó chính là mở hóa yếu tố về mặt trình tự Cách thức mà mình mở hóa nó là như thế nào? Khi chúng ta gặp từ thứ x t chúng ta đưa vào và bây giờ tạm thời chúng ta sẽ chưa còn biết là cái mạng này nó tính toán như thế nào chúng ta đi tính cái giá trị thứ st",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 1,
      "start_timestamp": "0:00:41",
      "end_timestamp": "0:01:34"
    }
  },
  {
    "page_content": "như thế nào chúng ta đi tính cái giá trị thứ st trường 1 rồi sau đó chúng ta đi tính cái giá trị output và khi chúng ta tính được cái từ thứ st trường 1 xong chúng ta lan truyền cái thông tin này đến cái nốt tiếp theo và chúng ta lại nhận cái thông tin tại thời điểm thứ st Và tại thời điểm thứ x t này, thì chúng ta sẽ kết hợp cả cái thông tin của quá khứ, tức là thông tin của cái x t-1. Khi đã xử lý cái từ x t-1 rồi, nó tạo ra cái thông tin là x t-1. Thì đây chính là quá khứ. Và cái quá khứ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 2,
      "start_timestamp": "0:01:31",
      "end_timestamp": "0:02:21"
    }
  },
  {
    "page_content": "x t-1. Thì đây chính là quá khứ. Và cái quá khứ này, nó sẽ kết hợp với thông tin của thời điểm hiện tại. Để tổng hợp thông tin, thì như vậy là ST là nó mang tính chất gọi là tổng hợp Tổng hợp thông tin Khi đó, việc đưa ra dự đoán giá trị y t y ngã t, nó sẽ mang đầy đủ thông tin của những từ trước đó và những từ hiện tại Và đồng thời là từ trước đó nó sẽ có trước, rồi nó sẽ kết hợp với từ hiện tại, thì nó sẽ giúp cho cái việc phán đoán này nó sẽ toàn nhiệt hơn. Và như vậy thì cái yếu tố hồi quy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 3,
      "start_timestamp": "0:02:12",
      "end_timestamp": "0:03:11"
    }
  },
  {
    "page_content": "toàn nhiệt hơn. Và như vậy thì cái yếu tố hồi quy nó thể hiện ở chỗ đó là cái quá trình này được lặp đi lặp lại. Cái ST này sẽ lại tiếp tục lan truyền đến cho cái thời điểm thứ T cộng 1. thì nó sẽ là quá khứ của ST-1. XT-1 là hiện tại mới kết hợp với quá khứ trước đó để tổng hợp thông tin và đưa ra output tiếp theo. Đây là cái dạng truyển khai, tức là cách thức mà chúng ta truyển khai các từ đầu vào trong một mô hình Recurrent Neural Network. Và viết như vậy thì nó cũng sẽ hơi tác quá. thì chút",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 4,
      "start_timestamp": "0:03:09",
      "end_timestamp": "0:03:51"
    }
  },
  {
    "page_content": "viết như vậy thì nó cũng sẽ hơi tác quá. thì chút nữa chúng ta sẽ có cái dạng gọi là dạng vút gọt Vào là thay vì chúng ta đưa vào x1, x2 trên xt thì ở đây chúng ta chỉ cần ký hiệu là x thôi và đầu ra sẽ là giá trị y ngã và ở đây chúng ta sẽ vẽ một cái vòng hồi quy x sẽ được đưa trở lại cho cái node s này và ở đây chúng ta sẽ có thống nhất với nhau về mặt ký hiệu Đối với cái dữ kiện đầu vào xt thì cái xt này thì t là có thể thay đổi cái độ dài, tức là t sẽ di chuyển từ t có thể giao động từ 1",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 5,
      "start_timestamp": "0:03:49",
      "end_timestamp": "0:04:42"
    }
  },
  {
    "page_content": "tức là t sẽ di chuyển từ t có thể giao động từ 1 cho đến t lớn. t sẽ thay đổi chiều dài của mình từ 1 cho đến t lớn. Và tại một cái thời điểm t hiện tại là xt, chúng ta sẽ đi tính cái giá trị dự đoán. Chúng ta sẽ đi tính giá trị dự đoán là ký hiệu bằng y ngã t. Và ở đây có một cái nụ ý cực kỳ quan trọng đó là các bộ tham số u, v và w này là chúng ta sẽ dùng chung, dùng chung cho mỗi bước tính toán. Cho ví dụ chúng ta tính với st-1, hay tính với xt, hay tính với xt-1, chúng ta đều sử dụng chung",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 6,
      "start_timestamp": "0:04:33",
      "end_timestamp": "0:05:39"
    }
  },
  {
    "page_content": "xt, hay tính với xt-1, chúng ta đều sử dụng chung các bộ trọng số này. S-T này được gọi là trạng thái ẩn. Đây là ký hiệu và quy ức về cách tặt tên cho mạng neural network về sau. X sẽ là input, S sẽ là dự đoán, S sẽ là trạng thái ẩn của mô hình. Và các cái bộ ma trận U, V và W chính là các cái tham số của mô bình Và như vậy thì ANN đã có thể, ENCODE có thể mã hóa được cái thứ tự, cái trình tự của các cái tự trong một phần bảng thông qua cái cơ chế là cơ chế hồi quy Bây giờ chúng ta sẽ đến với",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 7,
      "start_timestamp": "0:05:29",
      "end_timestamp": "0:06:39"
    }
  },
  {
    "page_content": "chế là cơ chế hồi quy Bây giờ chúng ta sẽ đến với các bước để xây dựng một mô hình dĩ dạng công thức Đầu tiên là bước số 1 là thiết kế hàm mô hình của mình Cho trước các quần vector là x1, xt, xt, xt, xt Thì ở đây chúng ta lưu ý, đây là World Vector, khái niệm World Vector thì chúng ta đã học ở trong vài trước rồi đó chính là Vector Embedding hay Vector Biểu diễn của từ W1 hoặc XT hòa này sẽ là Vector Biểu diễn của từ WT Rồi, và tại mỗi thời điểm hay còn gọi là timestag, tức là tại mỗi thời",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 8,
      "start_timestamp": "0:06:32",
      "end_timestamp": "0:07:27"
    }
  },
  {
    "page_content": "điểm hay còn gọi là timestag, tức là tại mỗi thời điểm t, thì chúng ta sẽ có nhận dữ kệ đầu vào là st. Và chúng ta sẽ tính toán cái giá trị trạng thái ẩn st dựa trên cái công thức này. dựa trên công thức này. Thì cái ST, ST sẽ có công thức như sau là bằng hàm kích hoa sigmoid hoặc là hàm tanh. Hàm này có thể là hàm sigmoid hoặc là hàm tanh hoặc là hàm tanh. Rồi, và nó sẽ phối hợp cái thông tin của qua khứ Đây là qua khứ và đây là hiện tại. Rồi Còn đây là hiện tại. Và hai cái ma trận U và W ở",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 9,
      "start_timestamp": "0:07:23",
      "end_timestamp": "0:08:31"
    }
  },
  {
    "page_content": "Còn đây là hiện tại. Và hai cái ma trận U và W ở đây, nó sẽ giúp cho chúng ta ánh sạ hai cái vector là XT và ST về cùng một cái không gian. và sau đó nó sẽ tổng hợp thông tin lại với nhau tổng hợp thông tin lại, rồi từ đó qua hàm kích hoàng để ra trạng thái ẩn trạng thái ẩn ST, nhưng mà ST nó đã chứa đầy đủ thông tin chứa đầy đủ thông tin để giúp cho mình đưa ra giá trị dự đoán ST là đủ thông tin để mình dự đoán Và để dự đoán kết quả thì chúng ta sẽ nâng với ma trận V và qua hàm Submax để ra",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 10,
      "start_timestamp": "0:08:26",
      "end_timestamp": "0:09:09"
    }
  },
  {
    "page_content": "ta sẽ nâng với ma trận V và qua hàm Submax để ra y-t Trong một số tài liệu, người ta sẽ ký hiệu là tất cả ma trận tham số người ta sẽ để là U chuyển vị, hoặc là W chuyển vị, rồi V chuyển vị Thì trong tài liệu này thì chúng ta sẽ lựa chọn cách thức ký hiệu u, v, w sao cho nó gọn nhất thật ra là cả hai cách thì đều giống nhau thôi ha đều đều khác đó chỉ là u, trong cái u chuẩn bị trong các tài liệu trước thì nó chính là cái chuẩn bị cho tức là nó sẽ có cái kích thước nó khác so với lại cái u của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 11,
      "start_timestamp": "0:09:06",
      "end_timestamp": "0:10:01"
    }
  },
  {
    "page_content": "sẽ có cái kích thước nó khác so với lại cái u của cái hệ thống bài dạng ở đây ví dụ như nếu u ở cái hiện trước ở trong cái bài dạng trước hoặc là cái bài hình dạng khác nó có kích thước là v nhân với là n V là số tập, số từ trong từ điển N là chiều dài của vector biểu diễn Ở bên đây, U sẽ có kích thước là N x V Nó sẽ ngược lại một chút Nhưng cái đó thì không quá quan trọng Ý nghĩa của nó vẫn giống nhau và bài tập cho chúng ta đó chính là cho trước các thông tin về cái độ dài của cái Xt Xt và y",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 12,
      "start_timestamp": "0:09:58",
      "end_timestamp": "0:10:54"
    }
  },
  {
    "page_content": "các thông tin về cái độ dài của cái Xt Xt và y ngã t là như sau Xt là vector có 8.000 triều, có 8.000 phần tử Xt là một vector 100 triều và y ngã t là một vector có 8.000 triều hay 8.000 phần tử Câu hỏi đặt ra là kích thước của tham số u, v và w trong trường hợp này sẽ là bao nhiêu? Hay nói cách khác, đó là u sẽ thuộc 1 cái r bao nhiêu nhân với lại bao nhiêu? Chúng ta sẽ cùng làm thử bài tập như sau. Đầu tiên, chúng ta sẽ phải bám vào 2 cái công thức này. Đây là hai cái công thức để giúp cho",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 13,
      "start_timestamp": "0:10:40",
      "end_timestamp": "0:11:35"
    }
  },
  {
    "page_content": "thức này. Đây là hai cái công thức để giúp cho chúng ta xác định được cái độ dài của u, v và w. Thì hàm activation, ở đây giả xử như chúng ta gọi là hàm sigmoid luôn đi ha. Thì đây là một cái hàm mà theo kiểu là thực hiện trên từng phần tử hay còn gọi là 11. 11 whites nó sẽ tính trên phần tử từng phần tử, do đó qua hàm sigmoid nó sẽ không dứt nó sẽ không làm thay đổi kích thước của cái vector của mình bí dụ đầu vào của hàm sigmoid này nó là một ma trận hoặc một vector nào đó thì qua hàm sigmoid",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 14,
      "start_timestamp": "0:11:30",
      "end_timestamp": "0:12:30"
    }
  },
  {
    "page_content": "trận hoặc một vector nào đó thì qua hàm sigmoid nó sẽ không làm thay đổi như vậy, chúng ta đã biết st là một ma trận kích thức xin lỗi là một vector có 100 phần tử hay biết như dạng ba trận thì nó sẽ là 100 nhân 1 như vậy thì toàn bộ cái phép cộng này nó sẽ là 100 nhân 1 mà cái phép cộng này thì nó cũng là 11Y rất là tính trên từng vần tử do đó thì hai cái này uxt và wxt triều 1 nó cũng là kết quả của nó cũng là các cái vector có kích thước là 100 x 1 Như vậy thì chúng ta sẽ bám vào cái nhận",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 15,
      "start_timestamp": "0:12:21",
      "end_timestamp": "0:13:29"
    }
  },
  {
    "page_content": "100 x 1 Như vậy thì chúng ta sẽ bám vào cái nhận xét này để dự đoán, để xác định xem u, v, w là kích thước bao nhiêu thì chúng ta sẽ lấy ra cái u nhân xt trước thì u của mình nó sẽ là kích thước bao nhiêu nhân bao nhiêu, mình chưa biết Xt là 8.000 x 1 Xt là 8.000 x 1 Và ở đây sẽ là bao nhiêu? Mình không biết Và đầu ra thì nó sẽ tạo ra là một cái vector có kích thước là 100 x 1 để x,y và x,t có thể nhân được với nhau, giá trị ở đây phải khớp số cột của u tương ứng với số dòng của x do đó đáp số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 16,
      "start_timestamp": "0:13:19",
      "end_timestamp": "0:14:12"
    }
  },
  {
    "page_content": "của u tương ứng với số dòng của x do đó đáp số của mình sẽ là 8.000 và khi nhân 2 cái ma trận này với nhau thì cái 100 nó sẽ tạo ra vét tơ là 100 nhân 1 như vậy thì ở đây số của mình nó sẽ là 100 như vậy u của mình sẽ có kích thước đó là 100 nhân cho 8 ngàn Tương tự như vậy, chúng ta sẽ thực hiện thao tác cho phép biến đổi là w nhân với nền st-1 W của mình sẽ là bằng bao nhiêu nhân bao nhiêu? Mình chưa biết St-1, st cũng tương tự như st-1, đó là một vector có kích thước là 100 nhân 1 Và thời",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 17,
      "start_timestamp": "0:14:07",
      "end_timestamp": "0:15:11"
    }
  },
  {
    "page_content": "là một vector có kích thước là 100 nhân 1 Và thời này output của nó sẽ ra là 1 cái vector cũng là 100 nhân 1 luôn. Rồi, như vậy thì chúng ta sẽ dùng các quy tắc về số chiều của nhân 2 ma trận để 2 ma trận W và ST có thể nhân được với nhau. Thì ở đây, số này phải giống với số này, đó là 100. Số cột của W sẽ giống với lại số hàng của ST, thì là 100 sẽ khớp với 100. và ở đây sẽ là 100 luôn như vậy w của mình sẽ là cái vector w của mình sẽ là một cái ma trận kích thước là 100 nhận quay lại 100 rồi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 18,
      "start_timestamp": "0:15:08",
      "end_timestamp": "0:15:40"
    }
  },
  {
    "page_content": "ma trận kích thước là 100 nhận quay lại 100 rồi thời tháng xông lên thì chúng ta sẽ tính xem cái ma trận v sẽ là bao nhiêu thì tương tự như sigmoid, softmax Max cũng là một cái hàm đảm bảo dựng nguyên số chiều khi chúng ta biến đổi Y của mình sẽ là một cái ma trận kích thước là 8.000 x 1 V của mình sẽ là kích thước là bao nhiêu? Mình không biết, mình sẽ để ở đây ST sẽ là 100 x 1 như vậy ở đây chúng ta muốn thực hiện được phép nhân này thì số cột của V sẽ phải là 100 và output của mình là ra 1",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 19,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "của V sẽ phải là 100 và output của mình là ra 1 vector ra 1 ma trận kích thước là 8.000 x 1 như vậy ở đây nó sẽ phải là 8.000 tóm lại, V sẽ là 1 ma trận kích thước là 8.000 x 100 Vì vậy, chúng ta đã có được 3 cái đáp án cho bài tập ở đây.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TqKBlC-zyKY",
      "filename": "TqKBlC-zyKY",
      "title": "[CS431 - Chương 7] Part 2_1: Kiến trúc mạng RNN",
      "chunk_id": 20,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Tiếp theo, chúng ta sẽ xem xét về mối quan hệ về mặt ngữ nghĩa của các từ này. Chúng ta sẽ xét 4 từ là king, queen, man và woman. Nếu chúng ta lấy vector king trừ cho king, chúng ta sẽ có vector đỏ. Lấy woman trừ cho man, chúng ta sẽ có vector đỏ. và người ta quan sát rằng là hình như hai cái vector này sắp xỉ nhau đó là cái vector nó thể hiện mối quan hệ về mặt giới tính giữa giới tính nam và giới tính nữ nếu như giới tính nam gọi là men thì giới tính nữ gọi là woman thì truyền đựng như vậy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UfLLBOPvgOU",
      "filename": "UfLLBOPvgOU",
      "title": "[CS431 - Chương 6] Part 5_2: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:44"
    }
  },
  {
    "page_content": "giới tính nữ gọi là woman thì truyền đựng như vậy người đàn ông mà quyền lực là king thì người phụ nữ quyền lực là queen, nữ hoàng Chúng ta sẽ xét một ví dụ để kiểm tra tính mốt quan hệ về mặt giới tính đó là King chúng ta sẽ xét xem các pattern này có gần nhau hay không nếu chúng ta đặt Queen là x thì x chừ King có bằng woman chừ cho men hay không thì ở đây, ở đây là x nè Rồi, và chúng ta xem coi x chúng ta sẽ kiểm tra xem x có xấp xỉ, hoặc có gần với từ win hay không Nếu như x-king là x-king,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UfLLBOPvgOU",
      "filename": "UfLLBOPvgOU",
      "title": "[CS431 - Chương 6] Part 5_2: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 1,
      "start_timestamp": "0:00:43",
      "end_timestamp": "0:02:09"
    }
  },
  {
    "page_content": "với từ win hay không Nếu như x-king là x-king, x-king là woman-man Khi muốn tính x, chúng ta đem vết king qua bên tay phải và thay dấu trừ thành dấu cộng Vì vậy chúng ta sẽ thấy woman và king là positive tức là dấu cộng, men là âm thì ở đây chúng ta sẽ dùng model.modeSimilarity và posity tức là dấu cọng là CoolMan và King và negative sẽ là Man thì để xem kết quả ra như thế nào kết quả là tựa gần với x này nhất Từ gần với x này nhất là Win Từ gần tiếp theo là Monarch Từ tiếp theo là Princess Từ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UfLLBOPvgOU",
      "filename": "UfLLBOPvgOU",
      "title": "[CS431 - Chương 6] Part 5_2: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 2,
      "start_timestamp": "0:02:00",
      "end_timestamp": "0:03:10"
    }
  },
  {
    "page_content": "tiếp theo là Monarch Từ tiếp theo là Princess Từ gần nhất là Win Vậy thì rõ ràng là cái mệnh đề này là đúng Tức là cái vẻ thuyết này là đúng Vector màu đỏ này là trùng nhau Hoặc là có giá trị sắp xịn nhau Bây giờ chúng ta sẽ xem cái góc độ của câu này. Cái góc độ này tí dạng là câu hỏi. Nếu như người đàn ông mà quyền lực thì gọi là vua, thì hỏi người phụ nữ quyền lực là gì? Thì đây chính là cái dạng formulae, cái dạng mà đưa về câu hỏi giả thuyết. Nếu như người đàn ông là quỳnh lực, người phụ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UfLLBOPvgOU",
      "filename": "UfLLBOPvgOU",
      "title": "[CS431 - Chương 6] Part 5_2: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 3,
      "start_timestamp": "0:03:08",
      "end_timestamp": "0:03:57"
    }
  },
  {
    "page_content": "Nếu như người đàn ông là quỳnh lực, người phụ nữ là quỳnh lực là gì? thì chúng ta sẽ có được công thức này Vậy thì bây giờ chúng ta sẽ thử trả lời câu hỏi sau Nếu vua mà là đàn ông thì hỏi nữ hoàng là gì? ở đây chúng ta sẽ có nếu vua king trừ man tức là king là đàn ông thì hỏi win là gì? tức là x từ cái này chúng ta sẽ suy ra là x sẽ là bằng x đem qua trái, win thì vẫn nguyên Cộng cho MEN, KING là dấu cộng, KÔN là dấu trừ Cộng này là KÔN, KÔN là KÔN và negati sẽ là king thì ở đây chúng ta nên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UfLLBOPvgOU",
      "filename": "UfLLBOPvgOU",
      "title": "[CS431 - Chương 6] Part 5_2: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 4,
      "start_timestamp": "0:03:51",
      "end_timestamp": "0:05:18"
    }
  },
  {
    "page_content": "KÔN và negati sẽ là king thì ở đây chúng ta nên có một câu lệnh in ra cho nó hợp lý một chút cho nó dễ hiểu một chút thì ở đây gọi là kết quả trung gian này chúng ta sẽ gọi là resolve Và đây chúng ta sẽ in ra màn hình là if king is a man then win is a Rồi, thì chúng ta sẽ lấy cái kết quả cuối cùng Result không xin lỗi, cái kết quả đầu tiên, tức là cái vector nào, cái tư nào gần với x này nhất nếu vua là đàn ông thì nữ hoàng là phụ nữ, nữ mẹ thì đây là một trong những cái tính chấp bảo toàn về",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UfLLBOPvgOU",
      "filename": "UfLLBOPvgOU",
      "title": "[CS431 - Chương 6] Part 5_2: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 5,
      "start_timestamp": "0:05:02",
      "end_timestamp": "0:06:32"
    }
  },
  {
    "page_content": "đây là một trong những cái tính chấp bảo toàn về quan hệ về giới tính Bây giờ chúng ta sẽ làm các ví dụ tương tự các bài tập cho mối quan hệ về thủ đô quốc gia danh từ số nhiều, động từ và ba Đầu tiên là cho quan hệ thủ đô quốc gia Thủ đô của Việt Nam là Hà Nội, thì thủ đô của Đức là gì? x, đối x, đối trừ đây là mối quan hệ về đất nước, thủ đô hỏi đất nước Germany thì thủ đô sẽ là gì? chúng ta sẽ đưa công thức này ra là x sẽ là bằng Germany Việt Nam và Hà Nội đem qua bên đáy phải đúng không?",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UfLLBOPvgOU",
      "filename": "UfLLBOPvgOU",
      "title": "[CS431 - Chương 6] Part 5_2: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 6,
      "start_timestamp": "0:06:10",
      "end_timestamp": "0:07:52"
    }
  },
  {
    "page_content": "Nam và Hà Nội đem qua bên đáy phải đúng không? Việt Nam và Hà Nội đem qua bên phải đối trừ x đem qua bên trái Germany cộng cho Hà Nội và trường cho Việt Nam Copying nội dung trên đây Germany, Hà Nội, Negative là Việt Nam và ở đây chúng ta sẽ có câu là If Hà Nội is the capital of Vietnam, then the capital of Germany is... Rồi, chúng ta sẽ xem cái kết quả của mình là gì Nếu Hà Nội là thủ đô của Việt Nam thì thủ đô của Đức sẽ là Berlin Và chúng ta hoàn toàn có thể thay thế cho những đất nước khác",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UfLLBOPvgOU",
      "filename": "UfLLBOPvgOU",
      "title": "[CS431 - Chương 6] Part 5_2: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 7,
      "start_timestamp": "0:07:49",
      "end_timestamp": "0:08:55"
    }
  },
  {
    "page_content": "hoàn toàn có thể thay thế cho những đất nước khác Ví dụ như là China, Trung Quốc và Bắc Kinh Thì đây là mối quan hệ về thủ đô quốc gia Tương tự như vậy, chúng ta sẽ có danh từ số x số nhiều Để cho nhanh, chúng ta sẽ có books Tương ứng sẽ là books có s Chúng ta sẽ xem trong từ box Tương ứng là gì Chúng ta sẽ đem x qua X sẽ là bằng box cộng cho books có s trừ cho books Rồi, như vậy là Box, đây sẽ là Book và đây sẽ là Book không có S Để cho nhanh thì chúng ta không cần ghi câu dẫn nhập và đồng tư",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UfLLBOPvgOU",
      "filename": "UfLLBOPvgOU",
      "title": "[CS431 - Chương 6] Part 5_2: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 8,
      "start_timestamp": "0:08:42",
      "end_timestamp": "0:10:17"
    }
  },
  {
    "page_content": "chúng ta không cần ghi câu dẫn nhập và đồng tư với ba tương tự như vậy cho mối quan hệ đồng tư với ba ví dụ như đồng tư có Gold thì tương lý là Gold x sẽ là bằng tag cộng cho run chừng cho go Rồi Tag Rồi Và negative sẽ là Go Tagging Thì đây chính là vấp 3 của từ Tag Như vậy thì trong cái tutorial này thì chúng ta đã cùng lượt qua môi in word to back và thử nghiệm với một số cách để tính vector biểu diễn, so sánh giữa các từ với nhau thông qua vector biểu diễn và kiểm tra xem mối quan hệ về mặt",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UfLLBOPvgOU",
      "filename": "UfLLBOPvgOU",
      "title": "[CS431 - Chương 6] Part 5_2: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 9,
      "start_timestamp": "0:10:07",
      "end_timestamp": "0:10:24"
    }
  },
  {
    "page_content": "biểu diễn và kiểm tra xem mối quan hệ về mặt thử nghĩa của các từ trong câu.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UfLLBOPvgOU",
      "filename": "UfLLBOPvgOU",
      "title": "[CS431 - Chương 6] Part 5_2: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 10,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Một trong những mô hình rất nổi tiếng, phổ biến hiện nay là mô hình Wortuvac. Mô hình Wortuvac được Thomas Picolop và các cộng sự giới thiệu vào năm 2013, tính đến thời điểm hiện nay đã được hơn 11 năm. Và mô hình này thì nó sẽ bao gồm hai cái mô hình con, tức là hai cái phương pháp hay hướng tiếp cận con, đó chính là Skip RAM và Continuous Battle Work. Thì chúng ta sẽ nói chi tiết hơn về hai cái mô hình này trong những cái phần tiếp theo. Đầu tiên đó là môn SkipRam. Ý tưởng của SkipRam đó chính",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UJNyIptbcNM",
      "filename": "UJNyIptbcNM",
      "title": "[CS431 - Chương 6] Part 4_1: Mô hình Word2Vec",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:58"
    }
  },
  {
    "page_content": "đó là môn SkipRam. Ý tưởng của SkipRam đó chính là chúng ta sẽ dự đoán các từ xung quanh khi có một từ ở giữa. Tô vàng ở đây là từ Wt Và từ ở giữa này, chúng ta sẽ phải đoán xem từ thứ t trừ 2 là gì Đoán từ thứ t trừ 1 là gì Đoán từ thứ t cộng 1 là gì Và đoán từ thứ t cộng 2 là gì Vì vậy, từ trái sang phải, chúng ta phải chỉ số của mình là chạy từ t trừ 2, t trừ 1 Và t cộng 1, t cộng 2 Vì vậy, chúng ta sẽ phải đoán từ thứ T khi chúng ta sẽ có cho trước từ thứ WT và chúng ta sẽ phải đoán từ thứ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UJNyIptbcNM",
      "filename": "UJNyIptbcNM",
      "title": "[CS431 - Chương 6] Part 4_1: Mô hình Word2Vec",
      "chunk_id": 1,
      "start_timestamp": "0:00:42",
      "end_timestamp": "0:01:46"
    }
  },
  {
    "page_content": "trước từ thứ WT và chúng ta sẽ phải đoán từ thứ T-1, T-2, T-1, T-2 Đó là ý tưởng của SkipRamp khi học mối quan hệ về mặt ngửi cảnh của từ Rồi, và ở đây chúng ta sẽ mô hình hóa cái việc dự đoán này dưới dạng là một cái công thức sát xuất Để dự đoán cái từ thứ T-2 thì chúng ta sẽ đưa về cái biểu diễn là cái công thức sát xuất là P của WT-2 Chúng ta đoán từ thứ T trừ 2 Cho trước, hay là khi biết trước từ thứ T Vậy là đây là công thức sát xuất có điều kiện Công thức sát xuất có điều kiện Và điều",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UJNyIptbcNM",
      "filename": "UJNyIptbcNM",
      "title": "[CS431 - Chương 6] Part 4_1: Mô hình Word2Vec",
      "chunk_id": 2,
      "start_timestamp": "0:01:39",
      "end_timestamp": "0:02:32"
    }
  },
  {
    "page_content": "điều kiện Công thức sát xuất có điều kiện Và điều kiện ở đây là chúng ta phải biết trước từ WT Tương tự như vậy, thì cho từ thứ T trừ 1 Chúng ta sẽ có lập, phải tính được sát xuất của từ thứ T trừ 1 khi cho trước cái thường thư tê rồi, t của Wt cộng bồ khi biết trước từ thư tê và hai cái từ hai cái từ này thì tương ứng, nó chính là cái ngữ cảnh bên ngoài với cái cửa sổ cái cửa sổ là bằng 2, cái kích thước của cửa sổ bằng 2 nghĩa là sao? một từ, nếu mà chúng ta đoán ra càng xa Từ xa thì rất là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UJNyIptbcNM",
      "filename": "UJNyIptbcNM",
      "title": "[CS431 - Chương 6] Part 4_1: Mô hình Word2Vec",
      "chunk_id": 3,
      "start_timestamp": "0:02:28",
      "end_timestamp": "0:03:13"
    }
  },
  {
    "page_content": "nếu mà chúng ta đoán ra càng xa Từ xa thì rất là khó. Từ thứ 10, 15, 20 rất là khó. Thông thường khi có một từ ở giữa, chúng ta có thể đoán được những từ phía trước trong một bán kính tương đối nhỏ thôi. Trong trường hợp này, cái bán kính của mình được thể hiện là Windowsize là bằng 2. Cứ tự nhiên ngoại cho ngửi cảnh bên ngoài bên tay phải là Windowsize bằng 2. Và tờ mà mình ở giữa thì nó sẽ là tại thời điểm thứ T Rồi, và tương tự như vậy chúng ta sẽ dịch chuyển sang tờ tiếp theo Như vậy là T",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UJNyIptbcNM",
      "filename": "UJNyIptbcNM",
      "title": "[CS431 - Chương 6] Part 4_1: Mô hình Word2Vec",
      "chunk_id": 4,
      "start_timestamp": "0:03:04",
      "end_timestamp": "0:03:55"
    }
  },
  {
    "page_content": "ta sẽ dịch chuyển sang tờ tiếp theo Như vậy là T của mình sẽ được dịch chuyển sang tờ từ banking Và chúng ta sẽ phải dự đoán xem cái từ kết đó hai bước Đó là sát suất là bao nhiêu phần trăm? Đó là từ turning Sát suất bao nhiêu phần trăm? Cái từ thứ T triều một đó là into Rồi, xác xúc của cái từ tiếp theo, ngay tiếp theo là WT cộng 1 là Crisis Rồi xác xúc của cái từ thứ T cộng 2 là S là bao nhiêu khuôn trăm? Vì vậy là nhiệm vụ của chúng ta là phải đi xây dựng một cái mô hình để làm sao ước lượng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UJNyIptbcNM",
      "filename": "UJNyIptbcNM",
      "title": "[CS431 - Chương 6] Part 4_1: Mô hình Word2Vec",
      "chunk_id": 5,
      "start_timestamp": "0:03:53",
      "end_timestamp": "0:04:41"
    }
  },
  {
    "page_content": "đi xây dựng một cái mô hình để làm sao ước lượng được các cái xác xúc này Và với mỗi cái thời điểm thứ T, với tê chạy từ 1 cho đến tê lớn 1 là từ đầu tiên của câu và T lớn là từ cuối cùng của câu chúng ta cho biết trước từ ở giữa là WT và chúng ta sẽ dự đoán từ ngữ cảnh xung quanh với một cửa sổ cố định là M trong trường hợp, ví dụ ở trên M là bằng 2 như vậy thì ta sẽ có công thức cho likelihood là l theta là bằng tích với t chạy từ 1 cho đến t. với t chạy từ 1 cho đến t hoa và ở đây chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UJNyIptbcNM",
      "filename": "UJNyIptbcNM",
      "title": "[CS431 - Chương 6] Part 4_1: Mô hình Word2Vec",
      "chunk_id": 6,
      "start_timestamp": "0:04:35",
      "end_timestamp": "0:05:29"
    }
  },
  {
    "page_content": "với t chạy từ 1 cho đến t hoa và ở đây chúng ta sẽ phải làm sao đó để nhâm xét xuất của những từ ngữ cảnh bên ngoài với g là chạy từ trường m cho đến m với g là phải khác không Vì trong trường hợp g bằng 0, t cũng g chính là bằng t, tức là cái từ tức là t của wt khi trả chứt t. Thì đoàn là mình đoán cái từ ở giữa, khi biết từ ở giữa thì đây là một cái điều rất là vô lấy. Sát số này thì nó luôn luôn là bằng 1 rồi. Vì vậy đó chúng ta cũng không có cần thiết phải đưa vào đây nữa. Vì vậy đó thì g",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UJNyIptbcNM",
      "filename": "UJNyIptbcNM",
      "title": "[CS431 - Chương 6] Part 4_1: Mô hình Word2Vec",
      "chunk_id": 7,
      "start_timestamp": "0:05:21",
      "end_timestamp": "0:06:04"
    }
  },
  {
    "page_content": "cần thiết phải đưa vào đây nữa. Vì vậy đó thì g sẽ phải khác không. Và cái likelihood này, công thức likelihood này, L theta này, thì mình sẽ phải làm sao để cực đại hóa, mình sẽ đi tính tính max. Và cái hàm mục tiêu của mình thông thường nó sẽ là hàm loss và mình phải đi minimize. Và cộng với việc là cái việc mà tính tích này, thì các giá trị sát suất này thông thường là những con số rất là bé, nó bé hơn một. Và tích của các xác xúc này, YOLGO thì nó sẽ có sự hướng là tiến đến 0. Dẫn đến là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UJNyIptbcNM",
      "filename": "UJNyIptbcNM",
      "title": "[CS431 - Chương 6] Part 4_1: Mô hình Word2Vec",
      "chunk_id": 8,
      "start_timestamp": "0:06:00",
      "end_timestamp": "0:06:42"
    }
  },
  {
    "page_content": "thì nó sẽ có sự hướng là tiến đến 0. Dẫn đến là cái khả năng biểu diễn của máy tính của mình khi làm việc với tích của các con số dỏ không không, rất là thấp. Tức là nó sẽ dần làm tròn thành số 0. Do đó thì chúng ta sẽ thiết kế lại là hàm mục tiêu Loss theta là âm của trung bình Locked likelihood của L. Tức là chúng ta sẽ đi tính lóc của hằng này, chúng ta sẽ đi tính lóc của L đi tính lóc của L, và khuyến biệt là tính lóc của hàm tích nó sẽ đi về tổng của các hàm lóc, dự ám của trung bình đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UJNyIptbcNM",
      "filename": "UJNyIptbcNM",
      "title": "[CS431 - Chương 6] Part 4_1: Mô hình Word2Vec",
      "chunk_id": 9,
      "start_timestamp": "0:06:38",
      "end_timestamp": "0:07:33"
    }
  },
  {
    "page_content": "về tổng của các hàm lóc, dự ám của trung bình đây là dự ám, còn trung bình, đây chính là trung bình rồi, và lóc của tích sẽ là bằng tổng của các cái lóc do đó thì công thức ở trên nó sẽ được chuyển về như vậy này Thay vì chúng ta sẽ đi tìm giá trị lớn nhất, thì chúng ta sẽ phải đi tìm giá trị nhỏ nhất của nốc sát xuất này. Và như vậy chúng ta đặt ra một câu hỏi đó là làm sao chúng ta tính được sát xuất P của P cộng G cho WT với WT là tham số của môn của mình. là tham số mồ hình thì ý tưởng đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UJNyIptbcNM",
      "filename": "UJNyIptbcNM",
      "title": "[CS431 - Chương 6] Part 4_1: Mô hình Word2Vec",
      "chunk_id": 10,
      "start_timestamp": "0:07:30",
      "end_timestamp": "0:08:14"
    }
  },
  {
    "page_content": "môn của mình. là tham số mồ hình thì ý tưởng đó là chúng ta sẽ sử dụng một cái mạng NeuroNetwork với một lỡ bẩn duy nhất thôi và cái đầu ra của mình sẽ là một cái hàm sóc mắt thì ở bên đây chúng ta sẽ có cái kiến trúc của cái mạng NeuroNetwork rõ ràng là cái mạng NeuroNetwork này nó cũng là một cái mạng học sâu nhưng mà nó rất là ngắn nó chỉ có duy nhất một lỡ bẩn thôi duy nhất một lỡ bẩn và toàn bộ h1, h2, hn này thì người ta sẽ ký nhiệm là h Và để từ input layer chuyển tính ra được xn layer",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UJNyIptbcNM",
      "filename": "UJNyIptbcNM",
      "title": "[CS431 - Chương 6] Part 4_1: Mô hình Word2Vec",
      "chunk_id": 11,
      "start_timestamp": "0:08:06",
      "end_timestamp": "0:09:23"
    }
  },
  {
    "page_content": "Và để từ input layer chuyển tính ra được xn layer thì chúng ta sẽ có một ma trận là ma trận w ma trận w này thì sẽ có kích thước là v nhưng n trong đó v chỉ số b là số từ trong tự điện số tư trong tự điện còn n n là số chiều của cái output của mình hay nói cách khác đây chính là số chiều của cái vector biểu diễn của cái tư của tự đầu vào. Vì vậy, chúng ta sẽ có công thức cho mạng Neural Network này. Layer biến đổi đầu tiên, đó là tính hit the layer, chúng ta sẽ có công thức là h, là bằng W,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UJNyIptbcNM",
      "filename": "UJNyIptbcNM",
      "title": "[CS431 - Chương 6] Part 4_1: Mô hình Word2Vec",
      "chunk_id": 12,
      "start_timestamp": "0:09:19",
      "end_timestamp": "0:10:29"
    }
  },
  {
    "page_content": "layer, chúng ta sẽ có công thức là h, là bằng W, nhân WX. Vì vậy, chúng ta thấy là công thức này rất đơn giản và không có hàm sigmoid, hoặc không có hàm kích hoạt, và nó chỉ đơn giản là một cái phép biến đổi thuyến tính sẽ là bằng W nhân của x và với x là một cái one-hop encode của cái từ Wt như vậy thì đầu vào của mình sẽ có cái dạng như sau là 000010 Trong đó, đây chính là vị trí của từ WT trong từ điện. Vị trí của từ thứ WT trên từ điện. Vì vậy, cái vector này sẽ là vector 1 hot và có chi số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UJNyIptbcNM",
      "filename": "UJNyIptbcNM",
      "title": "[CS431 - Chương 6] Part 4_1: Mô hình Word2Vec",
      "chunk_id": 13,
      "start_timestamp": "0:10:24",
      "end_timestamp": "0:11:08"
    }
  },
  {
    "page_content": "cái vector này sẽ là vector 1 hot và có chi số chiều rất là lớn. Thì thông thường W, cái V này có khả năng là lên đến 1 triệu. Rồi, cái lớp tiếp theo là lớp Output. Thì nó sẽ có cái công thức đó là giá trị dự đoán. Đây là giá trị dự đoán, tài mộ cái này sẽ là giá trị dự đoán. Rồi, sẽ là bằng SOAPMAX của W phải. W phải thì sẽ là ngược lại của W. W thì nó sẽ có kích thước là n nhân v-v. Và chúng ta cũng lần nữa chúng ta sẽ nhân tích vô hướng với nhạc h và đưa vào hàm sop bắt. Thì mục tiêu của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UJNyIptbcNM",
      "filename": "UJNyIptbcNM",
      "title": "[CS431 - Chương 6] Part 4_1: Mô hình Word2Vec",
      "chunk_id": 14,
      "start_timestamp": "0:11:05",
      "end_timestamp": "0:12:00"
    }
  },
  {
    "page_content": "nhạc h và đưa vào hàm sop bắt. Thì mục tiêu của việc đưa vào hàm sop bắt đó chính là nó sẽ đưa về cái không gian sát xuất. Nó sẽ đưa về cái không gian sát xuất trong đó tình phần tử trong cái A này. Nó sẽ có cái giá trị từ 0 cho đến 1. và tổng tất cả các phần tử ở đây thì nó sẽ là bằng một Và nhiệm vụ của chúng ta là chúng ta sẽ phải đi tối ưu khoáng hét cái hàm loss theta là bằng công thức là trung bình, âm của trung bình, log-line input hồi nãy đúng không? Vì khi biết trước WT, thì x này của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UJNyIptbcNM",
      "filename": "UJNyIptbcNM",
      "title": "[CS431 - Chương 6] Part 4_1: Mô hình Word2Vec",
      "chunk_id": 15,
      "start_timestamp": "0:11:52",
      "end_timestamp": "0:12:47"
    }
  },
  {
    "page_content": "đúng không? Vì khi biết trước WT, thì x này của mình chính là WT, nó chính là cái đầu vào. Và đưa ra cái sát xuất dự đoán trong từ thứ T cộng Z thì nó sẽ được thể hiện ở trong phân bố sát xuất của cái thần y nhã này. Và các giá trị này, giá trị nhiều mà càng cao thì nó sẽ càng thể hiện sát xuất của việc dự đoán đó Rồi, ở đây chúng ta lưu ý là nó sẽ có một cái hiệu, đó là ind, tức là ind là ví tác của chữ index Cho thấy index của từ thứ t cộng z, tức là cho biết vị trí của từ thứ t cộng z trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UJNyIptbcNM",
      "filename": "UJNyIptbcNM",
      "title": "[CS431 - Chương 6] Part 4_1: Mô hình Word2Vec",
      "chunk_id": 16,
      "start_timestamp": "0:12:39",
      "end_timestamp": "0:13:26"
    }
  },
  {
    "page_content": "tức là cho biết vị trí của từ thứ t cộng z trong tự điện của mình Ví dụ như là từ thứ T cộng G của mình, đó là ở vị trí số 3, vị trí thứ 100, rồi vị trí thứ 120, vị trí thứ 130. Đó là các cái giá trị chỉ số của các cái từ thứ T-1, T-2, rồi T-1, T-2. Thì chúng ta sẽ chỉ tính tổng của các cái lốc của các cái giá trị output này. Vì vậy, từ công thức này, chúng ta sẽ đi tối ưu qua tìm theta sau khi tổng này là nhỏ nhất. Hay là tìm min. Sau khi tối ưu xong, chúng ta sẽ có các trạm số, là các gia",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UJNyIptbcNM",
      "filename": "UJNyIptbcNM",
      "title": "[CS431 - Chương 6] Part 4_1: Mô hình Word2Vec",
      "chunk_id": 17,
      "start_timestamp": "0:13:22",
      "end_timestamp": "0:13:46"
    }
  },
  {
    "page_content": "ưu xong, chúng ta sẽ có các trạm số, là các gia trình W. Chúng ta luôn hít là ở đây, hệ thống ký hiệu nó hơi khác một chút xíu. Các bạn sẽ hỏi là W với theta là gì? thì mình cũng xin lỗi đó là trong trường hợp này, hệ thống ký hiệu của mình trước đây mình hay sử dụng cho các môn đó là theta thì trong hình này mình lấy từ cái bài bao góc của tác giả thì thật ra theta của mình chính là cái w này theta 1 của mình chính là cái w này và theta 2 của mình chính là cái w phải gột cái theta 1 và theta 2",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UJNyIptbcNM",
      "filename": "UJNyIptbcNM",
      "title": "[CS431 - Chương 6] Part 4_1: Mô hình Word2Vec",
      "chunk_id": 18,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "chính là cái w phải gột cái theta 1 và theta 2 thì mình chính là cái mộ tham số theta Rồi, ở đây thì nếu mà theo cái ký hiệu ở đây thì lẽ ra nó phải là loss của Rất nhiều nha Rồi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UJNyIptbcNM",
      "filename": "UJNyIptbcNM",
      "title": "[CS431 - Chương 6] Part 4_1: Mô hình Word2Vec",
      "chunk_id": 19,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Trong vòng tiếp theo, chúng ta sẽ cùng tìm hiểu về hướng tiếp cận học sâu trong lĩnh vực xử lý ngu ở tự nhiên. Học sâu, chúng ta nhắc lại, đó là một cái nhánh của representation learning. Tức là chúng ta sẽ tìm cách học từ dữ liệu thô. Dữ liệu đồ vào trước đây của chúng ta phải là các feature vector. Tức là chúng ta đã dựa trên chi thức chuyên gia để đưa vào bên trong các mạng neural. Bây giờ, với mạng học sau, chúng ta chỉ cần đưa dữ liệu đầu vào là dữ liệu nguyên bản. Và chúng ta không cần",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=utOha-d0prc",
      "filename": "utOha-d0prc",
      "title": "[CS431 - Chương 6] Part 2: Hướng tiếp cận Deep learning cho các bài toán NLP",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:38"
    }
  },
  {
    "page_content": "vào là dữ liệu nguyên bản. Và chúng ta không cần phải có kiến thức của chuyên gia để rút trích ra thành những đặc trưng quan trọng nữa. Mà tự mô hình sẽ đi tìm cách để học và biểu diễn các dữ liệu hoặc là các đặc trưng đó cho mình. Và đương nhiên, chúng ta sẽ phải cung cấp cho các mô hình máy học này rất nhiều dữ liệt. Và khi cung cấp cho nó rất nhiều dữ liệu thì nó sẽ đúc kết ra được những tri thức chung của tất cả những dữ liệu đó. Và tất cả những thành tựu liên quan đến học sâu hiện nay mà",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=utOha-d0prc",
      "filename": "utOha-d0prc",
      "title": "[CS431 - Chương 6] Part 2: Hướng tiếp cận Deep learning cho các bài toán NLP",
      "chunk_id": 1,
      "start_timestamp": "0:00:33",
      "end_timestamp": "0:01:10"
    }
  },
  {
    "page_content": "những thành tựu liên quan đến học sâu hiện nay mà có được là nhờ những yếu tố sau. Đầu tiên đó là yếu tố dữ liệu ngày càng lớn. Trước đây thì thật ra internet thì nó đã có từ những năm 1990 trở về trước. từ những ngày 1990. Tuy nhiên, cho đến những năm gần đây, chúng ta thấy là mạng internet đã có mỹ thành tựu rất là đáng kể. Thứ nhất, đó là nó đã phủ sống được cho rất là nhiều người có thể cùng tiếp cận. Và với việc phủ sống được rất nhiều người có khả năng tiếp cận, thì mỗi người trong chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=utOha-d0prc",
      "filename": "utOha-d0prc",
      "title": "[CS431 - Chương 6] Part 2: Hướng tiếp cận Deep learning cho các bài toán NLP",
      "chunk_id": 2,
      "start_timestamp": "0:01:07",
      "end_timestamp": "0:01:44"
    }
  },
  {
    "page_content": "có khả năng tiếp cận, thì mỗi người trong chúng ta sẽ có khả năng là một cái người tạo ra một cái nội dung có giá trị trên mạng internet, thông qua các cái mạng xã hội. Và các mạng xã hội hiện nay đã góp phần tạo ra rất nhiều kho dữ liệu quan trọng cho các hệ thống AI hiện nay để có thể sử dụng dữ liệu đó để huấn luyện cho các mô hình máy học. Và chúng ta có thể kể đến một số mạng xã hội quan trọng đã góp phần cho thành tựu của xử lý môn người tự nhiên, ví dụ như Wiki, Pedia. các trang mạng xã",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=utOha-d0prc",
      "filename": "utOha-d0prc",
      "title": "[CS431 - Chương 6] Part 2: Hướng tiếp cận Deep learning cho các bài toán NLP",
      "chunk_id": 3,
      "start_timestamp": "0:01:42",
      "end_timestamp": "0:02:27"
    }
  },
  {
    "page_content": "nhiên, ví dụ như Wiki, Pedia. các trang mạng xã hội khác, ví dụ như là Start Overflow cho những bạn nào làm về lập trình thì sẽ biết rõ trang này. Các mạng xã hội khác mà cung cấp rất nhiều những diễn liệu dịch thuật, ví dụ như là YouTube Trên trang Youtube thì chúng ta thấy là bên cạnh cái video góc, nó sẽ có các cái caption, tức là các cái bảng phiên ra lời thoại và có rất nhiều những tình nguyện viên họ đã phiên ra những ngôn ngữ khác nhau. Ví dụ như video góc thì nói về tiếng Anh và nội",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=utOha-d0prc",
      "filename": "utOha-d0prc",
      "title": "[CS431 - Chương 6] Part 2: Hướng tiếp cận Deep learning cho các bài toán NLP",
      "chunk_id": 4,
      "start_timestamp": "0:02:19",
      "end_timestamp": "0:03:09"
    }
  },
  {
    "page_content": "Ví dụ như video góc thì nói về tiếng Anh và nội dung của lời thoại là đương nhiên là họ sẽ tạo ra tiếng Anh Đồng thời sẽ có một số người cộng tác, họ tạo ra những lời thoại cho rất nhiều nguyên vô hưởng khác trong đó có tiếng Việt. Như vậy thì kênh YouTube này cũng góp phần ngạc cung cấp cho các kho diễn liệu về máy học hiện nay một cách đáng trẻ. Và một cái lý do nữa để khiến cho học sâu phát triển trong những năm gần đây chính là sức mạnh tính toán ngày càng tăng. trước đây thì chúng ta có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=utOha-d0prc",
      "filename": "utOha-d0prc",
      "title": "[CS431 - Chương 6] Part 2: Hướng tiếp cận Deep learning cho các bài toán NLP",
      "chunk_id": 5,
      "start_timestamp": "0:03:01",
      "end_timestamp": "0:03:38"
    }
  },
  {
    "page_content": "toán ngày càng tăng. trước đây thì chúng ta có các cdu và các cdu này cho dù nó tăng tốc độ đến độ đạo đi chăng nữa thì tại một thời điểm nó cũng chỉ có thể thực hiện được khoảng một trong còn cdu là một cái thiết bị phần cưng khác cho phép mình có thể tính toán xong xong rất nhiều cái phép toán tương tự và độc lập nhau tại một thời điểm nó có thể tính các phép toán tương tự độc lập nhau Đồng thời, không thể không kể đến các mô hình cũng như các tụ tán ngày nay đã cải tiến rất nhiều. Các mô",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=utOha-d0prc",
      "filename": "utOha-d0prc",
      "title": "[CS431 - Chương 6] Part 2: Hướng tiếp cận Deep learning cho các bài toán NLP",
      "chunk_id": 6,
      "start_timestamp": "0:03:27",
      "end_timestamp": "0:04:16"
    }
  },
  {
    "page_content": "các tụ tán ngày nay đã cải tiến rất nhiều. Các mô hình có thể giúp chúng ta học được nhiều dữ liệu hơn với thời gian huấn luyện ít hơn và tránh được rất nhiều hiện tượng đó là overfitting, tức là chỉ là tốt cho dữ liệu trend nhưng không tốt cho dữ liệu test như vậy. thì thành tựu của các cái này học sâu hiện nay là đã giúp cho Deep Learning phát triển một cách vượt mập. Rồi, và sơ đổ ở bên đây thì chúng ta có thể thấy là trước đây các hệ thống của mình nó sẽ dựa trên rule, hoặc là những cái hệ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=utOha-d0prc",
      "filename": "utOha-d0prc",
      "title": "[CS431 - Chương 6] Part 2: Hướng tiếp cận Deep learning cho các bài toán NLP",
      "chunk_id": 7,
      "start_timestamp": "0:04:08",
      "end_timestamp": "0:04:57"
    }
  },
  {
    "page_content": "mình nó sẽ dựa trên rule, hoặc là những cái hệ thống gọi là kinh điển thì nó đều phải có những cái hand design program, tức là các cái chương trình này sẽ do những cái chi thức của các chuyên gia họ thiết kế ra. Và ở cái mức độ là Classic Machine Learning, thì nó sẽ có các feature, có các công cụ để mapping giữa các feature và thậm chí là các chuyên gia họ sẽ phải thiết kế các đặc trưng này. Ví dụ khi chúng ta làm việc trên hình ảnh, thì chúng ta biết là mối quan hệ giữa các pixel với các nguồn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=utOha-d0prc",
      "filename": "utOha-d0prc",
      "title": "[CS431 - Chương 6] Part 2: Hướng tiếp cận Deep learning cho các bài toán NLP",
      "chunk_id": 8,
      "start_timestamp": "0:04:47",
      "end_timestamp": "0:05:37"
    }
  },
  {
    "page_content": "biết là mối quan hệ giữa các pixel với các nguồn đồng chập, chúng ta sẽ thiết kế các phép viễn đổi là filter và trọng số của các filter sẽ là do chuyên gia họ thiết kế. Tương tự như vậy, trong lĩnh vực xử lý ngôn ngựt, tương nhiên chúng ta sẽ có những cái trick, những cái mẹo để giúp cho học các mô hình, ví dụ như LSTM, hoặc là phẩm mô độc sâu. Tuy nhiên thì trước đây người ta không có sử dụng các mô hình mà tự vấn luyện để tạo ra các trọng số mà họ phải thi kế trước các trọng số dựa trên một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=utOha-d0prc",
      "filename": "utOha-d0prc",
      "title": "[CS431 - Chương 6] Part 2: Hướng tiếp cận Deep learning cho các bài toán NLP",
      "chunk_id": 9,
      "start_timestamp": "0:05:31",
      "end_timestamp": "0:06:08"
    }
  },
  {
    "page_content": "mà họ phải thi kế trước các trọng số dựa trên một số luật, ví dụ như là môn dựa trên bias để thống kê xem là cái từ này xuất hiện, thì sát xuất của cái từ tiếp theo sẽ là bao nhiêu? Họ sẽ thống kê. Rồi, và gần đây thì Representation Learning và điển hình đó là Deep Learning, thì nó sẽ đưa vào những cái Simple Feature và thậm chí như mình phải có đề cập đó, Nó là chúng ta không cần phải đưa đặc trưng của nó và chúng ta có thể đưa dữ liệu thô đầu vào thì máy vẫn có thể học được. Rồi. Và lĩnh vượt",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=utOha-d0prc",
      "filename": "utOha-d0prc",
      "title": "[CS431 - Chương 6] Part 2: Hướng tiếp cận Deep learning cho các bài toán NLP",
      "chunk_id": 10,
      "start_timestamp": "0:06:07",
      "end_timestamp": "0:06:45"
    }
  },
  {
    "page_content": "thì máy vẫn có thể học được. Rồi. Và lĩnh vượt học sau nó đã có những thành tựu vượt bập trong một số bài toán. Không phải trong một số bài toán mà trong rất nhiều bài toán. Và nổi tiếng nhất chính là cái bài toán về dịch máy, về chatbot, về gợi ý nội dung trong email. Và một số mô hình nổi tiếng gần đây chúng ta được nghe rất là nhiều, đó chính là Transformer. Tất cả các mô hình trong sự yên ngôn ngữ tự nhiên hiện nay đều có gốc từ kiến trúc Transformer. Ví dụ như, con chatbot rất nổi tiếng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=utOha-d0prc",
      "filename": "utOha-d0prc",
      "title": "[CS431 - Chương 6] Part 2: Hướng tiếp cận Deep learning cho các bài toán NLP",
      "chunk_id": 11,
      "start_timestamp": "0:06:42",
      "end_timestamp": "0:07:22"
    }
  },
  {
    "page_content": "Transformer. Ví dụ như, con chatbot rất nổi tiếng hiện nay đó là ChatGBT. ChatGBT thì cái chữ T chính là Transformer. Chuyện T là một trong những mô hình ngôn ngữ lớn và có chữ T thì T ở đây cũng chính là Transformer. Và dưới đây là, ở trên đây đó là những hình ảnh trập ra từ một con bốt của CorePilot, của người phát triển bởi Microsoft, thì chúng ta có thể yêu cầu dịch một đoạn quang mạng, từ tiếng Anh sang tiếng Việt. Chúng ta có thể soạn email một cách dễ dàng hơn bằng cách chúng ta chỉ cần",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=utOha-d0prc",
      "filename": "utOha-d0prc",
      "title": "[CS431 - Chương 6] Part 2: Hướng tiếp cận Deep learning cho các bài toán NLP",
      "chunk_id": 12,
      "start_timestamp": "0:07:15",
      "end_timestamp": "0:07:33"
    }
  },
  {
    "page_content": "một cách dễ dàng hơn bằng cách chúng ta chỉ cần gõ vai đưa khóa, là cái hệ thống sẽ tự nhắc cho chúng ta, cái từ tiếp theo sẽ đi là gì, chúng ta chỉ cần nhấn phím tác, là nập tức nó có thể thành thiện cái nội dung cho mình. Các cái nội dung mà nó sẽ chét sau đây, nó sẽ dựa trên những cái nội dung trao đổi trước đó của mình. Một cách từ đầu.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=utOha-d0prc",
      "filename": "utOha-d0prc",
      "title": "[CS431 - Chương 6] Part 2: Hướng tiếp cận Deep learning cho các bài toán NLP",
      "chunk_id": 13,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Ý tưởng của Transformer đó là nếu như trong các kiến trúc cũ là Recurrent Neural Network, thì trong quá trình tính toán chúng ta sẽ phải lan truyền thông tin một cách tuần tự. Nghĩa là tại cái từ ở đây, khi chúng ta xử lý tính toán đến cái từ cuối cùng trong giao đoạn encode, thì chúng ta sẽ phải lan truyền tuần tự đi lên, đi qua, đi lên, đi qua. Như vậy thì nó sẽ tố rất nhiều bước xử lý tuần tự Và thông tin của cái từ đầu tiên này khi đến được nơi này thì nó cũng đã bị mai một, cũng đã bị mất",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:38"
    }
  },
  {
    "page_content": "nơi này thì nó cũng đã bị mai một, cũng đã bị mất đi rất là nhiều Thì đó chính là cái điểm yếu của Recurrent Neural Network Và đồng thời cái việc thực hiện tuần tự này cũng sẽ khiến cho chúng ta không thể xử lý song song sử dụng các sức mạnh của GPU Còn kiến trúc của Transformer Cơ chế cell extension cho phép chúng ta xử lý xong xong Tại vì khi chúng ta tính toán tại đây, chúng ta không cần phải phụ thuộc vào các giá trị được tính toán tại đây Tức là các nodes ở trên cùng sẽ được thực hiện một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 1,
      "start_timestamp": "0:00:31",
      "end_timestamp": "0:01:30"
    }
  },
  {
    "page_content": "là các nodes ở trên cùng sẽ được thực hiện một cách độc lập Chúng ta muốn tính toán tại vị trí này, tại hidden này, thì chúng ta sẽ phải tính toán ở đây trước, rồi sau đó mới đến đây, tính đến đây, xong chúng ta mới đến đây được. Còn ở đây là các node ở đây là tính độc lập, mà độc lập thì có thể sử dụng GPU được. Do đó thì mỗi số phép tính xong xong của mình sẽ không phụ thuộc vào chiều dài của chuỗi. Tức là khi cái chuỗi này, cái chuỗi này mà dài, rất là dài, thì nó vẫn có thể thực hiện xong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 2,
      "start_timestamp": "0:01:17",
      "end_timestamp": "0:01:57"
    }
  },
  {
    "page_content": "dài, rất là dài, thì nó vẫn có thể thực hiện xong xong được. Và đồng thời, chúng ta thấy các cái chết nối dày đặt này, nó sẽ cho phép chúng ta có thể tương tác, tương tác giữa các cái từ. Nếu như ở đây, để mà có thể tương tác được cái từ đầu tiên và cái từ cuối cùng, cái từ đầu tiên và cái từ cuối cùng này, Tại đây thì chúng ta sẽ cần có rất nhiều bước tuân tự mới lang truyền để mà có thể tương tác được Trong khi đó tại đây, cái từ đầu tiên, cái từ đầu tiên này, nó đã có thể tương tác được",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 3,
      "start_timestamp": "0:01:48",
      "end_timestamp": "0:02:43"
    }
  },
  {
    "page_content": "cái từ đầu tiên này, nó đã có thể tương tác được Thông qua cái lớp trước đó là lớp số 1, lớp số 2 sẽ dùng thông tin của lớp số 1 và nó cũng dựa trên thông tin của từ cuối cùng của lớp. Tại layer số 2, nó đã có thể truy xuất đến thông tin của từ đầu tiên và từ cuối cùng của lớp trước đó một cách trực tiếp và không cần thực hiện cách tuôn tự. Đây chính là những ưu điểm của Transformer. Và hình vẽ trên đây đó chính là sơ đồ kiến trúc của Transformer thì khi chúng ta mới bắt đầu chúng ta nhìn vô",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 4,
      "start_timestamp": "0:02:33",
      "end_timestamp": "0:03:13"
    }
  },
  {
    "page_content": "thì khi chúng ta mới bắt đầu chúng ta nhìn vô cái sơ đồ này chúng ta sẽ rất là rối vì nó có quá nhiều cái mô đun và chúng ta cũng không biết tại sao nó lại có những cái mô đun này Thế thì bây giờ tại cái bước này, tại cái hình vẽ này thì chúng ta chỉ cần hình dung đó là Transformer Power hai thành phần đó là Encoder và Decoder Đây là Encoder và đây là Decoder Và đây là kiến trúc của Encoder. Và đây là kiến trúc của Decoder. Chúng ta sẽ cùng đến với từng thành phần của Transformer. Đầu tiên đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 5,
      "start_timestamp": "0:03:12",
      "end_timestamp": "0:03:57"
    }
  },
  {
    "page_content": "với từng thành phần của Transformer. Đầu tiên đó chính là Encoder. Thì cái module xử lý tính toán đầu tiên của Encoder đó chính là cell retention. Cell retention chính là một cái module chính của Transformer. Đó là một cái module chính. Và với dữ kiện đầu vào là các tờ của mình hoặc là các token của mình qua input embedding Tức là chúng ta sẽ biến một tờ trong một văn bảng thành một vector biểu diễn Và vector biểu diễn này sẽ đến mô đun cell attention Và cell attention nó dựa trên cơ chế của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 6,
      "start_timestamp": "0:03:49",
      "end_timestamp": "0:04:36"
    }
  },
  {
    "page_content": "Và cell attention nó dựa trên cơ chế của attention Thế thì attention nó cũng na ná, nó cũng tương đương với lại một phép truy vấn trong bảng dữ liệu của mình Có điều nếu như truy vấn trong bảng dữ liệu của mình, chúng ta có một query ở đây Chúng ta sẽ trả trong cơ sở dữ liệu của mình các key value Thông qua chúng ta sẽ sort khớp dựa trên các key Vậy chúng ta lấy thông tin của key value Chúng ta sẽ hình dung các khái niệm là Query, Key và Value Chúng ta sẽ hình dung các khái niệm liên quan đến",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 7,
      "start_timestamp": "0:04:33",
      "end_timestamp": "0:05:33"
    }
  },
  {
    "page_content": "Chúng ta sẽ hình dung các khái niệm liên quan đến một ứng dụng trong thực tế đó chính là các hệ thống tìm kiếm về multimedia Query của mình chính là các keyword Chúng ta muốn tìm kiếm một video về Transformer thì ký quả sẽ là Transformer Architecture và ký là tiêu đề của các video trong kênh YouTube của chúng ta. Và cái value của mình trả về, nó chính là những cái nội dung của video của mình. Đó chính là nội dung video. Rồi, ví dụ như là mô tạm, mô tạm cái video. Thì đó chính là các cái value",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 8,
      "start_timestamp": "0:05:28",
      "end_timestamp": "0:06:17"
    }
  },
  {
    "page_content": "mô tạm cái video. Thì đó chính là các cái value của mình. Thì đây chính là sự liên tưởng đến cái hệ thống tìm kiếm các hệ thống truy vấn trong thế giới thực của mình. Còn attention của mình đó sẽ khác so với lại truy vấn trong bảng dữ liệu của mình ở chỗ đó là chúng ta sẽ phải trích xuất ra thông tin của rất nhiều của rất nhiều những cái key và value Ở đây là chúng ta trích xuất một lột chúng ta sẽ lấy ánh xạ chúng ta sẽ lấy mỗi cái query của mình, nó sẽ ánh xạ đến một cặp key và value Query",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 9,
      "start_timestamp": "0:06:11",
      "end_timestamp": "0:07:06"
    }
  },
  {
    "page_content": "mình, nó sẽ ánh xạ đến một cặp key và value Query Quy sẽ ánh sạ một lột đến một cái cập key và value Trong khi đó, ở Attention thì mỗi một cái query của mình Nó sẽ khớp với mỗi key, nó sẽ so khớp với các cái key này của mình Và nó sẽ trả về tổng tất cả các cái value Có điều ở đây chúng ta sẽ thấy là nó sẽ có trọng số nha Nếu có trọng số thì những kỳ và value liên quan đến query này thì nó mới có trọng số lớn. Còn những kỳ và value có trọng số thấp, tức là x có sự liên quan, thì khi nó cộng lại",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 10,
      "start_timestamp": "0:06:53",
      "end_timestamp": "0:07:45"
    }
  },
  {
    "page_content": "tức là x có sự liên quan, thì khi nó cộng lại thì nó sẽ ít tham gia vào giá trị output của mình. Trong hình hình này, chúng ta thấy đây là những cái giá trị mà có trọng số thấp. Đây là những cái giá trị có trọng số thấp. Còn những cái key value mà chúng ta tô đỏ ở đây chính là những cái mà có trọng số cao. Thì khi đó, tỷ trọng, trọng lượng thông tin của V1, V3, V4, khi chúng ta tổng hợp thông tin sẽ là nhiều nhất. V0, V2, V5, V6 Hàm lượng thông tin tổng gợp sẽ rất thấp Đó là sự khác nhau như",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 11,
      "start_timestamp": "0:07:39",
      "end_timestamp": "0:08:34"
    }
  },
  {
    "page_content": "tin tổng gợp sẽ rất thấp Đó là sự khác nhau như Attention với truy vấu trong mạng dữ liệu của mình Khi này, chúng ta sẽ có công thức cho cell Attention trong encoder của mình Bước số 1 là với mỗi một từ, cái này chính là emletting Embedding vector của mình Đây là embedding vector của một cái từ Với mỗi từ nó sẽ chia ra thành ba cái giá trị, đó là query, key và value tương ứng là các cái màu Chúng ta sẽ theo dõi dựa trên màu cho dễ dụt Query nó sẽ phải ánh sạ từ embedding vector đó về cái không",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 12,
      "start_timestamp": "0:08:25",
      "end_timestamp": "0:09:19"
    }
  },
  {
    "page_content": "phải ánh sạ từ embedding vector đó về cái không gian Về ánh sạ về cái không gian của query của mình XI sẽ nhân với MyTrậnK để ánh sạp về không gian của key của mình XI nhân với V để ánh sạp về không gian của kevalu của mình Sang cái bước thứ 2, chúng ta sẽ tính attention score giữa query và key Trong trường hợp này query và key của mình đã có cùng một số chiều và phải đưa về cùng một số chiều Chúng ta chỉ việc thực hiện cái phép tích vô ứng giữa một query và một key thứ chi bất kỳ. Chúng ta sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 13,
      "start_timestamp": "0:09:13",
      "end_timestamp": "0:10:07"
    }
  },
  {
    "page_content": "một query và một key thứ chi bất kỳ. Chúng ta sẽ trả về là relation, tức là sự liên hệ giữa query và key này. Query thứ y và key thứ chi này. Sau đó chúng ta sẽ chuẩn hóa, xe mũa thứ 3 là chúng ta chuẩn hóa giá trị này về không gian sát xuất. Thông qua hàm softmax và công thức của softmax, chúng ta sẽ có alpha az, chính là extension distribution hay extension score mà chúng ta đã được chọn hóa. Sài gòn số 4 là chúng ta sẽ tính tổng trọng số của các value, Tức là các trọng số alpha az này sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 14,
      "start_timestamp": "0:10:03",
      "end_timestamp": "0:10:47"
    }
  },
  {
    "page_content": "các value, Tức là các trọng số alpha az này sẽ nhân với value chương ứng để chúng ta trả kết quả về output i Tức là output cho query thứ i Output cho query thứ i của mình Và khi này thì chúng ta sẽ có Nếu chúng ta thực hiện trên vétter, vétter hóa Tức là chúng ta sẽ gom các query lại với nhau. Thì bước số 1, các từ x, y sẽ được gột lại vào x. Ở đây chúng ta sẽ thực hiện theo pass, thực hiện theo khối. Trong cái slide trước, là chúng ta tính trên từng từng. Trong slide này, chúng ta sẽ gom tất",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 15,
      "start_timestamp": "0:10:45",
      "end_timestamp": "0:11:36"
    }
  },
  {
    "page_content": "từng từng. Trong slide này, chúng ta sẽ gom tất cả các từ trong câu input của mình Vào thành một cái ma trận là ma trận x Khi đó chúng ta tính toán trên cái ma trận x này Nó sẽ tính toán nhanh hơn và có thể thực hiện được một cách xong xong Giờ sức mạnh tính toán của GPU XI này sẽ là một vector dạng cục như thế này của mình Chúng ta sẽ gom tất cả xI này lại với nhau x y sẽ gom tất cả các x y lại với nhau thì chúng ta sẽ có được 1 ma trận toàn bộ x y gom lại sẽ là ma trận x nguyên cộ hợp của các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 16,
      "start_timestamp": "0:11:32",
      "end_timestamp": "0:12:41"
    }
  },
  {
    "page_content": "x y gom lại sẽ là ma trận x nguyên cộ hợp của các x y sẽ là ma trận x và khi đó chúng ta cũng có phong thức tương tự như vậy x nâng với ma trận y x nhân với trận huy, thì chúng ta sẽ có x huy, x tương ứng trong không gian query x ca tương ứng là x khi nhân với nại ca thì chúng ta sẽ có trong không gian key và x v, tức là trong không gian value Bước thứ 2, chúng ta sẽ tính attention score giữa query và key của mình. Chúng ta sẽ có 3 trận là xquay nhân vài nạy xcar Khi này chúng ta sẽ tính giữa",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 17,
      "start_timestamp": "0:12:33",
      "end_timestamp": "0:13:20"
    }
  },
  {
    "page_content": "nhân vài nạy xcar Khi này chúng ta sẽ tính giữa các query và các key Chúng ta sẽ tính trên 1 chuỗi tất cả các cặp query và key với nhau Nhưng lưu ý là ở bước cell attention này query và key của mình sẽ tự tên Chúng ta sẽ có các vétter sau khi chúng ta đã chiếu về không gian query key và balloon Mỗi từ này sẽ đi so với các từ và thậm chí so với cả chính nó nữa để tính ra cái score xquay nhân với xk và truyển khai ra thì xquay chính là x nhân với lại ma trận quay xk chính là x nhân với lại ma",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 18,
      "start_timestamp": "0:13:17",
      "end_timestamp": "0:14:21"
    }
  },
  {
    "page_content": "lại ma trận quay xk chính là x nhân với lại ma trận k Tất cả chuyển vị Khi chúng ta truyển khai chuyển vị vào bên trong dấu hoạt này thì nó sẽ là x chuyển vị đem ra và k chuyển vị đem lên trước Như vậy thì công thức của ma trận biến đổi, xin lỗi, của ma trận attention score của mình đó chính là xquay nhân với nầy ca chuyển vị, nhân với x chuyển vị Và sang bước thứ 3, chúng ta sẽ tính Attention Distribution với hàm Sopart thì chúng ta sẽ có ma trận A Và sang bước số 4, chúng ta sẽ tổng hợp là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 19,
      "start_timestamp": "0:14:11",
      "end_timestamp": "0:14:21"
    }
  },
  {
    "page_content": "trận A Và sang bước số 4, chúng ta sẽ tổng hợp là Output, là bằng cái công thức ở đây Rồi, thì đây là cái công thức ở dạng Vector-Oá cho Cell Attention Và khi chúng ta triển khai hết Chúng ta sẽ có output là bằng softmax của xquay ca chuyển quỵ x chuyển quỵ Qua hàm softmax xong để tính ra được, đây là cái handfan Chúng ta sẽ nhân với lại cái sv để tổng hợp thông tin, đây sẽ là trọng số và toàn bộ cái này sẽ là tổng hợp thông tin tổng hợp toàn bộ những cái thông tin của giai đoạn cell retention,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 20,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "những cái thông tin của giai đoạn cell retention, tức là giai đoạn encode",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UXxELgk5Vws",
      "filename": "UXxELgk5Vws",
      "title": "[CS431 - Chương 10] Part 4_1: Kiến trúc Transformer: Bộ Encoder",
      "chunk_id": 21,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Trong bản hướng dẫn này, chúng ta sẽ cùng tìm hiểu về phương phát biểu diễn từ với mô hình Word2Vacc. Ở đây thì chúng ta sẽ có hai phần. Phần đầu tiên đó là biểu diễn từ và tính toán sự tương đồng giữa hai từ với nhau. Tiếp theo đó là chúng ta sẽ cùng khai thác một số quan hệ về mặt ngữ nghĩa mà mô hình biểu diễn từ như Word2Vacc có khả năng thực hiện được. Đối với phần về biểu diễn từ và sự tính toán tương đồng giữa các từ với nhau, chúng ta sẽ sử dụng thư viện GenSim. Nếu chúng ta sử dụng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:34"
    }
  },
  {
    "page_content": "sẽ sử dụng thư viện GenSim. Nếu chúng ta sử dụng Google Collab, thì mặc nhiên là chúng ta đã cài trước thư viện GenSim rồi. Đó là chúng ta không cần phải cài đặt lại. Nếu chúng ta sử dụng trên máy tính cá nhân của mình, thì mình sẽ phải cài bằng một trong hai cách sau. Một, đó là chúng ta có thể sử dụng lệnh PIP Install GenSim. 2. Chúng ta sử dụng konda mini-konda thì chúng ta sẽ dùng lệnh konda Install trừ C, anaconda, genshin Để sử dụng tư viện Genshin này, chúng ta sẽ khai báo Import",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 1,
      "start_timestamp": "0:00:31",
      "end_timestamp": "0:01:22"
    }
  },
  {
    "page_content": "tư viện Genshin này, chúng ta sẽ khai báo Import Genshin. Thời gian để chúng ta khởi động và Import Genshin có thể tốn khoảng vài giây. Thế thì, cái thương mịn GenSim này thì nó sẽ không có chứa sẵn mô hình đã được huấn luyện sẵn. Do đó thì chúng ta sẽ phải download từ các cái trang web mà đã được cung cấp bởi các cái nhóm nghiên cứu trên thế giới. Ví dụ như là ở đây có một cái nơi rất là nổi tiếng để chứa các cái mô hình huấn luyện sẵn cho việc biểu diễn từ của Colbag là fasttech.cc Đây là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 2,
      "start_timestamp": "0:01:13",
      "end_timestamp": "0:02:05"
    }
  },
  {
    "page_content": "biểu diễn từ của Colbag là fasttech.cc Đây là trang web của Facebook, của nhóm nghiên cứu trong Facebook. Họ đã huấn luyện sẵn các mô hình cho các ngôn ngữ tiếng Anh, tiếng Việt, và tiếng Trung. Rất nhiều thứ tiếng ở trên thế giới, nổi tiếng, phổ biến. Ở đây chúng ta chỉ việc lên đây để tải mô hình về. Tại cái mô hình về, thì ở trong cái codeblog này, chúng ta đã có sản một cái đường link để có thể tải được mô hình Word2Vacc cho tiếng Anh. Thì tên của cái mô hình này là Vector English, tức là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 3,
      "start_timestamp": "0:02:01",
      "end_timestamp": "0:02:52"
    }
  },
  {
    "page_content": "tên của cái mô hình này là Vector English, tức là biểu diễn từ cho các cái từ của ngôn ngữ tiếng Anh. Và tập dữ liệu này được huấn luyện từ Wikinews và tổng số từ của mình đã là 1 triệu từ Sau khi chúng ta đã biến lại cái vector biểu diễn thì vector biểu diễn sẽ là 300 triệu 300D có nghĩa là mỗi một token mỗi một từ sẽ biểu diễn như vạng một vector 300 triệu Sau khi tải và dạy nén, thì nó sẽ có được một file WikiNews 300D1V.b. Và cái file này sẽ được import ở trong đoạn code ở dưới đây. Thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 4,
      "start_timestamp": "0:02:44",
      "end_timestamp": "0:03:44"
    }
  },
  {
    "page_content": "sẽ được import ở trong đoạn code ở dưới đây. Thì thời gian để tải và dạy nén có thể tốn của mình là khoảng một phút. Thì cũng nói luôn đó là mô hình Word2MAC này được train trên hàng tỷ, hàng trăm triệu tài liệu. Ở đây thì chúng ta sẽ tranh tổ thời gian, chúng ta sẽ chạy cái lệnh này, lệnh này thì có thể tốn từ 3 đến 4 phút. 4 phút, đã là lóát mô hình lên, thì tập dữ liệu wiki, News này 300D 1 triệu, 1 triệu, tốt cầm này được trend trên hàng tỷ văn bản và nó cần sử dụng đến rất nhiều những cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 5,
      "start_timestamp": "0:03:37",
      "end_timestamp": "0:04:24"
    }
  },
  {
    "page_content": "văn bản và nó cần sử dụng đến rất nhiều những cái GPU và trend trong khoảng thời gian là vài tuần. Với tài nguyên tính toán của chúng ta, con những người dùng cá nhân thì rất khó để mà chúng ta có thể để tập hợp với mô hình vô tô bách này. Do đó thì việc sử dụng một cái Pre-Train Mode cho một cái mô hình đã hữu luyện sẵn trước đó đó là khả thi hơn. Và chúng ta sẽ khai thác cái vô tô bách này để giải quyết một số cái bài toán về sau. Có thể nói ví dụ như là bài toán phân loại, văn bản, bài toán",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 6,
      "start_timestamp": "0:04:21",
      "end_timestamp": "0:04:59"
    }
  },
  {
    "page_content": "dụ như là bài toán phân loại, văn bản, bài toán dịch máy, bài toán tâm tác phân bản, vâng vâng. Thì tất cả những cái bài toán đó, thì mô hình Deep Learning sử dụng cho các bài toán đó đều phải có một bước gọi là Emitting, là Work Emitting. Tức là các từ của mình thay vì chúng ta xử lý dạng chuỗi, chúng ta sẽ đưa nó về Vector biểu diễn. Work To Back là một trong phương pháp biểu diễn phổ biến và được sử dụng rất nhiều trong các môn học sau. Chúng ta sẽ chờ 2 phút nữa để mô hình có thể đoát được",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 7,
      "start_timestamp": "0:04:56",
      "end_timestamp": "0:05:57"
    }
  },
  {
    "page_content": "ta sẽ chờ 2 phút nữa để mô hình có thể đoát được Bản chất của mô hình này chính là 1 cái ma trận Nếu như ở đây chúng ta thấy có 2 thông số là 300D 1 triệu Đây là một cái ma trận có kích thước là 1 triệu nhưng 300. 1 triệu nhưng 300, tức là khoảng 300 triệu. Đây là một cái mồ hình tham số, là một cái ma trận có kích thước là 1 triệu nhưng 300 triệu. Thì chúng ta thấy là một cái file này nó rất là nợ. Trong đây chúng ta thấy là cái file của mình khi đã nén, khi đã nén thì nó là khoảng 600 mê.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 8,
      "start_timestamp": "0:05:53",
      "end_timestamp": "0:06:53"
    }
  },
  {
    "page_content": "khi đã nén, khi đã nén thì nó là khoảng 600 mê. Thnga mô hình Ph override , trong 2 الشIANH quelque dование nữa ta có thể sử dụng cho th Debatte thì chúng ta sử dụng cái tái tử đó là chỉ một, mở hoặc vô, đóng hoặc vô và ở đây chúng ta sẽ truyền vào cái từ mà chúng ta muốn biểu diễn thì ví dụ như trong đây chúng ta muốn biểu diễn cái từ kinh, vô thì chúng ta sẽ truyền vào cái chuỗi là kinh sau đây ví dụ nếu chúng ta muốn truyền cái chuỗi khác, vật nguyên Chúng ta chỉ việc thay cái chuỗi King và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 9,
      "start_timestamp": "0:06:49",
      "end_timestamp": "0:07:32"
    }
  },
  {
    "page_content": "nguyên Chúng ta chỉ việc thay cái chuỗi King và chuỗi Queen, rất là đơn giản. Và cái kết quả trả về của cái model King này là chính là cái vector biểu diễn của thừa King. Và vector này, cái King vector này, nó sẽ là một cái vector 300 triều. Và việc so sánh tất cả các cái từ với nhau thì nó sẽ tương đương với việc chúng ta sẽ đi so sánh các cái vector biểu diễn của các cái từ này. Ví dụ nếu chúng ta có cái từ win, ở dưới đây chúng ta thấy là có từ win, chúng ta có cái win vector, chúng ta có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 10,
      "start_timestamp": "0:07:25",
      "end_timestamp": "0:08:11"
    }
  },
  {
    "page_content": "từ win, chúng ta có cái win vector, chúng ta có cái từ char, thì chúng ta sẽ có char vector. Và muốn so sánh hai cái từ win với char với nhau thì chúng ta sẽ sử dụng là winvec.dot charvec. thì ở đây đó chính là theo tác tích vô hướng. Tích vô hướng tức là chúng ta sẽ tính sự tương đồng giữa hai character. Nếu độ tương đồng này càng cao, thì giá trị tích vô hướng này sẽ càng cao. Nếu độ tương đồng thấp, thì tích vô hướng này sẽ thấp, tức là hai từ này không có tương đồng nhau. Bên cạnh đó, chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 11,
      "start_timestamp": "0:08:07",
      "end_timestamp": "0:08:47"
    }
  },
  {
    "page_content": "này không có tương đồng nhau. Bên cạnh đó, chúng ta cũng hoàn toàn có thể sử dụng những độ đo khác, độ đo về độ tương đồng, ví dụ như chúng ta có thể sử dụng độ đo coxin hoặc là sử dụng những độ đo không thuộc dòng Similarity, như là độ đo phạm cắt Rồi, thì ở đây chúng ta thấy là đã tốn hết 4 phút để loát cái mô mình này lên và bây giờ chúng ta sẽ tín hình chạy thử Rồi, chúng ta sẽ in ra cái kinh quét này là gì và chúng ta thấy là nó sẽ ra một cái vector. Thì nếu như bằng mắt thường chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 12,
      "start_timestamp": "0:08:41",
      "end_timestamp": "0:09:31"
    }
  },
  {
    "page_content": "cái vector. Thì nếu như bằng mắt thường chúng ta nhìn vô đây, chúng ta sẽ không thể hiểu được cái ý nghĩa của cái vector này. Vậy thì ở đây chúng ta sẽ thử quan sát xem là cái kích thước của king vector này là gì. Thì đó là một cái vector 300 triều. Rồi, và như vậy thì thật lẽ chúng ta có đề cập đến cái việc đó là để so sánh giữa hai cái từ với nhau. Chúng ta sẽ tính cái factor biểu diễn của đó và sau đó chúng ta sẽ tính tích phối hướng Ở đây chúng ta đã có sản là Win và Kai, chúng ta sẽ xem",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 13,
      "start_timestamp": "0:09:23",
      "end_timestamp": "0:10:12"
    }
  },
  {
    "page_content": "chúng ta đã có sản là Win và Kai, chúng ta sẽ xem Win và Kai nó như thế nào Thế thì nó sẽ ra là sự tương đồng giữa từ Win và từ Kai là 1.5, 1.51 Thế thì nếu như chúng ta nhìn vô 1.51 chúng ta không thể biết được rằng đây là hai cái từ có sự tương đồng cao hay không Như vậy thì muốn so sánh được thì chúng ta sẽ phải có thêm một cái từ khác Thì ở đây chúng ta sẽ có thêm một từ nữa là từ King Thì từ King thì chúng ta đã khai báo ở đây rồi Do đó thì chúng ta sẽ tính thêm là Win và King, Pacto Thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 14,
      "start_timestamp": "0:10:01",
      "end_timestamp": "0:11:05"
    }
  },
  {
    "page_content": "chúng ta sẽ tính thêm là Win và King, Pacto Thì để xem coi Win và Car, tức là nữ hoàng và xe hơi với lại nữ hoàng và vua thì từ nào sẽ có sự tương đồng hơn ở đây thì chúng ta sẽ in ra là similarity của win và k là bằng Rồi, ở đây thì chúng ta sẽ có là Similarity của Win và King để xem coi giá trị này sẽ là bằng bao nhiêu. Và ở đây thì chúng ta sẽ kiểm tra sự tương đối. Với đây chúng ta thấy giữa win và king, chúng ta thấy độ tương đồng là 3,2 cao gấp đôi hơn gấp đôi so với lại win và ca Thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 15,
      "start_timestamp": "0:10:54",
      "end_timestamp": "0:11:40"
    }
  },
  {
    "page_content": "cao gấp đôi hơn gấp đôi so với lại win và ca Thì điều này nó cũng thể hiện là 2 từ vua và nữ hoàng nó có tương đồng cao hơn so với lại nữ hoàng và xe Thì cái này cũng khá là dễ hiểu tại vì win và king nếu mà chúng ta xét trong phạm vi về đời sống của mình đúng không? thì đây là hai từ thường được sử dụng đi chung với nhau. Trong khi đó Win và Ca, thì hai cái từ này ít đi chung với nhau nên độ tương đồng của nó thấp hơn. Nhắc lại, sự tương đồng giữa hai từ sẽ thể hiện bởi chức năng ngữ phát. Ví",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 16,
      "start_timestamp": "0:11:32",
      "end_timestamp": "0:13:17"
    }
  },
  {
    "page_content": "hai từ sẽ thể hiện bởi chức năng ngữ phát. Ví dụ như Win, những câu nào mà chúng ta dùng từ Win thì đều có thể thay thế, không phải đều có thể thay thế mà chúng ta có thể thay thế với từ King mà nó vẫn đảm bảo được yếu tố về mặt ngữ phát và ý nghĩa của cái câu. Bây giờ chúng ta sẽ thử xét trên 3 từ đặc biệt, đó là từ love, like và từ hate để minh họa vây trò về mặt bộ pháp 3 character là Love, Like, Hate Nếu như về mặt cảm quan thì các bạn sẽ cảm nhận là love và like đều là thích, yêu thích thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 17,
      "start_timestamp": "0:13:05",
      "end_timestamp": "0:13:53"
    }
  },
  {
    "page_content": "nhận là love và like đều là thích, yêu thích thì hai cái từ này sẽ có mối quan tương đồng, cao hơn sau với love và hate Yêu, yêu, ghét thì hai cái từ này các bạn cảm nhận được là đối nhật nhau về mặt ngữ nghĩa Bây giờ chúng ta sẽ xem điều đó có đúng hay không Giữa Love và Like Ở đây sẽ là lớp Giữa Love và Hate Nếu chúng ta sử dụng ngữ nghĩa thì chúng ta sẽ xem lớp và hay rõ ràng là độ tương đồng sẽ không cao bằng lớp và head, đúng không? Vì vậy chúng ta sẽ chạy ra thử nè. Lớp và head, mình chia",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 18,
      "start_timestamp": "0:13:48",
      "end_timestamp": "0:14:24"
    }
  },
  {
    "page_content": "ta sẽ chạy ra thử nè. Lớp và head, mình chia chạy cái lại này. Rồi, thì giữa lớp và line chúng ta thấy là độ tương đồng thấp, Trong khi đó, love và hate thì lại độ tương đồng cao hơn. Nó mâu thuẫn về mặt ý nghĩa tại vì hồi nãy chúng ta cảm nhận, chúng ta đoán rằng là love và hate nó đối nhệt nhau về mặt ngữ nghĩa thì lẽ ra độ tương đồng phải thấp hơn. Thế thì điều này sẽ giải thích như thế nào? Ở đây thì chúng ta sẽ thấy love và hate trong cái moment work emitting thì nó sẽ không quan tâm về",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 19,
      "start_timestamp": "0:14:21",
      "end_timestamp": "0:15:10"
    }
  },
  {
    "page_content": "moment work emitting thì nó sẽ không quan tâm về mặt ngữ nghĩa và nó quan tâm về mặt vai trò ngữ pháp trong câu nào mà cái từ A có thể thay được cho từ B một cách dễ dàng và không thay đổi về mặt khấu trúc ngữ pháp cũng như là về ý nghĩa không phải thay đổi về mặt ý nghĩa mà là có thể thay đổi và không có vi phạm những nguyên trách về mặt ngữ pháp thì hai cái từ đó có độ tương đồng cao thế thì cái từ love và cái từ la, từ hate thì nó có cái vai trò ngữ pháp giống nhau Đó là điều đồng từ Nhưng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 20,
      "start_timestamp": "0:14:58",
      "end_timestamp": "0:15:18"
    }
  },
  {
    "page_content": "trò ngữ pháp giống nhau Đó là điều đồng từ Nhưng mà riêng cái từ line, nếu như các bạn trả trên từ điện á Thì cái từ line này có rất nhiều vai trò Line này có thể vừa là adverb, vừa có thể là verb, vừa có thể là các loại rất nhiều những loại từ khác Đó Thì adjective nữa, hoặc tính từ Nếu mà nói về mặt ngừng phát thì từ love không có tương đồng với từ like nhiều như từ love với từ hate Đây là một bí dụ cho các bạn cảm nhận được hai từ love, hate và like có mối quan hệ về mặt ngừng phát như thế",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 21,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "và like có mối quan hệ về mặt ngừng phát như thế nào Love và hate có mối quan hệ ngừng phát giống nhau nhiều hơn so với love và like nên sự tương đồng sẽ càng hơn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=WAiLM7OFU9A",
      "filename": "WAiLM7OFU9A",
      "title": "[CS431 - Chương 6] Part 5_1: Hướng dẫn lập trình với Word2Vec",
      "chunk_id": 22,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Phương thức 9 là phương thức build và trend trong đáp build, sẽ tiến hành định nghĩa kiến trúc của mạng ANN. Chúng ta sẽ cùng nhìn lại kiến trúc của mình. Đầu tiên, đó chính là emitting layer. Xin lỗi, đầu tiên của mình là input. input này sẽ chứa các chỉ số index của các từ trong câu comment, trong câu review của mình nó chỉ chứa chỉ số chứ không có nưu dữ liệu gốc bắt đầu lấp input này sẽ có đầu vào của mình Họ làm MaxReviewLang, tức là ở đây là 500 Vậy ở đây sẽ là một vector 500 triều, 500",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wKMBVF_bJdw",
      "filename": "wKMBVF_bJdw",
      "title": "[CS431 - Chương 8] Part 4_2: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:53"
    }
  },
  {
    "page_content": "là 500 Vậy ở đây sẽ là một vector 500 triều, 500 phần tử Input của mình sẽ là một vector có 500 phần tử Cho dù review ngắn hay review dài thì cũng sẽ đều có 500 phần tử Lớp tiếp theo chính là lớp Embedding Embezdin này sẽ có thông tin vô capsize, tức là số lượng từ của mình. Chúng ta sẽ xem xét dictionary. Do môn của mình chưa được lát lên, nhưng khi lát lên xong, quật toback.vector chính là ma trận trọng số đã được u luyện của mình. thì nó sẽ có hai thông số về kích thước, đó là Dictionary",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wKMBVF_bJdw",
      "filename": "wKMBVF_bJdw",
      "title": "[CS431 - Chương 8] Part 4_2: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 1,
      "start_timestamp": "0:00:47",
      "end_timestamp": "0:01:43"
    }
  },
  {
    "page_content": "có hai thông số về kích thước, đó là Dictionary Land, tức là tổng số từ trong tập tự điện của mình và Emitting Land, tức là chiều của vector mà mình dự kiến mình sẽ biểu diễn thì ở đây là khoảng 1 triệu, Dictionary là gần 1 triệu và Emitting Land là 300 triệu, do ở đây mình sẽ dùng là 300 triệu, 1 triệu Giờ đây chúng ta sẽ để là msding dictionary land, tức là số từ trong từ điện của mình ở đây sẽ là EmbeddingLayers và như để cập hồi nãy, tức là cái thông số EmbeddingLayers này chúng ta hoàn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wKMBVF_bJdw",
      "filename": "wKMBVF_bJdw",
      "title": "[CS431 - Chương 8] Part 4_2: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 2,
      "start_timestamp": "0:01:35",
      "end_timestamp": "0:02:31"
    }
  },
  {
    "page_content": "là cái thông số EmbeddingLayers này chúng ta hoàn toàn có thể thay đổi chúng ta hoàn toàn có thể thay đổi và cho cái mô hình của mình nó học EmbeddingLayers này luôn thay vì làm một cái Layers tỉnh, nhưng mà trong cái ví dụ này thì chúng ta đang xem sẽ nó làm một cái Layers tỉnh Emitting Initializer Emitting Layer sẽ có các ký Regularizer Ở phía sau thì chúng ta đã có một cái bộ cốt tương ứng cho mstm. Ở đây là nó sẽ có thêm hai thông số nữa. Thông số đầu tiên đó chính là WAI. WAI chính là mô",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wKMBVF_bJdw",
      "filename": "wKMBVF_bJdw",
      "title": "[CS431 - Chương 8] Part 4_2: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 3,
      "start_timestamp": "0:02:26",
      "end_timestamp": "0:03:41"
    }
  },
  {
    "page_content": "số đầu tiên đó chính là WAI. WAI chính là mô hình của tuvac mà chúng ta đã học trước đây. Và ở đây chúng ta sẽ sử dụng nhưng không hề hướng luyện lại. Chúng ta sẽ sử dụng không vui lệ lại do đó, trainable sẽ là bằng phone, tức là emitting này có được train lại hay không? Ở đây là không, chúng ta sẽ không train lại, chúng ta sẽ tái sử dụng luôn cho bộ trọng số của mô hình của tu BAC. Rồi, và ở đây thì chúng ta sẽ cùng truyền vào input layer thì ở đây nó sẽ có thông tin là input, input này là kết",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wKMBVF_bJdw",
      "filename": "wKMBVF_bJdw",
      "title": "[CS431 - Chương 8] Part 4_2: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 4,
      "start_timestamp": "0:03:31",
      "end_timestamp": "0:04:29"
    }
  },
  {
    "page_content": "đây nó sẽ có thông tin là input, input này là kết quả của lớp biến đổi trước đó Cái lớp tiếp theo đó chính là lớp ANN, và lớp ANN này thì nó sẽ cho chúng ta biết Các hình ảnh sẽ biết kết thước của hít đình layer là bao nhiêu, và kết thước của quá trình biến đổi là ST là bao nhiêu Sau khi thực hiện emitting layer, lưu ý là nó không phải là vétter 32 triều, mà nó sẽ là 300 triều Trong trường hợp tổng quát thì cái output này, số chiều của XT này có thể là con số bất kỳ để chúng ta định nghĩa Và mô",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wKMBVF_bJdw",
      "filename": "wKMBVF_bJdw",
      "title": "[CS431 - Chương 8] Part 4_2: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 5,
      "start_timestamp": "0:04:18",
      "end_timestamp": "0:05:13"
    }
  },
  {
    "page_content": "thể là con số bất kỳ để chúng ta định nghĩa Và mô hình của mình sẽ học cái ms-inlayer Còn trong trường hợp này ms-inlayer của mình là tỉnh thì ở đây sẽ là 300 Và qua đây thì chúng ta sẽ qua cái Arrange cell thì chúng ta sẽ tỉnh ra cái ST ST này chính là cái vector của cái trạng ai ở ở đây Và nó có thể là 64 chiều, ở đây chúng ta để 64 chiều Sau đó, chúng ta sẽ thực hiện phép biến đổi là DENSE, tức là kết nối đầy đủ để từ ST này biến thành output. Và ở đây chúng ta phân loại nhị phân nên ở đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wKMBVF_bJdw",
      "filename": "wKMBVF_bJdw",
      "title": "[CS431 - Chương 8] Part 4_2: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 6,
      "start_timestamp": "0:05:09",
      "end_timestamp": "0:06:06"
    }
  },
  {
    "page_content": "Và ở đây chúng ta phân loại nhị phân nên ở đây sẽ là 1 hẻm activation sẽ là sigmoid. Đầu ra của mình sẽ là 1 node và activation sẽ là sigmoid. Đầu vào là hít đần, hít đần là kết quả của layer trước đó, là simple INN Chúng ta có thể để là 64, theo những ký sơ đồ sẽ là 64 Hàm loss của mình sẽ sử dụng binary cross entropy, sử dụng Hadam Độ đo đánh giá của mình sẽ là accuracy, rồi chúng ta sẽ fit Bây giờ chúng ta sẽ phát triển bài pop phát triển Bây giờ chúng ta không có nhiều thời gian Mô hình của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wKMBVF_bJdw",
      "filename": "wKMBVF_bJdw",
      "title": "[CS431 - Chương 8] Part 4_2: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 7,
      "start_timestamp": "0:05:58",
      "end_timestamp": "0:07:02"
    }
  },
  {
    "page_content": "giờ chúng ta không có nhiều thời gian Mô hình của Thuvac đã được đặt lên rồi Bây giờ chúng ta sẽ cùng xem Chất thước của DictionaryLang là bao nhiêu Và EmittingLang là bao nhiêu Chúng ta đã loát mô hình rồi nên mình sẽ không phải loát lại nữa Tại vì nó sẽ tốn nhất 3 phút Đây là DictionaryLang là bằng 900 ngàn và msding của mình sẽ là 300 Tức là cái kích thước của tập tự điển của mình sẽ là gần 1 triệu Bây giờ chúng ta sẽ chạy ANN Rồi chúng ta sẽ khởi tạo một lớp lối tượng là ANN và gọi hàm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wKMBVF_bJdw",
      "filename": "wKMBVF_bJdw",
      "title": "[CS431 - Chương 8] Part 4_2: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 8,
      "start_timestamp": "0:06:58",
      "end_timestamp": "0:07:45"
    }
  },
  {
    "page_content": "sẽ khởi tạo một lớp lối tượng là ANN và gọi hàm build Build này sẽ dựng lên kiến trúc của mạng ANN của mình Rồi sau đó chúng ta sẽ tiến hành train Vì việc train này cũng tốn của chúng ta khoảng 3-4 phút Do là cái mạng ANN thì nó không có thực hiện tích toán xong xong được, các cái bước của mình nó đều thực hiện tuần tự nên cái tốc độ kính toán của mình nó sẽ rất là chậm Rồi thì chúng ta quan sát ở đây là cái loss của mình là đang 0.7 và accuracy của mình đang là khoảng 51-52% loss của mình nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wKMBVF_bJdw",
      "filename": "wKMBVF_bJdw",
      "title": "[CS431 - Chương 8] Part 4_2: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 9,
      "start_timestamp": "0:07:38",
      "end_timestamp": "0:08:45"
    }
  },
  {
    "page_content": "của mình đang là khoảng 51-52% loss của mình nó đang có số hướng giảm xuống Đây là 1 trên 3 tóc Nếu như chương trình này chạy xong, thì trong history này sẽ lưu lót của quá trình bố luyện của mình. Cô chênh này chạy xong thì trong history này nó sẽ lưu lọc của quá trình phương đuổi của mình Chúng ta sẽ in ở đây Để quan sát trọng số của mô hình của mình thì chúng ta sẽ dùng rn.model.layer Chúng ta cũng có thể grid một phương thức lấy trọng số Nhưng mà ở đây cho nhanh thì chúng ta có thể để",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wKMBVF_bJdw",
      "filename": "wKMBVF_bJdw",
      "title": "[CS431 - Chương 8] Part 4_2: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 10,
      "start_timestamp": "0:08:40",
      "end_timestamp": "0:10:06"
    }
  },
  {
    "page_content": "Nhưng mà ở đây cho nhanh thì chúng ta có thể để rn.model.layer Và chúng ta sẽ lấy layer số 2 Tại sao? Tại vì đây là layer số 0, là input layer, sau đó sẽ là layer số 1, là emitting layer Thì hai cái này là không có tham số hữu đuệnh nào Chủ yếu cái tham số hữu đuệnh của mình sẽ nằm ở lớp ANN này Nằm ở lớp ANN này, do đó nó sẽ nằm ở layer số 2, 0, 1, 2 Và getway thì chúng ta sẽ có 3 cái bộ trọng số là UVW. Ở xe 3 trọng số thì ở đây chúng ta đang lấy là cái trọng số thứ 3 trong 3 cái bộ thèm số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wKMBVF_bJdw",
      "filename": "wKMBVF_bJdw",
      "title": "[CS431 - Chương 8] Part 4_2: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 11,
      "start_timestamp": "0:09:48",
      "end_timestamp": "0:10:57"
    }
  },
  {
    "page_content": "lấy là cái trọng số thứ 3 trong 3 cái bộ thèm số là UVW. sau một thời gian, mô hình của mình đã kết thúc quá trình huấn luyện và nó đang ở trong giai đoạn là kết thúc thì cho mỗi cái Epoch thì nó sẽ chiến khoảng là 110 giây hông, khoảng trung bình là 110 giây rồi, thì ở đây chúng ta sẽ in ra để xem coi cái loss của mình nó như thế nào nhưng mà về quan sát thì chúng ta thấy là cái loss của mình nó giảm nhưng mà đến cái Epoch thứ 3 là nó bắt đầu nó không còn giảm được nữa Và accuracy đạt cao nhất",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wKMBVF_bJdw",
      "filename": "wKMBVF_bJdw",
      "title": "[CS431 - Chương 8] Part 4_2: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 12,
      "start_timestamp": "0:10:49",
      "end_timestamp": "0:12:01"
    }
  },
  {
    "page_content": "không còn giảm được nữa Và accuracy đạt cao nhất là 66% Đây là kết quả khi chúng ta trend với 3 cái ghi tóc của mình Và đây là cái biểu đồ, nó dạm xuống và bắt đầu tăng lên Trọng số sẽ là 64, tại vì trong sơ đồ này, ST của mình có 64, nên trọng số của mình ở đây không có là 32, mà là 64. Bây giờ chúng ta sẽ tiến hành dự đoán trên dữ liệu test và xem xem model của mình có độ chính xác là bao nhiêu phần trăm Trong phần nguyện đánh giá, chúng ta sẽ kiểm tra xem nếu y lớn hơn 0.5 thì nó tư ứng là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wKMBVF_bJdw",
      "filename": "wKMBVF_bJdw",
      "title": "[CS431 - Chương 8] Part 4_2: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 13,
      "start_timestamp": "0:11:56",
      "end_timestamp": "0:13:45"
    }
  },
  {
    "page_content": "kiểm tra xem nếu y lớn hơn 0.5 thì nó tư ứng là nhãn 1 Còn ngược lại thì nó sẽ là bằng 0 Và chúng ta sẽ đưa nó về vector sau đó Chúng ta sẽ kiểm tra xem cái Y predict này có khớp với lại Y test hay không Có bằng bằng Y test hay không Sau đó chúng ta sẽ tính tổng tất cả những cái mẫu cho cái chất quả bằng nhau Chi cho tổng số mẫu của mình, tức là chi cho E-test, thì khi đó chúng ta sẽ có được độ chính xác Chúng ta sẽ cùng quan sát xem kết quả của mình khi mà chúng ta dự đoán trên tập dữ liệu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wKMBVF_bJdw",
      "filename": "wKMBVF_bJdw",
      "title": "[CS431 - Chương 8] Part 4_2: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 14,
      "start_timestamp": "0:13:44",
      "end_timestamp": "0:15:00"
    }
  },
  {
    "page_content": "của mình khi mà chúng ta dự đoán trên tập dữ liệu XTAC là bằng bao nhiêu? Simple ANN sẽ thay bằng LSTM Rồi, chúng ta sẽ thay bằng LSTM Ở đây chúng ta sẽ để là 64 Rồi, đây sẽ để là m-adding Cho nó dễ hiểu nha Rồi, đây sẽ là hidden Bây giờ, chúng ta tạo lớp đối tượng LXStamp và train lại từ đầu Bắt đầu quá trình train của mình Đồ chính xác của chúng ta trong trường hợp này là 74% thì so với lại phiên bản dùng ANN chỉ có 68% điều này cho thấy đó là khi chúng ta sử dụng LSTN cell với cùng cấu hình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wKMBVF_bJdw",
      "filename": "wKMBVF_bJdw",
      "title": "[CS431 - Chương 8] Part 4_2: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 15,
      "start_timestamp": "0:14:55",
      "end_timestamp": "0:15:44"
    }
  },
  {
    "page_content": "khi chúng ta sử dụng LSTN cell với cùng cấu hình tương tự như ANN là cùng hand-made layer cũng có phần dance activation range và số hidden và số chiều của vector trạng thay ẩn của mình là 64 chiều thì ở đây chúng ta phải LSTM cho kết quả chính xác hơn như vậy thì qua cái tutorial này thì chúng ta đã cùng thực hiện cài đặt cái mạng ANN biến thể và biến thể của đó đó là LSTM Một cách tổ bát thì chúng ta cũng có thể không sử dụng MS-DIN layer như là một cái ma trận cố định này Mà chúng ta có thể",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wKMBVF_bJdw",
      "filename": "wKMBVF_bJdw",
      "title": "[CS431 - Chương 8] Part 4_2: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 16,
      "start_timestamp": "0:15:33",
      "end_timestamp": "0:15:44"
    }
  },
  {
    "page_content": "là một cái ma trận cố định này Mà chúng ta có thể cho mô hình của mình có thể học, tự học MS-DIN này Thay vì sử dụng một cái ma trận chỉnh Và cái kết quả thì khi chúng ta sử dụng cái ma trận MS-DIN, xin lỗi cái lớp layer Lớp msding này được huấn luyện cho kết quả càng tốt hơn sau việc sử dụng một cuộc msding cố định.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wKMBVF_bJdw",
      "filename": "wKMBVF_bJdw",
      "title": "[CS431 - Chương 8] Part 4_2: Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
      "chunk_id": 17,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Để trực qua hóa, chúng ta sẽ phải log ra tham số cho nguồn hình này. Chúng ta sẽ có hai cái là theta1 và theta2. Trong đó, thành phần theta2 là cái mà chúng ta sẽ quan sát đầu tiên, xem có giá trị của nó như thế nào. để lấy giá trị tham số đầu tiên theta2, chúng ta sẽ lấy Neural Network.get weight và chúng ta sẽ truyền vô layer na layer số 2 Rồi, như vậy thì chúng ta sẽ thấy là các giá trị của Theta 2 này sẽ có 8 giá trị thật cả. 8 giá trị này tương ứng là trọng số của các nốt trong mô hình này.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XBS1JuTrxVI",
      "filename": "XBS1JuTrxVI",
      "title": "[CS431 - Chương 2] Part 5b_2: Cài đặt mạng neural network",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:12"
    }
  },
  {
    "page_content": "ứng là trọng số của các nốt trong mô hình này. Và cái trọng số này thì nó sẽ thể hiện là cái vai trò khi đưa ra cái output cuối cùng thì tôi sẽ tin cậy vào cái node nào. Tôi sẽ tin cậy vào cái node nào. Vậy thì trong số 8 cái node này thì đâu đó có những node có độ tin cậy thấp, ví dụ như là 0.3, 0.5, 0.4. nhưng cũng có những nốt độ tin cậy rất cao, dự như là lưu ý là độ tin cậy ở đây chúng ta sẽ thể hiện ở chỗ trì việc đối chứ không phải là sự lớn bé về mặt đại số của nó. Nó như vậy là trừ 17,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XBS1JuTrxVI",
      "filename": "XBS1JuTrxVI",
      "title": "[CS431 - Chương 2] Part 5b_2: Cài đặt mạng neural network",
      "chunk_id": 1,
      "start_timestamp": "0:00:55",
      "end_timestamp": "0:02:01"
    }
  },
  {
    "page_content": "bé về mặt đại số của nó. Nó như vậy là trừ 17, 14, 11, 11, trừ 80 đó là những cái nốt có độ tin cậy rất cao. Với các đồ tên trẻ cao, chúng ta sẽ tìm cách trực quan hóa đường thẳng được tạo bởi các trọng số Đến các đồ tên trẻ cao, chúng ta đã tìm hiểu cách trực quan hóa đường thẳng trong bài logistic regression Chúng ta sẽ viết một cái vòng for, đoạn đầu là chúng ta vẽ các điểm data lên thôi. Điều này là đường thẳng có độ tin cải cao, tức là 0, 1, 0, 1, 2, 3, 4, 5, 6, 7 Chúng ta sẽ visualize các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XBS1JuTrxVI",
      "filename": "XBS1JuTrxVI",
      "title": "[CS431 - Chương 2] Part 5b_2: Cài đặt mạng neural network",
      "chunk_id": 2,
      "start_timestamp": "0:01:52",
      "end_timestamp": "0:03:20"
    }
  },
  {
    "page_content": "0, 1, 2, 3, 4, 5, 6, 7 Chúng ta sẽ visualize các neuron có độ tin tài cao và để vẽ đường thẳng. Với từng neuron thì chúng ta phải biến đổi công thức của mình từ dạng y bằng as cộng b. Trong đó a chính là trừ param 0 chia cho param 1. Rồi, lưu ý là cái param này là trọng số cho các cạnh nối đến cái x1 của x2, rồi param 0, đó chính là cái trọng số, param 0 chính là cái trọng số nối đến đây. Nấu đến thành phần bias Param 1 là nấu đến x1 là tương ứng với x1 Param 2 là trọng số tự uống với lại x2",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XBS1JuTrxVI",
      "filename": "XBS1JuTrxVI",
      "title": "[CS431 - Chương 2] Part 5b_2: Cài đặt mạng neural network",
      "chunk_id": 3,
      "start_timestamp": "0:03:12",
      "end_timestamp": "0:04:22"
    }
  },
  {
    "page_content": "ứng với x1 Param 2 là trọng số tự uống với lại x2 Cho Theta 1 Bây giờ chúng ta phải tính Theta 1 trước bằng neuralnet.get-weight và ở đây chúng ta sẽ truyền là 1 bây giờ chúng ta sẽ lấy thành phần biass thì nó chính là bằng theta 1 thành phần parem Theta là bằng theta 1,0 Bây giờ chúng ta sẽ có công thức này Chúng ta sẽ thế vào để tính ra phương trình đường thẳng cho các node của mình Rồi, thì A sẽ là bằng trừ 3 rem 0, R chia cho 3 rem phục b sẽ là trừ bias chi cho params bột và ở đây params ở",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XBS1JuTrxVI",
      "filename": "XBS1JuTrxVI",
      "title": "[CS431 - Chương 2] Part 5b_2: Cài đặt mạng neural network",
      "chunk_id": 4,
      "start_timestamp": "0:04:18",
      "end_timestamp": "0:06:02"
    }
  },
  {
    "page_content": "là trừ bias chi cho params bột và ở đây params ở đây chúng ta phải lấy là để biết chúng ta sẽ tính như thế nào, chúng ta sẽ in nó ra trước Để xem nó như thế nào BiasParam là các bộ 8 giá trị tương ứng với 8 lốt Trong đó, Param sẽ có 2 thành phần là cho W1 và W2 Vì vậy, mình muốn lấy ra thêm phần nào thì mình sẽ phải truyền thêm cái chỉ số nữa Tức là mình truyền vào chỉ số node thứ mấy Ở đây sẽ là params Params và không node thứ idx Vị trí là thứ idx Params 1 idx Params 1 idx Params 1 idx Bây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XBS1JuTrxVI",
      "filename": "XBS1JuTrxVI",
      "title": "[CS431 - Chương 2] Part 5b_2: Cài đặt mạng neural network",
      "chunk_id": 5,
      "start_timestamp": "0:05:55",
      "end_timestamp": "0:07:12"
    }
  },
  {
    "page_content": "idx Params 1 idx Params 1 idx Params 1 idx Bây giờ chúng ta sẽ vẽ nó lên Lt.plot và hai điểm của mình ở đây chúng ta sẽ lấy điểm từ trừ 1 cho đến 1 rồi, tương ứng đường lấy điểm từ trừ 1 cho đến 1 và điểm theo trục x2 hay trục y chúng ta ký hiệu ở đây Công thức này là a x b A x 1 b A x 1 b Rồi, thì ở đây chúng ta sẽ thấy là gì? Các cái điểm, xin lỗi các cái neuron mà có đội thiên trạng cao thì nó sẽ cắt Cái đường thẳng, xin lỗi nó sẽ cắt tập điểm của mình ra làm 2 phần Trong đó, 1 nửa, 1 ví dụ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XBS1JuTrxVI",
      "filename": "XBS1JuTrxVI",
      "title": "[CS431 - Chương 2] Part 5b_2: Cài đặt mạng neural network",
      "chunk_id": 6,
      "start_timestamp": "0:06:57",
      "end_timestamp": "0:07:53"
    }
  },
  {
    "page_content": "của mình ra làm 2 phần Trong đó, 1 nửa, 1 ví dụ như cái đường màu tím nè, thì 1 nửa bên đây phải Thì nó đều là những cái điểm màu đỏ nhưng nửa bên trái lẫn lộn cả màu đỏ và màu xanh. Tất cả những đường thẳng vỏ lại cũng bắt tiến chất như vậy. Ví dụ như đường màu đỏ, nửa dưới sẽ là toàn màu đỏ nhưng nửa bên trên thì có lẫn màu đỏ và màu xanh. Vì vậy, mỗi đường thẳng này là một quyết classifier sẽ là một bộ phần lớp yếu. Tổ hợp của nhiều quyết classifier này thì nó sẽ giúp chúng ta tạo thành một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XBS1JuTrxVI",
      "filename": "XBS1JuTrxVI",
      "title": "[CS431 - Chương 2] Part 5b_2: Cài đặt mạng neural network",
      "chunk_id": 7,
      "start_timestamp": "0:07:49",
      "end_timestamp": "0:08:35"
    }
  },
  {
    "page_content": "này thì nó sẽ giúp chúng ta tạo thành một bộ Strong Passifier, một bộ phân lớp mạnh tức là các đường màu xanh dương, màu xanh lá, màu tím, màu đỏ hợp lại thì nó sẽ giúp chúng ta tắt cái buồng màu đỏ ra buồng rìa bên ngoài và buồng màu xanh, cái điểm màu xanh là những điểm nằm ở bên trong Bây giờ chúng ta sẽ thử Visualize các điểm mà có độ tin cậy kém Ví dụ như là điểm số 012234 và chúng ta sẽ xem tự các điểm có tinh cải kém 2234 Với các điểm có độ tinh cải thấp thì nó sẽ đi xuyên qua Ví dụ như",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XBS1JuTrxVI",
      "filename": "XBS1JuTrxVI",
      "title": "[CS431 - Chương 2] Part 5b_2: Cài đặt mạng neural network",
      "chunk_id": 8,
      "start_timestamp": "0:08:31",
      "end_timestamp": "0:09:27"
    }
  },
  {
    "page_content": "độ tinh cải thấp thì nó sẽ đi xuyên qua Ví dụ như ở đây có hai đường thẳng là một kem và một xanh, nó đi xuyên qua tập điểm của mình. Vì việc đi xuyên qua tập điểm này thì nó sẽ không đóng góp cho việc là phân toàn, đó là một đỏ hoặc là một xanh. Thì nó sẽ không có nhiều giá trị tiến cải. Vì vậy, qua cái bài tập này, qua cái phép cài đặt này, chúng ta một lần nữa ứng dụng thư biện Keras để cài đặt cho kiến trúc mạng là NoRain Network Và một cách tổn quát thì sau này chúng ta có thể mở rộng lên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XBS1JuTrxVI",
      "filename": "XBS1JuTrxVI",
      "title": "[CS431 - Chương 2] Part 5b_2: Cài đặt mạng neural network",
      "chunk_id": 9,
      "start_timestamp": "0:09:22",
      "end_timestamp": "0:10:09"
    }
  },
  {
    "page_content": "tổn quát thì sau này chúng ta có thể mở rộng lên thành nhiều hít đền layer hơn Ví dụ như ở đây chúng ta sẽ có là hít đền số 1 thì sẽ truyền vào cho lớp tiếp theo là hít đền 1 Rồi sẽ ra hít đền 2 Hít đền 2 sẽ truyền vào đây Đúng không? Để ra hít đền số 3 Và hít đền 3 truyền vào để ra cái output và số node ở đây chúng ta cũng có thể gia giảm có thể là 16, 32 thì đây là cách thức cài đặt cho mạng Leroy Network sử dụng thư biệt Keras",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XBS1JuTrxVI",
      "filename": "XBS1JuTrxVI",
      "title": "[CS431 - Chương 2] Part 5b_2: Cài đặt mạng neural network",
      "chunk_id": 10,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề, học sâu, machine learning, logistic regression, imageNet. hay cái loại cách khác, các mô hình Deep Learning nó chính là một cái hàm hợp của rất nhiều những cái hàm thành phần và muốn tìm được mô hình tối ưu thì chúng ta sẽ phải đi tính đạo hàm của hàm hợp này thì ở đây chúng ta sẽ nhắc lại cái bảng tra đạo hàm của hàm hợp ví dụ như nếu đối với cái hàm bín đầu vào, đầu vào của mình là một bín x thì chúng ta sẽ có cái công thức như bên này x là đĩa số thì nếu như bên đây thì u của mình sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=YSi7G6hDhks",
      "filename": "YSi7G6hDhks",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.2: Ôn tập nền tảng giải tích (Part 2)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:03"
    }
  },
  {
    "page_content": "x là đĩa số thì nếu như bên đây thì u của mình sẽ là một cái hàm số thì lúc đó chúng ta sẽ có công thức để tra để tính đạo hàm theo từng dạng, ví dụ như dạng đa thức dạng hàm mũ rồi dựa hàm scene code, thàm lượng giá rồi hàm logarith thì đây là những cái dạng cản tra để giúp chúng ta có thể tính nhanh các đạo hàm của hàm hợp Đây sẽ là một số ví dụ để thực hành tính đạo hàm của hàm hợp. Với ví dụ của bài số 1 này, chúng ta sẽ vận dụng tính chất đạo hàm hàm hợp với hàm đa thức. Với bài số 1 này,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=YSi7G6hDhks",
      "filename": "YSi7G6hDhks",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.2: Ôn tập nền tảng giải tích (Part 2)",
      "chunk_id": 1,
      "start_timestamp": "0:00:59",
      "end_timestamp": "0:01:54"
    }
  },
  {
    "page_content": "hàm hàm hợp với hàm đa thức. Với bài số 1 này, chúng ta có thể nhõm ra được đạo hàm của hàm hợp này. Đạo hềm của cái hàm này đó chính là bằng 10 của 5x cộng 2 mùa 9 Rồi sau đó chúng ta sẽ nhân với lại cái đạo hềm của cái 5x cộng 2 này chính là bằng 5 Vậy 5 nhân 10 là 50 Nếu như cái bài này chúng ta làm chi tiết thì nó sẽ là như sau Thì chúng ta sẽ tính cái đạo hềm y phải Nếu chúng ta xem đoàn này là u, thì nó sẽ là bằng u mũ 10 Với u là bằng 5x cộng 2 Và chúng ta sẽ dùng công thức ở đây để tra",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=YSi7G6hDhks",
      "filename": "YSi7G6hDhks",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.2: Ôn tập nền tảng giải tích (Part 2)",
      "chunk_id": 2,
      "start_timestamp": "0:01:45",
      "end_timestamp": "0:03:03"
    }
  },
  {
    "page_content": "cộng 2 Và chúng ta sẽ dùng công thức ở đây để tra Đạo hẻm của y sẽ là 10u mũ 9 10u mũ 9 nhân với đạo hẻm của u 10 nhân với u mũ 9, trong trường hợp này n của mình là 10 nhân với đạo hẻm của u phải U là 5x cầu 2, đạo hẻm của nó theo x chính là 5 Vậy thì nó sẽ là bằng 10 U ở đây chúng ta thế vô chính là 5x cộng 2 tất cả bộ 9 nhân với u phải ở đây là 5 Vậy nó sẽ là bằng cái công thức như ở đây Thì chúng ta sẽ vận dụng đạo hẻm của hẻm hợp để tính Và ở đây chúng ta sẽ có thêm một cái khái niệm nữa",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=YSi7G6hDhks",
      "filename": "YSi7G6hDhks",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.2: Ôn tập nền tảng giải tích (Part 2)",
      "chunk_id": 3,
      "start_timestamp": "0:02:58",
      "end_timestamp": "0:03:51"
    }
  },
  {
    "page_content": "ở đây chúng ta sẽ có thêm một cái khái niệm nữa đó là trong trường hợp cái hẻm của mình nó phụ thuộc vào nhiều biến hay còn gọi là hàm đa biến hàm F sẽ phụ thuộc vào các biến x, y và z thì nếu như đạo hàm riêng theo biến x ở đây chúng ta lưu ý là hàm này phụ thuộc bởi nhiều biến số nhưng khi tính đạo hàm riêng theo từng biến thì chúng ta sẽ xem các biến còn lại như là tham số hay là các cái khác là hàng số vậy thì ở đây chúng ta sẽ có một cái ví dụ F x y, tức là F là một cái hàm vụ thuộc bởi 2",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=YSi7G6hDhks",
      "filename": "YSi7G6hDhks",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.2: Ôn tập nền tảng giải tích (Part 2)",
      "chunk_id": 4,
      "start_timestamp": "0:03:47",
      "end_timestamp": "0:04:34"
    }
  },
  {
    "page_content": "dụ F x y, tức là F là một cái hàm vụ thuộc bởi 2 cái biến số x và y thì khi đó đạo hàm riêng của f theo cái biến x thì chúng ta sẽ xem y như là hàng số như vậy là nguyên cái vế này nó là hàng số mà đạo hàm của hàng số thì nó bằng 0 đúng không? và 2x đạo hàm của 2x bình phương thì nó chính là bằng 4x như vậy thì cái kết quả của cái đạo hàm riêng này chính là 4x Tương tự như vậy thì chúng ta sẽ có đạo hàm của f theo a, thì chúng ta sẽ xem cái 2x bên này là hàng số và trừ 4 này là hàng số Như vậy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=YSi7G6hDhks",
      "filename": "YSi7G6hDhks",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.2: Ôn tập nền tảng giải tích (Part 2)",
      "chunk_id": 5,
      "start_timestamp": "0:04:27",
      "end_timestamp": "0:05:11"
    }
  },
  {
    "page_content": "này là hàng số và trừ 4 này là hàng số Như vậy thì đạo hàm của thằng này theo a chính là bằng trừ 3 Và đây là sẽ một số cái bài tập để cho chúng ta có thể thực hành tính toán Và ở đây chúng ta lưu ý là nó sẽ có một cái đạo hàm riêng bật 1 và đạo hàm riêng bật 2 thì bản chất đạo hàm riêng bật 2, chứ là chúng ta tính đạo hàm bật 1 2 lần thôi thì trong phạm vi bài tập này thì chúng ta không cần thiết phải làm bài này và chúng ta ôm lại trên đạo hàm riêng bật 1 là được rồi Tiếp theo, và cuối cùng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=YSi7G6hDhks",
      "filename": "YSi7G6hDhks",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.2: Ôn tập nền tảng giải tích (Part 2)",
      "chunk_id": 6,
      "start_timestamp": "0:05:07",
      "end_timestamp": "0:05:57"
    }
  },
  {
    "page_content": "riêng bật 1 là được rồi Tiếp theo, và cuối cùng trong khái niệm liên quan đến đạo hàm đó chính là radian khái niệm radian đối với 1 cái hàm số ở đây chúng ta lưu ý, đây là r n, tức là từ hàm này sẽ là ảnh xạ của vector r n về một giá trị số scalar là số thật, tức là ảnh xạ từ một vector n vần tử về một giá trị số thật, thì khi đó radian của f Radiant của F chính là một vector n-phần tử Nếu đầu vào có n-phần tử thì Radiant của nó sẽ là n-phần tử Về mặt ký hiệu là NapLA Nó là ký hiệu NapLA NapLA",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=YSi7G6hDhks",
      "filename": "YSi7G6hDhks",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.2: Ôn tập nền tảng giải tích (Part 2)",
      "chunk_id": 7,
      "start_timestamp": "0:05:51",
      "end_timestamp": "0:06:50"
    }
  },
  {
    "page_content": "Về mặt ký hiệu là NapLA Nó là ký hiệu NapLA NapLA của F theo x chính là bằng các đào hàm ri Là vector của các đào hàm ri Vì vậy chúng ta có thể gọi là nabla của f theo cái biến x, theo cái vector x thì đúng hơn. Học tì sẽ là bằng một vector trong nó từng vần tẩu sẽ là đạo hàm riêng của f theo x thành phần x1, đạo hàm riêng của f theo cái thành phần x2, cho đến đạo hàm riêng của f theo thành phần x thứ n. Như vậy thì đây sẽ là một vector. Như vậy đảng chất Raiden chỉ là một vector các cái đạo",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=YSi7G6hDhks",
      "filename": "YSi7G6hDhks",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.2: Ôn tập nền tảng giải tích (Part 2)",
      "chunk_id": 8,
      "start_timestamp": "0:06:39",
      "end_timestamp": "0:07:45"
    }
  },
  {
    "page_content": "đảng chất Raiden chỉ là một vector các cái đạo hàm riêng Rồi, thì ở đây chúng ta sẽ có một cái bài tập Nếu như Fxy có công thức đạo hàm như sau, thì khi đó nabla của Fx sẽ là vector, tại vì nó sẽ có phụ thuộc của 2 biến đồng vào, do đó vector này sẽ là vector 2 chiều. và ở đây nó sẽ là đạo hàm riêng của F theo biến x và đạo hàm riêng của biến F theo biến y thì lúc đó nó sẽ là bằng gì? nếu như ở đây chúng ta tính đạo hàm riêng của F theo x thì lúc này nó sẽ là bằng 5y Trừ 14x, cộng 3, đó là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=YSi7G6hDhks",
      "filename": "YSi7G6hDhks",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.2: Ôn tập nền tảng giải tích (Part 2)",
      "chunk_id": 9,
      "start_timestamp": "0:07:41",
      "end_timestamp": "0:08:44"
    }
  },
  {
    "page_content": "lúc này nó sẽ là bằng 5y Trừ 14x, cộng 3, đó là thành phần thứ nhất Đạ hàm của f theo y sẽ là bằng 5x Đây là hàng số, xem như hàng số đối với y Rồi, trừ 2y và trừ 6 Đây chính là radian của F Dưới đây sẽ làm bài tập để chúng ta có thể thực hành tính toán và ôm tập lại khái niệm radian Kết thúc nội dung ôm tập của ngày hôm nay, chúng ta sẽ tiến hành làm bài WIS và đặt những câu hỏi để cùng thảo luận",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=YSi7G6hDhks",
      "filename": "YSi7G6hDhks",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.4.2: Ôn tập nền tảng giải tích (Part 2)",
      "chunk_id": 10,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Tiếp theo, chúng ta sẽ cùng tìm hiểu về lịch sử hình thành và thành tựu của trí tuệ nhân tạo nói chung. Và học sâu deep learning nó riêng, thì thuộc ngữ về AI, trí tuệ nhân tạo hay còn gọi là tên tiếng Anh là artificial intelligence. cùng tìm hiểu về lịch sử hình thành và thành tựu của trí tuệ nhân tạo nói chung và học sâu Deep Learning nó riêng thì thuộc ngữ về AI, trí tuệ nhân tạo hay còn gọi là tên tiếng Anh là Artificial Intelligence không phải có trong vài năm ở đây đâu mà thật ra là ngay",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=zuBsXtdWlyQ",
      "filename": "zuBsXtdWlyQ",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.2: Lịch sử AI và thành tựu của Deep learning",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:35"
    }
  },
  {
    "page_content": "có trong vài năm ở đây đâu mà thật ra là ngay từ khi máy tính ra đời thì các nhà khoa học máy tính họ đã núng nấu cho mình một ước mơ đó là làm sao cho máy tính có khả năng thông minh như con người Và khái niệm này đã có từ những năm 1950. Sau đó thì các thành tự của AI thực ra tại thời điểm này cũng không có quá nhiều. Mãi cho đến khi vào những năm 1980, đó là các nhà khoa học họ mới thấy rằng là nếu mà chúng ta thiết kế các thuộc toán một cách tường minh, tức là có giải thuật rõ ràng thì nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=zuBsXtdWlyQ",
      "filename": "zuBsXtdWlyQ",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.2: Lịch sử AI và thành tựu của Deep learning",
      "chunk_id": 1,
      "start_timestamp": "0:00:32",
      "end_timestamp": "0:01:15"
    }
  },
  {
    "page_content": "tường minh, tức là có giải thuật rõ ràng thì nó sẽ bị mất đi tính linh động mà linh động là một cái thứ mà trí tuệ nhân tạo cần phải có tại vì chúng ta biết rằng là bộ đạo của chúng ta nó có khả năng linh động có khả năng học và phát triển theo thời gian do đó thì cái khái niệm Machine Learning, máy học đã có từ thời điểm này và ý tưởng của máy học đó chính là gì? là chương trình của chúng ta sẽ không phát triển một cách tự minh mà chúng ta sẽ được học từ dữ liệu Tức là nếu như chúng ta nạp cho",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=zuBsXtdWlyQ",
      "filename": "zuBsXtdWlyQ",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.2: Lịch sử AI và thành tựu của Deep learning",
      "chunk_id": 2,
      "start_timestamp": "0:01:10",
      "end_timestamp": "0:01:46"
    }
  },
  {
    "page_content": "học từ dữ liệu Tức là nếu như chúng ta nạp cho máy tính dữ liệu thì nó sẽ thông minh dần theo thời gian Còn nếu chúng ta ngưng không nạp cho nó dữ liệu thì nó sẽ cứ diễn nguyên như vậy hoài Thì cái khái niệm về Machine Learning nó đã có từ thời điểm này Sau đó thì đến những năm 2010 và đặc biệt là cục bốc năm 2012 Vào năm này thì mô hình học sâu đã có những bước phát triển nhẹ vọt cho kết quả phân loại hình ảnh tốt hơn so với các phương pháp máy học truyền thống và kỹ nguyên của Deep Learning",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=zuBsXtdWlyQ",
      "filename": "zuBsXtdWlyQ",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.2: Lịch sử AI và thành tựu của Deep learning",
      "chunk_id": 3,
      "start_timestamp": "0:01:41",
      "end_timestamp": "0:02:30"
    }
  },
  {
    "page_content": "học truyền thống và kỹ nguyên của Deep Learning có từng thời điểm này. Sau đây là một số thành tựu của Deep Learning có thể nói là nổi tiếng nhất và được cộng đồng biết nhiều nhất đó chính là thông qua các game máy tính có khả năng chiến thắng được các chuyên gia trong lĩnh vực Ví dụ như máy tính có thể thắng được các kỷ tướng cà vai máy tính có thể thắng được các game thủ trong các game như Star Rap, War Rap Rồi máy tính cũng có thể thay thế cho các chuyên gia trong lĩnh vực xử lý dữ liệu về",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=zuBsXtdWlyQ",
      "filename": "zuBsXtdWlyQ",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.2: Lịch sử AI và thành tựu của Deep learning",
      "chunk_id": 4,
      "start_timestamp": "0:02:23",
      "end_timestamp": "0:03:02"
    }
  },
  {
    "page_content": "các chuyên gia trong lĩnh vực xử lý dữ liệu về tài chính Rồi máy tính cũng có khả năng xử lý ảnh Ví dụ như gần đây chúng ta thấy là có các hệ thống xe tự lái thì xe tự lái này sẽ vận dụng những công nghệ liên quan đến xử lý hình ảnh. Rồi ứng dụng của Amazon Go, tức là chúng ta sẽ đến các cửa hàng tiện lợi và chúng ta chỉ cần lấy hàng đi và máy tính sẽ tự động trừ tiền của chúng ta dựa trên việc quan sát các hàng chúng ta lấy ra khỏi cửa hàng tiện lợi này. Ngoài ra thì chúng ta còn rất nhiều",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=zuBsXtdWlyQ",
      "filename": "zuBsXtdWlyQ",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.2: Lịch sử AI và thành tựu của Deep learning",
      "chunk_id": 5,
      "start_timestamp": "0:02:58",
      "end_timestamp": "0:03:40"
    }
  },
  {
    "page_content": "tiện lợi này. Ngoài ra thì chúng ta còn rất nhiều những ứng dụng liên quan đến xử lý hình ảnh khác. Rồi gần đây thì chúng ta thấy cái thành tượng trong xử lý ngôn ngữ tự nhiên và chúng ta thấy có rất nhiều những cái ứng dụng, ví dụ như là ChatGVT, Copilot Ví dụ Copilot là lập trình viên chúng ta có thể tạo, có thể sử dụng Copilot để mà lập trình một cách rất là nhanh chóng, chúng ta chỉ cần mô tả cho máy tính biết là chúng ta đang muốn viết những cái hàm cơ bản nào máy tính sẽ tự tạo cho chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=zuBsXtdWlyQ",
      "filename": "zuBsXtdWlyQ",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.2: Lịch sử AI và thành tựu của Deep learning",
      "chunk_id": 6,
      "start_timestamp": "0:03:34",
      "end_timestamp": "0:04:14"
    }
  },
  {
    "page_content": "cái hàm cơ bản nào máy tính sẽ tự tạo cho chúng ta những cái hàm cơ bản đó và thực ra chúng ta cũng có thể sửa lại để cho chương trình nó có thể hoàn thiện tốt hơn nhưng mà cơ bản sùa cốt mà máy tính tạo ra nó cũng đã chiếm đến 70-80% giúp chúng ta giảm được thời gian rất là đáng kể. Rồi máy tính, trí tệ nhân tạo đã có rất nhiều thành tự trong những lĩnh vực vấn dụng như là y sinh, chăm sóc sức khỏe về tài chính, tư mại điện tử hoặc là cho môi trường thông minh thì chắc hẳn chúng ta biết xe tự",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=zuBsXtdWlyQ",
      "filename": "zuBsXtdWlyQ",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.2: Lịch sử AI và thành tựu của Deep learning",
      "chunk_id": 7,
      "start_timestamp": "0:04:06",
      "end_timestamp": "0:04:49"
    }
  },
  {
    "page_content": "thông minh thì chắc hẳn chúng ta biết xe tự lái, rồi các ứng dụng trợ lý ảo, Cortana, rồi chúng ta cũng có thể điều khiển TV bằng giọt nói, có thể đăng nhập các thiết bị di động bằng gương mặt, thì đây là những ứng dụng của Deep Learning để làm cho môi trường tương tác của chúng ta ngày càng thông minh hơn. Về demo thì chúng ta có thể tham khảo với đường dẫn như sau, đó là github.com feature Copilot thì trong cái demo này thì chúng ta sẽ thấy máy tính sẽ tự động tạo ra các cái dòng code để hỗ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=zuBsXtdWlyQ",
      "filename": "zuBsXtdWlyQ",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.2: Lịch sử AI và thành tựu của Deep learning",
      "chunk_id": 8,
      "start_timestamp": "0:04:39",
      "end_timestamp": "0:05:29"
    }
  },
  {
    "page_content": "tính sẽ tự động tạo ra các cái dòng code để hỗ trợ cho các lập trình viên chỉ dựa trên những cái câu mô tả, ngắn gọn, hộp vào rồi gần đây thì chúng ta thấy là máy tính có khả năng tạo ra nội dung, ví dụ như một trong những cái ứng dụng thư mại nổi tiếng hiện nay liên quan đến việc tạo nội dung hình ảnh, đó chính là Các công ty, ví dụ như OpenAI, họ đã tạo ra mô đồ Deli2. Mô hình này có cho phép sáng tạo ra những hình ảnh dựa trên câu mô tả đầu vào. Trong lĩnh vực nghiên cứu, chúng ta sẽ có mô",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=zuBsXtdWlyQ",
      "filename": "zuBsXtdWlyQ",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.2: Lịch sử AI và thành tựu của Deep learning",
      "chunk_id": 9,
      "start_timestamp": "0:05:24",
      "end_timestamp": "0:06:14"
    }
  },
  {
    "page_content": "vào. Trong lĩnh vực nghiên cứu, chúng ta sẽ có mô hình stable diffusion, cũng tương tự như vậy. là một cái Open Source để cho chúng ta có thể tự triển khai một cái mô hình máy họp sâu để tạo hình ảnh Đây là một số cái ví dụ, ví dụ như chúng ta có một cái câu mô tả như thế này và máy tính sẽ tự động tạo ra với 3 cái mô hình như trên Với cùng một câu mô tả thì Mitsuni đã tạo ra tấm ảnh như thế này rồi Deli2, Deli2 tạo ra tấm ảnh như thế này và StableDiffusion thì điểm chung của cả 3 mô hình này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=zuBsXtdWlyQ",
      "filename": "zuBsXtdWlyQ",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.2: Lịch sử AI và thành tựu của Deep learning",
      "chunk_id": 10,
      "start_timestamp": "0:06:10",
      "end_timestamp": "0:06:30"
    }
  },
  {
    "page_content": "thì điểm chung của cả 3 mô hình này chúng ta thấy là nó đều rất là nghệ thuật phải không Và tất cả những cái demo, những cái ứng dụng đã đề cập ở trên thì đều sử dụng ở máy tính để học từ rất nhiều dữ liệu đó là dựa trên học sau",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=zuBsXtdWlyQ",
      "filename": "zuBsXtdWlyQ",
      "title": "[CS431 - Các kĩ thuật học sâu và ứng dụng] Video 1.2: Lịch sử AI và thành tựu của Deep learning",
      "chunk_id": 11,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Trong phần tiếp theo thì chúng ta sẽ cùng tìm hiểu về một trong những biến thể rất mạnh và hiệu quả đó chính là Bi-Directional ANN hay còn gọi là ANN 2 chiều. Chúng ta sẽ xem xét bài toán đơn giản trước, đó là bài toán Sendiment Analysis. Và ở trong mô hình RNN mà chúng ta đã được tìm hiểu trước đây, chúng ta sẽ ký hiệu bằng hệ thống các cái node như thế này. Mỗi cái node này sẽ có các cái theo thác xử lý. Và chúng ta lấy ví dụ như chúng ta đưa vào một câu comment, câu bình luận về một cái bộ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Cu7kGoRaE0",
      "filename": "_Cu7kGoRaE0",
      "title": "[CS431 - Chương 8] Part 2: Một số biến thể của RNN: Bidirectional RNN",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:44"
    }
  },
  {
    "page_content": "vào một câu comment, câu bình luận về một cái bộ phim là The movie was terribly exciting. Cứ khi đưa vào một từ, chúng ta sẽ tính toán các giá trị ẩn Dự nhiên ở đây là S1, S2, S3, cho đến S5 Và các giá trị ẩn này sẽ được tổng hợp thông tin lại Tổng hợp thông tin lại để tính ra cái node ở trên cổng Và đây là cái node output Và cái việc tổng hợp thông tin này sẽ được thực hiện là Element-Wise, Min hoặc là Max chúng ta sẽ tính toán trên cấp độ đó là từng phần tử giữa các giá trị S1, S2, S3 đó sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Cu7kGoRaE0",
      "filename": "_Cu7kGoRaE0",
      "title": "[CS431 - Chương 8] Part 2: Một số biến thể của RNN: Bidirectional RNN",
      "chunk_id": 1,
      "start_timestamp": "0:00:32",
      "end_timestamp": "0:01:30"
    }
  },
  {
    "page_content": "là từng phần tử giữa các giá trị S1, S2, S3 đó sẽ tính trên từng phần tử của cái vector này và sau khi tổ họp xong thì chúng ta sẽ đưa ra dự đoán đó là positive hay là negative hay là neutral Chúng ta sẽ biểu diễn nó dưới dạng một dạng thứ hai, đó là dạng vector. Khi đưa về dạng biểu diễn vector của Anen, nó sẽ giúp chúng ta hình dung được giá trị trạng thái ẩn. nó sẽ là một cái vector, thì nó sẽ mang tính chất đại diện nhiều hơn và dựa trên màu sắc của các cái vector này trong những slide-out,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Cu7kGoRaE0",
      "filename": "_Cu7kGoRaE0",
      "title": "[CS431 - Chương 8] Part 2: Một số biến thể của RNN: Bidirectional RNN",
      "chunk_id": 2,
      "start_timestamp": "0:01:22",
      "end_timestamp": "0:02:11"
    }
  },
  {
    "page_content": "sắc của các cái vector này trong những slide-out, nó sẽ giúp cho chúng ta thuận tiện trong việc hiểu cách thức hoạt động của mô hình hơn thay vì là chúng ta dùng cái node như thế này như vậy thì ở đây là ở dạng biểu diện vector và tất cả các cái vector mà ở trạng thái ẩn S1 cho nên S5 ở đây, chúng ta sẽ được thực hiện thao tác trung bình hoặc là thao tác max trên từng phần tử để ra một vector tổng hợp và vector này chúng ta sẽ thấy là nó có một xăng động và do nó đã tổng hợp thông tin của các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Cu7kGoRaE0",
      "filename": "_Cu7kGoRaE0",
      "title": "[CS431 - Chương 8] Part 2: Một số biến thể của RNN: Bidirectional RNN",
      "chunk_id": 3,
      "start_timestamp": "0:02:09",
      "end_timestamp": "0:02:54"
    }
  },
  {
    "page_content": "xăng động và do nó đã tổng hợp thông tin của các trạng thay ở trước đó Bây giờ chúng ta sẽ qua động cơ tại sao lại có kiến trúc mạng Bidirectional INN chúng ta sẽ để ý rằng là từ terribly ở đây từ terribly này khi kết hợp Với những thông tin ngữ khảnh tại thời điểm trước đó Đúng không? Là từ words, từ movie và từ the Các từ này không mang tính chất thể hiện trạng thái trạm xuất Các từ the movie, words đều là những từ trung tính Nhưng khi chúng ta mắc gặp tới từ terribly Thì nếu mà nguyên bản",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Cu7kGoRaE0",
      "filename": "_Cu7kGoRaE0",
      "title": "[CS431 - Chương 8] Part 2: Một số biến thể của RNN: Bidirectional RNN",
      "chunk_id": 4,
      "start_timestamp": "0:02:48",
      "end_timestamp": "0:03:39"
    }
  },
  {
    "page_content": "ta mắc gặp tới từ terribly Thì nếu mà nguyên bản của từ terrible Thì đây chính là một cái từ thể hiện tính chất tiêu cực Đó, nó thể hiện tính chất tiêu cực Và nhưng rõ ràng là cái từ ngay phía sau đó là cái từ Exciting Cái từ Exciting này mang tính chất là tích cực Thì khi đó cái từ Terripley này chỉ mang tính chất là thể hiện cái mức độ của việc Exciting thôi là cực kỳ là thú vị, cực kỳ là tích cực Như vậy thì khi chúng ta xử lý đến cái từ thứ S4 chúng ta chỉ nhận được các thông tin của quá",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Cu7kGoRaE0",
      "filename": "_Cu7kGoRaE0",
      "title": "[CS431 - Chương 8] Part 2: Một số biến thể của RNN: Bidirectional RNN",
      "chunk_id": 5,
      "start_timestamp": "0:03:36",
      "end_timestamp": "0:04:25"
    }
  },
  {
    "page_content": "S4 chúng ta chỉ nhận được các thông tin của quá khứ tức là những cái từ S1, S2, S3 truyền đến cho S4 mà chúng ta không được thấy cái từ ở phía sau đó chính là cái từ Exciting và phải nhờ có cái từ Exciting này thì nó mới giúp cho chúng ta hoàn thiện ý nghĩa của từ theoretically này hơn do đó chúng ta cần phải có ngữ cảnh của các từ bên tay phải nữa chứ không phải chỉ có những từ bên tay trái thì đó chính là động cơ của bidirectional AR net như vậy chúng ta sẽ phải có kiến trúc như thế nào đó để",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Cu7kGoRaE0",
      "filename": "_Cu7kGoRaE0",
      "title": "[CS431 - Chương 8] Part 2: Một số biến thể của RNN: Bidirectional RNN",
      "chunk_id": 6,
      "start_timestamp": "0:04:19",
      "end_timestamp": "0:05:01"
    }
  },
  {
    "page_content": "chúng ta sẽ phải có kiến trúc như thế nào đó để có thể duyệt được các câu của mình theo chiều ngược lại nữa thì như vậy nó mới hoàn thiện Và đó chính là ý tưởng của biến thể Bidirectional Ionate. Ý tưởng của đó nó có thể tóm gặp lại đó chính là tổng hợp thông tin ngựa cảnh từ cả hai phía. Và ở đây thì chúng ta sẽ thấy là cái màu sắc của chúng ta là màu xanh, đúng không? Là tương ứng với lại các cái vector trạng thái ẩn ở theo cái chiều là từ trái sang phải. sau đó khi đến được từ cuối cùng,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Cu7kGoRaE0",
      "filename": "_Cu7kGoRaE0",
      "title": "[CS431 - Chương 8] Part 2: Một số biến thể của RNN: Bidirectional RNN",
      "chunk_id": 7,
      "start_timestamp": "0:04:56",
      "end_timestamp": "0:05:44"
    }
  },
  {
    "page_content": "trái sang phải. sau đó khi đến được từ cuối cùng, chúng ta có thể thực hiện thao tác ngược lại và biểu diễn bằng vector bộ cam và từ Exciting này sẽ được duyệt ở đây sau đó sẽ lan truyền tổng hợp thông tin với từ Terribly thì tại đây, khi từ Exciting này mang nghĩa thích cực kết hợp với từ Terribly thì nó sẽ tạo ra cho chúng ta cái nghĩa tích cực. Rồi, và cứ như vậy chúng ta lan truyền đến đầu và tại cái vị trí số 1, tại cái time step t là bằng 1 này, thì chúng ta thấy cái vector 1 xanh, nó đã",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Cu7kGoRaE0",
      "filename": "_Cu7kGoRaE0",
      "title": "[CS431 - Chương 8] Part 2: Một số biến thể của RNN: Bidirectional RNN",
      "chunk_id": 8,
      "start_timestamp": "0:05:38",
      "end_timestamp": "0:06:31"
    }
  },
  {
    "page_content": "1 này, thì chúng ta thấy cái vector 1 xanh, nó đã được kết nối, nó đặt khoảnh khát, khoảnh khát thế này, rất là nối, với lại cái vector ẩn mà đã tổng hợp theo chiều từ phải sang trái. Và như vậy thì cái vector tại đây đã có đầy đủ thông tin hơn Đổi thông tin ngự cảnh từ bên tay phải và bên tay trái truyền về Và cứ như vậy chúng ta sẽ thực hiện cho tất cả những vector cho những time steps quả lại Và nếu xét về công thức biểu diễn cho kiến trúc mạng bidirectional này thì chúng ta sẽ có công thức",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Cu7kGoRaE0",
      "filename": "_Cu7kGoRaE0",
      "title": "[CS431 - Chương 8] Part 2: Một số biến thể của RNN: Bidirectional RNN",
      "chunk_id": 9,
      "start_timestamp": "0:06:25",
      "end_timestamp": "0:07:09"
    }
  },
  {
    "page_content": "bidirectional này thì chúng ta sẽ có công thức sau ở đây chúng ta có một lưu ý đó là với cái vector này là tại vị trí của terribly thì thông tin ngữ cảnh của từ terribly đã được tổng hợp toàn diện hơn từ cả hai phía thì đầu tiên, đó là tại một time step t chúng ta sẽ có cái s mà với mỗi cái mũi tên từ trái sang phải thì chúng ta sẽ có cái công thức là anand feedforward Anand Fit Forward của thông tin của quá khứ Và lưu ý là thông tin của quá khứ này cũng lấy theo chiều từ trái sang phải Sau đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Cu7kGoRaE0",
      "filename": "_Cu7kGoRaE0",
      "title": "[CS431 - Chương 8] Part 2: Một số biến thể của RNN: Bidirectional RNN",
      "chunk_id": 10,
      "start_timestamp": "0:07:04",
      "end_timestamp": "0:07:54"
    }
  },
  {
    "page_content": "này cũng lấy theo chiều từ trái sang phải Sau đó chúng ta kết hợp với thông tin của hiện tại Thì nó sẽ ra được trạng thái ẩn của vết, nó sẽ ra được trạng thái ẩn ST Tương tự như vậy, chúng ta sẽ có vector trải thay ẩn theo chiều từ phải sang trái, là st này Thì công thức cho nó sẽ là Anand Backward, backward tức là đi theo chiều ngược Thì chúng ta sẽ tổng hợp thông tin của st trừ 1 và st trừ 1 này là dấu mũi tên từ phải sang trái và tương tự như đây cũng là quá khứ nhưng mà lưu ý là quá khứ cho",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Cu7kGoRaE0",
      "filename": "_Cu7kGoRaE0",
      "title": "[CS431 - Chương 8] Part 2: Một số biến thể của RNN: Bidirectional RNN",
      "chunk_id": 11,
      "start_timestamp": "0:07:53",
      "end_timestamp": "0:08:40"
    }
  },
  {
    "page_content": "đây cũng là quá khứ nhưng mà lưu ý là quá khứ cho cái đường Backward kết hợp với thông tin hiện tại thì chúng ta sẽ có được cái ST theo chiều Backward và cuối cùng đó là chúng ta sẽ tổng hợp thông tin ST bằng cách chúng ta thực hiện cái phép conca nối thì cái dấu chấm phải này ở đây đó chính là phép nối nối hai cái vector ST theo chiều Forward và ST theo chiều backwards để tạo thành một vector trạng thái ẩn. Và dựa trên thông tin của trạng thái ẩn này, ST sẽ giúp chúng ta đưa ra gia trị dự đoán",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Cu7kGoRaE0",
      "filename": "_Cu7kGoRaE0",
      "title": "[CS431 - Chương 8] Part 2: Một số biến thể của RNN: Bidirectional RNN",
      "chunk_id": 12,
      "start_timestamp": "0:08:35",
      "end_timestamp": "0:09:20"
    }
  },
  {
    "page_content": "này, ST sẽ giúp chúng ta đưa ra gia trị dự đoán một cách thông tin có chứa đầy đủ tàn diện hơn từ hai phía. Và như vậy, chúng ta sẽ có một số nhận xét sau. Thứ nhất là bidirectional thì nó sẽ phù hợp đối với những bài toán mà chúng ta có khả năng tiếp cận được thông tin, tiếp cận được toàn bộ nội dung của dương liệu đồ vào. Tức là sao? Nếu như chúng ta có được input và chúng ta có thể đọc được hết nội dung của toàn bộ, chúng ta đọc được hết nội dung toàn bộ của cái X1, X2 cho đến Xt, chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Cu7kGoRaE0",
      "filename": "_Cu7kGoRaE0",
      "title": "[CS431 - Chương 8] Part 2: Một số biến thể của RNN: Bidirectional RNN",
      "chunk_id": 13,
      "start_timestamp": "0:09:16",
      "end_timestamp": "0:10:11"
    }
  },
  {
    "page_content": "dung toàn bộ của cái X1, X2 cho đến Xt, chúng ta đọc được hết này và mới đưa ra được dự đoán. Đó là những bài toán cho FET có khả năng sử dụng được B-Directional Và nếu như vậy thì rõ ràng language model là không phù hợp Tại vì language model chỉ cho FET nhìn từ trái sang phải và dự đoán từ tiếp theo thôi Do đó thì chúng ta không phù hợp sử dụng B-Directional cho language model Mô hình BERT là B-Directional Encoder Representation for Transformers Đây là một trong những mô hình, một biến thể của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Cu7kGoRaE0",
      "filename": "_Cu7kGoRaE0",
      "title": "[CS431 - Chương 8] Part 2: Một số biến thể của RNN: Bidirectional RNN",
      "chunk_id": 14,
      "start_timestamp": "0:10:04",
      "end_timestamp": "0:10:46"
    }
  },
  {
    "page_content": "Đây là một trong những mô hình, một biến thể của B-Directional, đã thể hiện ở trong cái chữ này Vô cùng hiệu quả, được sử dụng rất phổ biến hiện nay thì hiện nay ở Việt Nam chúng ta có một mô hình đó là phở bờt và cũng dựa trên mô hình của B-Directional này và từ nay về sau thì chúng ta sẽ có mẹo đó là bất cứ bài toán nào mà chúng ta được phép khả năng tiếp cận toàn bộ nội dung của dữ liệu đồ vào được phép tiếp cận toàn bộ nội dung của dữ liệu đồ vào ví dụ bài toán dịch máy, bài toán tống tắc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Cu7kGoRaE0",
      "filename": "_Cu7kGoRaE0",
      "title": "[CS431 - Chương 8] Part 2: Một số biến thể của RNN: Bidirectional RNN",
      "chunk_id": 15,
      "start_timestamp": "0:10:37",
      "end_timestamp": "0:10:46"
    }
  },
  {
    "page_content": "đồ vào ví dụ bài toán dịch máy, bài toán tống tắc văn bản Chúng ta được phép đọc hết nội dung của nầu vào này trước khi dịch, trước khi tống tắc Đó là những bài tán chúng ta có thể sử dụng bidirectional Bidirectional thì thông thường luôn chỉ có thể làm cho kết quả của mô bình tốt hơn mà thôi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Cu7kGoRaE0",
      "filename": "_Cu7kGoRaE0",
      "title": "[CS431 - Chương 8] Part 2: Một số biến thể của RNN: Bidirectional RNN",
      "chunk_id": 16,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Để giải quyết bánh đai này, chúng ta sẽ đi qua các thành phần của kiến trúc LSTM. Đầu tiên, đó là cái hàm FT. Mục tích của nó chính là quyết định xem cái gì cần nhớ hay cần giữ lại, Cái gì thì cần quên Với những thông tin của quá khứ Thì ở đây chúng ta sẽ có một trục xuyên suốt toàn bộ Có một cái trục để đi xuyên suốt Cái chuỗi ký tự của mình Cái chuỗi câu của mình Đó là trục C C là viết tác của chữ là contact Contact cell ở đây có cái cổng đầu tiên, nó gọi là ForgetGay và chúng ta để ý là cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Km_A2iRUds",
      "filename": "_Km_A2iRUds",
      "title": "[CS431 - Chương 8] Part 1_2: Một số biến thể của RNN: LSTM",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:10"
    }
  },
  {
    "page_content": "tiên, nó gọi là ForgetGay và chúng ta để ý là cái cổng forget gate này thì có cái hàm có sử dụng một cái hàm là hàm sigmoid thì trong cái hàm sigmoid thì cái miền giá trị của nó là từ 0 cho đến 1 thế thì với cái hàm sigmoid này nó sẽ giúp cho chúng ta điều hướng thông tin ví dụ nếu cái kết quả trả ra cho cái fst này nè tức là cái kết quả trả ra tại đây nè kết quả trả ra tại đây mà bằng 0 Nếu kết quả này mà bằng không, thì điều gì sẽ xảy ra? Cái giá trị không này nhân với lại contact cell là CT",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Km_A2iRUds",
      "filename": "_Km_A2iRUds",
      "title": "[CS431 - Chương 8] Part 1_2: Một số biến thể của RNN: LSTM",
      "chunk_id": 1,
      "start_timestamp": "0:01:02",
      "end_timestamp": "0:01:45"
    }
  },
  {
    "page_content": "giá trị không này nhân với lại contact cell là CT triệu một. Tức là nó đang thực hiện việc quên đi thông tin của quá khứ trong CT triệu một. Nếu giá trị này là bằng một, hoặc giá trị gần bằng một, thì nó sẽ giữ lại gần như toàn bộ thông tin của quá khứ. và nó truyền tới tiếp theo. Thì đó chính là ý đồ của forget gate. Tức là nó sẽ biết có nên nhớ hay quên thông tin của quá khứ hay không thông qua việc sử dụng hàm sích môi. Và để đưa ra quyết định là có quên hay không thì nó phải dựa vào thông",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Km_A2iRUds",
      "filename": "_Km_A2iRUds",
      "title": "[CS431 - Chương 8] Part 1_2: Một số biến thể của RNN: LSTM",
      "chunk_id": 2,
      "start_timestamp": "0:01:38",
      "end_timestamp": "0:02:38"
    }
  },
  {
    "page_content": "là có quên hay không thì nó phải dựa vào thông tin của trạng thái ẩn trước đó là st-1 và thông tin từ x hiện tại, đầu vào hiện tại là xt mô đuổi thứ 2 đó chính là cổng thông tin input gay đó gọi là input gay input gay này nó sẽ quyết định xem là thông tin xt này chúng ta có đưa nó vào bên trong đưa nó vào bên trong cái Context cell này hay không? Ở đây chúng ta thấy có một cái mũi tên tức là sau khi chúng ta tính cái này xong đúng không? chúng ta sẽ nhân với lại cái thông tin đi qua cái cổng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Km_A2iRUds",
      "filename": "_Km_A2iRUds",
      "title": "[CS431 - Chương 8] Part 1_2: Một số biến thể của RNN: LSTM",
      "chunk_id": 3,
      "start_timestamp": "0:02:32",
      "end_timestamp": "0:03:14"
    }
  },
  {
    "page_content": "ta sẽ nhân với lại cái thông tin đi qua cái cổng này rồi sau đó chúng ta sẽ cộng nó vào cái Context cell thì cái giá trị ở đây ở đây chúng ta sẽ sử dụng một cái hàm sigmoid và tương tự như vậy tương tự như vậy thì cái sigmoid này nó sẽ nhận cái giá trị là từ 0 cho đến 1 Nếu như hàm sigmoid trả ra giá trị gần bằng 0, tức là nó nói rằng là chúng ta sẽ không cần nạt thông tin của XT vào bên trong cổng này thì thực tế chúng ta thấy là có những từ không quá quan trọng trong một câu ví dụ như là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Km_A2iRUds",
      "filename": "_Km_A2iRUds",
      "title": "[CS431 - Chương 8] Part 1_2: Một số biến thể của RNN: LSTM",
      "chunk_id": 4,
      "start_timestamp": "0:03:10",
      "end_timestamp": "0:03:52"
    }
  },
  {
    "page_content": "không quá quan trọng trong một câu ví dụ như là những cái trong tiếng Anh, chúng ta sẽ có những cái mạo từ fun hoặc là những giới từ in, out, off, v.v. thì đó là những từ kém quan trọng do đó thì cái cổng này nó sẽ có xu hướng là lọc bỏ những cái thông tin không quan trọng để không đưa vào bên trong cái contact scale thì cái IT này chính là cái ký hiệu của cái chữ input Tiếp theo đó là cái cổng output cái cổng này thì chúng ta, chúc đợi chúng ta sẽ nói sau cái cổng output đây là ký hiệu bằng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Km_A2iRUds",
      "filename": "_Km_A2iRUds",
      "title": "[CS431 - Chương 8] Part 1_2: Một số biến thể của RNN: LSTM",
      "chunk_id": 5,
      "start_timestamp": "0:03:47",
      "end_timestamp": "0:04:40"
    }
  },
  {
    "page_content": "ta sẽ nói sau cái cổng output đây là ký hiệu bằng chữ O hơn nó sẽ quyết định xem là chúng ta có lấy thông tin từ CT này chúng ta có lấy thông tin từ... xin lỗi đến đây, đến thời điểm này thì nó đã tính ra CT rồi chúng ta có lấy thông tin của trục contact cell đi ra để thực hiện tính toán giá trị output này không? thì output này sẽ quyết định xem là có lấy hay không nếu qua hàm sigmoid này mà nó nhận giá trị là không hoặc là gồng bằng không thì khi không nhân với giá trị này, tức là nó đang khóa",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Km_A2iRUds",
      "filename": "_Km_A2iRUds",
      "title": "[CS431 - Chương 8] Part 1_2: Một số biến thể của RNN: LSTM",
      "chunk_id": 6,
      "start_timestamp": "0:04:31",
      "end_timestamp": "0:05:24"
    }
  },
  {
    "page_content": "không nhân với giá trị này, tức là nó đang khóa nó khóa cái thông tin này lại, không cho cái thông tin từ CT này đi ra cái ST còn nếu như giá trị này sắp xỉ lợ, nó tiến về 1 tức là nó sẽ cho phép lấy cái thông tin của CT đi ra để tính toán cho cái giá trị output và C-NHT, tức là cái thông tin sau khi chúng ta đã xào nấu, ổng học sâu thông tin sau khi chúng ta đã xào nấu giữa cái quá khứ là ST và cái ST trường 1, và cái thông tin đầu vào của mình đó là cái hiện tại. Chúng ta trộn 2 cái thông tin",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Km_A2iRUds",
      "filename": "_Km_A2iRUds",
      "title": "[CS431 - Chương 8] Part 1_2: Một số biến thể của RNN: LSTM",
      "chunk_id": 7,
      "start_timestamp": "0:05:18",
      "end_timestamp": "0:05:59"
    }
  },
  {
    "page_content": "đó là cái hiện tại. Chúng ta trộn 2 cái thông tin này lại với nhau để tạo ra một cái thông tin tổng hợp. Và cái thông tin tổng hợp này thì nó cứ tính thoái nhưng mà cái việc cái C ngã T này có đưa vào bên trong cái trục nó có đưa vào bên trong cái trục cần text cell hay không đó là phụ thuộc vào cái input gay này Cái việc là có đưa nó vào hay không đó là do hàng đoạt Còn cái hàm tăng này là nó sẽ tổng hợp thông tin của quá khứ và hiện tại Thì cái module này nó cũng tương tự như cái A nó cũng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Km_A2iRUds",
      "filename": "_Km_A2iRUds",
      "title": "[CS431 - Chương 8] Part 1_2: Một số biến thể của RNN: LSTM",
      "chunk_id": 8,
      "start_timestamp": "0:05:55",
      "end_timestamp": "0:06:33"
    }
  },
  {
    "page_content": "cái module này nó cũng tương tự như cái A nó cũng tương tự như cái ANN cell nó cũng tương tự như ANN cell phiên bản, tạo ra phiên bản đầu tiên của mình nhiệm vụ đó là để rút chích thông tin cần thiết của cell hiện tại để đưa vào Contacts Vector nhưng mà lưu ý là rút chích thông tin cần thiết thôi còn có đưa vào hay không, nó sẽ phụ thuộc vào input gate này Rồi, và cuối cùng là ở công thức này công thức tính CT này, tức là nó sẽ được cập nhật tại đây nó sẽ cập dạng, ct mặc dù nó vi là ở đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Km_A2iRUds",
      "filename": "_Km_A2iRUds",
      "title": "[CS431 - Chương 8] Part 1_2: Một số biến thể của RNN: LSTM",
      "chunk_id": 9,
      "start_timestamp": "0:06:29",
      "end_timestamp": "0:07:12"
    }
  },
  {
    "page_content": "tại đây nó sẽ cập dạng, ct mặc dù nó vi là ở đây nhưng mà chúng ta phải hiểu là cái nguồn thông tin đó là nó đã được thay đổi tại cái vị trí này thì ở đây là chúng ta dùng cái tán tử cộng tán tử cộng nghĩa là gì? đây là cái thông tin tổng hợp đây là cái thông tin tổng hợp tại thời điểm hiện tại còn đây là cái thông tin của quá khứ nhưng mà lưu ý đó là cái quá khứ này á nó có chứa thông tin nhiều hay không thì nó nằm ở cái phần quyết định là do cái forget gate ví dụ đến đây forget gate là bằng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Km_A2iRUds",
      "filename": "_Km_A2iRUds",
      "title": "[CS431 - Chương 8] Part 1_2: Một số biến thể của RNN: LSTM",
      "chunk_id": 10,
      "start_timestamp": "0:07:08",
      "end_timestamp": "0:07:48"
    }
  },
  {
    "page_content": "cái forget gate ví dụ đến đây forget gate là bằng sắp xửng bằng 0 là số rất là bé tức là nó đã quên sạch thông tin rồi như vậy đến đây thì cái lượng thông tin đi tiếp nó gần như là không còn còn hiện tại cũng thương tượng như vậy do cái cổng input nó sẽ quyết định xem là cái hàm lượng thông tin của cái xt khi đưa vào cái contact cell này này khi đưa vào cái contact cell này là nhiều hay ít đó là do cái cổng này Còn ở đây là sự tổng hợp thông tin của qua phương và hiện tại Cuối cùng, đó là ST ST",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Km_A2iRUds",
      "filename": "_Km_A2iRUds",
      "title": "[CS431 - Chương 8] Part 1_2: Một số biến thể của RNN: LSTM",
      "chunk_id": 11,
      "start_timestamp": "0:07:43",
      "end_timestamp": "0:08:44"
    }
  },
  {
    "page_content": "của qua phương và hiện tại Cuối cùng, đó là ST ST thì ở đây chúng ta sẽ là hềm tanh của CT Ở đây đằng trước Sau đó thì chúng ta sẽ tính cái ST thôi Thì công thức này nó cũng rất là đơn giản Nó sẽ là bằng OT nhưng hềm tanh của CT Rồi, thì ở đây chúng ta sẽ có một cái nhầm lẫn trong công thức một chút xíu Ở đây là hàm tang này, là hàm tang của xe ngã T Hàm tang này là của xe ngã T À xin lỗi, đây là hàm tang của CT, đúng rồi Rồi, như vậy thì ở đây là cái thông tin CT nè Chuyển qua hàm tăng và đến",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Km_A2iRUds",
      "filename": "_Km_A2iRUds",
      "title": "[CS431 - Chương 8] Part 1_2: Một số biến thể của RNN: LSTM",
      "chunk_id": 12,
      "start_timestamp": "0:08:40",
      "end_timestamp": "0:09:38"
    }
  },
  {
    "page_content": "là cái thông tin CT nè Chuyển qua hàm tăng và đến đây Ct ở đây là thông tin contact cell Và việc quyết định xem có lấy thông tin của Ct này ra hay không Có lấy thông tin của Ct không Thì nó sẽ phụ thuộc vào giá trị OT là đến từ cổng output gate Output gate này sẽ quyết định xem có lấy hay không Sau khi đã có được ST này rồi, chúng ta sẽ thực hiện việc dự đoán và việc dự đoán này cũng tương tự, chúng ta sẽ thực hiện tương tự như cái ANN bình thường, tương tự như phiên bản ANN bình thường đó là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Km_A2iRUds",
      "filename": "_Km_A2iRUds",
      "title": "[CS431 - Chương 8] Part 1_2: Một số biến thể của RNN: LSTM",
      "chunk_id": 13,
      "start_timestamp": "0:09:26",
      "end_timestamp": "0:10:18"
    }
  },
  {
    "page_content": "tương tự như phiên bản ANN bình thường đó là có trạng thái ẩn, chúng ta sẽ nhân với vector V để qua hàm softmax để tính giá trị output Và như vậy thì chúng ta thấy với phiên bản của ANN và LSTM ANN thì chúng ta chỉ có duy nhất một cái cổng là tăng là để tổng hợp thông tin của XT và đưa vào bên trong tính toán giá trị ST tiếp theo Và nó tương ứng chính là cái module này, về mặt ý nghĩa đó là nó tương ứng với module này, nó tổng hợp thông tin của trạng thái hiện tại và phoá khứ. Tuy nhiên, trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Km_A2iRUds",
      "filename": "_Km_A2iRUds",
      "title": "[CS431 - Chương 8] Part 1_2: Một số biến thể của RNN: LSTM",
      "chunk_id": 14,
      "start_timestamp": "0:10:13",
      "end_timestamp": "0:10:53"
    }
  },
  {
    "page_content": "trạng thái hiện tại và phoá khứ. Tuy nhiên, trong phiên bản LSTM, nó có thêm 3 cổng khác và cộng thêm 1 mô đu nữa đó là contacts. Cái cổng này sẽ giúp cho chúng ta có nên quên thông tin của quá khứ hay không. Cổng này sẽ giúp cho chúng ta xác định xem có nên đưa thông tin của trạng thái hiện tại vào cái cổng CT, vào cái contact cell hay không. Và cái cổng này sẽ giúp chúng ta xác định xem là lượng thông tin chúng ta lấy ra từ CP, tức là compact cell này là nhiều hx Thì nó có thêm 3 cái cổng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Km_A2iRUds",
      "filename": "_Km_A2iRUds",
      "title": "[CS431 - Chương 8] Part 1_2: Một số biến thể của RNN: LSTM",
      "chunk_id": 15,
      "start_timestamp": "0:10:45",
      "end_timestamp": "0:10:55"
    }
  },
  {
    "page_content": "cell này là nhiều hx Thì nó có thêm 3 cái cổng này, và nhờ 3 cái cổng này sẽ giúp chúng ta điều hướng được thông tin để từ đó giúp cho radian của mình trong quá trình tính toán nó sẽ được trở nên hiệu quả hơn Và đó chính là việc mà LSTM có thể giúp chúng ta phần nào giải quyết được hiện tượng Phanixing Radiant, là hiện tượng tiêu biến Radiant.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_Km_A2iRUds",
      "filename": "_Km_A2iRUds",
      "title": "[CS431 - Chương 8] Part 1_2: Một số biến thể của RNN: LSTM",
      "chunk_id": 16,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề, học sâu, machine learning, imageNet. Chủ đề, học sâu, machine learning, imageNet. Đầu của Deep Learning trong những vật sự liên ngu ngự tự nhiên thì RNN là gần như là một trong những kiến trúc mà được sử dụng rất là phổ biến và gần như tất cả các bài báo đều xoay xung quanh các biến thể của mạng RNN này. Và nội dung chính của ngày hôm nay chúng ta sẽ cùng giới thiệu qua loại dữ liệu dạng chuỗi. thì đối với những loại dữ liệu dạng chuỗi, nó sẽ có những tính chất gì, nó có gì khác so với",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:02"
    }
  },
  {
    "page_content": "nó sẽ có những tính chất gì, nó có gì khác so với những loại dữ liệu khác. Và từ đó thì chúng ta sẽ biết rằng là liệu có thể sử dụng được các kiến trúc mạng trước đây, ví dụ như là Neural Network bình thường cho loại dữ liệu dạng chuỗi này hay không. Trong phần thứ hai thì chúng ta sẽ cùng tìm hiểu sâu hơn về kiến trúc mạng Recurrent Neural Network, xem các cấu phần của mạng Railcuren Neural Network là cái gì và cách thức tính toán như thế nào. Và cuối cùng, chúng ta sẽ cùng tìm hiểu về một số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 1,
      "start_timestamp": "0:00:58",
      "end_timestamp": "0:01:40"
    }
  },
  {
    "page_content": "Và cuối cùng, chúng ta sẽ cùng tìm hiểu về một số bánh đề của mạng ANN hiện nay đang gấp gập phải và các giải pháp để giúp chúng ta giải quyết những bánh đề đó như thế nào. Đầu tiên đó là loại dữ liệu dàn chuỗi. thì dữ liệu dạng chuỗi nó xuất hiện ở trong một số hình thức ví dụ như là loại dữ liệu quang bản, là dữ liệu âm thanh, hoặc là dữ liệu giá chứng khoá thế thì thế nào gọi là dữ liệu dạng chuỗi? Dữ liệu dạng chuỗi nó sẽ được mô hình hóa dữ dạng là XT, XT cộng 1, v.v. thì đầu ra giá trị",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 2,
      "start_timestamp": "0:01:37",
      "end_timestamp": "0:02:25"
    }
  },
  {
    "page_content": "dữ dạng là XT, XT cộng 1, v.v. thì đầu ra giá trị tiếp theo nó sẽ đi phụ thuộc vào giá trị ở phía trước thông thường trong các nội dung của mình, không phải các từ xt và xt cộng 1 nó độc lập nhau mà nó có sự phụ thuộc lẫn nhau cái từ thứ t cộng 1 nó sẽ có mối quan hệ phụ thuộc với lại từ thứ t Và trong tổng thuệ một câu hoặc là một đoạn âm thanh hoặc là giá trứng khoán v.v. thì tùy vào trình tự xuất hiện của các giá trị lầu vào mà mình sẽ có ý nghĩa khác nhau hoàn toàn Ví dụ, đối với văn bản",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 3,
      "start_timestamp": "0:02:19",
      "end_timestamp": "0:03:10"
    }
  },
  {
    "page_content": "nghĩa khác nhau hoàn toàn Ví dụ, đối với văn bản thì chúng ta hay có câu đó là bí dụ từ tiếng Anh đi là mình sẽ dễ minh họa nhất là Do you understand? Chắc là bạn có hiểu không? Thì cái từ do này nó đặt ở phía trước nên ở đây chính là cái câu hỏi Nhưng cũng ba cái từ này nếu như chúng ta đặt ở cái trình tự khác Ví dụ như là you do understand thì nó lại ra một câu khẳng định là bạn hiểu rồi đó còn ở trên đó là bạn có hiểu không? đó thì trình tự xuất hiện của các từ xt và xt cộng 1 rất là quan",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 4,
      "start_timestamp": "0:03:05",
      "end_timestamp": "0:04:01"
    }
  },
  {
    "page_content": "xuất hiện của các từ xt và xt cộng 1 rất là quan trọng do đó dữ liệu dạng chuỗi chúng ta cần phải chú ý đến cái ứng tố này đó là trình tự Và chúng ta sẽ so sánh một số loại dữ liệu với nhau để xem cái sự khác biệt của nó là gì. Đối với dữ liệu chuỗi và cụ thể ở đây, chúng ta sẽ lấy một ví dụ đó là dữ liệu văn bản. Ví dụ để minh họa cho dữ liệu này là một câu, một đoạn văn. Ví dụ như là bầu trời xanh và bãi biển ống ánh. thì dữ liệu hình ảnh chúng ta sẽ có ví dụ đó là 1 tấm hình như thế này rồi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 5,
      "start_timestamp": "0:03:52",
      "end_timestamp": "0:04:46"
    }
  },
  {
    "page_content": "ta sẽ có ví dụ đó là 1 tấm hình như thế này rồi đối với dữ liệu mà dạng đặc trưng ví dụ như các thuộc tính của 1 học sinh chúng ta có các thuộc tính ví dụ như là thuộc tính đầu tiên là lớp 7 thuộc tính thứ 2 15 tuổi thuộc tính thứ 3 là điểm toán thuộc tính thứ 4 là điểm văn Thực tính thứ 5, đó là định trung bình, ví dụ vậy Về các biểu diễn thông thường của loại dữ liệu dạng chuỗi đó chính là chúng ta sẽ sử dụng dạng danh sách các từ hay gọi là string trong lập trình của mình, gọi là string hoặc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 6,
      "start_timestamp": "0:04:41",
      "end_timestamp": "0:05:35"
    }
  },
  {
    "page_content": "trong lập trình của mình, gọi là string hoặc là mảng các ký tư Ờ? Rồi Trong loại dữ liệm là hình ảnh thì chúng ta sẽ có cách biểu diễn phổ biến đó chính là dữ liệm ma trận 2 chiều đối với những cái ảnh mà không có màu hay còn gọi là ảnh mức sáng, ảnh grayscale Và tensor 3 chiều đối với ảnh màu Và ảnh màu này có 3 canh màu thông thường là Red, Green, Blue, Đỏ, Xanh lá và Xanh dương. Còn để biểu diễn cho dữ liệu dưới dạng đặc trưng của một đối tượng, người ta thường hay sử dụng đó là biểu diễn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 7,
      "start_timestamp": "0:05:28",
      "end_timestamp": "0:06:21"
    }
  },
  {
    "page_content": "người ta thường hay sử dụng đó là biểu diễn dưới dạng vector. Rồi, và tiếp theo đó là về hệ thống ký hiệu. Đối với dĩa liệu dàn chuỗi, chúng ta hay ký hiệu đó là W1, W2, cho đến Wt trong đó t chính là số từ trong một câu hoặc là độ dài Và chúng ta có một lưu ý đó là độ dài của văn bản t này là có thể thay đổi, t có thể là rất ít, ví dụ như chỉ là bằng một nhưng nó cũng có thể rất là nhiều, ví dụ như có thể lên đến hàng ngày. Còn đối với dữ liệu hình ảnh, chúng ta sẽ ký hiệu nó dưới dạng là ma",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 8,
      "start_timestamp": "0:06:17",
      "end_timestamp": "0:06:59"
    }
  },
  {
    "page_content": "hình ảnh, chúng ta sẽ ký hiệu nó dưới dạng là ma trận. Ví dụ trong trường hợp này, chúng ta sử dụng ma trận 2 chiều. Còn đối với tensor 3 chiều thì nó sẽ phức tạp hơn một chút. Đối với ma trận 2 chiều, chúng ta sẽ có 2 thông số, đó là bề ngang và bề cao là để thể hiện kích thước của hình ảnh của mình. Bề ngang và bề cao, và chúng ta cũng lưu ý, đó là bề ngang và bề cao, bề rộng và bề dài, trong đây là dùng từ bề rộng và bề dài thì hoàn toàn có thể thay đổi được. Có thể thay đổi, thì chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 9,
      "start_timestamp": "0:06:45",
      "end_timestamp": "0:07:43"
    }
  },
  {
    "page_content": "thể thay đổi được. Có thể thay đổi, thì chúng ta thấy là các ảnh của mình có thể có những độ phân giải khác nhau. Có những ảnh rất là nhỏ nhưng mà có những ảnh rất là to. Còn khi biểu diễn cho đặc trưng của một đối tượng thì thông thường chúng ta sẽ phải biểu diễn dưới dạng là một vector với n phần tử và n này phải là cố định, n này sẽ là không thay đổi. và không thay đổi. Và tính chất của các phần tử trong dữ liệu này của mình đó là, đầu tiên đối với dữ liệu dạng chuỗi thì hồi nãy chúng ta đã",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 10,
      "start_timestamp": "0:07:27",
      "end_timestamp": "0:08:27"
    }
  },
  {
    "page_content": "với dữ liệu dạng chuỗi thì hồi nãy chúng ta đã có trình bày rồi là tính trình tự. Nó rất là quan trọng. Từ thứ hai, mà đứng sau từ thứ một thì nó sẽ có một ý nghĩa Nhưng từ thứ W2 mà đứng trước từ W1 thì nó lại có một ý nghĩa khác giống như ví dụ ở trên Do đó thì ở đây chúng ta sẽ có mối quan hệ đó là các phần tử trong dữ liệu phụ thuộc theo một chiều thời gian Tại sao ở đây mình lại dùng cái từ là thời gian? Tại vì ngôn gốc của ngôn ngữ nó xuất phát là từ giọng nói, từ tiếng nói. Thì khi tiếng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 11,
      "start_timestamp": "0:08:15",
      "end_timestamp": "0:08:53"
    }
  },
  {
    "page_content": "phát là từ giọng nói, từ tiếng nói. Thì khi tiếng nói của mình mà mình cất ra, thì nó đi theo cái chuỗi là cái chuỗi thời gian. Lúc mà nó đưa vô bên trong qua cái đường là thính giác, thì nó sẽ là đi theo cái chuỗi thời gian. thì đó là tại sao mình lại dùng cái từ đó là phụ thuộc theo chiều thời gian tương tự như vậy cho dữ liệu âm thanh và chiến khoán, thì cái T này hàm ý đó là thời gian Đối với dữ liệu hình ảnh thì cái sự phụ thuộc này là nó sẽ phụ thuộc ở hai chiều nó phụ thuộc ở cả hai",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 12,
      "start_timestamp": "0:08:49",
      "end_timestamp": "0:09:35"
    }
  },
  {
    "page_content": "nó sẽ phụ thuộc ở hai chiều nó phụ thuộc ở cả hai chiều và hai chiều này nó gọi là chiều không gian bề ngang, bề cao, nó gọi là chiều không gian Trong khi đó, dữ liệu đặc trưng thì các phần tử này đột lập nhau. Nghĩa là sao? Nếu như chúng ta quy ước là thành phần đầu tiên là lớp, thành phần thứ hai là tuổi, thành phần thứ ba là điểm toái, thành phần thứ tư là điểm văn, thành phần thứ năm là điểm trung bình. Vì nếu như chúng ta đổi lại trình tượng này, ví dụ chúng ta đưa điểm toán lên trước, sau",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 13,
      "start_timestamp": "0:09:27",
      "end_timestamp": "0:10:08"
    }
  },
  {
    "page_content": "này, ví dụ chúng ta đưa điểm toán lên trước, sau đó sẽ đến điểm văn, sau đó đến điểm trung bình, sau đó là lớp và sau đó là tuổi Thì cái thông tin của cái đặc trưng này nó vẫn bảo toàn, nó không hề thay đổi cái nội dung Nó không thay đổi cái nội dung của cái đặc trưng Trong khi đó cũng là cái từ do từ you từ understand nhưng nếu chúng ta đảo lại thứ tự cho nhau Do lên trước, do ra sau thì đó là khảo định Nhưng mà do lên trước, do ra sau thì đó là câu hỏi thì tự nhiên cái ý nghĩa của cái đoạn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 14,
      "start_timestamp": "0:10:07",
      "end_timestamp": "0:10:49"
    }
  },
  {
    "page_content": "là câu hỏi thì tự nhiên cái ý nghĩa của cái đoạn văn, của cái văn bạng đó là bị thay đổi hoàn toàn. Cũng tương tự như vậy cho hình ảnh. Nếu như chúng ta đưa các đối tượng hình ảnh này lên trên các vị trí khác nhau, dịu như đưa đám mây xuống dưới, đưa mặt trời lên trên, thì tự nhiên nó sẽ tạo ra một tấm hình có cái kích thước, xử lời nó có cái ý nghĩa khác nhau hoàn toàn. Do đó thì ở đây hai cái loại dữ liệu chuỗi và hình ảnh thì nó bị phụ thuộc lẫn nhau, nó bị rèn buộc lẫn nhau. Trong khi đó,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 15,
      "start_timestamp": "0:10:47",
      "end_timestamp": "0:11:31"
    }
  },
  {
    "page_content": "lẫn nhau, nó bị rèn buộc lẫn nhau. Trong khi đó, dữ liệu vector thì nó sẽ độc lập. Và ý tưởng để áp dụng cho loại dữ liệu văn bản lên trên các mô hình máy học, đúng không? Thì đó chính là chúng ta có những ý tưởng đầu tiên để kế thừa những thành tựu của mạng neural network trước đây. Thế thì, khó khăn đầu tiên mà chúng ta khi áp dụng dữ liệu dạng chuỗi vào một mạng Neuro Network là chúng ta có một nhận xét như sau. Văn bảng có độ dài không cố định. Ví dụ, đối với câu này, ở đây độ dài của văn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 16,
      "start_timestamp": "0:11:27",
      "end_timestamp": "0:12:16"
    }
  },
  {
    "page_content": "Ví dụ, đối với câu này, ở đây độ dài của văn bảng này là 2. Nhưng ở câu sau, bầu trời xanh và biển vàng ống ánh việu vậy, thì cái độ dài của mình có thể lên đến là 10 chữ trong khi đó, cái mạng Neuro Network của mình, cái đầu vào của mình nó lại cố định thì chúng ta đã học cái mạng Neuro Network rồi đầu vào của mình nếu như nó chỉ có 4 Neuron thì xuyên xuất từ cái quá trình huấn luyện cho đến quá trình dự đoán nó cũng hoàn toàn có thể là, nó sẽ giữ nguyên là 4 Neuron Có bạn sẽ hỏi là tại sao ở",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 17,
      "start_timestamp": "0:12:13",
      "end_timestamp": "0:12:50"
    }
  },
  {
    "page_content": "giữ nguyên là 4 Neuron Có bạn sẽ hỏi là tại sao ở trong cái mạng CNN, đúng không? thì các cái ảnh của mình khi chúng ta đưa vào một cái mạng CNN đưa vào một cái mạng CNN thì nó sẽ làm một cái thao tác đó là Scale mình sẽ đưa một cái ảnh rất to Scale về đúng cái tỷ lệ scale về đúng tỷ lệ mà cái mạng CNN này nó dận làm đầu vào, đúng không? thì đối với ảnh, nó lại là một loại dữ liệu đặc biệt các cái trình tượng, các cái điểm ảnh nó phụ thuộc theo trình tượng không gian, đúng không? nhưng mà một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 18,
      "start_timestamp": "0:12:46",
      "end_timestamp": "0:14:05"
    }
  },
  {
    "page_content": "trình tượng không gian, đúng không? nhưng mà một cái ảnh to, một cái ảnh to đó, ví dụ như chúng ta có một cái đối tượng ở đây khi chúng ta thu nhỏ nó lại để tạo ra thành một cái ảnh nhỏ Đáp ứng được cái yêu cầu đầu vào của cái mạng CNN thì về mặt ngũ nghĩa là chúng ta nhìn vô tâm ảnh này chúng ta vẫn biết được cái ý nghĩa của nó hay nói cách khác là ý nghĩa nó không thay đổi Còn ở đây, cái đoạn văn của mình mình sẽ không có cái cách nào để mà mình nén mình nén cái đoạn văn này về cái dạng là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 19,
      "start_timestamp": "0:13:41",
      "end_timestamp": "0:14:51"
    }
  },
  {
    "page_content": "mình nén mình nén cái đoạn văn này về cái dạng là một cái vector 4 chiều cố định là số chiều Có bạn sẽ nói, tôi dùng giải pháp là backward có được hay không? Tức là mọi câu trong quan bản, hoặc mọi từ trong quan bản sẽ đưa về cái dạng là một cái vector cố định số chiều. Ví dụ cái từ tuyệt, nó tương ứng sẽ là 0100. Từ quá là 0010 Và khi chúng ta sử dụng mô hình Backup Work thì chúng ta sẽ trộn hai vector này lại với nhau để tạo thành một vector đó là 0,1,1,0 để ra vector biểu diễn cho từ quả và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 20,
      "start_timestamp": "0:14:49",
      "end_timestamp": "0:15:27"
    }
  },
  {
    "page_content": "là 0,1,1,0 để ra vector biểu diễn cho từ quả và số từ của vector biểu diễn này nó sẽ đều cố định là V tức là số phần tử trong tập Dixelory, trong tập tiểu điển tương tự như vậy cho câu bầu trời xanh và biển vàng ống ánh, ví dụ vậy thì nó cũng sẽ biểu diễn với một vector có số chiều là b tại như vậy thì nó cố định số chiều Với giải pháp này, nó sẽ bị một vấn đề là nó không đảm bảo được yếu tố về mặt trình tự Trình tự Tại sao? Tại vì cái câu Do you understand? Với cái câu là You do understand Sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 21,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "understand? Với cái câu là You do understand Sẽ có cùng cái vector biểu diễn Nó sẽ có cùng một cái vector biểu diễn Ví dụ như là Một, không, không, không, một, một, không, few, by cả hai từ này đều có cùng cách biểu diện. Thì như vậy là tính đảm bảo của mạng Neural Network là cho cái phần tính thứ tự của ban bản là không đảm bảo. Và đó chính là những cái rào cản để cho chúng ta không thể sử dụng cái mạng Neural Network một cách trực tiếp với cái loại diễn liệu là ban bản hoặc là cho các cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 22,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "cái loại diễn liệu là ban bản hoặc là cho các cái loại diễn liệu dạng chuỗi khác.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_KvZN8-SyvQ",
      "filename": "_KvZN8-SyvQ",
      "title": "[CS431 - Chương 7] Part 1: Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
      "chunk_id": 23,
      "start_timestamp": null,
      "end_timestamp": null
    }
  }
]