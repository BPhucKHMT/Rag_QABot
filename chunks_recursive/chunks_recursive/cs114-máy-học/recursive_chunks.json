[
  {
    "page_content": "Chủ đề, học sâu, Machine Learning, logistic regression, ImageNet. bước xong là bước số 4 Tiếp theo chúng ta sẽ thay đổi một số config để xem khi chúng ta thay đổi những siêu tham số của thuật toán gom cụm Thế thì ở đây khi chúng ta chọn ra số Cluster là 10 thì thực tế là vì chúng ta không biết trước cái nhãn của mình Mẫu dữ liệu nào là nhãn nào do đó cái việc mà chúng ta chọn 10 cụm thì khả năng cao là nó vẫn sẽ có những cái điểm mà nó đan xen giữa các đối tượng thuộc về các lớp khác nhau và để",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=-wnlmTs1xvc",
      "filename": "-wnlmTs1xvc",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 2)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:20"
    }
  },
  {
    "page_content": "các đối tượng thuộc về các lớp khác nhau và để có thể tăng độ đồng nhất của các cụm để tăng độ đồng nhất của các cụm thì chúng ta sẽ tăng số cụm lên Ví dụ như ở đây chúng ta đang chọn là k là 10 thì khi chúng ta tăng số cụm này lên gấp đôi Lên gấp đôi Thì chúng ta sẽ theo dõi xem thuật toán này của mình Các điểm của mình nó như thế nào Ở đây chúng ta sẽ cho chạy lại rồi rồi thì tăng số cụm lên thì chúng ta hy vọng rằng là các điểm ảnh trong một cụm thì nó sẽ giống nhau nhiều hơn Khi cụm càng bé",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=-wnlmTs1xvc",
      "filename": "-wnlmTs1xvc",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 2)",
      "chunk_id": 1,
      "start_timestamp": "0:01:15",
      "end_timestamp": "0:02:20"
    }
  },
  {
    "page_content": "thì nó sẽ giống nhau nhiều hơn Khi cụm càng bé thì khả năng nó sẽ co lại và ít có tình huống các chữ số khác nhau nhưng nó lại nằm trong một cụm Khi chúng ta chia nhỏ nó ra thì nó sẽ ra như thế này Với mỗi cụm chúng ta sẽ lấy mẫu và hiển thị tấm ảnh đó lên Cụm số 2 là con số 0, cụm số 3 là con số 1 Cụm số 7 là số 8 rất nhiều, cụm số 8 là toàn số 1, cụm số 9 là số 6 khi chia thành cụm, khả năng cao sẽ gom những điểm có cùng một chữ cái và chữ số vào chung một nhóm bây giờ chúng ta sẽ thử nghiệm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=-wnlmTs1xvc",
      "filename": "-wnlmTs1xvc",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 2)",
      "chunk_id": 2,
      "start_timestamp": "0:02:10",
      "end_timestamp": "0:03:40"
    }
  },
  {
    "page_content": "vào chung một nhóm bây giờ chúng ta sẽ thử nghiệm thêm một số hướng tiếp cận khác Ví dụ như hiện giờ chúng ta đang gom cụm trên cái không gian mà đã giảm số chiều rồi và dù sao thì việc mà chúng ta giảm số chiều này xuống thì nó cũng sẽ khiến cho mô hình của mình bị mất mát rất nhiều dữ liệu Do đó để mà có thể gom cụm chính xác hơn thì thay vì chúng ta gom cụm trên tập dữ liệu đã được giảm chiều thì chúng ta sẽ làm trên tập dữ liệu gốc thì tập dữ liệu gốc của mình nó sẽ nằm ở trong biến đó là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=-wnlmTs1xvc",
      "filename": "-wnlmTs1xvc",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 2)",
      "chunk_id": 3,
      "start_timestamp": "0:03:33",
      "end_timestamp": "0:04:31"
    }
  },
  {
    "page_content": "dữ liệu gốc của mình nó sẽ nằm ở trong biến đó là xImage do đó thì chúng ta sẽ truyền vào xImage ở đây rồi bây giờ chúng ta sẽ chạy thử thuật toán gom cụm và đương nhiên là vì trên dữ liệu gốc của mình có 784 chiều nó sẽ lớn hơn rất nhiều lần so với lại không gian 2 chiều của mình nên thuật toán này sẽ chạy chậm hơn Rồi, và nó tốn hết 20 giây để chạy chương trình này Sau khi chúng ta đã gom cụm trên số chiều gốc xong thì chúng ta sẽ tìm cách trực quan hóa Rồi Và ở đây chúng ta thấy là một số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=-wnlmTs1xvc",
      "filename": "-wnlmTs1xvc",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 2)",
      "chunk_id": 4,
      "start_timestamp": "0:04:25",
      "end_timestamp": "0:05:28"
    }
  },
  {
    "page_content": "quan hóa Rồi Và ở đây chúng ta thấy là một số cụm thì nó đã gom các cái điểm mà có cùng một cái mẫu dữ liệu là nó sẽ nằm về cùng một cái màu Ví dụ như ở đây chúng ta thấy là các cái cụm này là cùng một cái màu Nhưng mà một số cái cụm khác thì nó sẽ bị rải ra và nó đan xen lẫn nhau Thế thì bây giờ chúng ta sẽ cùng xem xem Rồi, thì khi chúng ta làm trên cái dữ liệu gốc thì chúng ta thấy là cụm số 0 là toàn số 4 là toàn số 4 rồi cụm số 1 là toàn số 1 cụm tiếp theo đó là toàn số 1 cụm tiếp theo nữa",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=-wnlmTs1xvc",
      "filename": "-wnlmTs1xvc",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 2)",
      "chunk_id": 5,
      "start_timestamp": "0:05:22",
      "end_timestamp": "0:06:14"
    }
  },
  {
    "page_content": "1 cụm tiếp theo đó là toàn số 1 cụm tiếp theo nữa là toàn số 1 và có một cái số thì chúng ta thấy cái số này mặc dù chúng ta biết đó là có thể là số 9 nhưng mà nó cũng khá là ốm, tức là nó cũng khá là giống cái con số 1 của mình rồi cụm tiếp, cụm số 4 này nè, thì là gồm toàn số 0 còn cụm số 3 thì có số 3 và số 5 là nó sẽ đan xen nhau rồi cụm số 5 là toàn số 7 rồi cụm số 6 thì gồm toàn số 4, nhưng mà đâu đó cái số này thì mình mình nhìn nó vẫn có thể hiểu là tại sao nó gom vô cụm số 4 là vì cách",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=-wnlmTs1xvc",
      "filename": "-wnlmTs1xvc",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 2)",
      "chunk_id": 6,
      "start_timestamp": "0:06:07",
      "end_timestamp": "0:07:03"
    }
  },
  {
    "page_content": "thể hiểu là tại sao nó gom vô cụm số 4 là vì cách viết của nó cũng khá giống toàn số 2, toàn số 6 Rõ ràng chúng ta thấy là khi chúng ta sử dụng thuật toán gom cụm trên cùng một cái trên dữ liệu gốc, thì độ chính xác của nó cao hơn là vì nó không bị mất mát thông tin Hãy subscribe cho kênh Ghiền Mì Gõ để không bỏ lỡ những video hấp dẫn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=-wnlmTs1xvc",
      "filename": "-wnlmTs1xvc",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 2)",
      "chunk_id": 7,
      "start_timestamp": "0:06:57",
      "end_timestamp": "0:07:03"
    }
  },
  {
    "page_content": "Cuối cùng là chúng ta sẽ cùng thử nghiệm sử dụng một cái phương pháp trực quan hóa nâng cao hơn so với lại cái phương pháp mà sử dụng PCA đó là chúng ta sử dụng phương pháp t-SNE. Thì ở đây chúng ta sẽ tạo ra à một prompt một cái prompt đó là sử dụng à t-SNE để trực quan hóa các cái dữ liệu đã được phân cụm. Rồi đầu tiên đó là nó sẽ giảm t-SNE từ 7 84 chiều xuống còn hai chiều. Đó thì đây là cái code của mình và mình sẽ lấy mẫu ha. Tức là vì cái thuật toán này nó khá là phức tạp tính cái chi phí",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=3RrUezdwQ-s",
      "filename": "3RrUezdwQ-s",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 3)",
      "chunk_id": 0,
      "start_timestamp": "0:00:15",
      "end_timestamp": "0:01:33"
    }
  },
  {
    "page_content": "toán này nó khá là phức tạp tính cái chi phí tính toán lớn. Do đó mình chỉ lấy trên một cái tập hợp con là khoảng 5000 mẫu thôi. Đó thì cái tập gốc của mình sẽ lấy là 5000 mẫu. Rồi cái cụm của mình thì cũng sẽ ờ lấy trong khoảng 5000 mẫu này. Thì cái cụm này chính là những cái cụm mà chúng ta đã gom trong cái lần thử trước đây trên cái tập dữ liệu mà full 784 chiều chứ không phải là thử nghiệm trên hai chiều. Rồi sau đó khai báo khởi tạo là t-SNE. Đó. Rồi sau đó chúng ta sẽ fit transform cái X",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=3RrUezdwQ-s",
      "filename": "3RrUezdwQ-s",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 3)",
      "chunk_id": 1,
      "start_timestamp": "0:01:29",
      "end_timestamp": "0:02:19"
    }
  },
  {
    "page_content": "Đó. Rồi sau đó chúng ta sẽ fit transform cái X subset này vào t-SNE thì nó sẽ ra là cái X t-SNE tức là đã giảm số chiều với thuật toán t-SNE. Rồi sau đó chúng ta sẽ ờ vẽ cái biểu đồ này ra. Đó thì chúng ta thấy là những cái điểm nào mà nó cùng một cái màu là nó sẽ cùng một cái cụm thì nó sẽ có cùng một cái màu. Đó color nè là nó sẽ là cái cụm của subset đó. thì nó sẽ là cùng một cái màu. Thế thì à rõ ràng chúng ta thấy về mặt không gian những cái điểm này à những cái điểm này là những cái điểm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=3RrUezdwQ-s",
      "filename": "3RrUezdwQ-s",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 3)",
      "chunk_id": 2,
      "start_timestamp": "0:02:14",
      "end_timestamp": "0:02:59"
    }
  },
  {
    "page_content": "điểm này à những cái điểm này là những cái điểm mà nó được tô cùng một cái màu thì rõ ràng là nó nằm ở gần nhau và đồng thời đó là nó sẽ cùng một cái cụm ở trong cái không gian gốc. Khi chúng ta chiếu lên trên cái không gian hai chiều thì nó vẫn còn giữ cái tính chất đó là những cái điểm mà cùng một cụm thì nó sẽ nằm gần nhau. Đâu đó thì chúng ta thấy nó vẫn sẽ có những cái điểm mà nó bị lạc. Ví dụ như cái điểm màu tím ở đây, cái điểm nhỏ nhỏ ở đây thì nó là lẫn từ các điểm ở phía trên chiếu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=3RrUezdwQ-s",
      "filename": "3RrUezdwQ-s",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 3)",
      "chunk_id": 3,
      "start_timestamp": "0:02:55",
      "end_timestamp": "0:03:33"
    }
  },
  {
    "page_content": "ở đây thì nó là lẫn từ các điểm ở phía trên chiếu xuống. Thì rõ ràng là cái việc mà chúng ta giảm chiều dữ liệu từ không gian rất là lớn là 784 chiều xuống không gian hai chiều thì cũng không thể nào mà đảm bảo tất cả các cái điểm của mình nó đều phải ờ đi vào đúng cái cụm của mình được đúng không? Nhưng mà nhìn chung là chúng ta thấy nó đã được gom những cái điểm xấp xỉ nhau về cùng một cái khu vực. Thì đâu đó chúng ta thấy là trong cái hình này thì có những cái khu vực là cái điểm của mình nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=3RrUezdwQ-s",
      "filename": "3RrUezdwQ-s",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 3)",
      "chunk_id": 4,
      "start_timestamp": "0:03:26",
      "end_timestamp": "0:04:08"
    }
  },
  {
    "page_content": "thì có những cái khu vực là cái điểm của mình nó tách ra làm hai thì có thể là hai cái giá trị ở hai cái khu vực này là hai cái giá trị khác nhau. Rồi như vậy thì trong phần này chúng ta đã tiến hành à thử nghiệm với lại thuật toán k-means à gom nhóm và trực quan hóa trên cái không gian ít chiều hơn đó là à thuật toán t-SNE cũng như là PCA. Thì hy vọng rằng là trong những cái phần tiếp theo thì chúng ta có thể khai thác được hai cái mô hình học không có giám sát này một cách hiệu quả trong cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=3RrUezdwQ-s",
      "filename": "3RrUezdwQ-s",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 3)",
      "chunk_id": 5,
      "start_timestamp": "0:04:00",
      "end_timestamp": "0:04:38"
    }
  },
  {
    "page_content": "không có giám sát này một cách hiệu quả trong cái việc đó là gom nhóm dữ liệu. Đặc biệt đó là trong lĩnh vực về à xử lý dữ liệu lớn. cái việc mà dữ liệu gán nhãn là rất là chi phí tốn kém, rất là cao. Do đó, cái việc gom nhóm dữ liệu này nó sẽ giúp cho chúng ta xác định được cái phân bố của dữ liệu và đưa những cái dữ liệu mà có cùng tính chất về cùng một cái cách mà chúng ta sẽ xử lý với cái dữ liệu của cùng một nhóm. Như vậy rồi cái việc trực quan hóa dữ liệu sẽ giúp cho chúng ta thấy được là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=3RrUezdwQ-s",
      "filename": "3RrUezdwQ-s",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 3)",
      "chunk_id": 6,
      "start_timestamp": "0:04:33",
      "end_timestamp": "0:05:09"
    }
  },
  {
    "page_content": "hóa dữ liệu sẽ giúp cho chúng ta thấy được là dữ liệu của mình nó có những cái tính chất giống nhau như thế nào, nó gần nhau như thế nào trong cái không gian đã giảm chiều.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=3RrUezdwQ-s",
      "filename": "3RrUezdwQ-s",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 3)",
      "chunk_id": 7,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề, học sâu, machine learning, logistic regression, ImageNet. thì hạn chế của một cái mạng perceptron đơn lẻ, tức là chỉ có một perceptron đó là nó chỉ có thể tạo ra được một cái ranh giới quyết định thẳng hoặc là tuyến tính hay nói cách khác đó là bản chất của cái perceptron đơn lẻ này nó chính là một cái logistic, là một cái logistic regression là một mô hình hồi quy luận lý do đó nó chỉ có thể giải quyết được các bài toán tuyến tính chỉ tách được các bài toán phân tách chỉ giải quyết được",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:11"
    }
  },
  {
    "page_content": "được các bài toán phân tách chỉ giải quyết được các bài toán phân tách tuyến tính tuy nhiên nó sẽ thất bại trong bài toán phức tạp hơn ví dụ như ở đây chúng ta có một bài toán đó là XOR đầu vào của mình sẽ là các giá trị là X đầu vào của mình sẽ là các giá trị là X1 và X2 output X1 XOR X2 là XOR XOR là nếu hai giá trị khác nhau thì X1 XOR X2 là 1 nếu hai giá trị X1, X2 giống nhau thì X1 XOR X2 là 0 class 1 là những điểm màu xanh class 0 là những điểm màu đỏ thì chúng ta thấy là không thể nào",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 1,
      "start_timestamp": "0:01:09",
      "end_timestamp": "0:02:03"
    }
  },
  {
    "page_content": "điểm màu đỏ thì chúng ta thấy là không thể nào chúng ta có thể chia tách tập màu đỏ và màu xanh ra làm hai phần được không thể tách ra bằng hai phần mà với một đường thẳng dù như thế này cũng không tách được như thế này cũng không tách được mà chúng ta chỉ có thể tách được bằng một dạng đường như thế này phía bên này là class phía bên này là class còn bên trong khu vực này là class 1 do đó giải pháp là chúng ta sẽ kết hợp nhiều cái neuron lại, nhiều cái perceptron này lại và tổ chức thành các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 2,
      "start_timestamp": "0:01:56",
      "end_timestamp": "0:03:05"
    }
  },
  {
    "page_content": "nhiều cái perceptron này lại và tổ chức thành các lớp, một cái lớp đầu vào, một hoặc là nhiều cái lớp ẩn, một hoặc nhiều tối thiểu là bằng một thì nó mới được gọi là Multi-Layer Perceptron, một cái lớp đầu ra Và đây chính là kiến trúc đa lớp, hay gọi là MLP Multi-Layer Perceptron để tạo ra MLP này có khả năng học được ranh giới quyết định dạng phi tuyến tính là ranh giới quyết định dạng phi tuyến tính thì đây là ranh giới quyết định và ranh giới này thì sẽ bao gồm hai đường này về phía bên đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 3,
      "start_timestamp": "0:03:00",
      "end_timestamp": "0:03:43"
    }
  },
  {
    "page_content": "này thì sẽ bao gồm hai đường này về phía bên đây là một cái ranh giới, về phía bên này là một ranh giới, còn phần ở giữa sẽ là một ranh giới thì rõ ràng là đây là một cách phân chia theo kiểu là phi tuyến tính do đó nó có thể giải quyết được các bài toán mà một perceptron đơn lẻ không thể làm được ở đây chúng ta sẽ có một vấn đề cần phải bàn, đó chính là hàm kích hoạt hàm kích hoạt sẽ quyết định tín hiệu đầu ra của một cái neuron chúng ta biết là một cái neuron sẽ nhận tín hiệu đầu vào là bao",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 4,
      "start_timestamp": "0:03:37",
      "end_timestamp": "0:04:36"
    }
  },
  {
    "page_content": "là một cái neuron sẽ nhận tín hiệu đầu vào là bao gồm bias, X1, X2, cho đến Xn rồi sau đó nó sẽ truyền tín hiệu đầu ra để truyền tín hiệu đầu ra thì chúng ta sẽ có một cái mô đun kích hoạt ở đây Phần đầu là tuyến tính rồi, nhân và cộng Phần đầu sẽ là tổng trọng số của tín hiệu đầu vào với trọng số của các cạnh nối này Đó là một hàm tuyến tính Và để quyết định xem tín hiệu đầu ra nó như thế nào thì chúng ta sẽ cần một hàm kích hoạt Và hàm này thì phải là một hàm phi tuyến Đây là một hàm có vai",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 5,
      "start_timestamp": "0:04:29",
      "end_timestamp": "0:05:11"
    }
  },
  {
    "page_content": "phải là một hàm phi tuyến Đây là một hàm có vai trò rất quan trọng hay nói cách khác là quan trọng nhất tại vì nếu như không có hàm kích hoạt phi tuyến tính này thì bản chất của mô hình của mình nó cũng chỉ là một cái mạng tuyến tính mà một mạng tuyến tính thì không thể giải quyết được các bài toán phức tạp tại sao chúng ta lại phải cần một cái hàm kích hoạt tốt quan trọng như thế này thì chúng ta sẽ có cái lý do nếu không có cái hàm kích hoạt phi tuyến thì mỗi lớp của một cái mạng thực hiện",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 6,
      "start_timestamp": "0:05:05",
      "end_timestamp": "0:05:51"
    }
  },
  {
    "page_content": "phi tuyến thì mỗi lớp của một cái mạng thực hiện một cái phép biến đổi tuyến tính tức là mặc dù chúng ta thấy số lượng neuron rất là nhiều chúng ta thấy là để chúng ta vẽ cho tiết kiệm thời gian thì chúng ta sẽ vẽ cái dấu x như thế này, tức là hàm ý kết nối đầy đủ mặc dù số node đầu ra có thể là hàng chục hàng trăm neuron nhưng nếu không có hàm kích hoạt thì suy cho cùng ở đây nó cũng chỉ là một phép biến đổi tuyến tính thì giả sử như ở layer số 1 này là Z ký hiệu là số 1 còn đầu vào của mình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 7,
      "start_timestamp": "0:05:48",
      "end_timestamp": "0:07:13"
    }
  },
  {
    "page_content": "1 này là Z ký hiệu là số 1 còn đầu vào của mình sẽ là Z0 thì Z1 nếu như không có hàm kích hoạt thì bản chất nó chỉ là bản chất chỉ là Z1 là bằng W1 nhân với lại Z0 rồi giả sử sau đó chúng ta lại tiếp tục có một cái mạng thứ hai xin lỗi một cái lớp biến đổi thứ hai thì Z2 Z2 bằng W2 nữa, tức là các trọng số ở layer này W2 nhân cho Z1 Nhân với tín hiệu từ lớp số 1 truyền vào Z1 lấy từ công thức này W1 W2 Nên thử tích của W và dùng W phẩy nào đó thôi rồi nhân với lại Z Z0 như vậy thì chúng ta thực",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 8,
      "start_timestamp": "0:07:10",
      "end_timestamp": "0:08:25"
    }
  },
  {
    "page_content": "rồi nhân với lại Z Z0 như vậy thì chúng ta thực hiện biến đổi rất là nhiều biến đổi rất là nhiều nhưng cuối cùng thì cái Z2 tức là cái output sau hai lớp biến đổi bản chất của nó cũng chỉ là một cái phép biến đổi tuyến tính của cái lớp Z0 đầu vào này hay dùng cái thuật ngữ của bên đại số tuyến tính đó là Z2 chính là một tổ hợp tuyến tính của Z0 Cho dù ở giữa chúng ta có biến đổi một trăm lớp đi nữa mà không có hàm kích hoạt Thì cuối cùng Z2 cũng chỉ là một tổ hợp tuyến tính của Z0 Thì do đó hàm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 9,
      "start_timestamp": "0:08:17",
      "end_timestamp": "0:09:23"
    }
  },
  {
    "page_content": "chỉ là một tổ hợp tuyến tính của Z0 Thì do đó hàm kích hoạt này giúp cho chúng ta phi tuyến tính hóa Thì khi đó nếu chúng ta có hàm kích hoạt giả sử như hàm này ký hiệu là sigmoid này đi thì khi đó Z2 của chúng ta sẽ là bằng sigmoid của W2 nhân với đầu ra là Z1 và Z1 thì nó lại bằng sigmoid của W1 Z0 Nhờ hàm kích hoạt ở giữa này, nó sẽ không cho phép chúng ta lấy W2 nhân với W1 để tạo thành tổ hợp tuyến tính Vai trò của hàm kích hoạt là giúp chúng ta phi tuyến hóa cái output các lớp phía sau",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 10,
      "start_timestamp": "0:09:08",
      "end_timestamp": "0:10:06"
    }
  },
  {
    "page_content": "ta phi tuyến hóa cái output các lớp phía sau nhằm giúp chúng ta giải quyết được những bài toán phức tạp Rồi, kết luận hàm kích hoạt phi tuyến chính là chìa khóa của Deep Learning về sau các mô hình Deep Learning mà muốn giải quyết được bài toán siêu phức tạp thì buộc chúng ta sẽ phải sử dụng hàm kích hoạt phi tuyến tính và có rất nhiều loại hàm kích hoạt khác nhau thì có thể kể đến đó là hàm sigmoid là một trong những hàm cơ bản nhất Đặc điểm của nó đó là với đầu vào X thì X này có thể thuộc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 11,
      "start_timestamp": "0:10:03",
      "end_timestamp": "0:10:49"
    }
  },
  {
    "page_content": "của nó đó là với đầu vào X thì X này có thể thuộc miền R tức là từ âm vô cùng cho đến dương vô cùng nhưng mà qua hàm sigmoid này thì nó sẽ đưa về một giá trị nằm trong khoảng từ 0 cho đến 1 như vậy thì chúng ta gọi là nén mọi giá trị đầu vào và nó sẽ thu về khoảng từ 0 cho đến 1 và nó rất hữu ích trong việc biểu diễn xác suất tại vì chúng ta biết rồi xác suất của một biến cố đó là một con số từ 0 cho đến 1 do đó nó sẽ hữu ích trong việc biểu diễn yếu tố xác suất và dạng thức của hàm này sẽ có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 12,
      "start_timestamp": "0:10:45",
      "end_timestamp": "0:11:33"
    }
  },
  {
    "page_content": "yếu tố xác suất và dạng thức của hàm này sẽ có cái dạng như sau lưu ý ở đây là nó sẽ tiệm cận, không chạm về 0 nó sẽ cứ tiệm cận về 0 thôi chứ không chạm về 0 Còn ở đây sẽ là tiệm cận về 1, chứ nó không chạm đến 1 Nhưng mà cơ bản là khi giá trị Z đầu vào này từ 5 trở lên thì sigmoid của 5 là nó xấp xỉ 1, gần như chạm 1 Còn ở đây là sigmoid của trừ 5 Thì nó sẽ xấp xỉ là bằng 0 luôn Nó không bằng 0 nhưng mà nó sẽ gần như là chạm đến 0 Nó rất là mau bão hòa Với cái Z này là Cái đầu vào của mình là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 13,
      "start_timestamp": "0:11:31",
      "end_timestamp": "0:12:50"
    }
  },
  {
    "page_content": "bão hòa Với cái Z này là Cái đầu vào của mình là Chỉ khoảng từ trừ 5 tới 5 thôi nó đã bão hòa rồi Do đó thì cũng chính cái vấn đề này Nó sẽ gây ra cái hiện tượng gọi là Vanishing Gradient Tức là Tiêu biến cái đạo hàm Nếu như cái mô hình học sâu của mình mà có chứa quá nhiều cái hàm kích hoạt này Thì dẫn đến đó là mô hình sẽ mau bị bão hòa và bị tiêu biến đi đạo hàm tiêu biến đạo hàm này thì việc huấn luyện sẽ rất là chậm và thậm chí là không huấn luyện được do đó thì hàm sigmoid này nó đơn giản",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 14,
      "start_timestamp": "0:12:31",
      "end_timestamp": "0:13:26"
    }
  },
  {
    "page_content": "luyện được do đó thì hàm sigmoid này nó đơn giản về mặt công thức nhưng mà đây là được sử dụng trong những giai đoạn đầu sau đó, từ năm 2012 trở về sau, người ta sử dụng hàm Rectified Linear Unit Hàm Rectified Linear Unit sẽ có công thức như ở đây và dạng đồ thị hàm số sẽ như thế này Từ năm 2012 trở về sau, người ta hạn chế dùng gần như không sử dụng hàm sigmoid như hàm kích hoạt mà họ sử dụng hàm ReLU này thì đây là một sự lựa chọn phổ biến cho các lớp ẩn và nó sẽ giúp cho mình giảm thiểu đáng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 15,
      "start_timestamp": "0:13:23",
      "end_timestamp": "0:14:10"
    }
  },
  {
    "page_content": "các lớp ẩn và nó sẽ giúp cho mình giảm thiểu đáng kể hiện tượng Vanishing Gradient tức là hiện tượng tiêu biến đạo hàm tại vì với đầu vào của chúng ta Chúng ta chỉ làm từ trừ 5 trở đi hoặc từ trừ 5 trở về sau là tự nhiên đạo hàm của mình gần như xấp xỉ bằng 0 Tại vì nó gần như tiệm cận vô đường này Đạo hàm gần như bằng 0 Mà đạo hàm gần như bằng 0 thì quá trình cập nhật của mình sẽ rất chậm đối với thuật toán Gradient Descent Chốc nữa chúng ta sẽ bàn về giải thuật này để huấn luyện Tóm lại ReLU",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 16,
      "start_timestamp": "0:14:07",
      "end_timestamp": "0:14:56"
    }
  },
  {
    "page_content": "bàn về giải thuật này để huấn luyện Tóm lại ReLU sẽ giúp chúng ta giảm thiểu hiện tượng tiêu biến đạo hàm này Do đó tốc độ huấn luyện sẽ ổn định hơn Một loại hàm kích hoạt nữa đó là hàm Softmax Mục tiêu của hàm Softmax không phải là để tổng hợp thông tin Mục tiêu của nó là để nén phân bố xác suất cho nó có tổng là bằng 1 với đầu vào của mình là một vector ví dụ ở đây chúng ta có một vector qua hàm kích hoạt Softmax nó sẽ đưa về không gian xác suất không gian xác suất là từng phần tử này ví dụ ở",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 17,
      "start_timestamp": "0:14:51",
      "end_timestamp": "0:15:45"
    }
  },
  {
    "page_content": "không gian xác suất là từng phần tử này ví dụ ở đây chúng ta có Y ngã là cái vector này từng phần tử Y ngã này nó sẽ là lớn hơn 0 và bé hơn 1 trong đó lớn hơn 0 và bé hơn 1 trong đó tổng tất cả các Y ngã này thì đều là bằng 1 tổng các giá trị này cộng lại bằng 1 thì đây là một cái không gian xác suất và nó phục vụ thường là ở lớp cuối cùng để giúp chúng ta có thể dễ dàng đọc được kết quả của mình là sự lựa chọn bắt buộc cho các lớp output Bắt buộc cho lớp Output để cho bài toán phân loại đa lớp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 18,
      "start_timestamp": "0:15:38",
      "end_timestamp": "0:16:18"
    }
  },
  {
    "page_content": "cho lớp Output để cho bài toán phân loại đa lớp Và ưu điểm đó là cho kết quả ra dạng xác suất dễ diễn giải Ví dụ như với hình này chúng ta có thể nói là Output với dữ liệu đầu vào Ở đây là dữ liệu đầu vào trước khi qua hàm Softmax, đây là lớp cuối cùng Qua hàm Softmax sẽ chuẩn hóa output này thay vì các giá trị 1, 3, 5, 2, 0, 7, 1 không được chuẩn hóa Sau khi chuẩn hóa về xác suất xong, chúng ta sẽ đọc kết quả này và có thể kết luận theo ngôn ngữ của xác suất 90% là nó thuộc về lớp số 2 2% là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 19,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "của xác suất 90% là nó thuộc về lớp số 2 2% là thuộc về lớp số 1 5% là thuộc về lớp số 3 Vì việc đọc giá trị output này sẽ diễn đạt rất dễ hiểu Lớp có xác suất cao nhất chính là dự đoán của mình Như vậy mình kết luận mình kết luận đó là phân loại sẽ là cái class số 2",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=40kpU65MMoI",
      "filename": "40kpU65MMoI",
      "title": "[CS114 - Chương 8] Neural Network (Part 2)",
      "chunk_id": 20,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Như vậy thì chúng ta sẽ cùng tìm hiểu xem cái lỗi là do gì. Thì rõ ràng cái kích thước của cái nabla ở đây là không đúng. Như vậy thì chúng ta sẽ rã cái này ra thành từng bước của chúng ta sẽ tính cái sigmoid và xem coi cái kích thước của nó là bao nhiêu. Rồi tương tự như vậy chúng ta sẽ xem kích thước của Y_hat xem cái kích thước của nó là bao nhiêu. Còn cái nabla ở đây chúng ta sẽ tạm bỏ qua. Rồi thì cái sigmoid này nó sẽ có kích thước là một hàng và 110 cột. I sẽ là 110 à phần tử tức là một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5MgxCUTD9I4",
      "filename": "5MgxCUTD9I4",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 2)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:01:01"
    }
  },
  {
    "page_content": "hàng và 110 cột. I sẽ là 110 à phần tử tức là một cái vector. Vậy thì liệu hiệu của hai cái này nó có tính toán ra được đúng là một cái vector có 110 phần tử hay không? Thì chúng ta sẽ lấy cái này trừ cho y. Rồi chúng ta sẽ lấy cái này trừ cho y. Rồi tất cả là tính shape rồi để xem coi cái hiệu số này nó sẽ ra là như thế nào. Ok thì khi chúng ta trừ ra thì nó sẽ ra là một cái ma trận một hàng và 110 cột. Như vậy là nó đã đúng ý ta. 110 cái à dự đoán trừ cho 110 cái giá trị thực tế thì nó sẽ ra",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5MgxCUTD9I4",
      "filename": "5MgxCUTD9I4",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 2)",
      "chunk_id": 1,
      "start_timestamp": "0:00:53",
      "end_timestamp": "0:01:49"
    }
  },
  {
    "page_content": "đoán trừ cho 110 cái giá trị thực tế thì nó sẽ ra là 110 cái độ lỗi. Vậy thì chúng ta sẽ xem xem cái x của mình có kích thước là bao nhiêu. X của mình thì nó lại là có kích thước là 3 nhân cho 110. Thế thì ở đây chúng ta thấy là để mà chúng ta có thể tạo ra được cái nabla một cái vector có số chiều bằng với theta tức là 3 nhân 1 theta của chúng ta là 3 hàng và 1 thì ở đây chúng ta sẽ phải lấy cái x là 3 x 110 sau đó chúng ta đi nhân với lại 1 110 thì nó không nhân được do đó chúng ta sẽ chuyển",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5MgxCUTD9I4",
      "filename": "5MgxCUTD9I4",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 2)",
      "chunk_id": 2,
      "start_timestamp": "0:01:41",
      "end_timestamp": "0:02:29"
    }
  },
  {
    "page_content": "thì nó không nhân được do đó chúng ta sẽ chuyển vị cái này chuyển vị cái hiệu này chúng ta sẽ chuyển vị cái thành phần này. Như vậy thì chúng ta sẽ sửa lại cái code này là x à nhân thì chúng ta có thể là chấm dot hoặc là dùng cái ký hiệu đơn giản thôi đó là @. Rồi và ở đây sau khi chúng ta thực hiện cái phép trừ xong thì chúng ta sẽ thực hiện cái vector chuyển vị. Và khi chúng ta lấy cái x mà nhân với lại cái ma trận chuyển vị này thì nó sẽ ra một cái vector là 3 1. Do đó chúng ta sẽ không cần",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5MgxCUTD9I4",
      "filename": "5MgxCUTD9I4",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 2)",
      "chunk_id": 3,
      "start_timestamp": "0:02:23",
      "end_timestamp": "0:03:06"
    }
  },
  {
    "page_content": "cái vector là 3 1. Do đó chúng ta sẽ không cần phải tính mean nữa tại vì ở trong đây nó đã tổng hợp cho chúng ta các cái sai số luôn rồi. Có chăng thì ở đây chúng ta sẽ chia cho số phần tử của mình. Chia trung bình cho số phần tử của mình thì là len của y là số phần tử cho cái mẫu dữ liệu huấn luyện. Rồi bây giờ chúng ta sẽ cùng chạy thử xem cái code này nó thực thi như thế nào. Như vậy là về mặt cú pháp là nó đã thực đúng rồi. Có điều là chúng ta thấy nó sai à nó không có chia ra làm hai phần.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5MgxCUTD9I4",
      "filename": "5MgxCUTD9I4",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 2)",
      "chunk_id": 4,
      "start_timestamp": "0:03:00",
      "end_timestamp": "0:03:50"
    }
  },
  {
    "page_content": "thấy nó sai à nó không có chia ra làm hai phần. Vậy thì chúng ta sẽ thử xem à cái lỗi nó nằm ở đâu thì khả năng cao là nó sẽ nằm ở trong cái phần mà trực quan hóa để vẽ à các cái đường thẳng của mình. Thì đây là thành phần W1, đây là thành phần bias và đây là thành phần à W2 tức là W1 tức là W2 và 2 là thành phần cuối cùng à bias. Thì trong cái công thức này chúng ta sẽ thấy đây cái chỗ này là chúng ta đang copy y và chưa sửa nè. thì chỗ này lẽ ra nó phải là 2 tức là tương ứng là cái thứ 3 còn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5MgxCUTD9I4",
      "filename": "5MgxCUTD9I4",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 2)",
      "chunk_id": 5,
      "start_timestamp": "0:03:43",
      "end_timestamp": "0:04:36"
    }
  },
  {
    "page_content": "ra nó phải là 2 tức là tương ứng là cái thứ 3 còn nếu không thì chúng ta nên viết nó ra một cách tường minh để cho nó đỡ bị à bị nhầm khi mà chúng ta copy rồi thì cái thành phần W2 sẽ là bằng theta[1] và thành phần bias thì sẽ là bằng theta[2] và khi đó thì cái công thức của mình nó sẽ là W1 nhân với 0 cộng cho tất cả chia cho W2 rồi W1 nhân với 3.5 cộng cho bias chia cho W2. Rồi và bây giờ chúng ta sẽ cùng chạy lại cái đoạn code này. Rồi thì sau một lúc chúng ta sẽ thấy là cái đường thẳng của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5MgxCUTD9I4",
      "filename": "5MgxCUTD9I4",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 2)",
      "chunk_id": 6,
      "start_timestamp": "0:04:30",
      "end_timestamp": "0:05:42"
    }
  },
  {
    "page_content": "một lúc chúng ta sẽ thấy là cái đường thẳng của mình đã tiến vào đây và nó đã bắt đầu à có xu hướng là tách nó ra làm hai phần đó là vùng màu đỏ và màu xanh. Thì như vậy chúng ta thấy là cái code này nó sẽ có những cái điểm lợi gì so với lại những cái phiên bản à trước đây. Cái lợi thứ nhất đó là thay vì chúng ta tính đạo hàm cho từng thành phần thì bây giờ chúng ta sẽ tính hàng loạt. Chúng ta sẽ tính hàng loạt cho ờ nabla và nabla thì bao gồm các cái thành phần trọng số theo X và các cái đặc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5MgxCUTD9I4",
      "filename": "5MgxCUTD9I4",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 2)",
      "chunk_id": 7,
      "start_timestamp": "0:05:38",
      "end_timestamp": "0:06:15"
    }
  },
  {
    "page_content": "các cái thành phần trọng số theo X và các cái đặc trưng đầu vào cộng với lại cái thành phần bias. Nếu như không có dùng cái công thức dạng vector hóa này thì chúng ta sẽ thấy là cái cái việc chúng ta tính đạo hàm chúng ta có bao nhiêu tham số, chúng ta sẽ phải copy cái code này ra bấy nhiêu dòng khiến cho cái code của mình nó bị phình to ra. Rồi tương tự như vậy trong cái bước cập nhật chúng ta cũng sẽ phải có bao nhiêu tham số thì chúng ta sẽ cập nhật bấy nhiêu dòng nó sẽ khiến cho code của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5MgxCUTD9I4",
      "filename": "5MgxCUTD9I4",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 2)",
      "chunk_id": 8,
      "start_timestamp": "0:06:11",
      "end_timestamp": "0:06:45"
    }
  },
  {
    "page_content": "cập nhật bấy nhiêu dòng nó sẽ khiến cho code của mình nó bị rối. Và một cách tổng quát thì cái phương pháp này, cái phương pháp code theo dạng vector hóa này nó chỉ gom lại thành một dòng code để tính gradient và một dòng code thì để cập nhật cái tham số. Còn cái bước ở đây thì chúng ta vì lý do trực quan hóa nên chúng ta mới tách nó ra để cho nó dễ nhìn. Chứ còn cái code này thì thực ra cũng không cần thiết trong cái quá trình mà mô hình của chúng ta huấn luyện. Thì đó chính là những cái điểm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5MgxCUTD9I4",
      "filename": "5MgxCUTD9I4",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 2)",
      "chunk_id": 9,
      "start_timestamp": "0:06:41",
      "end_timestamp": "0:07:19"
    }
  },
  {
    "page_content": "ta huấn luyện. Thì đó chính là những cái điểm mạnh của à cái mô hình của mình. Có lẽ ở đây thì chúng ta có thể sửa lại một chút nữa đó là cái thành phần mà nabla thì ở đây là chúng ta có thể ở đây chúng ta cũng không cần phải đặt cái biến phụ ha và dùng chính cái cái nabla là cái gradient theo từng cái thành phần để kiểm tra cái điều kiện dừng. Và sau khi chúng ta chờ một lúc sau thì chúng ta đã thấy đó là cái đường phân lớp của chúng ta đã tách ra làm hai và vì cái trường hợp này là cái tình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5MgxCUTD9I4",
      "filename": "5MgxCUTD9I4",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 2)",
      "chunk_id": 10,
      "start_timestamp": "0:07:13",
      "end_timestamp": "0:07:50"
    }
  },
  {
    "page_content": "ra làm hai và vì cái trường hợp này là cái tình huống mà số tập màu đỏ nó nhiều hơn gấp 10 lần so với tập màu xanh nên à chúng ta thấy là cái đường thẳng của mình nó bị đẩy về phía các cái điểm màu xanh dấu cộng này nhiều hơn. Chúng ta sẽ thấy cái khoảng cách này nè từ điểm màu đỏ gần nhất nó xa. Còn cái điểm màu xanh đến cái đường thẳng này thì nó ngắn đó. thì do cái lực đẩy từ các cái à mẫu dữ liệu mà có số đông nó sẽ khiến cho mô hình của mình nó bị mất cân bằng. Thì đây cũng có thể là chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5MgxCUTD9I4",
      "filename": "5MgxCUTD9I4",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 2)",
      "chunk_id": 11,
      "start_timestamp": "0:07:47",
      "end_timestamp": "0:07:54"
    }
  },
  {
    "page_content": "nó bị mất cân bằng. Thì đây cũng có thể là chúng ta đã gặp trong những phần trước. Đây là một cái điểm yếu khi mà chúng ta huấn luyện với những cái mô hình mà có quá nhiều mẫu.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5MgxCUTD9I4",
      "filename": "5MgxCUTD9I4",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 2)",
      "chunk_id": 12,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chúng ta sẽ cùng đến với cái nhóm thuật toán phân cụm à tiếp theo đó là hierarchical clustering hay là phân cụm à phân tần. Thì à nếu như ở trong cái phần thuật toán phân cụm theo kiểu phân hoạch thì một cái điểm dữ liệu của mình nó chỉ thuộc duy nhất bởi một cái cụm. Thì đối với thuật toán à à đối với cái nhóm mà hierarchical clustering tức là phân cụm phân tần thì một cái điểm dữ liệu của mình nó có thể vừa thuộc cái cụm số 6 mà vừa thuộc cái cụm số 8 và thuộc cái cụm số 9. Ví dụ bên đây cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5vs9uLeRy3Q",
      "filename": "5vs9uLeRy3Q",
      "title": "[CS114 - Chương 5] Cluster - Phần 4",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:58"
    }
  },
  {
    "page_content": "cụm số 8 và thuộc cái cụm số 9. Ví dụ bên đây cái điểm dữ liệu thứ tư thì vừa thuộc cái cụm số 6 mà vừa thuộc cái cụm số 9. Tức là nó thuộc hai cái cụm và nó sẽ có tình trạng đó là cụm lớn thì nó sẽ chứa luôn những cái cụm nhỏ hơn. Đó thì đây là một chuỗi các cái cụm nó nó lồng nhau. 9 thì chứa 8 và 6. 8 thì chứa 6 và 3, 6 thì chứa 1 và 2. Rồi thì ở đây chúng ta sẽ có hai cái à loại thuật toán phân cụm phân tần. Đầu tiên đó là cái phương pháp à hai cái phương pháp ha. Phương pháp đầu tiên đó là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5vs9uLeRy3Q",
      "filename": "5vs9uLeRy3Q",
      "title": "[CS114 - Chương 5] Cluster - Phần 4",
      "chunk_id": 1,
      "start_timestamp": "0:00:53",
      "end_timestamp": "0:01:38"
    }
  },
  {
    "page_content": "cái phương pháp ha. Phương pháp đầu tiên đó là bottom up hoặc hay còn gọi là agglomerative là xây dựng cái sơ đồ phân loại à xây dựng cái sơ đồ phân loại hay còn gọi là dendrogram. À từ cái cấp dưới cùng chúng ta sẽ đi từ cái cấp dưới cùng tức là từ cái điểm dữ liệu của mình đó. Chúng ta sẽ đi từ cái cụm này. Từ cái điểm dữ liệu này sẽ là một cái cụm đầu tiên. Sau đó nó sẽ hợp nhất với cái cặp cụm mà giống nhau nhất hoặc là gần nhau nhất. Giống nhất hoặc là gần nhất. Thì ví dụ chúng ta thấy là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5vs9uLeRy3Q",
      "filename": "5vs9uLeRy3Q",
      "title": "[CS114 - Chương 5] Cluster - Phần 4",
      "chunk_id": 2,
      "start_timestamp": "0:01:33",
      "end_timestamp": "0:02:23"
    }
  },
  {
    "page_content": "nhất hoặc là gần nhất. Thì ví dụ chúng ta thấy là ở đây chúng ta rã ra thành một cụm. Mỗi điểm này là một cụm. Rồi sau đó là hai cái điểm này, hai cái cụm này là gần nhau nhất thì chúng ta sẽ bắt đầu gom nó lên thành một cụm. Đó. Rồi sau đó chúng ta lại tiếp tục à gom. Rồi sau đó chúng ta lại thấy là hai cái điểm này thì cái cụm này và cái điểm này rồi cái cụm này và điểm à cái cụm này và điểm này là gần nhau nhất. Chúng ta sẽ lại gom Cứ như vậy chúng ta sẽ gom. Thì đây là cái kiểu phân cụm gom",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5vs9uLeRy3Q",
      "filename": "5vs9uLeRy3Q",
      "title": "[CS114 - Chương 5] Cluster - Phần 4",
      "chunk_id": 3,
      "start_timestamp": "0:02:18",
      "end_timestamp": "0:03:10"
    }
  },
  {
    "page_content": "chúng ta sẽ gom. Thì đây là cái kiểu phân cụm gom tầng. Và phương pháp thứ hai à phương pháp đầu đó là phương pháp về bottom up tức là đi từ dưới lên. Còn cái phương pháp thứ hai á đó là phương pháp top down là divisive thì nó sẽ ngược lại. Đó là từ các cái điểm này chúng ta đã xem nó tất cả chúng ta đã xem nó như là một cụm lớn rồi. Rồi sau đó chúng ta lại chẻ nhỏ xuống đó. À ban đầu là à không có cái này ha. Rồi chúng ta lại gom. Rồi sau đó ở đây chúng ta tiếp tục lại gom. Đó thì là đi từ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5vs9uLeRy3Q",
      "filename": "5vs9uLeRy3Q",
      "title": "[CS114 - Chương 5] Cluster - Phần 4",
      "chunk_id": 4,
      "start_timestamp": "0:03:03",
      "end_timestamp": "0:03:58"
    }
  },
  {
    "page_content": "ở đây chúng ta tiếp tục lại gom. Đó thì là đi từ trên xuống. Thì giữa hai cái phương pháp là top down và bottom up thì top down nó sẽ không phổ biến bằng bottom up. Hay nói cách khác đó là cái phương pháp agglomerative clustering thì nó sẽ là phổ biến hơn cái phương pháp phân chia dạng divisive method. Và ban đầu với mỗi dữ liệu thì chúng ta sẽ tạo thành một cái cụm. Tức là một điểm dữ liệu là tương ứng đã là một cụm rồi, hay còn gọi là một nút. Và chúng ta sẽ lần lượt gom nhóm hợp nhất từ từ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5vs9uLeRy3Q",
      "filename": "5vs9uLeRy3Q",
      "title": "[CS114 - Chương 5] Cluster - Phần 4",
      "chunk_id": 5,
      "start_timestamp": "0:03:52",
      "end_timestamp": "0:04:34"
    }
  },
  {
    "page_content": "Và chúng ta sẽ lần lượt gom nhóm hợp nhất từ từ các cái cụm có cái khoảng cách ngắn nhất lại với nhau. Thì cái tiêu chí thế nào gọi là ngắn nhất giữa các cụm? thì sẽ được à đề cập ở trong những cái slide tiếp theo. Rồi sau đó chúng ta cứ lặp đi lặp lại cái việc này. Tiếp tục hợp nhất hợp nhất gom cụm gom cụm. Cuối cùng tất cả các cái nút thì đều được gom về một cái cụm lớn một cái cụm lớn và cụm này sẽ là bao phủ hết tất cả các cái điểm dữ liệu của mình. Thì cái thuật toán đó là Agglomerative",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5vs9uLeRy3Q",
      "filename": "5vs9uLeRy3Q",
      "title": "[CS114 - Chương 5] Cluster - Phần 4",
      "chunk_id": 6,
      "start_timestamp": "0:04:29",
      "end_timestamp": "0:05:10"
    }
  },
  {
    "page_content": "của mình. Thì cái thuật toán đó là Agglomerative là đầu vào của chúng ta sẽ có một cái tập dữ liệu là D. Và lúc này thì chúng ta sẽ không có cho trước cái số cụm chúng ta cần chia là K. Rồi với mỗi điểm dữ liệu trong cái tập D này thì chúng ta sẽ xem nó như là một cái cụm. Sau đó chúng ta sẽ đi tính cái khoảng cách giữa tất cả các cái cặp điểm. Tất cả cặp điểm X1 với X2, X1 với X3, X1 với XM, X2 với X3 X vân vân. tất cả các cặp điểm để tìm ra cái cặp nào là nhỏ nhất. Tìm hai cái cụm mà gần nhau",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5vs9uLeRy3Q",
      "filename": "5vs9uLeRy3Q",
      "title": "[CS114 - Chương 5] Cluster - Phần 4",
      "chunk_id": 7,
      "start_timestamp": "0:05:04",
      "end_timestamp": "0:05:51"
    }
  },
  {
    "page_content": "cặp nào là nhỏ nhất. Tìm hai cái cụm mà gần nhau nhất. Tìm hai cụm gần nhau nhất. Sau đó gộp thành thì giả sử như cụm 2 và cụm 3 cho cái khoảng cách gần nhất thì chúng ta sẽ gom hai cái thằng này lại với nhau để tạo thành một cụm. Rồi sau đó lại tiếp tục so x1 với cái cụm 23, x1 với cái cụm 4, x1 với cụm 5. Cứ như vậy và gộp hai cụm này thành một cái cụm mới để đặt tên là C. Ví dụ đây là C thì tính khoảng cách từ C đến tất cả các cái cụm còn lại. Thì vì chúng ta mới hình thành cái cụm mới nên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5vs9uLeRy3Q",
      "filename": "5vs9uLeRy3Q",
      "title": "[CS114 - Chương 5] Cluster - Phần 4",
      "chunk_id": 8,
      "start_timestamp": "0:05:45",
      "end_timestamp": "0:06:29"
    }
  },
  {
    "page_content": "Thì vì chúng ta mới hình thành cái cụm mới nên chúng ta phải tính cái giá trị cho nó. Còn các cái cụm kia thì chúng ta tái sử dụng lại và tiếp tục tìm hai cụm gần nhất. Rồi lại gộp lại rồi lại à tính khoảng cách cho cái cụm mới được sáp nhập mới được gom lại đó hợp nhất lại lại tiếp tục đi tính khoảng cách và chúng ta sẽ lập cho đến khi nào chỉ còn duy nhất một cụm thì trong cái sơ đồ này chúng ta sẽ thấy đây là một cái kiểu à phân cụm theo kiểu là nested tức là à nó sẽ bao hàm gọi là gộp lại",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5vs9uLeRy3Q",
      "filename": "5vs9uLeRy3Q",
      "title": "[CS114 - Chương 5] Cluster - Phần 4",
      "chunk_id": 9,
      "start_timestamp": "0:06:19",
      "end_timestamp": "0:07:17"
    }
  },
  {
    "page_content": "là nested tức là à nó sẽ bao hàm gọi là gộp lại với nhau, gộp lại lẫn nhau. Thì ở cái vòng lập đầu tiên là hai cái điểm P1 và P2 là gần nhất nên nó sẽ gom lại thành một cụm. Rồi sau đó thì hai điểm P5 và P4 là gần nhất. Nó sẽ gom lại thành một cụm. Và tương ứng ở đây chúng ta thấy là P1 P2 gom lại thành một cụm. Thì bản chất giữa A và B đây chỉ là hai cái cách biểu diễn khác nhau. Một cái là Nested và một cái là dendrogram. rồi sau đó thì chúng ta sẽ thấy là cụm số 1 và P3 à cụm 1 và P3 là gần",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5vs9uLeRy3Q",
      "filename": "5vs9uLeRy3Q",
      "title": "[CS114 - Chương 5] Cluster - Phần 4",
      "chunk_id": 10,
      "start_timestamp": "0:07:10",
      "end_timestamp": "0:07:57"
    }
  },
  {
    "page_content": "ta sẽ thấy là cụm số 1 và P3 à cụm 1 và P3 là gần nhất. Tại vì hai cái thằng này à cụm 1 và P3 nó sẽ có cái khoảng cách gần hơn. Còn cái cụm số hai này nó bị tách biệt ra xa. Nó có một cái đoạn khoảng cách xa như thế này thì P3 sẽ được gom với lại P1 P2 để tạo thành một cụm mới. Đó cụm này có tên là 3. Sau đó thì chúng ta thấy là hai cái cụm này, hai cụm 3 và cụm 2 chỉ là hai cụm duy nhất thôi thì chắc chắn chúng ta sẽ gom nó lại thành một cái cụm số 4. Thì đây là cái cách hoạt động của thuật",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5vs9uLeRy3Q",
      "filename": "5vs9uLeRy3Q",
      "title": "[CS114 - Chương 5] Cluster - Phần 4",
      "chunk_id": 11,
      "start_timestamp": "0:07:49",
      "end_timestamp": "0:08:44"
    }
  },
  {
    "page_content": "cụm số 4. Thì đây là cái cách hoạt động của thuật toán. à gom cụm theo kiểu là à bottom up. Thì ở đây chúng ta sẽ có cái khoảng cái khái niệm nó gọi là khoảng cách giữa hai cụm. Bây giờ hai cái cụm này có rất nhiều điểm. Thì bây giờ chúng ta sẽ ờ định nghĩa thế thì có rất nhiều cái cách định nghĩa khác nhau. Cái cách đầu tiên đó là dựa trên à liên kết đơn. Tức là nó sẽ lấy ra một điểm à nó sẽ tính cái khoảng cách nhỏ nhất giữa hai điểm thuộc hai cụm này. Ví dụ như chúng ta thấy là trong cái cụm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5vs9uLeRy3Q",
      "filename": "5vs9uLeRy3Q",
      "title": "[CS114 - Chương 5] Cluster - Phần 4",
      "chunk_id": 12,
      "start_timestamp": "0:08:38",
      "end_timestamp": "0:09:21"
    }
  },
  {
    "page_content": "cụm này. Ví dụ như chúng ta thấy là trong cái cụm số 1 và cái cụm số hai thì khoảng cách nhỏ nhất giữa hai cái điểm trong hai cái cụm này chính là hai cái điểm này. Đó. Tại sao? Ví dụ như hai điểm này, chúng ta thấy là khoảng cách nó dài hơn so với hai điểm này hoặc là hai điểm này à thì nó có khoảng cách dài hơn. Do đó chúng ta sẽ lấy ra hai điểm ở mỗi cụm. Và ờ khoảng cách nào là ngắn nhất? Thì đó chính là cái khoảng cách theo kiểu là single link liên kết đơn. Thì với cái cách này nó sẽ dễ bị",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5vs9uLeRy3Q",
      "filename": "5vs9uLeRy3Q",
      "title": "[CS114 - Chương 5] Cluster - Phần 4",
      "chunk_id": 13,
      "start_timestamp": "0:09:15",
      "end_timestamp": "0:10:00"
    }
  },
  {
    "page_content": "liên kết đơn. Thì với cái cách này nó sẽ dễ bị ảnh hưởng bởi những cái điểm mà liên tiếp nhau. À nó sẽ cứ nối vô nối vô nối vô nối vô thành một cái cụm. Thì cái điều này nó sẽ làm cho cái cụm của mình nó sẽ liên tục phình ra và phình theo cái hướng là chiều dài dẹt như thế này. Còn cái kiểu thứ hai đó là liên kết hoàn chỉnh hay còn gọi là complete à complete link. Tức là nó sẽ lấy ra hai cái điểm xa nhất giữa hai cụm. Đây là điểm xa nhất. Rồi thì khoảng cách giữa hai cái cụm này chính là hai",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5vs9uLeRy3Q",
      "filename": "5vs9uLeRy3Q",
      "title": "[CS114 - Chương 5] Cluster - Phần 4",
      "chunk_id": 14,
      "start_timestamp": "0:09:56",
      "end_timestamp": "0:10:46"
    }
  },
  {
    "page_content": "thì khoảng cách giữa hai cái cụm này chính là hai cái điểm mà ở xa nhất ngoài rìa của hai cái cụm số 1 và cái cụm số hai này. Còn liên kết trung bình tức là chúng ta sẽ cộng tất cả những cái độ dài của các cái khoảng cách này lại với nhau. Rồi sau đó chúng ta cộng lại trung bình. Và cái cách thứ tư đó là trọng tâm. Tức là chúng ta sẽ lấy ra một điểm nằm ở trọng tâm của cái cụm thứ nhất. Rồi một cái điểm nằm ở trọng tâm của cái cụm thứ hai. Rồi sau đó chúng ta nối lại. Thì đây chính là cái cách",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5vs9uLeRy3Q",
      "filename": "5vs9uLeRy3Q",
      "title": "[CS114 - Chương 5] Cluster - Phần 4",
      "chunk_id": 15,
      "start_timestamp": "0:10:40",
      "end_timestamp": "0:11:17"
    }
  },
  {
    "page_content": "đó chúng ta nối lại. Thì đây chính là cái cách định nghĩa cái khoảng cách giữa hai cái cụm. Như vậy thì chúng ta đã cùng tìm hiểu về thuật toán một trong những cái thuật toán mà gom cụm phân tần. rất là phổ biến và sử dụng rất là phổ biến đó chính là thuật toán Agglomerative. Nó sẽ phổ biến hơn so với lại thuật toán là divisive tức là từ top down từ bottom up thì nó sẽ là phổ biến hơn. Và để làm được cái thuật toán này thì chúng ta sẽ cần phải có cái khái niệm đó là khoảng cách giữa hai cụm là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5vs9uLeRy3Q",
      "filename": "5vs9uLeRy3Q",
      "title": "[CS114 - Chương 5] Cluster - Phần 4",
      "chunk_id": 16,
      "start_timestamp": "0:11:13",
      "end_timestamp": "0:11:24"
    }
  },
  {
    "page_content": "cái khái niệm đó là khoảng cách giữa hai cụm là gì thì chúng ta sẽ có rất nhiều những cái khái niệm khác nhau. Và với mỗi cái khái niệm thì nó cũng đều có những cái điểm yếu và điểm mạnh. Thì chúng ta sẽ tùy theo cái dữ liệu của mình nó như thế nào để mà mình à chọn lựa cái thuật toán hoặc là chọn lựa cái khái niệm định nghĩa cho cái khoảng cách giữa hai cụm cho nó phù hợp với cái bài toán đó. Không chắc là có một cái phương pháp nào là tốt nhất.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=5vs9uLeRy3Q",
      "filename": "5vs9uLeRy3Q",
      "title": "[CS114 - Chương 5] Cluster - Phần 4",
      "chunk_id": 17,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Các loại tiếp theo, bên trong online đang cố gắng để khớp các Quick Giờ Klaus market đó là cho biết đặt quân cờ tiếp theo ở đâu và phần thưởng của mình đó sẽ là cộng 1 nếu thắng ở cuối trò chơi hoặc là bằng không nếu chúng ta thua, thì ở trên cái màn hình ở đây chúng ta thấy đó là những thuật toán AlphaGo, AlphaZero và AlphaStar thì ở đây chúng ta thấy là nổi tiếng nhất đó là AlphaGo đây là một thuật toán mà chơi cờ vây đã giúp cho máy tính có thể chiến thắng được các đại kiện tướng cờ vây sau",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6NcteEkIRBM",
      "filename": "6NcteEkIRBM",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 3",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:04"
    }
  },
  {
    "page_content": "chiến thắng được các đại kiện tướng cờ vây sau đó là có Alpha Zero, đây là một thuật toán chơi các thể loại cờ khác nhau và khó của việc chơi cờ, đặc biệt là cờ vây, chính là không gian trạng thái rất lớn Với không gian trạng thái lớn này, nó sẽ gây khó khăn cho mô hình của mình trong việc đưa ra quyết định, các hành động và một số cái khái niệm khác đó là chính sách của hành động hay còn gọi là policy ký hiệu là chữ P thì đây là một cái chiến lược của agent để chọn ra cái hành động tại mỗi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6NcteEkIRBM",
      "filename": "6NcteEkIRBM",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 3",
      "chunk_id": 1,
      "start_timestamp": "0:00:59",
      "end_timestamp": "0:01:47"
    }
  },
  {
    "page_content": "lược của agent để chọn ra cái hành động tại mỗi trạng thái nó sẽ chọn ra cái hành động tại mỗi trạng thái và nó chính là cái xác suất chọn cái hành động thì xét về đặc điểm thì nó có thể là một cái chiến thuật đơn giản nó có thể là một cái policy đơn giản đó là một cái bảng tra Nếu mà ở trạng thái này, thì chúng ta sẽ chọn cái hành động này Nếu mà ở trạng thái này thì chúng ta sẽ chọn cái hành động này Đó là một cái bảng tra Nhưng nó cũng có thể là một cái phức tạp Là một cái chiến thuật phức",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6NcteEkIRBM",
      "filename": "6NcteEkIRBM",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 3",
      "chunk_id": 2,
      "start_timestamp": "0:01:44",
      "end_timestamp": "0:02:28"
    }
  },
  {
    "page_content": "là một cái phức tạp Là một cái chiến thuật phức tạp Là một cái hàm số hoặc nó có thể là một cái mạng neural network Với cái dữ liệu đầu vào chúng ta sẽ chọn cái hành động tiếp theo là gì Ví dụ như người chơi cờ sẽ có chiến thuật là ưu tiên ăn những quân có giá trị cao trước Ví dụ như cụ thể là chúng ta ăn quân mã và quân hậu thì chúng ta sẽ ưu tiên ăn quân hậu trước Ví dụ vậy, thì đây cũng là một chiến thuật Thứ hai đó là trong xe tự lái thì nếu như chúng ta gặp cái trạng thái đó là đèn đỏ thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6NcteEkIRBM",
      "filename": "6NcteEkIRBM",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 3",
      "chunk_id": 3,
      "start_timestamp": "0:02:21",
      "end_timestamp": "0:03:06"
    }
  },
  {
    "page_content": "như chúng ta gặp cái trạng thái đó là đèn đỏ thì cái action, cái policy cho cái action chúng ta đó là chúng ta phải dừng, tại vì nếu như chúng ta không dừng thì có thể chúng ta sẽ bị phạt và lúc đó thì cái reward của mình nó sẽ bị đẩy về không rồi đèn xanh thì chúng ta sẽ là đi, thì đây chính là cái chiến thuật cho cái policy chính sách hành động cho xe tự lái chúng ta sẽ có nhiều loại policy khác nhau Đầu tiên là Deterministic Policy tức là với mỗi một trạng thái S thì chúng ta sẽ có duy nhất",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6NcteEkIRBM",
      "filename": "6NcteEkIRBM",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 3",
      "chunk_id": 4,
      "start_timestamp": "0:02:58",
      "end_timestamp": "0:03:44"
    }
  },
  {
    "page_content": "mỗi một trạng thái S thì chúng ta sẽ có duy nhất một hành động một hành động duy nhất là A và A sẽ là bằng P của S một cái S đầu vào thì chúng ta sẽ có duy nhất một cái Action trong xe tự lái thì ví dụ như hồi nãy chúng ta đã nói nếu chúng ta gặp đèn đỏ thì hành động luôn là trong game Cờ Vua thì một agent có chiến thuật cố định không bao giờ chọn nước đi nào khác ngoài nước đi tối ưu tại cái trạng thái đó tức là tại một cái trạng thái chúng ta sẽ có rất nhiều cách đi nhưng mà chúng ta sẽ luôn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6NcteEkIRBM",
      "filename": "6NcteEkIRBM",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 3",
      "chunk_id": 5,
      "start_timestamp": "0:03:41",
      "end_timestamp": "0:04:20"
    }
  },
  {
    "page_content": "sẽ có rất nhiều cách đi nhưng mà chúng ta sẽ luôn chọn duy nhất một nước đi tối ưu nhất chúng ta sẽ không thử những nước đi khác mà chúng ta chỉ chọn duy nhất một nước đi tối ưu Và cái Deterministic Policy này thì nó sẽ dùng khi mà môi trường của mình ít có tính ngẫu nhiên và nó có tính ổn định Nhưng trong lĩnh vực về game thì yếu tố ngẫu nhiên nó khá là cao tại vì nó sẽ dựa trên phong cách chơi của từng game thủ khác nhau Tại những thời điểm khác nhau họ sẽ có những yếu tố ngẫu nhiên khác nhau",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6NcteEkIRBM",
      "filename": "6NcteEkIRBM",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 3",
      "chunk_id": 6,
      "start_timestamp": "0:04:11",
      "end_timestamp": "0:05:00"
    }
  },
  {
    "page_content": "nhau họ sẽ có những yếu tố ngẫu nhiên khác nhau Nên thường trong game nó sẽ không dùng Deterministic Policy Agent đã học đủ tốt để không cần phải thử thêm không cần phải thử thêm những policy khác thì nó sẽ chọn đơn định Deterministic Policy Stochastic Policy Tại mỗi trạng thái S chúng ta sẽ ánh xạ thành phân phối xác suất cho các hành động tức là với 1 cái S đầu vào cho trước thì chúng ta sẽ có phân bố action A thì nó sẽ tương ứng là một cái phân bố xác suất cho trước S thì chúng ta sẽ có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6NcteEkIRBM",
      "filename": "6NcteEkIRBM",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 3",
      "chunk_id": 7,
      "start_timestamp": "0:04:56",
      "end_timestamp": "0:05:41"
    }
  },
  {
    "page_content": "phân bố xác suất cho trước S thì chúng ta sẽ có Action A này là xác suất chứ không phải lúc nào chúng ta cũng sẽ chọn Action A này thì đây là Stochastic Policy ví dụ như trong game Pac-Man khi chưa biết hướng đi nào tốt nhất thì Agent sẽ chọn cái hướng ngẫu nhiên đó là lên xuống trái phải để thử để thử và học từ cái kết quả mà mình trả về và Reinforcement Learning trong game phức tạp hơn, ví dụ như StarCraft, Quake thì Agent có thể pha trộn các hành động để khó đoán hơn cho đối thủ của mình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6NcteEkIRBM",
      "filename": "6NcteEkIRBM",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 3",
      "chunk_id": 8,
      "start_timestamp": "0:05:36",
      "end_timestamp": "0:06:10"
    }
  },
  {
    "page_content": "hành động để khó đoán hơn cho đối thủ của mình Stochastic Policy thì thường dùng khi môi trường của mình không chắc chắn và có yếu tố xác suất Vì trong game, đó là một trong những môi trường mà có yếu tố không chắc chắn Agent cần khám phá thêm nhiều hành động và trong tình huống đối kháng và tránh để đối phương đoán trước những hành vi của mình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6NcteEkIRBM",
      "filename": "6NcteEkIRBM",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 3",
      "chunk_id": 9,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề, học sâu, machine learning, ImageNet. Chủ đề, học sâu, machine learning, ImageNet. một kiến trúc Deep Learning và Deep Learning ngày nay được dựa trên tư tưởng của mạng Neural nhân tạo đó là kiến trúc gồm nhiều lớp và việc huấn luyện sẽ dựa trên một thuật toán Lan truyền ngược Backpropagation do đó bài học ngày hôm nay có vai trò vô cùng quan trọng và nền tảng cho lý thuyết về học sâu lý thuyết về máy học hiện đại hiện nay Mục tiêu của bài học này Đó là chúng ta sẽ hiểu được những khái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:02"
    }
  },
  {
    "page_content": "học này Đó là chúng ta sẽ hiểu được những khái niệm và nguồn cảm hứng của mạng Neural nhân tạo Có những khái niệm ví dụ như Neural là gì Lớp hay layer là gì Trọng số của mạng Neural Network là gì Độ lệch hay bias là gì Và chúng ta sẽ hiểu được cách thức mà mạng Neural nhân tạo hoạt động như thế nào Gọi là thuật toán Feedforward, tức là thuật toán Lan Truyền thuận rồi biết được quá trình huấn luyện của một cái mạng Neural như thế nào đó là thuật toán Backpropagation, Lan Truyền Ngược và một vài",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 1,
      "start_timestamp": "0:00:57",
      "end_timestamp": "0:01:33"
    }
  },
  {
    "page_content": "toán Backpropagation, Lan Truyền Ngược và một vài cái ứng dụng trong thực tế của mạng Neural nhân tạo thì đầu tiên chúng ta sẽ nói về cái nguồn cảm hứng là tại sao chúng ta lại cần có một cái mạng Neural nhân tạo thế thì đầu tiên chúng ta sẽ phải nói đến là trước cái mạng Neural nhân tạo thì các thuật toán mà truyền thống nó hiệu quả với dữ liệu mà phân tách, tuyến tính được Tức là những mô hình truyền thống trước đây chỉ hiệu quả được với những dữ liệu có thể phân tách một cách tuyến tính Tức",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 2,
      "start_timestamp": "0:01:29",
      "end_timestamp": "0:02:11"
    }
  },
  {
    "page_content": "dữ liệu có thể phân tách một cách tuyến tính Tức là chúng ta chỉ có thể vẽ được một đường thẳng để chia nó ra làm hai phần Thì đây là một đường tuyến tính để tách ra hai điểm màu vàng, màu cam và màu xanh Tuy nhiên trong thực tế thì dữ liệu của mình thường có một mối quan hệ rất phức tạp và thường sẽ là phi tuyến tính Ví dụ như các loại dữ liệu hình ảnh và dữ liệu văn bản để mà chúng ta có thể nhận diện được hình ảnh hoặc là phân loại văn bản vân vân thì đó là những bài toán mà không phải là có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 3,
      "start_timestamp": "0:02:02",
      "end_timestamp": "0:02:44"
    }
  },
  {
    "page_content": "vân thì đó là những bài toán mà không phải là có một mối quan hệ tuyến tính thì ở trong hình bên tay trái ở đây chúng ta sẽ thấy là 2 tập là màu xanh và màu cam thì nó sẽ không thể nào có thể chia tách ra được bằng một đường thẳng Ví dụ như chúng ta kẻ một cái đường như thế này hoặc là chúng ta kẻ một cái đường như thế này thì không có cách nào mà có thể chia nó ra làm hai phần không có cách nào để chia nó ra làm hai phần được với một đường thẳng mà chúng ta chỉ có thể là có một cái đường rất",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 4,
      "start_timestamp": "0:02:38",
      "end_timestamp": "0:03:23"
    }
  },
  {
    "page_content": "mà chúng ta chỉ có thể là có một cái đường rất là phức tạp như thế này nó sẽ đi len lỏi để mà chia tách nó ra làm hai phần thì đây nó gọi là một cái ví dụ về phân loại dữ liệu nhưng mà nó là phi tuyến Còn ở phía trên đây là một đường thẳng, thì đây chính là tuyến tính Và một đường thẳng thì không thể nào chia tách tập này ra được làm 2 Đó là cái động cơ tại sao chúng ta cần phải có một cái mạng Neural Mạng Neural này nó có khả năng tự học các cái mối quan hệ rất là phức tạp của dữ liệu Và dữ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 5,
      "start_timestamp": "0:03:14",
      "end_timestamp": "0:04:07"
    }
  },
  {
    "page_content": "cái mối quan hệ rất là phức tạp của dữ liệu Và dữ liệu này thì nó có cái mối quan hệ đó là phi tuyến Là một cái mối quan hệ phi tuyến Thì xuất phát điểm của cái mạng Neural Network này là nó lấy cái nguồn cảm hứng từ cái não của con người Từ não của con người thì ở trong hình bên đây đó là một cái kiến trúc và một cái tế bào của một cái tế bào não người Tạm viết tắt đây, thì cái tế bào não này sẽ nhận dữ liệu đầu vào, cái input này Thông qua các cái nhánh, thông qua các cái Dendritic Và nhánh",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 6,
      "start_timestamp": "0:04:00",
      "end_timestamp": "0:04:50"
    }
  },
  {
    "page_content": "cái nhánh, thông qua các cái Dendritic Và nhánh này nó sẽ tiếp nhận các cái tín hiệu đầu vào Và bên trong thì nó sẽ có cái phần thân tế bào Rồi cái phần lõi đó là cái phần nhân Thì cái phần lõi này nó sẽ nhận cái tín hiệu đầu vào và nó sẽ xử lý Sau đó nó sẽ lan truyền và nó kích hoạt Nó kích hoạt qua cái Axon này để mà truyền cái tín hiệu đầu ra truyền tín hiệu đầu ra đến các tế bào não khác các tế bào não khác sẽ xử lý đây là một kiến trúc về mặt sinh học của não người và tương ứng bên tay",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 7,
      "start_timestamp": "0:04:45",
      "end_timestamp": "0:05:45"
    }
  },
  {
    "page_content": "mặt sinh học của não người và tương ứng bên tay phải là chúng ta sẽ có một cái kiến trúc về một cái tế bào tế bào trong nháy kép của một cái mạng Neural nhân tạo đây là một cái tế bào, nó cũng sẽ nhận vào input các cái node và mỗi cái node này nó sẽ nhận tín hiệu kích hoạt từ các lớp trước ở đây sẽ có cạnh để thực hiện phép nhân, phép trọng số đây chính là weight toàn bộ cạnh này nó sẽ có trọng số là b, w1, w2, wn đó là trọng số của tế bào mạng Neural nhân tạo sau đó sẽ có 1 hàm kích hoạt phi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 8,
      "start_timestamp": "0:05:41",
      "end_timestamp": "0:06:35"
    }
  },
  {
    "page_content": "Neural nhân tạo sau đó sẽ có 1 hàm kích hoạt phi tuyến tính là 1 hàm kích hoạt hàm kích hoạt này là 1 hàm phi tuyến sau đó tính toán tạo ra giá trị output và output này sẽ truyền đến cho tất cả tế bào ở lớp phía sau và ý tưởng cốt lõi cũng lấy từ cấu trúc và cách thức hoạt động của não người đầu vào là input, đầu ra là output và có các nhân xử lý thì bên đây cũng sẽ có những nhân xử lý nhân xử lý này thì bao gồm là Tuyến tính và một cái là Phi tuyến nó sẽ xử lý tín hiệu đầu vào nhận tín hiệu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 9,
      "start_timestamp": "0:06:32",
      "end_timestamp": "0:07:14"
    }
  },
  {
    "page_content": "tuyến nó sẽ xử lý tín hiệu đầu vào nhận tín hiệu đầu vào xử lý tín hiệu và truyền tín hiệu đến các Neural khác nó sẽ truyền đi ra qua các Neural khác và nó sẽ kết nối hàng triệu Neural thành một mạng Neural phức tạp để hy vọng là có thể giải quyết được cùng một cái vấn đề của mình đó chính là ý tưởng của mạng Neural nhân tạo mạng Neural là gì? mạng Neural nhân tạo là một mô hình toán dẫu sao chúng ta thấy là chúng ta đưa nó về các dạng đồ thị nhưng cuối cùng nó cũng đều là những công thức tính",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 10,
      "start_timestamp": "0:07:11",
      "end_timestamp": "0:08:02"
    }
  },
  {
    "page_content": "cuối cùng nó cũng đều là những công thức tính toán nên nó được gọi là một mô hình toán bao gồm các Neural được tổ chức thành các lớp Mỗi một cái node này được gọi là 1 cái Neural Và nó được sắp xếp theo lớp Ví dụ nguyên một cái dọc này là 1 node, là 1 layer Nguyên cái dọc này là 1 layer, nguyên cái dọc này là 1 layer Cái cấu trúc của nó sẽ bao gồm 3 cái loại lớp chính Cái lớp đầu vào hay là lớp input layer thì đây là cái lớp input sau đó là lớp ẩn hidden layer hidden layer có thể có 1 hoặc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 11,
      "start_timestamp": "0:07:59",
      "end_timestamp": "0:08:55"
    }
  },
  {
    "page_content": "lớp ẩn hidden layer hidden layer có thể có 1 hoặc nhiều hidden layer lớp đầu ra là output layer chúng ta sẽ có 1 lớp như thế này để đưa ra được kết quả dự đoán lớp hidden layer sẽ học trích xuất các đặc trưng từ dữ liệu đầu vào Luồng xử lý của mình là chúng ta sẽ đi từ lớp input trước, nó sẽ truyền tín hiệu từ bên ngoài vào Sau đó lớp input này sẽ lan truyền đến hidden layer, cái lớp ẩn thứ nhất Lớp ẩn thứ nhất sau đó lại được tiếp tục lan truyền đến lớp hidden layer thứ 2 Lớp hidden layer thứ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 12,
      "start_timestamp": "0:08:52",
      "end_timestamp": "0:09:36"
    }
  },
  {
    "page_content": "đến lớp hidden layer thứ 2 Lớp hidden layer thứ 2 v.v.v. như vậy sẽ truyền cho đến lớp output layer cuối cùng Đó là luồng xử lý dữ liệu Sau đây thì chúng ta sẽ có một ví dụ để minh họa Đầu tiên, đó là chúng ta sẽ có cái ví dụ là rất là kinh điển trong các cái bài toán mà nhận diện đó chính là nhận diện chữ số viết tay là các cái con số từ 0 cho đến 9 bằng hình ảnh thì ở trong cái tập dữ liệu MNIST nó có kích thước là 28 x 28 tức là một cái ma trận có kích thước là 28 x 28 như thế này và trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 13,
      "start_timestamp": "0:09:29",
      "end_timestamp": "0:10:27"
    }
  },
  {
    "page_content": "có kích thước là 28 x 28 như thế này và trong đây nó sẽ có cái con số ví dụ vậy, đây là một cái con số sau đó chúng ta sẽ flatten tức là chúng ta biến từ dữ liệu dạng ma trận về dạng vector Vector này là một vector có 784 chiều sau đó qua mạng Neural Network Mạng Neural nhân tạo thì chúng ta sẽ truyền đến các lớp ẩn tiếp theo Lớp ẩn này có kết nối dày đặc Mỗi cái Neural này sẽ nối đến tất cả những cái Neural ở layer tiếp theo Nên nó được gọi là dày đặc Thì để đảm bảo cho tính thoáng của hình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 14,
      "start_timestamp": "0:10:18",
      "end_timestamp": "0:11:21"
    }
  },
  {
    "page_content": "dày đặc Thì để đảm bảo cho tính thoáng của hình ảnh thì chúng ta sẽ không có vẽ dày đặc như thế này Chúng ta chỉ cần vẽ dấu mũi tên thôi nhưng khi chúng ta ký hiệu là dense, hàm ý mỗi Neural ở lớp trước sẽ được kết nối đến tất cả các Neural ở lớp tiếp theo và con số 128 cho biết là output tức là tại layer này có bao nhiêu nodes 128 hàm ý là ở đây có 128 nodes 784 là cái node đầu vào ở đây và mỗi cái node đầu vào này sẽ nhận 1 cái giá trị Rồi, thì do là chúng ta làm trên cái bài toán nhận diện",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 15,
      "start_timestamp": "0:11:14",
      "end_timestamp": "0:12:16"
    }
  },
  {
    "page_content": "do là chúng ta làm trên cái bài toán nhận diện chữ viết tay chữ số viết tay từ 0 đến 9, tức là chúng ta có 10 lớp đối tượng chúng ta có 10 lớp đối tượng cần phải phân biệt 10 lớp đối tượng này tương ứng ở lớp kế cuối, chúng ta sẽ có 10 cái Neural tương ứng là mỗi 1 cái Neural này để cho biết nó có thuộc về cái chữ số ở vị trí đó hay không ví dụ như cái Neural này cho biết nó có thuộc về chữ số 0 hay không Neural này tương ứng là có thuộc về chữ số 1 hay không Neural cuối này cho biết nó có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 16,
      "start_timestamp": "0:12:08",
      "end_timestamp": "0:12:55"
    }
  },
  {
    "page_content": "chữ số 1 hay không Neural cuối này cho biết nó có thuộc về chữ số 9 hay không Và để việc dự đoán được chuẩn hóa về không gian xác suất Chúng ta sẽ sử dụng một hàm kích hoạt, đó là softmax Hàm kích hoạt softmax sẽ đưa các giá trị 10 cho giá trị này về một vector có 10 chiều Y ngã là giá trị dự đoán, là một vector 10 chiều Và vector 10 chiều này thì nó thỏa mãn điều kiện đó là tổng của các phần tử của y ngã này đó là bằng một tức là tổng các cái xác suất thuộc về từng cái chữ số này nè từng cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 17,
      "start_timestamp": "0:12:50",
      "end_timestamp": "0:14:00"
    }
  },
  {
    "page_content": "xác suất thuộc về từng cái chữ số này nè từng cái chữ số này nè sẽ là bằng một và trong đó từng cái thành phần y ngã_i này nè thì đều là lớn hơn 0 và bé hơn 1 thì đưa về cái không gian xác suất này để giúp cho chúng ta chuẩn hóa và biết được rằng là một cái dữ liệu đầu vào là một cái ma trận điểm ảnh như thế này nó sẽ thuộc về lớp chữ số nào một đơn vị cơ bản trong mạng Neural nhân tạo là Perceptron đây là khái niệm Perceptron mô hình đơn giản nhất trong mạng Neural nhân tạo là Perceptron nó sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 18,
      "start_timestamp": "0:13:55",
      "end_timestamp": "0:14:59"
    }
  },
  {
    "page_content": "trong mạng Neural nhân tạo là Perceptron nó sẽ có duy nhất dữ liệu đầu vào như vậy chúng ta sẽ có duy nhất một input 1 input layer và đầu ra là 1 output layer 1 output layer output layer chỉ có 1 Neural nó chỉ có duy nhất một Neural thì đây là một đơn vị cơ bản, là một mạng Neural nhân tạo đơn giản nhất trong Neural nhân tạo perceptron này nó sẽ có một hàm tuyến tính và một hàm kích hoạt phi tuyến tính Đầu vào là dữ liệu từ lớp dữ liệu đầu vào Trọng số của mình sẽ là bao gồm W1, W2, Wn Tương",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 19,
      "start_timestamp": "0:14:54",
      "end_timestamp": "0:15:59"
    }
  },
  {
    "page_content": "Trọng số của mình sẽ là bao gồm W1, W2, Wn Tương ứng với trọng số của các dữ liệu đầu vào của mình B là khái niệm gọi là bias Tạm dịch là độ lệch nó tương ứng với dữ liệu đầu vào của mình là x0 và x0 trong trường hợp này là bằng 1 bias này giống như trong phương trình đường thẳng của mình giống như trong phương trình AX cộng BI cộng thêm một thành phần nữa, nó không phụ thuộc vào X và Y, nó chính là C C này chính là bias Đây là một dạng phương trình A là hệ số đi kèm với biến x B là trọng số đi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 20,
      "start_timestamp": "0:15:54",
      "end_timestamp": "0:16:47"
    }
  },
  {
    "page_content": "A là hệ số đi kèm với biến x B là trọng số đi kèm với biến y C là trọng số tự do, không phụ thuộc vào x và y W1 là trọng số đi kèm với tín hiệu đầu vào là x1 W2 là trọng số đi kèm với dữ liệu x2 Wn là tương ứng trọng số đi kèm với đầu vào là xn Tuy nhiên, chúng ta sẽ có một trọng số nữa là b b này sẽ đi kèm với thành phần x0 tức là bằng 1 là không phụ thuộc vào x1, x2, xn Thì thành phần không phụ thuộc vào các biến đầu vào, đó gọi là độ lệch, bias Thì nó giúp cho mô hình của mình có tính tổng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 21,
      "start_timestamp": "0:16:42",
      "end_timestamp": "0:17:36"
    }
  },
  {
    "page_content": "Thì nó giúp cho mô hình của mình có tính tổng quát hơn Và quá trình tính toán sẽ bao gồm 2 bước Bước đầu tiên là tính tổng, tổng trọng số z Z này sẽ được tính là bằng (x0 * b), tức là b là b cộng cho w1, x1, w2, x2, wn, xn thì đây là một phép biến đổi tuyến tính phép biến đổi này là một phép biến đổi tuyến tính sau đó chúng ta sẽ tiến hành tính cái output y này thông qua cái hàm kích hoạt thì đây là hàm kích hoạt f và hàm này nó bắt buộc phải là một cái hàm phi tuyến nó bắt buộc phải là một cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 22,
      "start_timestamp": "0:17:33",
      "end_timestamp": "0:18:33"
    }
  },
  {
    "page_content": "một cái hàm phi tuyến nó bắt buộc phải là một cái hàm phi tuyến Chúng ta sẽ giải thích lý do tại sao nó phải là một hàm phi tuyến Tuy nhiên ở đây chúng ta thấy là nếu chúng ta nhìn kỹ lại thì perceptron này sẽ giống với một mô hình trước đây Để mà phân loại nhị phân, đó chính là mô hình Logistic Regression Về mặt bản chất thì Perceptron là một cái cấu phần, đơn vị cơ bản nhất trong mạng Neural nhân tạo là một mô hình Logistic Regression sau đây chúng ta sẽ lấy một ví dụ tính toán bằng số học",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 23,
      "start_timestamp": "0:18:30",
      "end_timestamp": "0:19:18"
    }
  },
  {
    "page_content": "chúng ta sẽ lấy một ví dụ tính toán bằng số học đầu vào là x0, trong trường hợp này là bằng 0 nhưng mà nếu nói về bias thì cái thành phần này phải là bằng 1 nhưng mà giả định ở đây là x0 là bằng 0 thực tế nó phải là bằng 1, bias luôn luôn là bằng 1 Rồi, thì khi đó, ờ... ờ, thực tế là ở đây là số 1 Rồi, cái slide này nó có một chút nhầm lẫn Ở đây là 1, thì khi đó là cái phép biến đổi đầu tiên là z thì z sẽ là bằng 1 nhân 1 là bằng 1 rồi 2 nhân cho 0.5 2 nhân cho 0.5 và 3 nhân với trừ 1 3 nhân",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 24,
      "start_timestamp": "0:19:14",
      "end_timestamp": "0:19:29"
    }
  },
  {
    "page_content": "cho 0.5 2 nhân cho 0.5 và 3 nhân với trừ 1 3 nhân với trừ 1 khi đó chúng ta cộng lại, thì 2 nhân với 0.5 là bằng 1 3 nhân với trừ 1 là trừ 3 1 trừ 3 cộng 1 là trừ 1 vậy z trong trường hợp này sẽ là bằng trừ 1 sau đó chúng ta gọi 1 hàm kích hoạt giả sử như hàm kích hoạt này đó là hàm có công thức sau f của z là bằng 1 nếu z lớn hơn 0 và f của z bằng 0 nếu trường hợp ngược lại thì đây bản chất chính là hàm ReLU Rectified Linear Unit khi z bằng trừ 1 tức là nó kích hoạt chỗ này là đáp số của mình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 25,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "1 tức là nó kích hoạt chỗ này là đáp số của mình f của -1 chính là bằng 0 như vậy output của mình lúc này sẽ là bằng 0 thì đây là một ví dụ chúng ta lan truyền chúng ta lan truyền giá trị của mình từ đầu cho đến cuối nhắc lại 1 lần nữa đó là x0 của mình là phải bằng 1 khi nói về bias thì x0 chắc chắn là bằng 1 chứ không thể nào bằng 0 là do có lỗi trong việc soạn slide nên cái này nó là bằng 0 thôi còn thực tế nó phải là luôn luôn là bằng 1",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=6spgb9hK5To",
      "filename": "6spgb9hK5To",
      "title": "[CS114 - Chương 8] Neural Network (Part 1)",
      "chunk_id": 26,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chúng ta sẽ cùng đến với một phần rất là quan trọng trong lập trình với các cái mô hình dựa trên gradient đó chính là vector hóa cái thuật toán gradient descent. Chúng ta sẽ cùng nhắc lại thuật toán Gradient Descent thì khởi tạo của chúng ta là sẽ có các cái tham số của mô hình là W và B. Trong đó W thì tương ứng là cái trọng số của cái đặc trưng đầu vào X B là cho cái thành phần là bias. Và dữ liệu đầu vào của chúng ta thì sẽ bao gồm là các cái mẫu dữ liệu là x1, x2 cho đến à x thứ m. Và tương",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:01:02"
    }
  },
  {
    "page_content": "mẫu dữ liệu là x1, x2 cho đến à x thứ m. Và tương tự như vậy thì y tức là cái nhãn đầu ra chúng ta cũng sẽ có i1, I2 cho đến im. Rồi thì khi chúng ta có được cái x và y chúng ta thế vào đây thì chúng ta sẽ tính được cái đạo hàm. À ở đây chúng ta lưu ý là cái chỉ số chúng ta sẽ không để dưới giống như trên hình ha. Và chúng ta để cái chỉ số ở trên thì cái cách viết của mình sẽ là x thứ nhất x thứ hai là mẫu dữ liệu thứ hai. Rồi mẫu dữ liệu thứ m. Rồi y thì cũng vậy. Y sẽ là mẫu dữ liệu thứ nhất,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 1,
      "start_timestamp": "0:00:55",
      "end_timestamp": "0:01:50"
    }
  },
  {
    "page_content": "Rồi y thì cũng vậy. Y sẽ là mẫu dữ liệu thứ nhất, thứ hai và thứ m. Thế thì ở đây chúng ta sẽ đặt câu hỏi là tại sao chúng ta không ký hiệu ở bên dưới để cho nó thuận tiện. Nó có lý do của nó đó là trong trường hợp mô hình này chúng ta mở rộng sang cái à đa biến tức là một cái đặc trưng đầu vào của chúng ta sẽ có nhiều biến đầu vào thì khi đó cái chỉ số bên dưới sẽ dùng để minh họa cho các cái đặc trưng của mình. Lấy ví dụ à mẫu dữ liệu thứ nhất chúng ta sẽ có đặc trưng thứ nhất của mẫu dữ liệu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 2,
      "start_timestamp": "0:01:47",
      "end_timestamp": "0:02:28"
    }
  },
  {
    "page_content": "chúng ta sẽ có đặc trưng thứ nhất của mẫu dữ liệu thứ nhất. Ví dụ như đây là diện tích của một căn nhà. Mẫu dữ liệu thứ nhất và cái đặc trưng thứ hai ví dụ như là cái số phòng ngủ rồi mẫu dữ liệu thứ nhất và cho cái mẫu đặc trưng thứ ba ví dụ như là chiều cao của tòa nhà. Ví dụ vậy thì đây chính là cái phần chỉ số bên dưới để dùng để đánh số cái số lượng đặc trưng số đặc trưng. Còn cái chỉ số ở phía trên đó là cái chỉ số của à thứ tự của mẫu. Thế thì bây giờ chúng ta sẽ tìm cách đơn giản hóa",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 3,
      "start_timestamp": "0:02:21",
      "end_timestamp": "0:03:15"
    }
  },
  {
    "page_content": "Thế thì bây giờ chúng ta sẽ tìm cách đơn giản hóa cái công thức này. Và ờ khi tính toán với cái dữ liệu mà đã được vector hóa thì cái thuật toán của mình nó cũng sẽ tính toán nhanh hơn. Vậy thì đầu tiên chúng ta sẽ xem xét ở cái không gian là đặc trưng trước. Nếu như chúng ta đặt theta nó sẽ bao gồm hai cái thành phần đó là W và B. Chúng ta gom nó lại thành một vector. Thì khi chúng ta tính toán chúng ta lấy cái thành phần trọng số của cái theta này chúng ta nhân với x nhân với x thì x của mình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 4,
      "start_timestamp": "0:03:10",
      "end_timestamp": "0:03:53"
    }
  },
  {
    "page_content": "này chúng ta nhân với x nhân với x thì x của mình nó cũng sẽ đi tương ứng. Ví dụ thành phần đầu tiên là W thì nó sẽ có là mẫu dữ liệu thứ thứ nhất và cái thành phần thứ hai chúng ta sẽ gán là 1. Tại sao là 1? Tại vì B ở đây là bias thì chúng ta xem như chúng ta có được cái đặc trưng mới và đặc trưng này cố định nó là 1 thì B nhân với 1 chính là bias w nhân với X thì khi đó theta nhân với X thì nó sẽ ra là wx 1 cộng cho thì chúng ta nhân cái theta chuyển vị thôi nhân với x thì nó sẽ ra cái công",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 5,
      "start_timestamp": "0:03:50",
      "end_timestamp": "0:04:42"
    }
  },
  {
    "page_content": "chuyển vị thôi nhân với x thì nó sẽ ra cái công thức như thế này. Và ở đây chúng ta sẽ không tính cho một mẫu dữ liệu X thứ nhất, một mẫu dữ liệu X thứ nhất mà chúng ta sẽ tính cho tất cả các cái mẫu dữ liệu của mình cho đến mẫu dữ liệu thứ m. Rồi và khi đó theta nhân x thì nó sẽ là một chuỗi à một vector các cái w x1 + b wx2 cộng b phẩy vân vân cho đến w x m cộng b. Rồi và khi đó thì chúng ta sẽ có cái vector này và chúng ta qua cái hàm sigmoid thì cái hàm sigmoid đó là một cái hàm mà nó sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 6,
      "start_timestamp": "0:04:33",
      "end_timestamp": "0:05:29"
    }
  },
  {
    "page_content": "thì cái hàm sigmoid đó là một cái hàm mà nó sẽ tính theo element-wise. Do đó thì à chúng ta sẽ tính toán hàm sigmoid trên từng cái phần tử của cái vector này. Như vậy thì cái công thức mà viết gọn lại của cái p của mình là cái xác suất là sẽ là bằng sigmoid của theta chuyển vị nhân x. Một cách tương tự đối với trường hợp mà cái biến số của mình nó có nhiều hơn một đặc trưng thì chúng ta cũng làm tương tự như vậy. Khi đó x của chúng ta nó sẽ là à mẫu dữ liệu thứ nhất và thành phần thứ nhất, đặc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 7,
      "start_timestamp": "0:05:23",
      "end_timestamp": "0:06:17"
    }
  },
  {
    "page_content": "mẫu dữ liệu thứ nhất và thành phần thứ nhất, đặc trưng thứ nhất, mẫu dữ liệu thứ nhất và thành phần thứ hai vân vân cho đến cái mẫu dữ liệu thứ nhất và thành phần thứ n. Rồi sau đó thì chúng ta sẽ có thêm cái bias và cứ như vậy cho đến cái mẫu dữ liệu thứ m thành phần đặc trưng thứ nhất. Mẫu dữ liệu thứ m thành phần đặc trưng thứ à thứ hai vân vân cho đến mẫu dữ liệu thứ m và thành phần đặc trưng thứ n. Và cuối cùng đó là bias. Thì khi chúng ta đưa về cái dạng như thế này thì khi chúng ta tính",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 8,
      "start_timestamp": "0:06:11",
      "end_timestamp": "0:07:02"
    }
  },
  {
    "page_content": "đưa về cái dạng như thế này thì khi chúng ta tính toán thì nó sẽ được tính hàng loạt. Và theta lúc này thì nó sẽ bao gồm là theta là bằng W1, W2 cho đến W thứ N. Và thành phần cuối cùng đó là bias. Rồi thì khi đó cái dự đoán cái xác suất dự đoán của chúng ta nó sẽ là một cái có công thức đó là sigmoid của theta chuyển vị nhân với x. Thì khi chúng ta lấy theta chuyển vị nhân x thì chúng ta sẽ lấy thành phần w1 nhân với lại cái x11 thành phần w2 nhân với x12 thành phần thứ n nhân với x1n và b",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 9,
      "start_timestamp": "0:06:53",
      "end_timestamp": "0:07:39"
    }
  },
  {
    "page_content": "nhân với x12 thành phần thứ n nhân với x1n và b nhân với 1. Rồi thì bây giờ chúng ta sẽ thử à lập trình cho cái à công thức mà mới này ha. Thì ở đây chúng ta sẽ có một cái Logistic Regression phiên bản vectorize. Và cái code ở bên đây thì chúng ta đã được copy nguyên vẹn từ cái code logistic cũ. Chúng ta copy qua đây. Rồi thì để cho nó đỡ rối chúng ta sẽ tắt cái kia đi. Và bây giờ chúng ta sẽ gom nhóm đầu tiên đó là cái tham số. Thì chúng ta sẽ tạo ra một cái biến theta là bằng np.array. Ừ. Và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 10,
      "start_timestamp": "0:07:34",
      "end_timestamp": "0:08:27"
    }
  },
  {
    "page_content": "tạo ra một cái biến theta là bằng np.array. Ừ. Và vì nó là dạng cột đúng không nên chúng ta sẽ có thành phần W1 thì nó là bằng 12. Rồi thành phần tiếp theo là 34. Còn nếu không chúng ta cũng có thể dùng một cái hàm random. Nhưng mà ở đây chúng ta muốn minh họa cái cách chuyển đổi code từ phiên bản trước sang phiên bản sau là dùng vector nên chúng ta dùng chính những cái con số này để cho chúng ta dễ theo dõi. Rồi và khi đó thì cái W1 W2 chúng ta sẽ bỏ đi nhưng mà chúng ta sẽ để cái comment để",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 11,
      "start_timestamp": "0:08:22",
      "end_timestamp": "0:09:08"
    }
  },
  {
    "page_content": "sẽ bỏ đi nhưng mà chúng ta sẽ để cái comment để chúng ta có thể đối chiếu. Sau khi chúng ta đã tạo xong cái theta ở đây chúng ta để thêm dấu chấm để cho nó đưa về dạng số thực ha. Tiếp theo thì chúng ta sẽ tạo cái biến X. Thế thì X của mình nó sẽ bao gồm à hai thành phần đó là X R và X à red. Và ở đây thì chúng ta đã có một cái bước là chúng ta sẽ trộn hai cái này lại là concatenate như thế này. Nhưng mà khi chúng ta concatenate lại á thì nó sẽ thiếu một cái thành phần đó là bias. Do đó chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 12,
      "start_timestamp": "0:09:03",
      "end_timestamp": "0:09:53"
    }
  },
  {
    "page_content": "thiếu một cái thành phần đó là bias. Do đó chúng ta sẽ bổ sung thêm cái bias phía sau thì x sẽ là bằng np.concatenate của cái x cũ. Số phần tử của mình thì sẽ là bằng ờ tổng số mẫu của cái x cũ ha là X.shape. Rồi và X.shape thì ở đây chúng ta sẽ lấy số hàng à số cột của mình số cột do đó thì sẽ là để là 1 và axis ở đây sẽ là bằng 0 tại vì chúng ta trồng lên nhau chứ không chúng ta không có nối chúng ta trồng lên rồi thì để kiểm chứng xem à cái giá trị này nó như thế nào thì chúng ta sẽ in lại x",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 13,
      "start_timestamp": "0:09:48",
      "end_timestamp": "0:10:59"
    }
  },
  {
    "page_content": "trị này nó như thế nào thì chúng ta sẽ in lại x và chúng ta sẽ in ra y để xem coi các cái giá trị nó ra đúng như chúng ta mong muốn hay không. Tương tự như vậy thì chúng ta cũng sẽ in ra theta. Rồi thì ở đây nó báo lỗi ở cái code chỗ này. Rồi chúng ta sẽ xem cái X ở trên nó ra cái gì. Và cái np.ones nó ra cái gì để mình xem xem là nó có concatenate được hay không. Rồi X thì chúng ta sẽ thấy đó là một cái ma trận gồm có hai hàng và rất nhiều cột ở đây ha. Hai hàng và rất nhiều cột. Còn ones à",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 14,
      "start_timestamp": "0:10:52",
      "end_timestamp": "0:12:24"
    }
  },
  {
    "page_content": "ở đây ha. Hai hàng và rất nhiều cột. Còn ones à thì nó là một cái vector. Đó là một cái vector. Do đó để mà trồng được thì chúng ta sẽ phải đưa cái ones này vào bên trong một cái array. Còn không thì chúng ta có thể tạo ra là thành một cái tuple. Chúng ta sẽ truyền vào một cái tuple rồi 1 và số cột của x thì nó sẽ hiểu rằng chúng ta đang tạo ra một cái ma trận thay vì chúng ta tạo một vector. Rồi bây giờ chúng ta sẽ kiểm tra lại xem cái lỗi nó nằm ở đâu. Rồi thì lỗi tiếp tục nó nằm ở trong cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 15,
      "start_timestamp": "0:12:17",
      "end_timestamp": "0:13:12"
    }
  },
  {
    "page_content": "ở đâu. Rồi thì lỗi tiếp tục nó nằm ở trong cái hàm concatenate này. Cannot interpret 10 data. Rồi bây giờ chúng ta sẽ xem lại ha. print X và in cái ones array này. Rồi. Ok. Thì cái hàm np.ones nó đã không có chạy được. Lý do đó là vì ờ ở đây chúng ta truyền vào shape đúng không? Ta phải để là cái dấu ngoặc đơn một và một cái cặp như thế này. Thì ở đây do có quá nhiều dấu ngoặc nên chúng ta dễ bị nhầm. Thì đây sẽ là một cái tuple. Đây sẽ là một cái tuple. của số 1 và số cột của mình. Rồi sau đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 16,
      "start_timestamp": "0:13:05",
      "end_timestamp": "0:13:56"
    }
  },
  {
    "page_content": "tuple. của số 1 và số cột của mình. Rồi sau đó cái thành phần thứ hai là của cái hàm np.ones. Và thành phần cuối cùng sẽ là cái à hàm print. Chúng ta sẽ in lại. Ok, đó là đúng rồi. Bây giờ chúng ta sẽ kiểm tra lại trong cái code ở đây. np.ones thì ở đây chúng ta chưa bọc nó lại ha. Bọc cái một và cái shape. Rồi thành phần thứ hai là hàm ones và thành phần thứ ba đó là chúng ta sẽ gom cái x và cái np.ones này lại. Và bây giờ chúng ta sẽ chạy lại. Như vậy là x của chúng ta đã ra đúng rồi. Thành",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 17,
      "start_timestamp": "0:13:50",
      "end_timestamp": "0:14:39"
    }
  },
  {
    "page_content": "Như vậy là x của chúng ta đã ra đúng rồi. Thành phần đầu tiên x1, thành phần thứ hai x2 và thành phần thứ ba là các cái con số 1 là bias. Y của chúng ta thì thành phần 0 là chiếm đa số là vì nó có gấp 10 lần so với lại thành phần là cái nhãn một. À ở đây cũng đã có gắn cái bias và hai cái trọng số của x1 và x2. Tiếp theo thì chúng ta sẽ tiến hành à cài đặt cái thuật toán gradient descent. thì cái công thức của mình nó sẽ không còn phức tạp như thế này mà nó chỉ đơn giản đó là nabla là bằng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 18,
      "start_timestamp": "0:14:33",
      "end_timestamp": "0:15:26"
    }
  },
  {
    "page_content": "thế này mà nó chỉ đơn giản đó là nabla là bằng np.mean của chúng ta sẽ lấy cái sigmoid của theta chuyển vị (là .T ha). Sau đó chúng ta nhân với lại và chúng ta dùng cái np.dot với lại cái ma trận X của mình ở trên đây. sigmoid của theta chuyển vị (là .T ha). Sau đó chúng ta nhân với lại và chúng ta dùng cái np.dot với lại cái ma trận X của mình ở trên đây. Rồi sau đó thì chúng ta sẽ trừ cho y. Rồi ở đây thì chúng ta sẽ tính sigmoid xong đúng không? Thì chúng ta sẽ trừ cho y. Và sau khi trừ cho",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 19,
      "start_timestamp": "0:15:16",
      "end_timestamp": "0:16:13"
    }
  },
  {
    "page_content": "Thì chúng ta sẽ trừ cho y. Và sau khi trừ cho y xong thì chúng ta không quên chúng ta nhân với lại cái à X của mình (chuyển vị) nhân với cái X của mình. Rồi sau đó là tính mean thì lúc này nabla theo theta nó sẽ là có cái công thức như thế này. Và bây giờ thì tương tự như vậy công thức cập nhật của mình cũng sẽ gọn gàng hơn. Chúng ta không phải tách ra làm nhiều phần như thế này mà theta là bằng theta. trừ cho alpha nhân với lại nabla. Rồi và ở bên dưới thì chúng ta cũng sẽ đi tính cái W0, W1,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 20,
      "start_timestamp": "0:16:09",
      "end_timestamp": "0:17:09"
    }
  },
  {
    "page_content": "bên dưới thì chúng ta cũng sẽ đi tính cái W0, W1, W2 và B. Thì W0 à W1 của chúng ta đó chính là cái theta mà phần tử đầu tiên. Rồi, W2 thì sẽ là thành phần theta theta 1. W1 thì là theta0 còn W2 thì sẽ là theta 1. Còn thành phần bias thì nó sẽ là thành phần số hai của theta là thành phần cuối cùng. Thì đương nhiên cái cách code như thế này thì nó sẽ hơi rối đối trong cái lúc mà chúng ta bóc tách cái tham số ra. Nhưng mà đây chỉ là phục vụ cho cái việc trực quan hóa nên lâu lâu chúng ta mới sử",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 21,
      "start_timestamp": "0:17:02",
      "end_timestamp": "0:17:48"
    }
  },
  {
    "page_content": "việc trực quan hóa nên lâu lâu chúng ta mới sử dụng thôi. Còn ở trên thì chúng ta sẽ phải dùng thường xuyên. À và lúc này thì chúng ta sẽ không còn cái đạo hàm mà chúng ta sẽ có nabla. Thế thì thôi để đơn giản chúng ta sẽ không để cái chữ nabla theta nữa và chúng ta chỉ để nabla thôi. Rồi thì nabla mà thành phần đầu tiên. Rồi thành phần thứ hai, thành phần đầu tiên là 0, thành phần thứ hai đó là 1 thành phần thứ hai đó là 1 và thành phần thứ ba đó là 2. Rồi bây giờ chúng ta sẽ kiểm tra code này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 22,
      "start_timestamp": "0:17:46",
      "end_timestamp": "0:17:48"
    }
  },
  {
    "page_content": "là 2. Rồi bây giờ chúng ta sẽ kiểm tra code này lại một lần nữa và chúng ta sẽ thực thi. Thì ở đây là nó đang có cái à lỗi size trong cái việc mà tính cái giá trị của mình thì là index error là 0, 1 và 2. Thì bây giờ chúng ta sẽ kiểm tra xem cái theta của mình à cái nabla của mình đó là cái kích thước của nó là bằng bao nhiêu thì chúng ta sẽ in nabla chấm shape. Rồi thì nabla chấm shape thì nó lại ra là một cái con số scalar. Thì đây là vô lý tại vì lẽ ra chúng ta kỳ vọng nó phải ra một cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 23,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "tại vì lẽ ra chúng ta kỳ vọng nó phải ra một cái vector có kích thước giống với lại kích thước của theta thì khi đó chúng ta mới có thể trừ được.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=7IdTMLLCPxw",
      "filename": "7IdTMLLCPxw",
      "title": "[CS114 - Chương 4] Vector hóa (Phần 1)",
      "chunk_id": 24,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề, học sâu, machine learning, ImageNet. Chủ đề, học sâu, machine learning, ImageNet. cho bài toán hồi quy. Quá trình sẽ gồm hai bước. Đầu tiên, chúng ta sẽ chia không gian đặc trưng. Không gian đặc trưng ở đây là tập hợp các giá trị có thể của x1, x2, xp hay còn gọi là các cột đặc trưng thành G vùng riêng biệt R1, R2 và Rg. R, ở đây là viết tắt của region Tiếp theo, với mỗi mẫu trong vùng R_g chúng ta đưa ra một giá trị dự đoán cho tất cả các mẫu thuộc về vùng đó chính là giá trị trung bình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:22"
    }
  },
  {
    "page_content": "mẫu thuộc về vùng đó chính là giá trị trung bình của các giá trị phản hồi cho các mẫu huấn luyện trong vùng R_g Giá trị phản hồi ở đây thực ra là giá trị hồi quy các bạn. Như vậy thì bây giờ chúng ta sẽ quan tâm quá trình phân tầng không gian đặc trưng nó sẽ diễn ra như thế nào và là thế nào chúng ta sẽ xây dựng các vùng R1, R2 và R_g. Đầu tiên chúng ta sẽ quan tâm đặc điểm của các vùng này Các vùng này sẽ được giới hạn trong các vùng có dạng hình chữ nhật và song song với các trục hay còn được",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 1,
      "start_timestamp": "0:01:14",
      "end_timestamp": "0:02:20"
    }
  },
  {
    "page_content": "chữ nhật và song song với các trục hay còn được gọi là các đặc trưng sau khi chúng ta quan tâm tới đặc điểm của các vùng, bước tiếp theo chúng ta sẽ đi tìm mục tiêu của bài toán đó là chúng ta sẽ cố gắng giảm thiểu giá trị Residual Sum Square Giả sử chúng ta sẽ có G vùng và chúng ta sẽ đi duyệt từng vùng và ứng với mỗi vùng, chúng ta sẽ đi duyệt từng mẫu trong vùng đó và chúng ta sẽ đi tính độ lỗi của giá trị dự đoán Y_i là giá trị thật của mẫu thứ i Y_i dự đoán cho vùng G được định nghĩa như",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 2,
      "start_timestamp": "0:02:08",
      "end_timestamp": "0:03:31"
    }
  },
  {
    "page_content": "thứ i Y_i dự đoán cho vùng G được định nghĩa như sau Ở đây để dễ hình dung hơn thì giả sử chúng ta sẽ có 1 cái cây có 3 vùng Chúng ta sẽ tưởng tượng sẽ có 1 cái cây như thế này Chúng ta có 3 vùng, ở đây thì chỉ số G sẽ chạy Chúng ta sẽ có vùng thứ nhất, vùng thứ 2 và vùng thứ 3 Trong này có 3 điểm dữ liệu Y của chúng ta sẽ chạy trong 3 mẫu này Ở đây chúng ta chỉ có 2 mẫu và ở đây chúng ta có 4 mẫu G bằng 1, G bằng 2, và G bằng 3 như vậy thì chúng ta có thể lưu ý rằng G của chúng ta sẽ tương",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 3,
      "start_timestamp": "0:03:27",
      "end_timestamp": "0:04:44"
    }
  },
  {
    "page_content": "ta có thể lưu ý rằng G của chúng ta sẽ tương đương với số nút lá của cái cây quyết định trong trường hợp này là cho bài toán hồi quy như vậy thì chúng ta sẽ có rất nhiều cách để chia không gian đặc trưng ban đầu thành các vùng về mặt lý thuyết thì chúng ta có thể tìm ra tất cả các tổ hợp các vùng sau đó chúng ta sẽ chọn Cái cách phân vùng mà sao cho chúng ta sẽ có độ lỗi thấp nhất. Tuy nhiên, việc này là không thể. Trong thực tế, để thay thế cho thuật toán vét cạn thì người ta sẽ sử dụng thuật",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 4,
      "start_timestamp": "0:04:39",
      "end_timestamp": "0:05:31"
    }
  },
  {
    "page_content": "thuật toán vét cạn thì người ta sẽ sử dụng thuật toán tham lam để đi giải quyết bài toán phân vùng. Như vậy, chúng ta có thể thấy rằng, việc tìm ra tất cả các tổ hợp, các vùng quyết định, hay nói cách khác là tìm ra tất cả các cây quyết định cho bài toán hồi quy là điều không thể. Như vậy, chúng ta sẽ tìm một hướng giải quyết khác. Đơn giản hơn, chúng ta sẽ tìm ra một lời giải không phải là hoàn hảo, tuy nhiên nhưng mang lại kết quả tốt. Đó là thuật toán tham lam. Ý tưởng của thuật toán này là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 5,
      "start_timestamp": "0:05:17",
      "end_timestamp": "0:06:18"
    }
  },
  {
    "page_content": "toán tham lam. Ý tưởng của thuật toán này là phân chia nhị phân đệ quy. Chúng ta sẽ tiếp cận phân chia vùng từ trên xuống. Cụ thể là, đầu tiên chúng ta sẽ bắt đầu từ đỉnh của cây quyết định. Tất cả các điểm đều thuộc về một vùng duy nhất. Có nghĩa là tất cả các điểm trong bộ dữ liệu được hồi quy với một giá trị duy nhất nếu như chúng ta dừng lại thuật toán tại đây. Nếu không, tiếp tục chúng ta sẽ tìm biến để phân chia hay nói cách khác là chúng ta sẽ đi lặp tất cả các cột đặc trưng trong dữ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 6,
      "start_timestamp": "0:06:06",
      "end_timestamp": "0:07:12"
    }
  },
  {
    "page_content": "ta sẽ đi lặp tất cả các cột đặc trưng trong dữ liệu đầu vào. Ứng với mỗi cột đặc trưng như vậy, chúng ta sẽ tìm ra một điểm phân chia x, hay còn được viết tắt là split point Đây là một cái từ mà chúng ta sẽ thấy quen thuộc trên các trang hướng dẫn của các diễn đàn về khoa học máy tính Và chúng ta sẽ đi tìm điểm x sao cho giảm thiểu ResidualSumSquare nhiều nhất so với vùng ban đầu Để dễ hình dung hơn, giả sử ban đầu tập dữ liệu của chúng ta sẽ gồm trên 2 điểm Và chúng ta sẽ chia thành 2 vùng dữ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 7,
      "start_timestamp": "0:06:59",
      "end_timestamp": "0:08:13"
    }
  },
  {
    "page_content": "trên 2 điểm Và chúng ta sẽ chia thành 2 vùng dữ liệu Giả sử đây chúng ta sẽ split theo hướng này Thì chúng ta sẽ có một số điểm bên trái và một số điểm bên phải Và độ giảm giá trị lỗi sẽ là nhiều nhất khi chúng ta chọn các split này Tiếp theo, chúng ta sẽ tiếp tục lặp lại cho từng vùng mà chúng ta đã chia tại bước đầu tiên Tương tự cách ở bước 2 cho đến khi chúng ta sẽ thỏa mãn tiêu chí dừng Bởi vậy đây là một thuật toán lặp cho nên chúng ta sẽ phải định nghĩa ra một tiêu chí dừng Một điểm đáng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 8,
      "start_timestamp": "0:08:05",
      "end_timestamp": "0:09:02"
    }
  },
  {
    "page_content": "định nghĩa ra một tiêu chí dừng Một điểm đáng lưu ý ở đây là thuật toán tham lam sẽ không nhìn về phía trước để xem xét các bước tiếp theo có nghĩa là khi mà chúng ta phân chia các vùng ở bước này thì chúng ta không còn quan tâm đến kết quả của bước trước nữa chúng ta coi như đây là một bộ dữ liệu mới Ở các slide trước, chúng ta có đề cập đến việc các vùng của chúng ta sẽ là các hộp hình chữ nhật và song song với các trục Ở đây, chúng ta sẽ có hai cách chia, hay là hai cách phân các vùng, cách",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 9,
      "start_timestamp": "0:08:52",
      "end_timestamp": "0:10:01"
    }
  },
  {
    "page_content": "cách chia, hay là hai cách phân các vùng, cách bên trái và cách bên phải Ở đây chúng ta có thể thấy rằng các vùng bên trái không thể được thực hiện bởi thuật toán mà chúng ta đã đề ra Ứng với thuật toán mà chúng ta đã mô tả trong các phần trước thì tại mỗi bước chúng ta sẽ phân chia dữ liệu thành 2 vùng Như vậy, chúng ta có thể hình dung là, giả sử ban đầu chúng ta sẽ có không gian đặc trưng gốc gồm x2 và x1 Tại bước đầu tiên, chúng ta sẽ đi lặp tất cả các điểm trong đặc trưng x1 sau đó chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 10,
      "start_timestamp": "0:09:51",
      "end_timestamp": "0:10:49"
    }
  },
  {
    "page_content": "tất cả các điểm trong đặc trưng x1 sau đó chúng ta sẽ chọn ra điểm split point có độ lỗi giảm nhiều nhất tương tự như vậy chúng ta sẽ đi lặp tất cả các điểm split point trong đặc trưng x2 và chúng ta chọn ra một điểm có độ giảm lỗi nhiều nhất và chúng ta sẽ coi hai giá trị giảm lỗi trong x1 và x2 bên nào có độ giảm lỗi cao nhất thì chúng ta sẽ tiến hành split như vậy giả sử đây chúng ta sẽ có giá trị giảm lỗi trong x1 là nhiều nhất và chúng ta sẽ tiến hành phân chia vùng chúng ta chọn một điểm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 11,
      "start_timestamp": "0:10:44",
      "end_timestamp": "0:12:04"
    }
  },
  {
    "page_content": "tiến hành phân chia vùng chúng ta chọn một điểm split point trong x1 và chúng ta sẽ tiến hành phân vùng như vậy chúng ta có thể thấy rằng ít nhất sẽ có một đường thẳng mà nó vuông góc với một trục đặc trưng ở đây thì chúng ta sẽ để ý rằng trong cách phân vùng bên trái không có đường thẳng nào mà cắt toàn bộ một vùng như trong ví dụ mà chúng ta vừa minh họa chúng ta có thể thấy rằng ở cách phân vùng thứ 2 Tiếp theo, chúng ta sẽ có vùng bên trái và vùng bên phải thì ở đây chúng ta sẽ có thể chia",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 12,
      "start_timestamp": "0:11:57",
      "end_timestamp": "0:13:04"
    }
  },
  {
    "page_content": "vùng bên phải thì ở đây chúng ta sẽ có thể chia theo cách này chúng ta coi như đây là một bộ dữ liệu độc lập chỉ còn R2 và R1 Tiếp theo, chúng ta coi phần dữ liệu bên phải là phần dữ liệu độc lập thì chúng ta có thể split thành 2 phần và tiếp theo chúng ta có thể coi cái phần còn lại là một bộ dữ liệu độc lập và chúng ta sẽ split và đây là một cách phân vùng hợp lệ Trong các slide trước, đặc biệt là thuật toán xây dựng cây quyết định hồi quy, chúng ta có đề cập đến khái niệm tiêu chí dừng. Đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 13,
      "start_timestamp": "0:12:51",
      "end_timestamp": "0:13:47"
    }
  },
  {
    "page_content": "ta có đề cập đến khái niệm tiêu chí dừng. Đây là một cái điểm quan trọng để giúp chúng ta dừng thuật toán. Về mặt lý thuyết, chúng ta sẽ dừng lại khi mà chúng ta phân chia các vùng, khi mà trong vùng đó chỉ còn một điểm dữ liệu duy nhất. Ứng với tình huống đó thì chúng ta sẽ tạo ra một cái cây quyết định rất là lớn và nó sẽ tốn về mặt chi phí tính toán. Thực tế thì nó có thể không tổng quát tốt cho dữ liệu mới. Tại vì chúng ta có thể hình dung rằng bản chất của bài toán cây quyết định thì tại",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 14,
      "start_timestamp": "0:13:39",
      "end_timestamp": "0:14:35"
    }
  },
  {
    "page_content": "rằng bản chất của bài toán cây quyết định thì tại mỗi nút lá, việc giá trị dự đoán nó sẽ có nghĩa là tất cả các điểm dữ liệu thuộc về một vùng. Nó sẽ được dự đoán bằng giá trị trung bình tất cả các mẫu dữ liệu thuộc về vùng đó. Hay nói cách khác, thì chúng ta dự đoán giá trị của một mẫu dữ liệu mới nó sẽ là giá trị trung bình của những mẫu dữ liệu mà có đặc trưng tương đồng với nó nhất. Thì tiêu chí dừng ở đây, có thể là chúng ta sẽ giới hạn kích thước của cây bằng cách kiểm soát hiện tượng quá",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 15,
      "start_timestamp": "0:14:28",
      "end_timestamp": "0:15:49"
    }
  },
  {
    "page_content": "thước của cây bằng cách kiểm soát hiện tượng quá khớp. Hiện nay, các tiêu chí phổ biến để định nghĩa tiêu chí dừng cho cây quyết định bao gồm số lượng mẫu trong một nút lá nhỏ hơn một số nhất định thì chúng ta sẽ dừng độ tinh khiết của nút lớn hơn một số nhất định Thực ra chúng ta sẽ có khái niệm về độ tinh khiết trong phần xây dựng cây quyết định cho bài toán phân loại Giả sử chúng ta sẽ có một dữ liệu phân loại gồm 3 mẫu thuộc lớp O và 1 mẫu thuộc lớp X Đây là bộ dữ liệu thứ nhất ví dụ Giả sử",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 16,
      "start_timestamp": "0:15:40",
      "end_timestamp": "0:16:53"
    }
  },
  {
    "page_content": "lớp X Đây là bộ dữ liệu thứ nhất ví dụ Giả sử chúng ta sẽ tiếp tục có một bộ dữ liệu thứ 2 D2 gồm 4 mẫu dữ liệu thuộc về lớp O thì chúng ta có thể thấy là giả sử chúng ta coi O như là nước cất và X ở đây là 1 hạt sạn đi Chúng ta có thể nói rằng bộ dữ liệu điểm 1 này chưa thuần khiết nước hoàn toàn, còn bộ dữ liệu thứ 2 này được coi là nước thuần khiết. Có nghĩa là nó sẽ không chứa các tạp chất khác. Và trong bài toán này, chúng ta sẽ nói rằng một bộ dữ liệu hay một vùng nó thuần khiết một lớp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 17,
      "start_timestamp": "0:16:45",
      "end_timestamp": "0:17:44"
    }
  },
  {
    "page_content": "bộ dữ liệu hay một vùng nó thuần khiết một lớp nào đó Chẳng hạn như chúng ta sẽ có một bộ D3, nó gồm 3 mẫu X, thì chúng ta có thể nói nó thuần khiết về một lớp X Ngoài ra thì tiêu chí dừng của chúng ta sẽ có thể định nghĩa bằng độ sâu của 1 cây Nếu như 1 cây mà có độ sâu quá lớn thì nó có thể là một hiện tượng quá khớp và khi mà trong tình huống tất cả các giá trị phản hồi ở đây là chúng ta sẽ hiểu là giá trị hồi quy đều giống hệt nhau hả? có nghĩa là một cái cây hồi quy như thế này mà tất cả",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 18,
      "start_timestamp": "0:17:41",
      "end_timestamp": "0:18:24"
    }
  },
  {
    "page_content": "là một cái cây hồi quy như thế này mà tất cả giá trị mà dự đoán cho bài toán hồi quy á ở đây nó giống nhau luôn v1 v1 v1 thì ở đây chúng ta có thể hình dung là giống như là trong cái môn nhập môn lập trình đó việc chúng ta viết các cái lệnh if cái lệnh điều kiện đó các bạn nhưng mà trong cái cây quyết định trong cái hàng cuối cùng á thì tất cả các cái công việc nó đều như nhau thì thôi chúng ta khỏi viết lệnh if thì còn hơn Và tương tự như vậy, trong tình huống này, tất cả các giá trị phản hồi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 19,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "trong tình huống này, tất cả các giá trị phản hồi đều giống như hệt nhau thì thôi chúng ta sẽ chỉ cần tạo ra một cái cây mà nó chỉ có một nút thôi Nói cách khác, thì chúng ta thà không phân chia các vùng còn hơn Chỉ đơn thuần là chúng ta kết luận là bộ dữ liệu này chỉ có một giá trị phản hồi mà thôi Hãy subscribe cho kênh Ghiền Mì Gõ để không bỏ lỡ những video hấp dẫn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8b8OKHIktqI",
      "filename": "8b8OKHIktqI",
      "title": "[CS114 - Chương 8] Decision Tree (Part 2)",
      "chunk_id": 20,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Tiếp theo thì chúng ta sẽ cùng à minh họa cho cái tình huống đó là dữ liệu của mình nó bị mất cân bằng. Thì ở đây chúng ta giả sử cái tập dữ liệu red của mình nó sẽ gấp nhiều lần so với lại tập dữ liệu green. Thì ở đây số mẫu thay vì chúng ta đánh đồng là đều n sample. Thì ở đây chúng ta sẽ cho à ví dụ như là 10 nhân cho n sample và y tương tự như vậy cũng sẽ là 10 nhân cho tổng số sample. Rồi bây giờ chúng ta sẽ chạy thử cái code này. Chúng ta thấy là cái đám điểm màu đỏ nó dày đặc đó. Rất là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8KfV0kYAUxk",
      "filename": "8KfV0kYAUxk",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 2)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:55"
    }
  },
  {
    "page_content": "thấy là cái đám điểm màu đỏ nó dày đặc đó. Rất là dày đặc. Rồi và các cái điểm màu xanh lá thì nó sẽ thưa hơn. Và chúng ta sẽ thấy là cái đường của mình á nó càng lúc là nó sẽ tiến về cái khu vực mà có thể tách ra làm hai phần. Rồi bây giờ chúng ta sẽ cùng à chờ đợi một lúc nữa để xem coi là cái à đường thẳng của mình nó sẽ dừng ở đâu. Nhưng mà ở trên hình chúng ta cũng có thể thấy sơ bộ được ha. Đó có thể thấy sơ bộ được. Đó là ờ cái tập điểm màu đỏ nó có một cái khoảng cách khá là xa à khá là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8KfV0kYAUxk",
      "filename": "8KfV0kYAUxk",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 2)",
      "chunk_id": 1,
      "start_timestamp": "0:00:49",
      "end_timestamp": "0:02:00"
    }
  },
  {
    "page_content": "đỏ nó có một cái khoảng cách khá là xa à khá là xa so với lại các cái tập điểm đó. Thì ở đây nó sẽ khá là xa. Còn các cái điểm màu xanh thì chúng ta thấy là cái khoảng cách nó ngắn hơn. Thì điều này cho thấy đó là cái lực đẩy của số lượng điểm mà gấp 10 lần nó sẽ khiến cho cái mô hình của mình bị lùi về bị lùi về phía của những cái tập điểm ít hơn. Còn những cái điểm ít hơn cái lực đẩy của mình nó ít nên đâm ra là nó sẽ bị kéo về phía của nó. Đó thì với cái ví dụ này chúng ta hoàn toàn có thể à",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8KfV0kYAUxk",
      "filename": "8KfV0kYAUxk",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 2)",
      "chunk_id": 2,
      "start_timestamp": "0:01:56",
      "end_timestamp": "0:02:49"
    }
  },
  {
    "page_content": "thì với cái ví dụ này chúng ta hoàn toàn có thể à thấy được cái tình huống đó là nếu như có một cái điểm nào đó mà ở gần cái khu vực này ví dụ có một cái điểm hình tròn ở đây thì rõ ràng chúng ta thấy với cái cách chúng ta sử dụng cái đường phân lớp này thì à cái điểm hình tròn này nó sẽ biến thành màu đỏ. Nhưng thực tế thì chúng ta thấy là nó xứng đáng là hướng về cái điểm màu xanh đúng không? Nó xứng đáng thuộc về cái bề màu xanh tại vì nó gần các cái điểm màu xanh hơn. Thì đó chính là cái à",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8KfV0kYAUxk",
      "filename": "8KfV0kYAUxk",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 2)",
      "chunk_id": 3,
      "start_timestamp": "0:02:42",
      "end_timestamp": "0:03:24"
    }
  },
  {
    "page_content": "các cái điểm màu xanh hơn. Thì đó chính là cái à tình huống mà khi cái dữ liệu của mình nó bị bất cân xứng nó sẽ khiến cho cái mô hình của mình nó sẽ bị bias vào các cái à quyết định. Cụ thể ở đây đó là bias vào cái tập màu đỏ. Tại vì khi nó tách nó bị đẩy lùi ra khỏi các điểm màu đỏ thì khi đó những cái điểm như thế này xác suất cao là nó sẽ kéo về cái điểm màu đỏ luôn để tạo ra cái sự gọi là bias. Sự bias của các cái điểm màu đỏ. Hay nói một cách nôm na đó là nếu như cái số điểm màu đỏ lớn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8KfV0kYAUxk",
      "filename": "8KfV0kYAUxk",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 2)",
      "chunk_id": 4,
      "start_timestamp": "0:03:19",
      "end_timestamp": "0:03:59"
    }
  },
  {
    "page_content": "cách nôm na đó là nếu như cái số điểm màu đỏ lớn hơn cái số điểm màu xanh thì khi chúng ta đưa ra cái dự đoán á thì một cách nhắm mắt chúng ta có thể dự đoán luôn cái điểm của mình đó là điểm màu đỏ tại vì xác suất của nó cao hơn cái điểm màu xanh nên khi chúng ta chọn cái điểm này là điểm màu đỏ thì khả năng chúng ta sẽ đạt được cái độ chính xác là cao. Tuy nhiên cái cách làm như vậy thì chúng ta thấy những cái điểm này mà được gán vào cái nhãn màu đỏ thì trong thực tế trong thực tế cái đường",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8KfV0kYAUxk",
      "filename": "8KfV0kYAUxk",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 2)",
      "chunk_id": 5,
      "start_timestamp": "0:03:55",
      "end_timestamp": "0:04:39"
    }
  },
  {
    "page_content": "màu đỏ thì trong thực tế trong thực tế cái đường thẳng để phân lớp á thì lẽ ra nó phải là đường như thế này. đó. Lẽ ra nó phải là đường như thế này thì chúng ta lại đi đẩy cái điểm của mình ra xa nó cái đường này ra và nó hướng về cái điểm màu xanh khiến cho cái việc mà ờ thử nghiệm khi chúng ta test thì cái độ chính xác của cái đường này nó sẽ kém. Còn cái đường nét đứt này nó chia ra đều hơn đó thì nó sẽ hiệu quả hơn. Rồi trong cái sơ đồ này chúng ta cũng thấy tương tự như vậy sau khi đã chạy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8KfV0kYAUxk",
      "filename": "8KfV0kYAUxk",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 2)",
      "chunk_id": 6,
      "start_timestamp": "0:04:33",
      "end_timestamp": "0:05:27"
    }
  },
  {
    "page_content": "ta cũng thấy tương tự như vậy sau khi đã chạy hội tụ xong thì chúng ta sẽ thấy là cái khoảng không ở bên đây à nó sẽ rộng rãi hơn. Còn ở bên đây cái khoảng không của mình à nó hẹp hơn. thì dẫn đến đó là cái mô hình của mình nó sẽ thiên hướng là bias để đẩy ra xa khỏi cái điểm màu đỏ để cho cái độ chính xác khi chúng ta dự đoán trên cái điểm màu đỏ là cao hơn tại vì nó chiếm số đông.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8KfV0kYAUxk",
      "filename": "8KfV0kYAUxk",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 2)",
      "chunk_id": 7,
      "start_timestamp": "0:05:21",
      "end_timestamp": "0:05:39"
    }
  },
  {
    "page_content": "Chúng ta tiếp tục nói về ứng dụng đơn giản minh họa cho bài toán image classification trong machine learning. Thì các bạn đã biết ha, nếu như các bạn sử dụng Google Collab mà xài CPU để chạy cái ấy mà ăn nguồn này thì nó sẽ chạy khá là chậm ha. Tại vì Google Collab cung cấp cho các bạn chỉ có hai core CPU ừ với một cấu hình không phải là mạnh lắm. Thì để cho cái thời gian training à trong machine learning cái khâu lâu nhất thì thường luôn sẽ là khâu training. Đó để chúng ta muốn kéo cái thời",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8VA4C-TS2cQ",
      "filename": "8VA4C-TS2cQ",
      "title": "[CS114 - Tutorial] Google Colab (Phần 4)",
      "chunk_id": 0,
      "start_timestamp": "0:00:15",
      "end_timestamp": "0:00:56"
    }
  },
  {
    "page_content": "khâu training. Đó để chúng ta muốn kéo cái thời gian training với cái ví dụ đơn giản này từ 2 tiếng đồng hồ xuống càng nhanh càng tốt thì các bạn có thể sử dụng GPU mà Google Collab cung cấp sẵn. Hoặc ừ ở đây chúng ta sẽ thử minh họa nếu như các bạn có một cái CPU hiện đại hơn ha. Ví dụ như CPU trên máy tính của các bạn đ cũng có thể xem là mạnh hơn đáng kể so với CPU ảo mà Google Collab cung cấp rồi. Thì để cho các bạn có một sự liên hệ ở đây, chúng ta sẽ thử chạy cái code này trên một con CPU",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8VA4C-TS2cQ",
      "filename": "8VA4C-TS2cQ",
      "title": "[CS114 - Tutorial] Google Colab (Phần 4)",
      "chunk_id": 1,
      "start_timestamp": "0:00:52",
      "end_timestamp": "0:01:41"
    }
  },
  {
    "page_content": "ta sẽ thử chạy cái code này trên một con CPU tương đối là tốt hơn. Thì phần này các bạn có thể chọn download cái collab Notebook về máy của mình ha. Và nếu như thường để chạy collab notebook trực tiếp trên thiết bị cá nhân thì chúng ta sẽ chạy bằng file. Chb. Đó. Ở đây các bạn có thể download cái file này về máy của mình để chạy. À và tiếp theo để chạy được cái code này các bạn sẽ phải cài đặt thư viện Tenser Flow vào máy. Rồi có nhiều cách để cài tensor flow ha. À nếu các bạn muốn thử đó, nếu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8VA4C-TS2cQ",
      "filename": "8VA4C-TS2cQ",
      "title": "[CS114 - Tutorial] Google Colab (Phần 4)",
      "chunk_id": 2,
      "start_timestamp": "0:01:35",
      "end_timestamp": "0:02:21"
    }
  },
  {
    "page_content": "tensor flow ha. À nếu các bạn muốn thử đó, nếu các bạn muốn thử sức thì các bạn có thể chọn cách phù hợp với máy của mình, có thể tải về cài đặt. Còn ở đây thì th bài giảng này sẽ minh họa cho các bạn chạy bằng sử dụng một cái công nghệ gọi là container. Chúng ta sẽ sử dụng như hướng dẫn ở đây ha. một cái tens flow docker image. Rồi à hầu hết các code machine learning nếu như chạy trong thực tế chạy trên thiết bị thật à không phải là code minh họa cho việc học thì người ta sẽ chạy trên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8VA4C-TS2cQ",
      "filename": "8VA4C-TS2cQ",
      "title": "[CS114 - Tutorial] Google Colab (Phần 4)",
      "chunk_id": 3,
      "start_timestamp": "0:02:15",
      "end_timestamp": "0:03:11"
    }
  },
  {
    "page_content": "minh họa cho việc học thì người ta sẽ chạy trên terminal. Rồi chúng ta di chuyển vào folder chữa cái file mã nguồn mà chúng ta đã download. À rồi thì ở đây a chúng ta đã có cái file Python của mình đó tải từ trên collab về. Tiếp theo các bạn có thể làm theo hướng dẫn. Nếu như các bạn chọn cài đặt tensor flow sử dụng docker một công nghệ container thì phổ biến nhất hiện nay các bạn có thể sử dụng là docker thì chúng ta đầu tiên là sẽ phải khởi tạo một cái máy có tạm có thể xem như là máy ảo ha",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8VA4C-TS2cQ",
      "filename": "8VA4C-TS2cQ",
      "title": "[CS114 - Tutorial] Google Colab (Phần 4)",
      "chunk_id": 4,
      "start_timestamp": "0:03:04",
      "end_timestamp": "0:03:57"
    }
  },
  {
    "page_content": "một cái máy có tạm có thể xem như là máy ảo ha bằng cái lệnh cứ pháp là Docker run như thế này. Chúng ta có thể mượn tạp à cái lệnh mà trang tensor flow cho hướng dẫn chúng ta ở đây. Nhưng ở đây thay vì chạy cái Jupiter Notebook ở đây chúng ta sẽ không cần phải sử dụng Jupiter Notebook trên máy tính cá nhân của mình ha. Khi chạy trên máy tính cá nhân thường người ta sẽ làm việc thông qua giao diện console. Và cái terminal phổ biến nhất hiện nay là B. Đó khi các bạn chạy lần đọc nếu như cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8VA4C-TS2cQ",
      "filename": "8VA4C-TS2cQ",
      "title": "[CS114 - Tutorial] Google Colab (Phần 4)",
      "chunk_id": 5,
      "start_timestamp": "0:03:52",
      "end_timestamp": "0:04:55"
    }
  },
  {
    "page_content": "nay là B. Đó khi các bạn chạy lần đọc nếu như cái container này chưa có trong máy thì Docker sẽ tự động tải về cho các bạn. Rồi và chắc là chúng ta sẽ tua nhanh cái phần này. Thì trong lúc chờ quá trình download này, chúng ta sẽ xem à trên máy tính của thầy thì CPU được sử dụng à CPU được sử dụng là con AMD Ryzen 9 5900X ha. Đây cũng là CPU hiện giờ vẫn tương đối là mới. Sau này thì chắc nó sẽ lạc hậu đi nhiều. Đó. Còn hiện giờ chúng ta có thể tham khảo giá bán của con này. Như nó cũng đã ra",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8VA4C-TS2cQ",
      "filename": "8VA4C-TS2cQ",
      "title": "[CS114 - Tutorial] Google Colab (Phần 4)",
      "chunk_id": 6,
      "start_timestamp": "0:04:38",
      "end_timestamp": "0:05:49"
    }
  },
  {
    "page_content": "tham khảo giá bán của con này. Như nó cũng đã ra vài đời CPU mới hơn rồi. Thì CPU này có 16 COE ha và CPU nó hỗ trợ công nghệ đa luồng nên nó sẽ nh có máy nhận diện là 32 Core đó nhanh hơn đáng kể so với CPU mà Google Collab sẽ cung cấp miễn phí cho các bạn và đây là container cài đặt sạ Tensor flow ha. Đó, khi các bạn chạy cái container này các bạn sẽ có thể có một cái version ừ cài đặt sẵn của Python và thư viện Tensor Flow trên máy của các bạn mà các bạn không cần phải quá quan tâm đến việc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8VA4C-TS2cQ",
      "filename": "8VA4C-TS2cQ",
      "title": "[CS114 - Tutorial] Google Colab (Phần 4)",
      "chunk_id": 7,
      "start_timestamp": "0:05:43",
      "end_timestamp": "0:06:48"
    }
  },
  {
    "page_content": "mà các bạn không cần phải quá quan tâm đến việc tải và install các cái package này. Đó, ở đây chúng ta có thể thử lệnh import tow. Ok. Và quá trình import thành công đó. Và chúng ta có thể thử ha à cái lệnh để kiểm tra version tenser flow được cài đặt. Chúng ta sẽ in phiên bản tensor flow ra màn hình. Rồi ở đây là phiên bản Tensor Flow 20 2.0 ha mới hơn là phiên bản Tensor Flow mà Google Collab cung cấp sẵn cho các bạn. Đó cũng là một ưu điểm nếu như các bạn chọn à chạy các cái code machine",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8VA4C-TS2cQ",
      "filename": "8VA4C-TS2cQ",
      "title": "[CS114 - Tutorial] Google Colab (Phần 4)",
      "chunk_id": 8,
      "start_timestamp": "0:06:42",
      "end_timestamp": "0:07:27"
    }
  },
  {
    "page_content": "nếu như các bạn chọn à chạy các cái code machine learning trên máy tính cá nhân của mình ha. Nếu các bạn biết cách cài đặt các công cụ thì các bạn có thể có một cái tool nó cập nhật hơn. Bây giờ để chạy được cái mã nguồn mà chúng ta đã download từ trên Google Collab về thì đầu tiên chúng ta phải cho cái máy ảo nó có quyền truy cập đến cái file mã nguồn này. Đó trong Docker thì chúng ta sẽ sử dụng một số các cái tùy chọn sau đây để chạy ứng dụng. À đầu tiên chúng ta phải a share cái folder hiện",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8VA4C-TS2cQ",
      "filename": "8VA4C-TS2cQ",
      "title": "[CS114 - Tutorial] Google Colab (Phần 4)",
      "chunk_id": 9,
      "start_timestamp": "0:07:20",
      "end_timestamp": "0:08:36"
    }
  },
  {
    "page_content": "À đầu tiên chúng ta phải a share cái folder hiện tại working directory với lại à cái máy ảo container đang cài đặt tensor flow của chúng ta. Rồi à thầy sẽ thêm một số các option cần thiết đó. Và đây có vẻ là các option cần thiết để chúng ta chạy. Rồi thì à trong cái container à đó trong cái máy ảo ha tạm gọi container là máy ảo à có cài đặt tensor flow. Chúng ta có thể chạy cái file mà chúng ta đã download từ trên Google Collab về bằng lệnh Python và tên file. À và khi cài thì chắc các bạn sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8VA4C-TS2cQ",
      "filename": "8VA4C-TS2cQ",
      "title": "[CS114 - Tutorial] Google Colab (Phần 4)",
      "chunk_id": 10,
      "start_timestamp": "0:08:29",
      "end_timestamp": "0:09:10"
    }
  },
  {
    "page_content": "và tên file. À và khi cài thì chắc các bạn sẽ thấy báo lỗi thiếu thư viện ha. À đó do máy ảo này mặc định cung cấp sẵn tensor flow nhưng có một số thư viện à một số cái lệnh import trong cái mã nguồn của chúng ta mà cái máy ảo này chưa có sẵn thì chúng ta sẽ phải install vào. Và việc install này nó chỉ có thay đổi trong máy ảo, nó sẽ không làm ảnh hưởng đến cái bảng cài đặt Python trên máy tính của các bạn nếu như các bạn chọn chạy trên máy của mình. Rồi chúng ta sẽ cần thư viện M blư viện",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8VA4C-TS2cQ",
      "filename": "8VA4C-TS2cQ",
      "title": "[CS114 - Tutorial] Google Colab (Phần 4)",
      "chunk_id": 11,
      "start_timestamp": "0:09:06",
      "end_timestamp": "0:10:17"
    }
  },
  {
    "page_content": "của mình. Rồi chúng ta sẽ cần thư viện M blư viện Tenser Flow digit set ha. Rồi bếp install rồi. Quá trình cài đặt successfully install ha. Quá trình cài đặt đã thành công. Chúng ta có thể chạy lại file thơ của mình. Và bây giờ quá trình training đã bắt đầu. Các bạn có thể thấy thời gian training cho mỗi mỗi lần lập của mỗi B 32 tấm ảnh ha. Trong quá trình training. Nếu như chúng ta sử dụng CPU của Google Collab cung cấp thì nó sẽ là gần 2 giây cho mỗi cái P 32 tấm ản. Còn trên CPU của chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8VA4C-TS2cQ",
      "filename": "8VA4C-TS2cQ",
      "title": "[CS114 - Tutorial] Google Colab (Phần 4)",
      "chunk_id": 12,
      "start_timestamp": "0:10:11",
      "end_timestamp": "0:11:09"
    }
  },
  {
    "page_content": "mỗi cái P 32 tấm ản. Còn trên CPU của chúng ta trên máy tính này thì thời gian là 200 m. nhanh hơn tốc độ khoảng gần như là gấp 10 lần. Và các bạn có thể xem ha, ở đây có ừ CPU 5950X này đã được huy động đến a gần đạt ngưỡng max. ở đây chưa đạt ngưỡng 100% của CPU này. Đó do bài toán của chúng ta cũng khá đơn giản và chúng ta chạy một batch cũng nhẹ chỉ có khoảng a 32 tấm ảnh thôi. Thì CPU này đang được huy động khoảng a 70% là dành cho Python. Đó đây là cái tiến trình chiếm nhiều tài nguyên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8VA4C-TS2cQ",
      "filename": "8VA4C-TS2cQ",
      "title": "[CS114 - Tutorial] Google Colab (Phần 4)",
      "chunk_id": 13,
      "start_timestamp": "0:11:02",
      "end_timestamp": "0:11:58"
    }
  },
  {
    "page_content": "Đó đây là cái tiến trình chiếm nhiều tài nguyên CPU nhất hiện tại là 70% và đang dùng hết 6 GB RAM ha. Và thời gian chúng ta chạy là nhanh hơn gấp 10 lần so với CPU của Google Collab. Đó, đây cũng là một con số hoàn toàn thuần túy là cơ học thôi các bạn ha. Colab cung cấp cho các bạn hai CPU core, còn máy tính này có 16 CPU core thì thời gian chạy là nhanh hơn gần gấp 10 lần. Và với mỗi step tức là mỗi một B 32 ảnh chúng ta khoảng 200 mây như vậy 582 B thì chúng ta sẽ hoàn thành một phòng lập",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8VA4C-TS2cQ",
      "filename": "8VA4C-TS2cQ",
      "title": "[CS114 - Tutorial] Google Colab (Phần 4)",
      "chunk_id": 14,
      "start_timestamp": "0:11:52",
      "end_timestamp": "0:13:00"
    }
  },
  {
    "page_content": "582 B thì chúng ta sẽ hoàn thành một phòng lập của quá trình training này. Đó các bạn à cứ nhân lên ha. Khoảng như vậy khoảng 100 giây là chúng ta sẽ xong một vòng lập và năm vòng lập thì có vẻ trên chúng ta sẽ dùng chưa tới 5 phút so với thời gian chạy trên Google collab là 2 giờ đồng hồ. Đó là các bạn sử dụng tuần túy là CPU để chạy một cái model machine learning và đặc biệt là sử dụng cả cái thư viện deep learning. Thì thời gian cho một cái bài toán đơn giản như thế này sẽ khoảng gần ừ 10",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8VA4C-TS2cQ",
      "filename": "8VA4C-TS2cQ",
      "title": "[CS114 - Tutorial] Google Colab (Phần 4)",
      "chunk_id": 15,
      "start_timestamp": "0:12:52",
      "end_timestamp": "0:13:46"
    }
  },
  {
    "page_content": "bài toán đơn giản như thế này sẽ khoảng gần ừ 10 phút cho một lần training. Đó, chúng ta có thể kiểm tra nhanh không biết là CPU 5950X hiện giờ có giá bao nhiêu nhỉ? Rồi đó chúng ta kiểm tra nhanh thôi. Thì CPU này có giá khoảng 7 triệu nếu các bạn mua kèm máy à mua rời thì là 8 triệu chỉ riêng CPU nha các bạn. Đó là cái giá các bạn sẽ phải a đầu tư nếu như chúng ta muốn sử dụng CPU cho machine learning. Còn nếu các bạn cần nhanh hơn nữa, chúng ta có thể xem xét đến option sử dụng GPU ha. Và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8VA4C-TS2cQ",
      "filename": "8VA4C-TS2cQ",
      "title": "[CS114 - Tutorial] Google Colab (Phần 4)",
      "chunk_id": 16,
      "start_timestamp": "0:13:41",
      "end_timestamp": "0:14:00"
    }
  },
  {
    "page_content": "ta có thể xem xét đến option sử dụng GPU ha. Và cách đơn giản nhất để có thể sử dụng GPU đó là các bạn dùng cái GPU Tesla T4 mà Google Collab cung cấp sẵn. À chắc là chúng ta sẽ dừng cái quá trình training ở đây. Rồi đó CPU này còn có thể vẫn chỉ mới sử dụng khoảng 70 đến 80% thôi. Chúng ta còn có thể sử dụng thêm nếu như các bạn tối ưu hóa cái mã nguồn các bạn tối ưu hóa lại size cho mỗi lần training ha. Rồi tuy nhiên chắc là chúng ta sẽ tạm dừng ở đây minh họa thế này các bạn cũng có thể hình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8VA4C-TS2cQ",
      "filename": "8VA4C-TS2cQ",
      "title": "[CS114 - Tutorial] Google Colab (Phần 4)",
      "chunk_id": 17,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "ở đây minh họa thế này các bạn cũng có thể hình dung được các phần nào tốc độ cho một cái CPU hiện đại ha.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=8VA4C-TS2cQ",
      "filename": "8VA4C-TS2cQ",
      "title": "[CS114 - Tutorial] Google Colab (Phần 4)",
      "chunk_id": 18,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chào mừng các bạn đến với môn học CS114 học máy. Và chúng ta sẽ cùng đến với một trong những bài học rất là thú vị đó chính là phân cụm hay clustering. Thì phân cụm là một trong những cái thuật toán mà nằm trong cái nhóm đó là học không giám sát. Và nếu mà xét về cái tính ứng dụng của học không giám sát thì nó sẽ có cái tính ứng dụng rất là cao. Lý do đó là vì hiện nay chúng ta thấy là với cái sự phát triển của mạng xã hội và internet thì cái dữ liệu trên internet có rất là nhiều. Tuy nhiên đại",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:55"
    }
  },
  {
    "page_content": "liệu trên internet có rất là nhiều. Tuy nhiên đại đa số trong đó là dữ liệu không có gán nhãn. Và chính vì cái dữ liệu không có gán nhãn thì nó sẽ thúc đẩy cho cái việc ứng dụng các cái thuật toán học không giám sát mà cụ thể đó là thuật toán phân cụm. Nó có rất nhiều đất để dụng vỏ. Trước khi đi vào chi tiết thì chúng ta sẽ cùng à ôn lại, nhắc lại một số cái khái niệm. Đầu tiên đó là khái niệm học có giám sát. học có giám sát hay còn gọi là supervised learning là chúng ta sẽ tìm cách để ước",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 1,
      "start_timestamp": "0:00:49",
      "end_timestamp": "0:01:28"
    }
  },
  {
    "page_content": "learning là chúng ta sẽ tìm cách để ước lượng cái hàm phân loại hoặc là cái hàm hồi quy. Đối với cái hàm phân loại thì chúng ta sẽ à có cái đầu ra đó là một cái giá trị thuộc về một cái phân lớp nào đó. Còn hồi quy thì đầu ra của cái hàm của mình nó sẽ là một cái giá trị dự đoán liên tục. Và cái việc ước lượng các hàm phân loại hoặc hồi quy này thì nó sẽ được dựa trên các cái cặp dữ liệu. các cái cặp dữ liệu có cái nhãn thì nhãn ở đây trong trường hợp này đó chính là giá trị Y. Thì Y ở đây nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 2,
      "start_timestamp": "0:01:22",
      "end_timestamp": "0:02:05"
    }
  },
  {
    "page_content": "hợp này đó chính là giá trị Y. Thì Y ở đây nó được gọi là nhãn hay còn gọi là label. Và ở trên hình ví dụ ở đây chúng ta thấy là các cái tập à các cái điểm của mình sẽ được gán bởi hai cái nhãn đó là hình tròn và chữ X thì tương ứng là hai cái phân lớp. Và tập dữ liệu huấn luyện của mình thì bao gồm là x1, y1. Trong đó cái chỉ số ở phía trên là cho biết cái thứ tự của cái mẫu dữ liệu của mình. Đây là mẫu dữ liệu thứ à đây là mẫu thứ thứ nhất. Đó cái chỉ số ở trên là để thể hiện cái mẫu thứ mấy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 3,
      "start_timestamp": "0:02:00",
      "end_timestamp": "0:02:42"
    }
  },
  {
    "page_content": "cái chỉ số ở trên là để thể hiện cái mẫu thứ mấy và tập dữ liệu này thì gồm có m mẫu. Thế thì với cái ví dụ này thì chúng ta có thể dễ dàng thấy là cái hàm à phân loại nếu mà sử dụng mô hình hồi quy tuyến tính nó sẽ tìm ra được một cái đường để phân tách hai cái tập điểm này ra làm hai một cách rất là dễ dàng là vì chúng ta đã có nhãn rồi. Bây giờ nhiệm vụ của chúng ta chỉ là đi tìm cái đường phân phân biệt giữa hai cái tập điểm này. Đối với học không giám sát thì chúng ta sẽ học từ cái dữ liệu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 4,
      "start_timestamp": "0:02:38",
      "end_timestamp": "0:03:17"
    }
  },
  {
    "page_content": "không giám sát thì chúng ta sẽ học từ cái dữ liệu mà không có không có gán nhãn. Hay nói cách khác là ở trong học có giám sát thì chúng ta sẽ cần một cái cặp là X và Y. Thì ở đây chúng ta không có cái dữ liệu Y mà chúng ta chỉ có dữ liệu X thôi. Và chúng ta sẽ phải à học như thế nào để xác định được cái cấu trúc và cái phân bố của dữ liệu. Thì trên hình chúng ta thấy cái dữ liệu của mình cũng là tương tự như học có giám sát nhưng ở đây chúng ta không hề có nhãn và tất cả đều được gán bằng cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 5,
      "start_timestamp": "0:03:10",
      "end_timestamp": "0:03:54"
    }
  },
  {
    "page_content": "không hề có nhãn và tất cả đều được gán bằng cái màu màu đen. Thế thì nhiệm vụ của chúng ta đó là phải tìm cách xác định được cái phân bố của cái dữ liệu này nó có tính chất như thế nào. Đó X ở đây chúng ta sẽ không có cái Y. Ở đây không có Y mà chỉ có X thôi. Rồi thì cái việc xác định phân bố nó sẽ dẫn đến chúng ta đến các cái bài toán đó là ví dụ như các cái điểm này chúng ta thấy là nó sẽ co cụm lại với nhau thì chúng ta sẽ gom nó lại thành một cụm. Rồi các cái điểm này thì nó co cụm với",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 6,
      "start_timestamp": "0:03:48",
      "end_timestamp": "0:04:27"
    }
  },
  {
    "page_content": "một cụm. Rồi các cái điểm này thì nó co cụm với nhau thì chúng ta sẽ gom lại thành một cụm. Thì đây là một cái ví dụ để minh họa cho cái việc đó là học không giám sát và xác định phân phân bố nó có liên quan như thế nào đến cái bài toán phân cụm của mình sẽ được nói chi tiết ở phía sau. Và dữ liệu ở đây thì chúng ta sẽ được gom thành hai cái cụm riêng biệt. Và à các cái thuật toán gom cụm thì có rất nhiều cái thuật toán khác nhau. Trong đó thì nổi tiếng nhất có thể kể đến đó là các thuật toán",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 7,
      "start_timestamp": "0:04:22",
      "end_timestamp": "0:05:04"
    }
  },
  {
    "page_content": "nổi tiếng nhất có thể kể đến đó là các thuật toán như là K-means hoặc là clustering. Và các cái thuật toán này thì đều có cái tính ứng dụng rất là cao trong thực tiễn. À nói về cái lĩnh vực về khoa học dữ liệu hoặc là khai thác dữ liệu. Thì cái việc áp dụng thuật toán gom cụm nó sẽ giúp cho chúng ta có thể xác định được cái phân khúc của thị trường. À ví dụ như chúng ta có thể xác định được cái phân khúc khách hàng của mình là thuộc nhóm nào. Ví dụ như nhóm là à trung cấp, nhóm cao cấp hoặc là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 8,
      "start_timestamp": "0:05:00",
      "end_timestamp": "0:05:41"
    }
  },
  {
    "page_content": "dụ như nhóm là à trung cấp, nhóm cao cấp hoặc là nhóm bình dân. Ví dụ vậy. Hoặc là ví dụ như trong một lớp học khi chúng ta à có một cái điểm à của một cái bạn sinh viên thì chúng ta sẽ xếp loại xem sinh viên đó là điểm trung bình khá hay là giỏi xuất sắc. À cũng có một cái ví dụ thú vị để cho chúng ta biết thế nào gọi là học không giám sát trên cái phân bố của dữ liệu. Giả sử như chúng ta có điểm của một bạn học sinh điểm ví dụ điểm trung bình ha. Rồi điểm trung bình của bạn này đó là à 8 điểm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 9,
      "start_timestamp": "0:05:33",
      "end_timestamp": "0:06:28"
    }
  },
  {
    "page_content": "Rồi điểm trung bình của bạn này đó là à 8 điểm 8,5 đi thì chúng ta được yêu cầu là xác định xem bạn này là thuộc cái phân khúc là trung bình khá hay yếu thì chúng ta có thể xác định được hay không? Thì câu trả lời đó là với chỉ có duy nhất điểm của một bạn học sinh là 8,5, chúng ta sẽ không thể biết được người sinh viên này là giỏi khá hay yếu. À chúng ta chỉ có thể biết được cái điểm trung bình này là giỏi khá à xuất sắc hay yếu khi chúng ta đặt nó trong cái phân bố của tất cả những cái bạn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 10,
      "start_timestamp": "0:06:21",
      "end_timestamp": "0:07:17"
    }
  },
  {
    "page_content": "đặt nó trong cái phân bố của tất cả những cái bạn còn lại trong lớp. Ví dụ nếu như chúng ta có cái phân bố như thế này và cái điểm 8,5 của bạn á là nó nằm ở đây thì bạn là giỏi. Nhưng nếu ở một cái cách tính điểm khác hoặc là một cái trường à trường học khác cái phân bố của mình nó không phải như vậy. À giả sử điểm 8,5 nó lại nằm ở đây thì có một số trường ví dụ như họ tính cái thang điểm của mình là trên 100 đi chẳng hạn thì 8,5 nó sẽ là yếu. Rồi cũng có một số trường thì họ lại tính theo cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 11,
      "start_timestamp": "0:07:10",
      "end_timestamp": "0:07:49"
    }
  },
  {
    "page_content": "cũng có một số trường thì họ lại tính theo cái hệ là là 20. Ví dụ đây là từ 0 cho đến 20 và 8,5 của mình đó là nằm ở đây thì lúc này đó là trung bình. Như vậy thì muốn biết, muốn phân phân cụm được hoặc là xác định được cái phân khúc của cái bạn học sinh này thì chúng ta phải đặt bạn đó ở bên trong cái phân bố của điểm của tất cả những bạn còn lại trong lớp. Ờ ngoài ứng dụng trong cái việc là xác định phân khúc thị trường hoặc là xác định cái phân khúc của một bạn học sinh trong lớp học thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 12,
      "start_timestamp": "0:07:43",
      "end_timestamp": "0:08:29"
    }
  },
  {
    "page_content": "phân khúc của một bạn học sinh trong lớp học thì chúng ta còn có những cái ứng dụng khác. Ví dụ như là phân tích thiên văn hoặc là à trong cái phân tích về à sinh học thông tin, sinh tin học à ví dụ như là xác định cái chuỗi DNA để xác định xem là cái người này có những cái khả năng là à thuộc cái nhóm DNA có thể mắc những cái bệnh nào. Ví dụ vậy. rồi phân tích mạng xã hội để xem coi một cái à người trên một cái người dùng trong mạng xã hội sẽ nằm trong một cái phân khúc nào. Ví dụ như là đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 13,
      "start_timestamp": "0:08:24",
      "end_timestamp": "0:09:07"
    }
  },
  {
    "page_content": "nằm trong một cái phân khúc nào. Ví dụ như là đây là tập hợp à những cái bạn mà những cái người dùng trên mạng xã hội mà à có cái mối quan hệ bạn bè với nhau ví dụ vậy. Rồi sau đó thì chúng ta sẽ lại có một cái phân khúc là cũng cái người đó nhưng mà lại nằm trong một cái hội nhóm về kinh doanh. Ví dụ vậy. Thế thì cái việc mà clustering có thể là chúng ta sẽ cluster cứng hoặc là cluster mềm. Cứng tức là một điểm thì nó thuộc về một cụm thôi. Nhưng cũng có thể là cluster mềm tức là một điểm nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 14,
      "start_timestamp": "0:09:02",
      "end_timestamp": "0:09:48"
    }
  },
  {
    "page_content": "cũng có thể là cluster mềm tức là một điểm nó sẽ thuộc về cả hai cụm. Đó thì nó sẽ tùy thuộc vào cái ngữ cảnh ứng dụng. Và chúng ta sẽ phát biểu cái vấn đề đó là với một tập hợp các cái điểm dữ liệu à chúng ta hãy nhóm chúng thành các cái cụm sao cho các cái điểm mà trong cùng một cụm thì có cái phân bố gần nhau. Còn các cái điểm mà à các cái cụm mà khác nhau thì nó sẽ phân bố xa. Các cùng một cụm thì nó sẽ gần. Ví dụ như những cái điểm này thì là nằm ở gần nhau thì chúng ta sẽ gom lại thành",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 15,
      "start_timestamp": "0:09:43",
      "end_timestamp": "0:10:23"
    }
  },
  {
    "page_content": "là nằm ở gần nhau thì chúng ta sẽ gom lại thành một cụm. Nhưng với những cái điểm khác rồi thì chúng ta thấy là đây là hai cái cụm khác nhau và hai cái cụm khác nhau này thì à hai ở hai cái điểm mà ở hai cụm khác nhau này thì chúng ta sẽ thấy là cái khoảng cách nó xa nhau. Nó có cái phân bố xa nhau. Còn những cái điểm mà nằm trong cùng một cụm thì nó sẽ có cái phân bố có cái tính phân bố là nó co cụm và nó gần nhau. Và thông thường thì các cái điểm nằm trong không gian nhiều chiều thì cái độ đo",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 16,
      "start_timestamp": "0:10:17",
      "end_timestamp": "0:11:03"
    }
  },
  {
    "page_content": "nằm trong không gian nhiều chiều thì cái độ đo mà được sử dụng để xác định xem cái tính gần nhau đó chính là các cái độ đo về khoảng cách. Và nổi tiếng nhất có thể nói là độ đo khoảng cách về Euclidean hoặc là khoảng cách về cosin. Lưu ý là cosin thì ban đầu được sử dụng để làm độ đo tương đồng. Tuy nhiên chúng ta sẽ có cái phiên bản là cosin để tính cái khoảng cách bằng cách là đảo cái trình tự của nó lại rồi độ đo ch vân vân. Và các cái khía cạnh cần phải quan tâm của một cái thuật toán gom",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 17,
      "start_timestamp": "0:11:01",
      "end_timestamp": "0:11:40"
    }
  },
  {
    "page_content": "cạnh cần phải quan tâm của một cái thuật toán gom của cái việc gom cụm đó là à thuật toán phân cụm thì chúng ta có thể là phân cụm theo kiểu là phân hoạch hay là partitional clustering. thì à một cái à phân cụm được gọi là phân hoạch. Thì khi chúng ta có các cái điểm dữ liệu và chúng ta sẽ chia nó thành các cái nhóm mà nó rời nhau hoàn toàn, tức là không có cái tình trạng điểm này sẽ nằm chung cụm với lại cái điểm kia. Đó thì cái đây là cái ví dụ mà chúng ta đã đề cập như hồi nãy. Thì chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 18,
      "start_timestamp": "0:11:35",
      "end_timestamp": "0:12:17"
    }
  },
  {
    "page_content": "mà chúng ta đã đề cập như hồi nãy. Thì chúng ta thấy là cái điểm A ở đây thì nó sẽ thuộc cái cluster số 1. Còn cái điểm A này nó sẽ không thuộc cái cluster số 2. Tương tự như vậy các cái điểm B bên đây thì nó chỉ thuộc cái cluster số 2 nhưng lại không được phép thuộc cái cluster số 1. Thì đây là phân hoạch theo kiểu là rõ ràng. Phân hoạch tức là à phân cụm theo kiểu cứng. Còn một cái kiểu nữa đó là phân cụm phân cấp hierarchical clustering. Thì ở đây chúng ta sẽ gom có thể là có nhiều cái chiến",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 19,
      "start_timestamp": "0:12:12",
      "end_timestamp": "0:12:52"
    }
  },
  {
    "page_content": "đây chúng ta sẽ gom có thể là có nhiều cái chiến thuật gom gom từ trên xuống hoặc là gom từ dưới lên top down hoặc là bottom up. Nhưng mà cái ý tưởng chung của nó đó là một điểm của mình thì có thể được phân cụm vào à nhiều nhóm khác nhau. Và cái nhóm này thì nó có cái kiểu là phân tầng. Ví dụ ở đây chúng ta sẽ à có một điểm điểm này, điểm A này nè, điểm A này thì có thể thuộc cái cụm này nhưng đồng thời chúng ta cái điểm A đó nó cũng có thể là thuộc cái cụm lớn ở bên ngoài. Đó, như vậy là một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 20,
      "start_timestamp": "0:12:47",
      "end_timestamp": "0:13:26"
    }
  },
  {
    "page_content": "thuộc cái cụm lớn ở bên ngoài. Đó, như vậy là một cái điểm A thì có thể là thuộc cụm nhỏ nhưng cũng đồng thời có thể thuộc cụm cụm lớn và cụm lớn nó sẽ chứa cái cụm nhỏ là kiểu phân tầng. Và à cái khía cạnh khác của phân cụm đó là liên quan đến cái độ đo khoảng cách thì chúng ta đã nói rồi ở trong cái slide trước là khoảng cách chúng ta có thể sử dụng nhiều cái loại à à độ đo khoảng cách khác nhau hoặc là chúng ta sử dụng cái độ đo về mặt tương đồng. Rồi chất lượng của phân cụm thì khoảng cách",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 21,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "đồng. Rồi chất lượng của phân cụm thì khoảng cách giữa các cụm sẽ được tối đa hóa. Các cái cụm của mình sẽ thông thường là sẽ được à tối đa hóa. Còn khoảng cách mà à trong cùng cụm những cái điểm nào mà trong cùng một cái nhóm thì nó sẽ co cụm lại với nhau. Còn những cái điểm nào mà có tính chất khác biệt hoàn toàn thì nó sẽ tối đa hóa cái khoảng cách đó.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9bI07JfvEh8",
      "filename": "9bI07JfvEh8",
      "title": "[CS114 - Chương 5] Cluster - Phần 1",
      "chunk_id": 22,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Để giải thích cho vấn đề này thì chúng ta sẽ vẽ cái sơ đồ như sau. Nếu như hai cái giá trị y và phi này á mà khớp với nhau tức là y = phi đó chúng ta thế vào cái hàm loss này sai số của mình lúc này nó không còn bằng 1 nữa mà nó bằng 0. Tương tự như vậy, khi chúng ta thế y mà bằng phi vào trên, ví dụ như là y = 0 và phi cũng bằng 0 thì khi chúng ta thế vào cái lỗi này, hàm lỗi binary cross entropy nó cũng sẽ là bằng 0 luôn. Như vậy thì chúng ta sẽ vẽ một cái biểu đồ đó là giữa hai cái hàm mà",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9Iv6TuyWU5g",
      "filename": "9Iv6TuyWU5g",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 2 (Phần 2)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:01:07"
    }
  },
  {
    "page_content": "sẽ vẽ một cái biểu đồ đó là giữa hai cái hàm mà cùng có cái giá trị nhỏ nhất à cùng có cái giá trị nhỏ nhất tại cái vị trí này đây là theta sao? Thì câu hỏi đó là ờ hàm nào sẽ là hàm tốt hơn? Thì chúng ta nhìn vô đây một là hàm có cái độ dốc lớn. Nói cách khác là khi chúng ta mới thay đổi những cái giá trị mà nó bị sai số, bị lệch với nhau thì cái loss của mình nó sẽ tiến lên thành một cái con số rất là lớn. Thì đây là cái độ dốc nó lớn nè. Và hàm thứ hai độ dốc sẽ thoai thoải hơn. Thì à dựa",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9Iv6TuyWU5g",
      "filename": "9Iv6TuyWU5g",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 2 (Phần 2)",
      "chunk_id": 1,
      "start_timestamp": "0:01:00",
      "end_timestamp": "0:01:50"
    }
  },
  {
    "page_content": "hàm thứ hai độ dốc sẽ thoai thoải hơn. Thì à dựa trên cái lý thuyết về đạo hàm, chúng ta biết rằng là cái đường số 1 nó sẽ cho cái độ dốc cao, tức là cái đạo hàm của mình nó lớn. Tức là đạo hàm của mình nó lớn. Đạo hàm của J theo theta và đạo hàm của J theo b là những cái con số lớn. Và khi chúng ta tính ra được những cái đạo hàm này lớn á thì cái quá trình cập nhật của chúng ta là theta là bằng theta trừ cho alpha nhân cho đạo hàm này. Thì nếu như cái giá trị đạo hàm này mà lớn thì rõ ràng là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9Iv6TuyWU5g",
      "filename": "9Iv6TuyWU5g",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 2 (Phần 2)",
      "chunk_id": 2,
      "start_timestamp": "0:01:42",
      "end_timestamp": "0:02:30"
    }
  },
  {
    "page_content": "như cái giá trị đạo hàm này mà lớn thì rõ ràng là cái bước nhảy này nó sẽ lớn và nó sẽ dễ dàng giúp cho chúng ta huấn luyện nhanh hơn và dễ thoát ra khỏi điểm cực tiểu cục bộ dễ hơn. Thì thì đó là chính là lý do tại sao chúng ta chọn cái hàm số 1 thay vì hàm số hai. Là vì với cái độ dốc cao thì sau này cái việc cập nhật tham số theta hoặc là b của mình nếu có thì nó sẽ nhanh hơn.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=9Iv6TuyWU5g",
      "filename": "9Iv6TuyWU5g",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 2 (Phần 2)",
      "chunk_id": 3,
      "start_timestamp": "0:02:25",
      "end_timestamp": "0:02:48"
    }
  },
  {
    "page_content": "Trong thực tế thì dữ liệu của chúng ta nó sẽ không hoàn toàn là một cái dữ liệu à tuyến tính mà nó có thể là một cái dữ liệu phi tuyến. À thế nào là một cái dữ liệu tuyến tính thì chúng ta có thể minh họa đó là x và đây là y ha. thì tuyến tính đó là nó sẽ đồng biến như thế này hoặc là nghịch biến đó thì một là đường đi lên hoặc là hai đó là đường đi xuống như thế này thì nó được gọi là tuyến tính còn cái dữ liệu phi tuyến đây là tuyến tính còn đối với cái dữ liệu phi tuyến á thì cái y của mình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=abEjEZwTp2o",
      "filename": "abEjEZwTp2o",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 4)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:58"
    }
  },
  {
    "page_content": "với cái dữ liệu phi tuyến á thì cái y của mình nó sẽ phụ thuộc bởi một cái hàm không phải là hàm bậc 1 đối với X mình lấy ví dụ đơn giản như là hàm bậc hai đi. Đó thì đây là các cái tập dữ liệu. Đây là các cái dữ liệu mà cho cái mối quan hệ giữa y và x đó là một cái mối quan hệ phi tuyến. Thì nếu đúng chúng ta sẽ phải tìm được một cái đường như thế này. Một cái đường dạng bậc hai như thế này. Thì đây là một cái ví dụ của phi tuyến. Thế thì cái mô hình hồi quy tuyến tính của mình liệu có sử dụng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=abEjEZwTp2o",
      "filename": "abEjEZwTp2o",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 4)",
      "chunk_id": 1,
      "start_timestamp": "0:00:52",
      "end_timestamp": "0:01:44"
    }
  },
  {
    "page_content": "hình hồi quy tuyến tính của mình liệu có sử dụng được cho cái dữ liệu phi tuyến hay không? Thì câu trả lời đó là được. Câu trả lời là được. Nếu chúng ta nhìn kỹ á thì cái sự cái mô hình mà hồi quy tuyến tính á thì cái chữ tuyến tính ở đây á nó không phải là thể hiện cái tuyến tính giữa cái đặc trưng và cái giá trị đầu ra. mà nó sẽ thể hiện cái mối quan hệ bậc mới của đạo hàm theo cái biến W và B, tức là cái biến mô hình của mình. Thì khi chúng ta lấy đạo hàm của W và B thì nó sẽ ra một cái hàm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=abEjEZwTp2o",
      "filename": "abEjEZwTp2o",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 4)",
      "chunk_id": 2,
      "start_timestamp": "0:01:37",
      "end_timestamp": "0:02:24"
    }
  },
  {
    "page_content": "lấy đạo hàm của W và B thì nó sẽ ra một cái hàm bậc một hay là một cái hàm tuyến tính. Đó thì cái khái niệm tuyến tính ở đây á nó chính là đang nói cho cái đạo hàm ờ của cái hàm lỗi của mình. Còn nếu như X và Y ở đây các bạn chọn ra những cái đặc trưng phù hợp thì hoàn toàn có thể sử dụng được. Lấy ví dụ đối với cái trường hợp phi tuyến ở đây thì chúng ta sẽ làm một cái nó gọi là feature engineering. Tức là chúng ta sẽ rút trích ra đặc trưng. Nếu như bình thường chúng ta sử dụng là wx + b thì ở",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=abEjEZwTp2o",
      "filename": "abEjEZwTp2o",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 4)",
      "chunk_id": 3,
      "start_timestamp": "0:02:18",
      "end_timestamp": "0:03:11"
    }
  },
  {
    "page_content": "như bình thường chúng ta sử dụng là wx + b thì ở đây chúng ta sẽ có một cái ờ đặc trưng mới đó là x bình phương. Rồi đồng thời chúng ta sẽ có thêm cả cái x và có thêm cái thành phần bias. thì cái công thức của mình lúc này nó sẽ mở rộng ra đó là một cái mô hình hồi quy tuyến tính đa biến. Lúc này thì W của mình nó sẽ không phải là một giá trị scalar mà chúng ta sẽ tách nó ra làm hai phần. Cụ thể đó là chúng ta sẽ có w1x cộng cho w2 x bình. Rồi sau đó chúng ta sẽ cộng thêm cái thành phần bias",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=abEjEZwTp2o",
      "filename": "abEjEZwTp2o",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 4)",
      "chunk_id": 4,
      "start_timestamp": "0:03:05",
      "end_timestamp": "0:03:59"
    }
  },
  {
    "page_content": "sau đó chúng ta sẽ cộng thêm cái thành phần bias nữa. Thì lúc này là chúng ta có nhiều hơn nhiều hơn một biến. Tức là đây là một cái mô hình hồi quy đa biến. Và nếu chúng ta nhìn nó theo cái góc độ là cái biến x đầu vào thì nó là bậc hai. Nhưng nếu chúng ta nhìn kỹ ở đây cái biến số của mình nó không phải là x hoặc là x bình phương mà biến số của mình chính là b, w1 và w2. Thế thì cả b, w1, w2 nó đều là những cái hàm bậc 1. Nó đều là những cái hàm bậc một nên nó vẫn là một cái mô hình hồi quy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=abEjEZwTp2o",
      "filename": "abEjEZwTp2o",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 4)",
      "chunk_id": 5,
      "start_timestamp": "0:03:54",
      "end_timestamp": "0:04:36"
    }
  },
  {
    "page_content": "hàm bậc một nên nó vẫn là một cái mô hình hồi quy tuyến tính. Đó, như vậy thì à trả lời tóm lại cho cái câu hỏi đó là dữ liệu mà có mối quan hệ phi tuyến thì chúng ta có sử dụng mô hình hồi quy tuyến tính được hay không? Thì câu trả lời đó chính là được. Và chúng ta sẽ phải sử dụng thêm một cái đặc trưng nữa nó gọi là đặc trưng x bình phương để cho cái mô hình của mình có thể học được với cái mối quan hệ phi tuyến tính này.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=abEjEZwTp2o",
      "filename": "abEjEZwTp2o",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 4)",
      "chunk_id": 6,
      "start_timestamp": "0:04:31",
      "end_timestamp": "0:04:41"
    }
  },
  {
    "page_content": "Chúng ta sẽ cùng đến với mô hình phân tích thành phần chính. Thì thuật toán ở đây được sử dụng đó chính là thuật toán PCA. Và chúng ta giả định rằng các cái điểm dữ liệu X của mình là nằm trong một cái không gian là D chiều. Và đây là một cái không gian có số chiều lớn và nằm gần một cái không gian con có cái chiều thấp hơn đó là P chiều. Thì cái không gian này được ký hiệu là J và P thì sẽ là nhỏ hơn so với D. Và không gian con J này á thì được sinh ra bởi một tập hợp các cái vectơ trực chuẩn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=b8HuVAc0cb4",
      "filename": "b8HuVAc0cb4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 3",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:57"
    }
  },
  {
    "page_content": "sinh ra bởi một tập hợp các cái vectơ trực chuẩn W1, W2, WP. Thì khi đó dữ liệu xấp xỉ bởi một cái tổ hợp tuyến tính là x mũ của à các cái vectơ trực chuẩn WK. Thế thì ở đây chúng ta sẽ có công thức là X sẽ xấp xỉ bằng với lại x ngã. Trong đó x ngã thì sẽ là bằng một cái tổ hợp tuyến tính của WK với lại JK và ký hiệu gọn lại thì nó sẽ là bằng W nhân J. Thế thì công thức này có thể sẽ hơi khó hiểu nếu mà chúng ta đưa về một cái không gian à có số chiều đủ nhỏ thì chúng ta có thể dễ hiểu hơn. Thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=b8HuVAc0cb4",
      "filename": "b8HuVAc0cb4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 3",
      "chunk_id": 1,
      "start_timestamp": "0:00:49",
      "end_timestamp": "0:01:39"
    }
  },
  {
    "page_content": "chiều đủ nhỏ thì chúng ta có thể dễ hiểu hơn. Thì giả sử như ở đây là chúng ta có cái không gian X à chúng ta giả sử các cái điểm dữ liệu X của mình nó nằm ở trong không gian là U1 và U2. Thì đây là cái không gian gốc ban đầu. Rồi ở đây chúng ta sẽ có một cái không gian con. Chúng ta sẽ có một cái không gian con. Thế thì cái điểm X của mình ban đầu nó sẽ có cái tọa độ là tính theo cái hệ trục tọa độ là U1 U2. Nhưng mà sau đó thì chúng ta sẽ chiếu nó trên cái không gian con à chiếu nó trên một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=b8HuVAc0cb4",
      "filename": "b8HuVAc0cb4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 3",
      "chunk_id": 2,
      "start_timestamp": "0:01:34",
      "end_timestamp": "0:02:22"
    }
  },
  {
    "page_content": "nó trên cái không gian con à chiếu nó trên một cái không gian con khác đó là W ờ W1 và W2. Rồi thì khi ở trên cái hệ trên cái không gian cũ á thì chúng ta sẽ chiếu xuống dưới trục U1 và U2. Nhưng mà khi chúng ta chiếu lên trên cái ờ không gian con mới thì chúng ta sẽ phải chiếu lên hai cái vectơ W1 và W2 này. Đó thì X của mình nó sẽ phải chiếu lên hai trục này. Và đây chính là cái phép chiếu. Đây chính là cái phép chiếu của mình. Trong đó JK nó chính là cái tọa độ. À trong đó J là một cái tọa",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=b8HuVAc0cb4",
      "filename": "b8HuVAc0cb4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 3",
      "chunk_id": 3,
      "start_timestamp": "0:02:18",
      "end_timestamp": "0:03:09"
    }
  },
  {
    "page_content": "chính là cái tọa độ. À trong đó J là một cái tọa độ ở trong cái ờ không gian W1 và W2. Còn W thì sẽ là ờ các vectơ trực chuẩn trong cái không gian con. Đó thì khi đó là J của mình JK này của mình nó sẽ thuộc một cái R^D. WK này cũng là thuộc R^D. Tại vì cho dù nó là cái hệ tọa độ của một cái không gian con đi chăng nữa thì cái cái tọa độ của nó cũng sẽ phải có cái số chiều giống như là cái số chiều ban đầu. Rồi thì khi đó là thằng này là vectơ D chiều, đây là vectơ D chiều nhân lại thì nó sẽ ra",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=b8HuVAc0cb4",
      "filename": "b8HuVAc0cb4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 3",
      "chunk_id": 4,
      "start_timestamp": "0:03:04",
      "end_timestamp": "0:03:49"
    }
  },
  {
    "page_content": "chiều, đây là vectơ D chiều nhân lại thì nó sẽ ra một cái giá trị scalar. Thì tích của hai cái này nó chính là cái hình chiếu. À nó tích của hai này chính là hình chiếu trên cái thành phần thứ k. Ví dụ chúng ta lấy cái J1 nhân với W1 chính là cái giá trị nó chính là cái giá trị tọa độ trên cái trục W1. Rồi W2 nhân với J2 chính là cái tọa độ trên à cái à vectơ trực chuẩn W2 ha. Đó thì bản chất đây chính là phép chiếu. Và thay vì chúng ta lưu x, chúng ta lưu cái tọa độ trong cái không gian U1 U2",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=b8HuVAc0cb4",
      "filename": "b8HuVAc0cb4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 3",
      "chunk_id": 5,
      "start_timestamp": "0:03:43",
      "end_timestamp": "0:04:35"
    }
  },
  {
    "page_content": "ta lưu cái tọa độ trong cái không gian U1 U2 thì bây giờ chúng ta sẽ lưu cái hình chiếu của nó. À chúng ta sẽ lưu cái hình chiếu của nó. Thì trong trường hợp nếu như cái P này nè mà nhỏ hơn so với lại D, P mà nhỏ hơn so với D tức là chúng ta bỏ đi một cái thành phần. Ví dụ chúng ta bỏ đi thành phần này thì khi đó là chúng ta chỉ tốn có một cái giá trị chúng ta chỉ cần tốn một giá trị để lưu cái tọa độ của x nhưng mà đương nhiên đây sẽ là tọa độ xấp xỉ chứ không thể nào mà chính xác tuyệt đối.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=b8HuVAc0cb4",
      "filename": "b8HuVAc0cb4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 3",
      "chunk_id": 6,
      "start_timestamp": "0:04:31",
      "end_timestamp": "0:05:13"
    }
  },
  {
    "page_content": "xấp xỉ chứ không thể nào mà chính xác tuyệt đối. Còn muốn chính xác tuyệt đối thì chúng ta phải có cái số chiều P bằng với lại D. Còn ở đây vì chúng ta đang muốn giảm số chiều tức là P nhỏ hơn D thì chúng ta sẽ phải chấp nhận đó là bỏ đi một cái thành phần nào đó. Tuy nhiên cái việc mà chọn lựa cái thành phần W nào thì chúng ta sẽ dựa trên cái phương sai. Nếu mà phương sai nhỏ thì chúng ta sẽ bỏ đi. Tại vì cái thông tin để giúp phân biệt các cái dữ liệu của mình nó không có nhiều. Và nếu mà",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=b8HuVAc0cb4",
      "filename": "b8HuVAc0cb4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 3",
      "chunk_id": 7,
      "start_timestamp": "0:05:09",
      "end_timestamp": "0:05:47"
    }
  },
  {
    "page_content": "cái dữ liệu của mình nó không có nhiều. Và nếu mà nhìn nó ở một cái góc độ là một cái mạng neural network thì đầu vào của mình sẽ là một cái vectơ D chiều và đầu ra sẽ là một cái vectơ J là có P chiều. Trong đó mỗi cái giá trị J1, J2, JP nó chính là cái kết quả của cái phép chiếu. Đó, nó chính là cái kết quả của cái phép chiếu. Các cái tọa độ của mình, cái điểm tọa độ của mình lên trên cái vectơ trực chuẩn chiếu lên cái vectơ trực chuẩn. W1. Tương tự như vậy, ở dưới sẽ là chiếu lên cái vectơ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=b8HuVAc0cb4",
      "filename": "b8HuVAc0cb4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 3",
      "chunk_id": 8,
      "start_timestamp": "0:05:42",
      "end_timestamp": "0:06:44"
    }
  },
  {
    "page_content": "tự như vậy, ở dưới sẽ là chiếu lên cái vectơ trực chuẩn W2. Đây là sẽ là à WP. Thì khi đó ờ thay vì lưu là X1, X2 cho đến XD thì chúng ta chỉ cần lưu Z1, Z2, ZP à nó sẽ đỡ tốn bộ nhớ hơn. Và các cái bước để thực hiện với PCA thì bước một đó là chúng ta sẽ đưa cái dữ liệu xoay quanh cái góc tọa độ của mình. Tức là nếu như cái dữ liệu gốc của mình nó như thế này thì chúng ta sẽ tìm cách là tịnh tiến cái hệ trục tọa độ về à về tâm của cái cụm này. Và tâm cụm này thì được biểu diễn bởi là x gạch",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=b8HuVAc0cb4",
      "filename": "b8HuVAc0cb4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 3",
      "chunk_id": 9,
      "start_timestamp": "0:06:36",
      "end_timestamp": "0:07:18"
    }
  },
  {
    "page_content": "Và tâm cụm này thì được biểu diễn bởi là x gạch tức là giá trị trung bình. Đó. Thì khi chúng ta lấy x tức là cái dữ liệu gốc trừ cho cái x trung bình này nè thì bản chất là chúng ta đang thực hiện một cái phép tịnh tiến. Chúng ta đang tịnh tiến cái à hệ trục tọa độ gốc à về cái hệ trục tọa độ mới là E1 E2. Trong đó cái gốc tọa độ nó sẽ nằm ngay trung tâm. Sau đó chúng ta sang bước số hai đó là chúng ta sẽ tính cái ma trận hiệp phương sai S. ma trận hiệp phương sai S để tìm ra các cái đặc trưng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=b8HuVAc0cb4",
      "filename": "b8HuVAc0cb4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 3",
      "chunk_id": 10,
      "start_timestamp": "0:07:13",
      "end_timestamp": "0:07:54"
    }
  },
  {
    "page_content": "hiệp phương sai S để tìm ra các cái đặc trưng liên quan đến nhau như thế nào thì bản chất của cái phép tính hiệp phương sai này chúng ta đang tìm à chúng ta đang tìm cái sự tương quan giữa các cái dữ liệu của mình thì nếu như dữ liệu nào mà có cái sự tương quan càng cao thì nó chứng tỏ là có cái sự tương đồng cao. đó thì khi chúng ta tính s là bằng trung bình cộng của X sau khi đã được ờ à đưa về cái trung tâm á đó thì là X mũ nhân với X mũ chuyển vị thì X là cái ma trận đã chuẩn hóa ở trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=b8HuVAc0cb4",
      "filename": "b8HuVAc0cb4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 3",
      "chunk_id": 11,
      "start_timestamp": "0:07:48",
      "end_timestamp": "0:08:28"
    }
  },
  {
    "page_content": "vị thì X là cái ma trận đã chuẩn hóa ở trong cái bước số 1 thì n là số mẫu dữ liệu của mình. Và sang bước số ba thì chúng ta sẽ tìm các cái thành phần chính à nơi mà dữ liệu có cái độ biến thiên lớn. Thì khi mà cái độ dữ liệu của mình nó có biến thiên lớn thì tức là cái hàm lượng thông tin của mình nó sẽ có nhiều hơn. Nó sẽ phân biệt giữa dữ liệu này với dữ liệu kia nhiều hơn. Và thành phần chính thứ nhất là PC1 là principal component 1 thì chúng ta sẽ có là hướng có cái phương sai lớn nhất hay",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=b8HuVAc0cb4",
      "filename": "b8HuVAc0cb4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 3",
      "chunk_id": 12,
      "start_timestamp": "0:08:23",
      "end_timestamp": "0:09:02"
    }
  },
  {
    "page_content": "ta sẽ có là hướng có cái phương sai lớn nhất hay là dữ liệu phân tán nhiều nhất. Thành phần chính thứ hai đó là PC2 thì là vuông góc với lại cái PC1. Tức là thành phần 2 phải vuông góc với thành phần PC1 và phương sai thì lớn thứ nhì. Ở trên là phương sai lớn thứ nhất còn ở đây sẽ là lớn thứ nhì. Và ở đây chúng ta sẽ có dấu ba chấm, tức là chúng ta sẽ tìm các cái thành phần chính tiếp theo à cho đến khi nào mà cái phương sai này nó không còn lớn nữa. Và các cái hướng này thì sẽ được xác định từ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=b8HuVAc0cb4",
      "filename": "b8HuVAc0cb4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 3",
      "chunk_id": 13,
      "start_timestamp": "0:08:56",
      "end_timestamp": "0:09:37"
    }
  },
  {
    "page_content": "nữa. Và các cái hướng này thì sẽ được xác định từ các cái vectơ riêng là eigenvector của ma trận hiệp phương sai. Và mức độ quan trọng của chúng được đo bằng cái giá trị riêng tức là eigenvalue. Và bước số 4 đó là chọn các cái hướng quan trọng nhất. Thì giả sử như chúng ta có N cái principle component thì chúng ta chỉ cần chọn P bé hơn D. Giả sử ban đầu là có D đúng không? Thì chúng ta chỉ cần chọn P bé hơn D cái component mà có cái độ quan trọng à cao nhất. Sau đó chúng ta sẽ biến đổi dữ liệu.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=b8HuVAc0cb4",
      "filename": "b8HuVAc0cb4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 3",
      "chunk_id": 14,
      "start_timestamp": "0:09:32",
      "end_timestamp": "0:10:14"
    }
  },
  {
    "page_content": "à cao nhất. Sau đó chúng ta sẽ biến đổi dữ liệu. thì à chúng ta sẽ chọn P thành phần hàng đầu để mà giải thích được cái phần lớn cái phương sai ví dụ như là 95% và biến đổi cái tập dữ liệu gốc bằng cách chiếu nó lên các cái thành phần hàng đầu này. Thì ở đây chúng ta sẽ có một cái ví dụ minh họa, một cái hình ảnh minh họa. Thì ở bước số một là chúng ta đã chuyển cái hệ trục tọa độ là về tâm của cái dữ liệu của mình E1 và E2. thì tâm cái gốc tọa độ của nó nó sẽ nằm ở trọng tâm của cái các cái dữ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=b8HuVAc0cb4",
      "filename": "b8HuVAc0cb4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 3",
      "chunk_id": 15,
      "start_timestamp": "0:10:09",
      "end_timestamp": "0:10:48"
    }
  },
  {
    "page_content": "của nó nó sẽ nằm ở trọng tâm của cái các cái dữ liệu. Sau khi chúng ta à phân tích cái thành phần chính thì chúng ta sẽ xác định được là cái thành phần U1 là cái thành phần mà có cái phương sai là lớn nhất. Còn cái thành phần U2 thì chúng ta thấy là cái phương sai của mình nó rất là bé và bé hơn so với lại U1 rất là nhiều. Do đó chúng ta sẽ không quan tâm đến cái thành phần U2 này. Và bước tiếp theo đó là chúng ta sẽ chiếu các cái điểm dữ liệu này. À ví dụ chúng ta chiếu dữ liệu xuống. chiếu dữ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=b8HuVAc0cb4",
      "filename": "b8HuVAc0cb4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 3",
      "chunk_id": 16,
      "start_timestamp": "0:10:43",
      "end_timestamp": "0:11:28"
    }
  },
  {
    "page_content": "À ví dụ chúng ta chiếu dữ liệu xuống. chiếu dữ liệu xuống. Đó thì cái màu đỏ, điểm màu đỏ này chính là cái tọa độ chính là cái giá trị của à trên cái trục U1 chính là cái giá trị của các cái điểm của mình trên cái trục U1 này. Và chúng ta sẽ lưu à chúng ta sẽ lưu các cái điểm màu đỏ này lại, các cái giá trị màu đỏ này lại. Lưu ý là giá trị là giá trị đại số ha. Chúng ta sẽ thay nói cách khác đó là chúng ta lưu cái giá trị scalar của cái phép chiếu các cái điểm màu xanh xuống dưới cái vectơ trực",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=b8HuVAc0cb4",
      "filename": "b8HuVAc0cb4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 3",
      "chunk_id": 17,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "các cái điểm màu xanh xuống dưới cái vectơ trực chuẩn U1. Và như vậy thì thay vì chúng ta với mỗi điểm này chúng ta cần lưu hai giá trị tương ứng là hình chiếu trên E1 và E2 thì bây giờ chúng ta chỉ cần lưu trên một giá trị thôi trên cái giá trị hình chiếu của U1.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=b8HuVAc0cb4",
      "filename": "b8HuVAc0cb4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 3",
      "chunk_id": 18,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Để tăng cái tính thuyết phục hơn thì chúng ta sẽ tìm cách là chỉ tập trung vào cái khu vực có cái điểm dữ liệu của mình. Thế thì ở đây chúng ta sẽ trực quan hóa thì chúng ta sẽ giới hạn lại cái view của mình. Đó là plt.xlim. Thì ví dụ như à dải giá trị của mình chúng ta thấy là chúng ta sẽ lấy xlim là từ trừ ờ trong cái khoảng này đi ha là từ -3 cho đến 4. Và y thì có thể là từ vì ở đây nó có bậc 4 đó. bậc 4 thì nó có thể là con số rất là lớn nên chúng ta sẽ lấy giá trị ở đây có thể lên đến là à",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CB_v2ZxAyKw",
      "filename": "CB_v2ZxAyKw",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 2)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:01:09"
    }
  },
  {
    "page_content": "chúng ta sẽ lấy giá trị ở đây có thể lên đến là à 50 và trục tung của mình á thì có thể là khoảng là 50. Rồi bây giờ chúng ta sẽ xem thử xem cái hình nó vẽ như thế nào. Đó thì ở đây là chúng ta đang đi hơi lố. Chúng ta đang đi hơi lố. À nếu như chúng ta tập trung vào cái khu vực này thì có thể là nó sẽ à rõ hơn. Đó. Rồi. Thế thì ở đây cái do cái giá trị khởi tạo của chúng ta nó quá lớn nên đâm ra khi chúng ta vẽ chúng ta sẽ nhìn thấy là nó đi quá xa. Đó thì ở đây chúng ta sẽ giảm nó xuống là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CB_v2ZxAyKw",
      "filename": "CB_v2ZxAyKw",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 2)",
      "chunk_id": 1,
      "start_timestamp": "0:01:00",
      "end_timestamp": "0:01:52"
    }
  },
  {
    "page_content": "quá xa. Đó thì ở đây chúng ta sẽ giảm nó xuống là còn khoảng năm thôi. Rồi và chúng ta sẽ chạy lại. Rồi ở đây thì chúng ta sẽ để là ví dụ như à cái bước nhảy của mình là 0.01 ha để cho cái tốc độ huấn luyện của nó nhanh hơn. À nhưng mà vì nó nhanh quá nên nó bị phân kỳ. Nó cứ chạy qua chạy lại nó bị phân kỳ. Rồi ở đây chúng ta sẽ để lại là 0.01 và x ở đây thì có thể chúng ta chỉ lấy trong cái phạm vi là -30 cho đến khoảng 5 thôi. À ở đây chúng ta bị nhầm ha. Tức là ở đây là xlim còn đây phải là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CB_v2ZxAyKw",
      "filename": "CB_v2ZxAyKw",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 2)",
      "chunk_id": 2,
      "start_timestamp": "0:01:47",
      "end_timestamp": "0:03:03"
    }
  },
  {
    "page_content": "bị nhầm ha. Tức là ở đây là xlim còn đây phải là ylim mới đúng. Tức là trục tung của mình là phải từ -50 cho đến khoảng là 5 thôi. Đó thì chúng ta thấy là cái mô hình của chúng ta đang tìm cách để fit vào. Chúng ta thấy là càng lúc nó đang fit vào các cái điểm nè. Nó bẻ cái đường bậc B để cho nó đi qua các cái điểm của mình. Tương tự như vậy, ở đây nó cũng sẽ cố gắng bẻ cái đường bậc B để đi qua các cái điểm của mình. Và chắc chắn là một cái đường cong bậc B thì dù sao khi chúng ta fit thì nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CB_v2ZxAyKw",
      "filename": "CB_v2ZxAyKw",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 2)",
      "chunk_id": 3,
      "start_timestamp": "0:02:57",
      "end_timestamp": "0:03:37"
    }
  },
  {
    "page_content": "cong bậc B thì dù sao khi chúng ta fit thì nó cũng sẽ có thể bẻ được tại vì cái tính tùy biến của hàm bậc B rất là cao nhưng à nó sẽ bị sai biệt ở cái khu vực mà ngoài rìa của cái phân bố. Tức là khi đi ra ngoài cái phạm vi của cái phân bố này á thì các cái điểm dữ liệu của mình sẽ bị sai. Thì cụ thể hơn chúng ta có thể nới cái phạm vi này ra là thành -5 thì chúng ta sẽ thấy rõ hơn. Từ -5 đó. Thì cho dù cái mô hình của mình khi mà nó huấn luyện thì cái đường này nó cũng sẽ là vọt lên. Nó sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CB_v2ZxAyKw",
      "filename": "CB_v2ZxAyKw",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 2)",
      "chunk_id": 4,
      "start_timestamp": "0:03:32",
      "end_timestamp": "0:04:25"
    }
  },
  {
    "page_content": "thì cái đường này nó cũng sẽ là vọt lên. Nó sẽ cong nó đi xuống sau rồi nó sẽ đi lên. Đó thì nó sẽ bị sai ở ngoài rìa của cái phân bố này. Nó sẽ bị sai ở ngoài rìa của phân bố. Nhưng mà đương nhiên khi mà huấn luyện đủ lâu thì có thể là cái đường bậc B nó sẽ bẻ ra và nó sẽ ép cho các cái giá trị của trọng số của W1, à hệ số của hàm bậc 4 của cái thành phần bậc 4 và thành phần bậc 3 là nó sẽ giảm xuống. Đó thì ở đây chúng ta sẽ lấy một cái ví dụ ha. Rồi bây giờ chúng ta sẽ in ra. Thì ví dụ như",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CB_v2ZxAyKw",
      "filename": "CB_v2ZxAyKw",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 2)",
      "chunk_id": 5,
      "start_timestamp": "0:04:17",
      "end_timestamp": "0:05:32"
    }
  },
  {
    "page_content": "ha. Rồi bây giờ chúng ta sẽ in ra. Thì ví dụ như chúng ta lặp khoảng 100 lần thì chúng ta sẽ in ra i bằng 0. Đó. Rồi và ở đây thì sẽ cộng bằng 1 if it % 5, ví dụ 100 đi. Rồi thì chúng ta sẽ in ra cái bộ trọng số W1. Rồi chúng ta chỉ cần in ra các cái thành phần là W1, W2, và W3, W4 thôi. Rồi bây giờ chúng ta sẽ xem coi là trong cái quá trình huấn luyện thì các cái bộ trọng số của mình nó đã cập nhật ra sao. À quên và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=CB_v2ZxAyKw",
      "filename": "CB_v2ZxAyKw",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 2)",
      "chunk_id": 6,
      "start_timestamp": "0:05:24",
      "end_timestamp": "0:05:59"
    }
  },
  {
    "page_content": "Bây giờ chúng ta sẽ thử chạy trên một cái GPU à gắn với máy tính thật của các bạn, không phải thông qua máy ảo Google Colab. Thì liệu các bạn có thể chạy được những cái chương trình machine learning này hay không? Thì trên nếu hồi nãy các bạn tinh ý thì sẽ thấy trên máy tính của thầy hiện tại cũng có một cái GPU ha. Đó là con AMD Radeon 6700XT. Đó, GPU này thì nó cũng đời cũ rồi, thậm chí nó còn cũ hơn cả con CPU nữa. Nếu chúng ta tra nhanh thử thông tin cái GPU này thì à hiện giờ chắc là chỉ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:01:08"
    }
  },
  {
    "page_content": "thông tin cái GPU này thì à hiện giờ chắc là chỉ còn những cái trang, những cái shop nào mà họ bán đồ cũ thì may ra họ còn mẫu GPU này thôi ha. Đó. Đây a à shop này cũng còn khá nhiều. Còn một số shop khác thì đây mẫu 6700XT này hiện bán với giá khoảng 7 triệu. Chắc là hàng tồn kho của họ. Còn nếu hàng nếu các bạn mua đồ second-hand thì chắc nó sẽ rẻ hơn nữa ha. Thì GPU này nó có cấu hình cũng a không phải là cao lắm ha. Nếu các bạn đây có thể thấy hình dáng của GPU về thông số thì nó chỉ có 12",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 1,
      "start_timestamp": "0:01:02",
      "end_timestamp": "0:01:58"
    }
  },
  {
    "page_content": "hình dáng của GPU về thông số thì nó chỉ có 12 GB V RAM thôi, ít hơn cả GPU Tesla T4 mà Google Colab sẽ cung cấp miễn phí của các bạn. Nhưng mẫu GPU này là mẫu GPU cho người dùng cá nhân và thường. Nó hướng đến việc ừ tăng tốc các cái thao tác xử lý liên quan đến đồ họa nhiều hơn là các cái thao tác xử lý tính toán trên máy chủ. Nên nó sẽ yêu cầu nguồn điện cao hơn và tốc độ chạy của nó thì nhiều khi cũng có thể là nhanh hơn Tesla T4 nhưng mà VRAM của nó sẽ hạn chế hơn. Và cái mẫu GPU này thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 2,
      "start_timestamp": "0:01:52",
      "end_timestamp": "0:02:33"
    }
  },
  {
    "page_content": "của nó sẽ hạn chế hơn. Và cái mẫu GPU này thì các bạn không thể gắn nhiều quá nhiều GPU vào cùng một máy như mẫu GPU dành riêng cho server được ha. Trên server một máy chủ có thể gắn đến hàng vài chục ha. Mấy cái nhiều nhất có thể gắn đến khoảng hai mươi mấy cái GPU. Còn nếu trong máy tính cá nhân của các bạn thì thường mỗi máy tính chỉ gắn được một GPU này mà thôi. Đó thì ở đây chúng ta có một GPU duy nhất. GPU này hiện tại cũng đang được sử dụng khá nhiều ha. Đó, đây là GPU duy nhất có trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 3,
      "start_timestamp": "0:02:27",
      "end_timestamp": "0:03:10"
    }
  },
  {
    "page_content": "khá nhiều ha. Đó, đây là GPU duy nhất có trong máy tính nên nó sẽ phải gánh cả các cái ứng dụng đồ họa mà máy đang chạy. Đặc biệt là ứng dụng quay video, bài giảng chẳng hạn thì các bạn thấy VRAM chưa gì là nó đã dùng gần phân nửa rồi đó. Và utilization đang cỡ khoảng 40%. Đây là chỉ là những phần mềm chạy trên máy và để quay video bài giảng này mà thôi ha. Chưa đụng gì đến machine learning. Thì nếu chúng ta cần, chúng ta vẫn có thể chạy cái code machine learning của chúng ta trên con GPU này.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 4,
      "start_timestamp": "0:03:05",
      "end_timestamp": "0:03:48"
    }
  },
  {
    "page_content": "machine learning của chúng ta trên con GPU này. Mặc dù nó sẽ không phải là tối ưu như là chạy trên một server đó. Nhưng nếu các bạn biết cách cài đặt các bạn vẫn có thể chạy được. Thì đầu tiên mỗi GPU tùy hãng sản xuất ha, tùy công nghệ mà để chạy các bạn sẽ phải có cách cài đặt TensorFlow khác nhau. Thông thường cách cài đặt dễ nhất nếu các bạn dùng cách cài đặt mặc định này nó chỉ work nếu như các bạn dùng GPU đời mới mà của hãng Nvidia. Còn với hãng AMD thì các bạn sẽ nếu như dùng Docker các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 5,
      "start_timestamp": "0:03:44",
      "end_timestamp": "0:04:48"
    }
  },
  {
    "page_content": "hãng AMD thì các bạn sẽ nếu như dùng Docker các bạn sẽ phải sử dụng docker hỗ trợ một cái công nghệ gọi là ROCm rồi. ROCm đây là cái Docker Hub image à tức là cái máy ảo chứa cài sẵn TensorFlow và cái thư viện tên là ROCm do AMD họ cung cấp và quản lý. Rồi đây là cái công cụ để giúp TensorFlow chạy được trên những cái GPU do hãng AMD họ phát hành. Thì các bạn sẽ thấy cái lệnh để chạy lệnh Docker run ở đây sẽ có cú pháp phức tạp hơn à tương đối là nhiều so với cái lệnh Docker dùng để chạy trên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 6,
      "start_timestamp": "0:04:40",
      "end_timestamp": "0:05:41"
    }
  },
  {
    "page_content": "là nhiều so với cái lệnh Docker dùng để chạy trên GPU Nvidia ha. Đó và ở đây thì chúng ta chạy thử thôi. Đó. Rồi và máy này thì đã có chạy trước đó. Rồi lệnh chạy sẽ bắt đầu như thế này ha. Docker run. Rồi chúng ta cũng share cái working directory hiện hành với cái máy ảo. Và đây là các cái thông số à mặc định theo gợi ý của Docker Hub do AMD họ đề xuất. Rồi và chúng ta share cái folder này ha. Chúng ta sẽ chạy một cái terminal bash để chúng ta có thể tương tác với lại cái máy ảo. Sau khi chạy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 7,
      "start_timestamp": "0:05:34",
      "end_timestamp": "0:06:23"
    }
  },
  {
    "page_content": "có thể tương tác với lại cái máy ảo. Sau khi chạy xong thì chắc các bạn còn nhớ công việc đầu tiên chúng ta sẽ phải install hai cái thư viện mà mặc định TensorFlow họ không cung cấp sẵn đó là Matplotlib và tensorflow-datasets. Rồi và máy này cài sẵn rồi thì nên nó chạy cũng khá là nhanh thôi ha. Requirement đều satisfied sẵn. Sau khi cài đặt xong thì à ở đây chúng ta sẽ tạm thời chia màn hình này làm hai phần. một bên để các bạn à quan sát tài nguyên được sử dụng của máy. Đó, bên dưới là tài",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 8,
      "start_timestamp": "0:06:19",
      "end_timestamp": "0:07:24"
    }
  },
  {
    "page_content": "nguyên được sử dụng của máy. Đó, bên dưới là tài nguyên được sử dụng của máy hiện hành ha. Và đây là tài nguyên ở phía bên tay phải trái của chúng ta là các cái tài nguyên của GPU ha. Và bên tay phải phía trên là các cái tài nguyên của ừ CPU. Bây giờ chúng ta sẽ chạy lại cái ừ máy ảo này ha. Ờ đầu tiên chúng ta vẫn chạy lại hai lệnh cài đặt à thư viện ha. Mặc dù thư viện này có cài sẵn rồi nhưng mà do máy ảo. Khi các bạn à tắt máy ảo Docker và các bạn chạy lại đôi khi các bạn sẽ được một cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 9,
      "start_timestamp": "0:07:21",
      "end_timestamp": "0:08:12"
    }
  },
  {
    "page_content": "các bạn chạy lại đôi khi các bạn sẽ được một cái máy ảo hoàn toàn mới tùy vào các tham số. Đó cả kỹ năng sử dụng Docker cũng là một kiến thức các bạn à nên trang bị. Nó cũng khá hữu ích trong rất nhiều tình huống. Đó, thư viện đã cài sẵn. Thì bây giờ chúng ta chạy file Python mà chúng ta đã tải về hồi nãy. Rồi và đầu tiên các bạn có thể nhận thấy ngay đó là chúng ta đang thực hiện quá trình training nhưng tốc độ nó đang khá chậm. nó lên tới hàng trăm mili giây cho mỗi batch và GPU ở bên này các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 10,
      "start_timestamp": "0:08:07",
      "end_timestamp": "0:09:07"
    }
  },
  {
    "page_content": "trăm mili giây cho mỗi batch và GPU ở bên này các bạn có thể thấy GPU utilization ở đây chỉ có vài chục phần trăm thôi và còn CPU utilization thì đang đến 80% như vậy có nghĩa là chúng ta vẫn chưa dùng được GPU ha. Đó nếu các bạn quan sát kỹ cái thông báo thông báo lúc mà nếu các bạn quan sát kỹ thông báo lúc trước khi bắt đầu vào quá trình training, các bạn sẽ thấy ở đây một thông báo warning là ignoring visible GPU device ha. có nghĩa là máy tính của chúng ta có cài có cắm GPU sẵn nhưng à",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 11,
      "start_timestamp": "0:09:03",
      "end_timestamp": "0:09:51"
    }
  },
  {
    "page_content": "tính của chúng ta có cài có cắm GPU sẵn nhưng à TensorFlow mặc định họ chỉ chạy ROCm ROCm TensorFlow ha, mà là một phiên bản TensorFlow khác do AMD họ cung cấp để các bạn sử dụng chung với card của AMD GPU thì họ chỉ hỗ trợ các cái version này thôi đây. Còn version của card 6700XT là version có mã hiệu mặc dù khác nhau chỉ một tí thôi ha. GFX 1031, nhưng phiên bản được hỗ trợ lại là 1030 nên card của chúng ta bị ignore không dùng đến. Muốn sử dụng đến cái card này chúng ta sẽ phải chạy với một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 12,
      "start_timestamp": "0:09:46",
      "end_timestamp": "0:10:40"
    }
  },
  {
    "page_content": "đến cái card này chúng ta sẽ phải chạy với một cái tham số hay một cái environment variable. Environment variable là các cái biến của hệ điều hành ha. Đây là biến của hệ điều hành không phải là biến trong Python nha. Và các bạn sẽ gán giá trị cho các cái biến này trước khi các bạn chạy lệnh Python. Đó. Ở đây chúng ta sẽ gán override GFX_VERSION à có nghĩa là chúng ta yêu cầu ROCm TensorFlow nhận cái GPU của chúng ta là phiên bản 10.3.0 chứ không phải là 10.3.1 như hồi nãy nó nhận ra. Đó. Rồi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 13,
      "start_timestamp": "0:10:35",
      "end_timestamp": "0:11:23"
    }
  },
  {
    "page_content": "phải là 10.3.1 như hồi nãy nó nhận ra. Đó. Rồi nếu như chúng ta chạy với tham số này với cái environment variable này thì bây giờ lệnh chạy của chúng ta rồi đã có một chút thay đổi ha. Đó các bạn có thể thấy ở đây thông báo là created device thay vì ignoring device. Và nếu như các bạn chạy thử trên máy mình bây giờ các bạn sẽ thấy máy sẽ bị lag khá đáng kể tại vì GPU đang được huy động. Đó các bạn nhìn bên tay phải ha. À có thể video bây giờ nó sẽ hơi giật tại vì GPU đang được huy động gần như",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 14,
      "start_timestamp": "0:11:15",
      "end_timestamp": "0:12:01"
    }
  },
  {
    "page_content": "sẽ hơi giật tại vì GPU đang được huy động gần như là 100% đó. Và nó xung đột với cái phần mềm thu quay phim ghi hình bài giảng. Nhưng mà bù lại các bạn có thể nhìn thấy thời gian chạy của một batch nó đang a cải thiện khá là nhiều ha. Đó. Rồi chúng ta sẽ tạm à dừng cái việc training model ở đây ha. Đó tại vì à GPU được utilize quá nhiều thì phần giao diện đồ họa mà các bạn làm việc với máy nó cũng sẽ bị ảnh hưởng ha. Do máy chúng ta chỉ có một GPU duy nhất thôi. Nó phải handle quá nhiều công",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 15,
      "start_timestamp": "0:11:55",
      "end_timestamp": "0:13:10"
    }
  },
  {
    "page_content": "GPU duy nhất thôi. Nó phải handle quá nhiều công việc. Đó. Đó cũng là nhược điểm giữa GPU của máy tính cá nhân so với các GPU trên server. Trừ khi các bạn có nhiều máy tính thì các bạn dùng riêng hoặc các bạn có máy tính nhiều GPU thì các bạn mới có thể tránh khỏi cái vấn đề này. Thì ở đây các bạn có thể thấy thời gian training cho một batch nó dao động khá là nhiều. Đó do GPU được huy động cho nhiều công việc khác nhau nhưng mà nhìn chung nó nhanh hơn là chúng ta sử dụng so với CPU. Rồi và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 16,
      "start_timestamp": "0:13:04",
      "end_timestamp": "0:13:55"
    }
  },
  {
    "page_content": "nhanh hơn là chúng ta sử dụng so với CPU. Rồi và cuối cùng hình như để cho cái quá trình training này mượt hơn thì các bạn sẽ phải set thêm một cái environment nữa. Environment này là của TensorFlow ha. Đó có nghĩa là TF_FORCE_GPU_ALLOW_GROWTH. Rồi do TensorFlow mặc định khi TensorFlow hoạt động nó sẽ chiếm dụng toàn bộ tài nguyên GPU của máy. Vì thường trên các cái máy server GPU sinh ra chỉ để phục vụ cho các cái hoạt động tính toán nên TensorFlow nó sẽ huy động toàn bộ cái GPU của máy. Còn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 17,
      "start_timestamp": "0:13:48",
      "end_timestamp": "0:00:24"
    }
  },
  {
    "page_content": "nó sẽ huy động toàn bộ cái GPU của máy. Còn trên máy tính cá nhân thì các bạn dùng GPU cho nhiều công việc thì các bạn set cái environment này bằng true. GPU allow growth có nghĩa là mặc định TensorFlow sẽ không chiếm dụng toàn bộ GPU mà khi nào nó cần dùng thì nó mới xin cấp phát tài nguyên. Việc này có thể khiến cho cái quá trình training nó chậm đi ha. nhưng mà bù lại nó không ảnh hưởng nhiều so với các cái process khác trên máy. Hi vọng là quá trình chạy với máy tính cá nhân các bạn set cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 18,
      "start_timestamp": "0:00:21",
      "end_timestamp": "0:01:15"
    }
  },
  {
    "page_content": "trình chạy với máy tính cá nhân các bạn set cái environment variable này thì quá trình chạy nó sẽ mượt mà hơn. Đó. Rồi và bây giờ các bạn có thể thấy GPU của chúng ta nó cũng đang được huy động khá là nhiều nhưng nó không bị chiếm dụng toàn bộ như lần trước. Và CPU thì đang nghỉ ngơi rất là chill ha. CPU utilization chỉ Bây giờ chúng ta sẽ thử chạy trên một cái GPU à gắn với máy tính thật của các bạn, không phải thông qua máy ảo Google Colab. Thì liệu các bạn có thể chạy được những cái chương",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 19,
      "start_timestamp": "0:01:08",
      "end_timestamp": "0:02:05"
    }
  },
  {
    "page_content": "liệu các bạn có thể chạy được những cái chương trình machine learning này hay không? Thì trên nếu hồi nãy các bạn tinh ý thì sẽ thấy trên máy tính của thầy hiện tại cũng có một cái GPU ha. Đó là con AMD Radeon 6700XT. Đó, GPU này thì nó cũng đời cũ rồi, thậm chí nó còn cũ hơn cả con CPU nữa. Nếu chúng ta tra nhanh thử thông tin cái GPU này thì à hiện giờ chắc là chỉ còn những cái trang, những cái shop nào mà họ bán đồ cũ thì may ra họ còn mẫu GPU này thôi ha. Đó. Đây a à shop này cũng còn khá",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 20,
      "start_timestamp": "0:01:58",
      "end_timestamp": "0:02:37"
    }
  },
  {
    "page_content": "này thôi ha. Đó. Đây a à shop này cũng còn khá nhiều. Còn một số shop khác thì đây mẫu 6700XT này hiện bán với giá khoảng 7 triệu. Chắc là hàng tồn kho của họ. Còn nếu hàng nếu các bạn mua đồ second-hand thì chắc nó sẽ rẻ hơn nữa ha. Thì GPU này nó có cấu hình cũng a không phải là cao lắm ha. Nếu các bạn đây có thể thấy hình dáng của GPU về thông số thì nó chỉ có 12 GB V RAM thôi, ít hơn cả GPU Tesla T4 mà Google Colab sẽ cung cấp miễn phí của các bạn. Nhưng mẫu GPU này là mẫu GPU cho người",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 21,
      "start_timestamp": "0:02:33",
      "end_timestamp": "0:03:19"
    }
  },
  {
    "page_content": "các bạn. Nhưng mẫu GPU này là mẫu GPU cho người dùng cá nhân và thường. Nó hướng đến việc ừ tăng tốc các cái thao tác xử lý liên quan đến đồ họa nhiều hơn là các cái thao tác xử lý tính toán trên máy chủ. Nên nó sẽ yêu cầu nguồn điện cao hơn và tốc độ chạy của nó thì nhiều khi cũng có thể là nhanh hơn Tesla T4 nhưng mà VRAM của nó sẽ hạn chế hơn. Và cái mẫu GPU này thì các bạn không thể gắn nhiều quá nhiều GPU vào cùng một máy như mẫu GPU dành riêng cho server được ha. Trên server một máy chủ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 22,
      "start_timestamp": "0:03:14",
      "end_timestamp": "0:03:57"
    }
  },
  {
    "page_content": "riêng cho server được ha. Trên server một máy chủ có thể gắn đến hàng vài chục ha. Mấy cái nhiều nhất có thể gắn đến khoảng hai mươi mấy cái GPU. Còn nếu trong máy tính cá nhân của các bạn thì thường mỗi máy tính chỉ gắn được một GPU này mà thôi. Đó thì ở đây chúng ta có một GPU duy nhất. GPU này hiện tại cũng đang được sử dụng khá nhiều ha. Đó, đây là GPU duy nhất có trong máy tính nên nó sẽ phải gánh cả các cái ứng dụng đồ họa mà máy đang chạy. Đặc biệt là ứng dụng quay video, bài giảng chẳng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 23,
      "start_timestamp": "0:03:50",
      "end_timestamp": "0:04:55"
    }
  },
  {
    "page_content": "Đặc biệt là ứng dụng quay video, bài giảng chẳng hạn thì các bạn thấy VRAM chưa gì là nó đã dùng gần phân nửa rồi đó. Và utilization đang cỡ khoảng 40%. Đây là chỉ là những phần mềm chạy trên máy và để quay video bài giảng này mà thôi ha. Chưa đụng gì đến machine learning. Thì nếu chúng ta cần, chúng ta vẫn có thể chạy cái code machine learning của chúng ta trên con GPU này. Mặc dù nó sẽ không phải là tối ưu như là chạy trên một server đó. Nhưng nếu các bạn biết cách cài đặt các bạn vẫn có thể",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 24,
      "start_timestamp": "0:04:48",
      "end_timestamp": "0:05:48"
    }
  },
  {
    "page_content": "nếu các bạn biết cách cài đặt các bạn vẫn có thể chạy được. Thì đầu tiên mỗi GPU tùy hãng sản xuất ha, tùy công nghệ mà để chạy các bạn sẽ phải có cách cài đặt TensorFlow khác nhau. Thông thường cách cài đặt dễ nhất nếu các bạn dùng cách cài đặt mặc định này nó chỉ work nếu như các bạn dùng GPU đời mới mà của hãng Nvidia. Còn với hãng AMD thì các bạn sẽ nếu như dùng Docker các bạn sẽ phải sử dụng docker hỗ trợ một cái công nghệ gọi là ROCm rồi. ROCm đây là cái Docker Hub image à tức là cái máy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 25,
      "start_timestamp": "0:05:41",
      "end_timestamp": "0:06:45"
    }
  },
  {
    "page_content": "ROCm đây là cái Docker Hub image à tức là cái máy ảo chứa cài sẵn TensorFlow và cái thư viện tên là ROCm do AMD họ cung cấp và quản lý. Rồi đây là cái công cụ để giúp TensorFlow chạy được trên những cái GPU do hãng AMD họ phát hành. Thì các bạn sẽ thấy cái lệnh để chạy lệnh Docker run ở đây sẽ có cú pháp phức tạp hơn à tương đối là nhiều so với cái lệnh Docker dùng để chạy trên GPU Nvidia ha. Đó và ở đây thì chúng ta chạy thử thôi. Đó. Rồi và máy này thì đã có chạy trước đó. Rồi lệnh chạy sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 26,
      "start_timestamp": "0:06:37",
      "end_timestamp": "0:07:31"
    }
  },
  {
    "page_content": "máy này thì đã có chạy trước đó. Rồi lệnh chạy sẽ bắt đầu như thế này ha. Docker run. Rồi chúng ta cũng share cái working directory hiện hành với cái máy ảo. Và đây là các cái thông số à mặc định theo gợi ý của Docker Hub do AMD họ đề xuất. Rồi và chúng ta share cái folder này ha. Chúng ta sẽ chạy một cái terminal bash để chúng ta có thể tương tác với lại cái máy ảo. Sau khi chạy xong thì chắc các bạn còn nhớ công việc đầu tiên chúng ta sẽ phải install hai cái thư viện mà mặc định TensorFlow họ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 27,
      "start_timestamp": "0:07:24",
      "end_timestamp": "0:08:23"
    }
  },
  {
    "page_content": "hai cái thư viện mà mặc định TensorFlow họ không cung cấp sẵn đó là Matplotlib và tensorflow-datasets. Rồi và máy này cài sẵn rồi thì nên nó chạy cũng khá là nhanh thôi ha. Requirement đều satisfied sẵn. Sau khi cài đặt xong thì à ở đây chúng ta sẽ tạm thời chia màn hình này làm hai phần. một bên để các bạn à quan sát tài nguyên được sử dụng của máy. Đó, bên dưới là tài nguyên được sử dụng của máy hiện hành ha. Và đây là tài nguyên ở phía bên tay phải trái của chúng ta là các cái tài nguyên của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 28,
      "start_timestamp": "0:08:17",
      "end_timestamp": "0:09:14"
    }
  },
  {
    "page_content": "phải trái của chúng ta là các cái tài nguyên của GPU ha. Và bên tay phải phía trên là các cái tài nguyên của ừ CPU. Bây giờ chúng ta sẽ chạy lại cái ừ máy ảo này ha. Ờ đầu tiên chúng ta vẫn chạy lại hai lệnh cài đặt à thư viện ha. Mặc dù thư viện này có cài sẵn rồi nhưng mà do máy ảo. Khi các bạn à tắt máy ảo Docker và các bạn chạy lại đôi khi các bạn sẽ được một cái máy ảo hoàn toàn mới tùy vào các tham số. Đó cả kỹ năng sử dụng Docker cũng là một kiến thức các bạn à nên trang bị. Nó cũng khá",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 29,
      "start_timestamp": "0:09:07",
      "end_timestamp": "0:09:55"
    }
  },
  {
    "page_content": "một kiến thức các bạn à nên trang bị. Nó cũng khá hữu ích trong rất nhiều tình huống. Đó, thư viện đã cài sẵn. Thì bây giờ chúng ta chạy file Python mà chúng ta đã tải về hồi nãy. Rồi và đầu tiên các bạn có thể nhận thấy ngay đó là chúng ta đang thực hiện quá trình training nhưng tốc độ nó đang khá chậm. nó lên tới hàng trăm mili giây cho mỗi batch và GPU ở bên này các bạn có thể thấy GPU utilization ở đây chỉ có vài chục phần trăm thôi và còn CPU utilization thì đang đến 80% như vậy có nghĩa",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 30,
      "start_timestamp": "0:09:51",
      "end_timestamp": "0:10:45"
    }
  },
  {
    "page_content": "CPU utilization thì đang đến 80% như vậy có nghĩa là chúng ta vẫn chưa dùng được GPU ha. Đó nếu các bạn quan sát kỹ cái thông báo thông báo lúc mà nếu các bạn quan sát kỹ thông báo lúc trước khi bắt đầu vào quá trình training, các bạn sẽ thấy ở đây một thông báo warning là ignoring visible GPU device ha. có nghĩa là máy tính của chúng ta có cài có cắm GPU sẵn nhưng à TensorFlow mặc định họ chỉ chạy ROCm ROCm TensorFlow ha, mà là một phiên bản TensorFlow khác do AMD họ cung cấp để các bạn sử",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 31,
      "start_timestamp": "0:10:40",
      "end_timestamp": "0:11:34"
    }
  },
  {
    "page_content": "TensorFlow khác do AMD họ cung cấp để các bạn sử dụng chung với card của AMD GPU thì họ chỉ hỗ trợ các cái version này thôi đây. Còn version của card 6700XT là version có mã hiệu mặc dù khác nhau chỉ một tí thôi ha. GFX 1031, nhưng phiên bản được hỗ trợ lại là 1030 nên card của chúng ta bị ignore không dùng đến. Muốn sử dụng đến cái card này chúng ta sẽ phải chạy với một cái tham số hay một cái environment variable. Environment variable là các cái biến của hệ điều hành ha. Đây là biến của hệ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 32,
      "start_timestamp": "0:11:26",
      "end_timestamp": "0:12:05"
    }
  },
  {
    "page_content": "cái biến của hệ điều hành ha. Đây là biến của hệ điều hành không phải là biến trong Python nha. Và các bạn sẽ gán giá trị cho các cái biến này trước khi các bạn chạy lệnh Python. Đó. Ở đây chúng ta sẽ gán override GFX_VERSION à có nghĩa là chúng ta yêu cầu ROCm TensorFlow nhận cái GPU của chúng ta là phiên bản 10.3.0 chứ không phải là 10.3.1 như hồi nãy nó nhận ra. Đó. Rồi nếu như chúng ta chạy với tham số này với cái environment variable này thì bây giờ lệnh chạy của chúng ta rồi đã có một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 33,
      "start_timestamp": "0:12:01",
      "end_timestamp": "0:13:19"
    }
  },
  {
    "page_content": "thì bây giờ lệnh chạy của chúng ta rồi đã có một chút thay đổi ha. Đó các bạn có thể thấy ở đây thông báo là created device thay vì ignoring device. Và nếu như các bạn chạy thử trên máy mình bây giờ các bạn sẽ thấy máy sẽ bị lag khá đáng kể tại vì GPU đang được huy động. Đó các bạn nhìn bên tay phải ha. À có thể video bây giờ nó sẽ hơi giật tại vì GPU đang được huy động gần như là 100% đó. Và nó xung đột với cái phần mềm thu quay phim ghi hình bài giảng. Nhưng mà bù lại các bạn có thể nhìn thấy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 34,
      "start_timestamp": "0:13:10",
      "end_timestamp": "0:15:03"
    }
  },
  {
    "page_content": "giảng. Nhưng mà bù lại các bạn có thể nhìn thấy thời gian chạy của một batch nó đang a cải thiện khá là nhiều ha. Đó. Rồi chúng ta sẽ tạm à dừng cái việc training model ở đây ha. Đó tại vì à GPU được utilize quá nhiều thì phần giao diện đồ họa mà các bạn làm việc với máy nó cũng sẽ bị ảnh hưởng ha. Do máy chúng ta chỉ có một GPU duy nhất thôi. Nó phải handle quá nhiều công việc. Đó. Đó cũng là nhược điểm giữa GPU của máy tính cá nhân so với các GPU trên server. Trừ khi các bạn có nhiều máy tính",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 35,
      "start_timestamp": "0:14:55",
      "end_timestamp": "0:14:53"
    }
  },
  {
    "page_content": "trên server. Trừ khi các bạn có nhiều máy tính thì các bạn dùng riêng hoặc các bạn có máy tính nhiều GPU thì các bạn mới có thể tránh khỏi cái vấn đề này. Thì ở đây các bạn có thể thấy thời gian training cho một batch nó dao động khá là nhiều. Đó do GPU được huy động cho nhiều công việc khác nhau nhưng mà nhìn chung nó nhanh hơn là chúng ta sử dụng so với CPU. Rồi và cuối cùng hình như để cho cái quá trình training này mượt hơn thì các bạn sẽ phải set thêm một cái environment nữa. Environment",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 36,
      "start_timestamp": "0:15:44",
      "end_timestamp": "0:15:39"
    }
  },
  {
    "page_content": "set thêm một cái environment nữa. Environment này là của TensorFlow ha. Đó có nghĩa là TF_FORCE_GPU_ALLOW_GROWTH. Rồi do TensorFlow mặc định khi TensorFlow hoạt động nó sẽ chiếm dụng toàn bộ tài nguyên GPU của máy. Vì thường trên các cái máy server GPU sinh ra chỉ để phục vụ cho các cái hoạt động tính toán nên TensorFlow nó sẽ huy động toàn bộ cái GPU của máy. Còn trên máy tính cá nhân thì các bạn dùng GPU cho nhiều công việc thì các bạn set cái environment này bằng true. GPU allow growth có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 37,
      "start_timestamp": "0:15:34",
      "end_timestamp": "0:16:16"
    }
  },
  {
    "page_content": "environment này bằng true. GPU allow growth có nghĩa là mặc định TensorFlow sẽ không chiếm dụng toàn bộ GPU mà khi nào nó cần dùng thì nó mới xin cấp phát tài nguyên. Việc này có thể khiến cho cái quá trình training nó chậm đi ha. nhưng mà bù lại nó không ảnh hưởng nhiều so với các cái process khác trên máy. Hi vọng là quá trình chạy với máy tính cá nhân các bạn set cái environment variable này thì quá trình chạy nó sẽ mượt mà hơn. Đó. Rồi và bây giờ các bạn có thể thấy GPU của chúng ta nó cũng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 38,
      "start_timestamp": "0:16:11",
      "end_timestamp": "0:16:48"
    }
  },
  {
    "page_content": "giờ các bạn có thể thấy GPU của chúng ta nó cũng đang được huy động khá là nhiều nhưng nó không bị chiếm dụng toàn bộ như lần trước. Và CPU thì đang nghỉ ngơi rất là chill ha. CPU utilization chỉ loanh quanh đâu đó cỡ 15% thôi. Chủ yếu là để lấy dữ liệu và đưa cho GPU làm việc. Trong khi đó con Ryzen 5950X nó đang khá là chill. À còn con 6700XT của chúng ta thì đấy các bạn thấy chỗ GFX à utilization này nó lên tới khoảng 90%. Nó đang làm việc à rất là vất vả nhưng mà bù lại đó. Nếu các bạn quan",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 39,
      "start_timestamp": "0:16:36",
      "end_timestamp": "0:16:48"
    }
  },
  {
    "page_content": "là vất vả nhưng mà bù lại đó. Nếu các bạn quan sát thời gian chạy cho mỗi batch cũng như thời gian chạy cho toàn bộ một vòng lập một epoch trong quá trình training nó đã giảm đi đáng kể. Đó với ở đây chúng ta thấy có một số epoch chạy nhanh nhất thì nó chỉ khoảng 14 giây thôi. Epoch đầu tiên thì luôn luôn là lâu do máy phải chuẩn bị dữ liệu. Còn các epoch sau thì các bạn thấy thường chạy rất là ổn định, chỉ khoảng 14 giây cho một epoch trong khi nếu các bạn còn nhớ với GPU ha cũng là GPU Tesla",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 40,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "nếu các bạn còn nhớ với GPU ha cũng là GPU Tesla T4 mà Nvidia cung cấp cho các bạn thì thời gian cho một epoch thường khoảng 50 giây. Đó như vậy cái GPU dân dụng của chúng ta, cái GPU cho người dùng cá nhân con 6700XT đó à cỡ 7 triệu mấy này nó chạy nhanh gần gấp ba lần GPU của Google Colab cung cấp ha. Nếu các bạn biết cách setup, cài đặt và sử dụng thì các bạn có thể sử dụng GPU trên máy tính cá nhân của mình. Nhưng mà không phải máy tính cá nhân của ai cũng có GPU này và không phải GPU nào",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 41,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "nhân của ai cũng có GPU này và không phải GPU nào cũng có thể hỗ trợ quá trình training nhé các bạn. Đó, các bạn sẽ phải học hỏi và làm quen với khá nhiều kiến thức mới nếu như các bạn muốn sử dụng máy của mình cho các cái hoạt động liên quan đến machine learning. Bù lại các bạn có thể có cái máy nó chạy nhanh hơn là cái phiên bản default miễn phí trên Google Colab ha. Thì trong môn này các bạn có thể sử dụng phương pháp nào cũng được ha. Các bạn có thể sử dụng Google Colab cho nó đơn giản để",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 42,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "có thể sử dụng Google Colab cho nó đơn giản để chúng ta làm quen với bài học. Và nếu muốn thì các bạn hoàn toàn vẫn có thể chạy cái chương trình Python này trên máy của mình. Trong các bài tiếp theo, chúng ta sẽ đi sâu hơn về ngôn ngữ lập trình Python. để giúp các bạn có thể tự tin làm chủ ngôn ngữ này cho những bài học sắp tới.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=cPenwbVXQGM",
      "filename": "cPenwbVXQGM",
      "title": "[CS114 - Tutorial] Google Colab (Phần 5)",
      "chunk_id": 43,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Tiếp theo thì để minh họa cho thuật toán Logistic Regression với phương pháp huấn luyện đó là Gradient Descent thì chúng ta cũng sẽ mô phỏng cái dữ liệu của mình ở trong cái không gian hai chiều. Thì trong trường hợp này thì hai cái chiều đặc trưng của mình nó sẽ được ký hiệu là x1 và x2. Và một cái điểm của mình thì sẽ là một cái tọa độ cặp tọa độ là x1 và x2. Đồng thời nó sẽ có cái nhãn y. Thì ví dụ như những cái điểm nào mà nằm ở phía trên thì cái nhãn của mình sẽ là bằng 0. Và những cái điểm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:54"
    }
  },
  {
    "page_content": "cái nhãn của mình sẽ là bằng 0. Và những cái điểm nào ở phía dưới thì nhãn của mình là bằng 1. Ví dụ vậy. Rồi đây là y = 0. Và chúng ta sẽ có rất nhiều những cái tập điểm. Và khi chúng ta huấn luyện thì chúng ta sẽ tìm ra được một cái đường để phân tách hai cái tập điểm này ra làm hai phần giống như ở trên à cái hình vẽ. Thế thì ở đây chúng ta sẽ dùng cái thư viện lên để khởi bước đầu tiên đó là khởi tạo à các cái dữ liệu. Thì ở đây X của mình nó sẽ là các cái giá trị là ờ các cái cặp giá trị",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 1,
      "start_timestamp": "0:00:49",
      "end_timestamp": "0:01:38"
    }
  },
  {
    "page_content": "nó sẽ là các cái giá trị là ờ các cái cặp giá trị thì các cái cặp giá trị này thì nó sẽ xoay xung quanh các cái điểm. Thì lấy ví dụ như ở đây chúng ta sẽ có một cái điểm tâm ha. cái tâm thì cái tâm này ví dụ như là 1 và 3. Rồi, tương tự như vậy, các cái điểm dấu cộng ở đây thì nó sẽ có một cái điểm tâm. Ví dụ như ở đây là 1 thì chúng ta sẽ có hai cái tập điểm thì np. array rồi thì chúng ta sẽ có cái tập điểm đó là 1 và 3 rồi. Rồi sau đó thì chúng ta sẽ cộng cho một cái đại lượng random noise",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 2,
      "start_timestamp": "0:01:34",
      "end_timestamp": "0:02:52"
    }
  },
  {
    "page_content": "ta sẽ cộng cho một cái đại lượng random noise với cái phân bố đó là 01 mean là bằng 0 và standard deviation là bằng 1. Rồi và ở đây thì nó sẽ có hai chiều. Do đó thì ở đây chúng ta sẽ sửa lại một chút xíu. Và cái kích thước của mình thì nó sẽ là bằng đầu tiên đó là số chiều của mình sẽ là hai hàng nhưng mà cái số dòng á thì nó sẽ là số mẫu dữ liệu. Thế thì n_samples ví dụ như chúng ta muốn mỗi một cái tập dữ liệu của mình, mỗi một cái nhóm dữ liệu của mình nó sẽ có 10 mẫu đi thì ở đây n sẽ là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 3,
      "start_timestamp": "0:02:44",
      "end_timestamp": "0:03:38"
    }
  },
  {
    "page_content": "của mình nó sẽ có 10 mẫu đi thì ở đây n sẽ là bằng bằng 10. Rồi bây giờ chúng ta sẽ in ra thử xem x của mình nó nhìn như thế nào ha. Thì chúng ta sẽ lưu lại và run. Đó. Rồi thì bây giờ chúng ta sẽ à khởi tạo cho các cái tập điểm thứ hai đó là các cái điểm mà dấu cộng. Thì đây là X_red thì tương ứng chúng ta sẽ có à không ở đây sẽ là x dùng cho đúng từ ha. Kẻo không chúng ta sẽ bị nhầm. Còn đây là X_green đi là hai tập điểm. Thì cái tập điểm thứ hai nó sẽ xoay xung quanh cái tọa độ đó là 3 và 1.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 4,
      "start_timestamp": "0:03:32",
      "end_timestamp": "0:04:31"
    }
  },
  {
    "page_content": "nó sẽ xoay xung quanh cái tọa độ đó là 3 và 1. Và bây giờ chúng ta sẽ vẽ lên trên cái thay vì chúng ta in ra thì chúng ta sẽ dùng hàm matplotlib pyplot as plt plt rồi chúng ta sẽ trực quan hóa. Thì đối với các cái điểm màu màu đỏ thì chúng ta sẽ dùng là plt.plot và tọa độ sẽ là X_red rồi lấy tọa độ là từ 0 cho đến : 0 và X_red tọa độ 1 lấy cho đến :. Và với mỗi một cái điểm này thì chúng ta sẽ dùng cái ký hiệu đó là r. ha màu đỏ và một cái dấu chấm. Rồi plt.show() tương tự như vậy thì chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 5,
      "start_timestamp": "0:04:20",
      "end_timestamp": "0:05:34"
    }
  },
  {
    "page_content": "Rồi plt.show() tương tự như vậy thì chúng ta sẽ có cái điểm màu xanh. Rồi và ở đây thì chúng ta sẽ dùng màu là màu green và dấu cộng. Rồi bây giờ chúng ta sẽ chạy thử để xem chương trình nó ra như thế nào. Thì rõ ràng chúng ta thấy là nó đã tách ra làm hai. Tuy nhiên vì cái nhiễu của chúng ta khá lớn nên có thể là nó sẽ có cái phần giao thoa ở đây một chút. Thì để tránh cái vùng giao thoa này thì chúng ta có thể nhân cái hệ số của mình thấp hơn một chút. Ví dụ như ở đây là 0.5. Rồi ở đây là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 6,
      "start_timestamp": "0:05:29",
      "end_timestamp": "0:06:23"
    }
  },
  {
    "page_content": "một chút. Ví dụ như ở đây là 0.5. Rồi ở đây là 0.5. Rồi đó thì chúng ta thấy là hai cái tập điểm của mình nó đã tách biệt ra hoàn toàn. Nó đã tách ra hoàn toàn. Tiếp theo thì chúng ta sẽ ờ cài đặt cái thuật toán Gradient Descent. Thế thì bước đầu tiên chúng ta cũng sẽ là khởi tạo. Bước thứ hai đó là chúng ta sẽ tính đạo hàm. Và chúng ta sẽ đi cập nhật cái tham số đó thì nó còn một cái bước số ba nữa đó là chúng ta sẽ đi cập nhật cái tham số rồi và chúng ta sẽ lặp lại cho đến khi nào mà cái đạo",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 7,
      "start_timestamp": "0:06:18",
      "end_timestamp": "0:07:08"
    }
  },
  {
    "page_content": "và chúng ta sẽ lặp lại cho đến khi nào mà cái đạo hàm của mình nó sẽ hội tụ tức là nó sẽ tiến về không. Rồi bây giờ chúng ta sẽ tiến hành cài đặt cho các cái bước đầu tiên đó là khởi tạo. Thế thì các cái tham số ở đây của mình á nó sẽ có các tham số đó là W1, W2 và B. Tại sao W1 và W2 mà không phải là W? Thì vì cái đặc trưng X của mình á là nó sẽ có hai phần là thành phần trục X1 và thành phần theo trục X2. Do đó nó sẽ phải có hai tham số. À ở trên phần khởi tạo dữ liệu thì chúng ta chưa có cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 8,
      "start_timestamp": "0:07:02",
      "end_timestamp": "0:08:10"
    }
  },
  {
    "page_content": "phần khởi tạo dữ liệu thì chúng ta chưa có cái dữ liệu y đúng không? Thì y của mình nó sẽ có hai phần đó là cho y_red sẽ là np.zeros. Và cái số mẫu dữ liệu của mình thì sẽ là X.shape shape rồi y_green thì sẽ là bằng np.ones tức là các cái giá trị là số 1 và X.shape rồi thì các cái giá trị W1 và W2 thì chúng ta khởi tạo ví dụ như cho là bằng 12. W2 là bằng 34. Rồi B sẽ là bằng 5. Thì đây là những cái con số mang tính chất rất là ngẫu nhiên. Sau đó thì chúng ta sẽ vào cái vòng lặp while True thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 9,
      "start_timestamp": "0:08:03",
      "end_timestamp": "0:09:03"
    }
  },
  {
    "page_content": "thì chúng ta sẽ vào cái vòng lặp while True thì trong cái while True này chúng ta sẽ đi tính đạo hàm theo từng phần ờ từng tham số. Thì chúng ta thấy là đạo hàm theo W thì nó sẽ là bằng 1/M à tức là trung bình cộng ha. 1/M của tổng thì đây là chính là trung bình cộng. Còn à cái thành phần này thì tương ứng nó sẽ là sigmoid của WX + b. Còn y đây chính là ground truth. Đây là ground truth. Do đó thì chúng ta sẽ tính toán cái đạo hàm của nó. Thì derivative theo W1 thì sẽ là bằng np.mean của hàm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 10,
      "start_timestamp": "0:08:54",
      "end_timestamp": "0:10:06"
    }
  },
  {
    "page_content": "derivative theo W1 thì sẽ là bằng np.mean của hàm sigmoid thì ở đây chúng ta chưa có. Chúng ta sẽ cài đặt cho cái hàm sigmoid def sigmoid của x thì nó sẽ là return cho 1 chia cho 1 cộng cho np.exp của -x. Rồi thì ở đây chúng ta sẽ có cái công thức của cái hàm sigmoid rồi thì bây giờ sẽ là sigmoid của W1 nhân cho X[:, 0] cộng cho W2 nhân cho X[:, 1] cộng cho B. Tất cả trừ cho Y xong rồi lại nhân với lại à X. X thì ở đây nếu chúng ta đang ở cái thành phần nào của W thì chúng ta sẽ lấy cái thành",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 11,
      "start_timestamp": "0:10:00",
      "end_timestamp": "0:11:23"
    }
  },
  {
    "page_content": "phần nào của W thì chúng ta sẽ lấy cái thành phần tương ứng đó. Ví dụ chúng ta đang tính đạo hàm cho W1 thì ở đây nó sẽ là X[:, 0]. Thế thì bây giờ X[:, 0] của mình là gì? X[:, 0] nó sẽ là X à để ở đây cho gọn chúng ta sẽ gom cả X_red và X_green lại với nhau thành một cái X chung là bằng à np.concatenate của X_red và X_green. Rồi tương tự như vậy y thì chúng ta cũng sẽ gom lại à là bằng nối với lại np. rồi chúng ta nối lại ha. Rồi sau khi nối xong thì chúng ta sẽ lấy cái thành phần x ờ đầu tiên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 12,
      "start_timestamp": "0:11:16",
      "end_timestamp": "0:12:26"
    }
  },
  {
    "page_content": "thì chúng ta sẽ lấy cái thành phần x ờ đầu tiên đó là X[:, 0] hai chấm. Thành phần x thứ hai sẽ là X ở trên là X của chúng ta là viết viết hoa. Do đó ở đây chúng ta cũng sẽ viết hoa. Rồi X[:, 0] cộng và ở đây sẽ là X X[:, 1]. À thì cái mean này của chúng ta sẽ đặt ở bên tính luôn cả cái phần X này. Rồi như vậy thì chúng ta sẽ sửa lại công thức này một chút xíu là sigmoid trừ y rồi tất cả sẽ nhân với X. Nếu mà chúng ta tách nó ra như thế này cho nó dễ nhìn ha. Đó thì cái np.mean này nè sẽ được",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 13,
      "start_timestamp": "0:12:17",
      "end_timestamp": "0:13:26"
    }
  },
  {
    "page_content": "nó dễ nhìn ha. Đó thì cái np.mean này nè sẽ được tính cho toàn bộ cái vế này. Đó trong đó cái vế này thì nó sẽ tách ra là hai phần. Phần bên trái sẽ là hiệu số của à sigmoid trừ y. Sigmoid trừ y. Còn bên phải sẽ là X của mình. Thì X ở đây là X[:, 0] ha. X[:, 0] tại vì chúng ta đang tính cho cái thành phần đầu tiên. Tương tự như vậy cho W2 thì chúng ta sẽ sửa lại đây sẽ là à nhân với lại X[:, 1]. Đó. Rồi và thành phần cuối cùng đó là đạo hàm theo B thì nó sẽ là à ở đây chúng ta sẽ không nhân với",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 14,
      "start_timestamp": "0:13:16",
      "end_timestamp": "0:14:31"
    }
  },
  {
    "page_content": "B thì nó sẽ là à ở đây chúng ta sẽ không nhân với X nữa. Chúng ta sẽ bỏ thành phần đi. Hay nói cách khác đây chính là 1. Rồi bây giờ chúng ta sẽ thử cập nhật cái W 1, W2 và B thì W1 sẽ là bằng W1 trừ cho alpha nhân cho đạo hàm của W1. Tương tự như vậy, chúng ta sẽ có W2 cập nhật lại rồi B đạo hàm theo theo B. Rồi và bây giờ thì điều kiện dừng đó là gì? Đó là if đạo hàm à if trị tuyệt đối của derivative của W1 đủ nhỏ and đạo hàm của W2 đủ nhỏ và đạo hàm theo B đủ nhỏ thì chúng ta sẽ break rồi và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 15,
      "start_timestamp": "0:14:25",
      "end_timestamp": "0:15:27"
    }
  },
  {
    "page_content": "hàm theo B đủ nhỏ thì chúng ta sẽ break rồi và alpha ở đây chúng ta chưa có alpha và epsilon thì giả sử alpha ở đây là bằng 0.1 và epsilon là bằng 0.00 01. Rồi bây giờ chúng ta sẽ thử chạy cái thuật toán này ha để xem có lỗi gì không. Rồi thì ở đây nó sẽ có cái lỗi trong cái quá trình mà tính W2 à không biết có cái lỗi gì tại sao 4 ờ 10 ở đây lại là 310 ha W2 rồi ở đây sẽ là W1 nhân cho X[:, 0] cộng W2 nhân X[:, 1] cộng B trừ Y rồi trừ cho X[:, 0] à ở đây phải là X[:, 0] chấm ở đây phải là X[:,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 16,
      "start_timestamp": "0:15:23",
      "end_timestamp": "0:16:10"
    }
  },
  {
    "page_content": "à ở đây phải là X[:, 0] chấm ở đây phải là X[:, 1] rồi chúng ta sẽ chạy lại rồi nhưng mà bây giờ làm sao chúng ta biết là cái thuật toán này nó đã chạy thành công thì chúng ta phải trực quan hóa. Thế thì để trực quan hóa cũng tương tự, chúng ta sẽ ờ có một cái bước trực quan chúng đầu tiên chúng ta sẽ plot các cái điểm dữ liệu ra trước sau đó thì chúng ta sẽ vẽ cái đường thẳng ra thì plt.plot plt.plot thì cái điểm bên biên trái của mình nó sẽ là điểm ví dụ như là à 1 và 3 à 1 và bên trái là 1",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 17,
      "start_timestamp": "0:16:06",
      "end_timestamp": "0:17:02"
    }
  },
  {
    "page_content": "điểm ví dụ như là à 1 và 3 à 1 và bên trái là 1 và bên phải của mình sẽ là ờ 4. Ví dụ vậy. Và tương ứng thì chúng ta sẽ có cái hệ số. Thế thì bây giờ hệ số của mình sẽ được tính như thế nào? bên bên trái của mình biên trái sẽ là 1 ha và biên phải của mình sẽ là 4. Thì bây giờ mình sẽ tính toán cái giá trị theo trục y như thế nào. Thế thì chúng ta biết rằng là cái bộ hệ số của chúng ta cho cái phương trình đường thẳng đó là w1x1 cộng cho w2x2 cộng cho b là bằng 0. Đó là bằng 0. Thế thì chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 18,
      "start_timestamp": "0:16:55",
      "end_timestamp": "0:17:58"
    }
  },
  {
    "page_content": "cho b là bằng 0. Đó là bằng 0. Thế thì chúng ta đã có x1 rồi. X1 của chúng ta là chúng ta lấy là 1 và 4. Thì khi chúng ta thế vào chúng ta sẽ có được cái x2. Thì x2 sẽ là bằng gì? Sẽ là bằng trừ của w1x1 cộng b tất cả chia cho w à chia cho w2. Thì đây là cái công thức để tính cho cái x2 của mình. Do đó thì ở đây chúng ta sẽ sửa lại nó sẽ là bằng trừ của w1x1 cộng b tất cả chia cho w2 ha. Thì à thế vô w1 chính là cái giá trị w1 của mình. Rồi x1 thì thực ra nó chính là giá trị từ đầu tiên nó sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 19,
      "start_timestamp": "0:17:51",
      "end_timestamp": "0:19:21"
    }
  },
  {
    "page_content": "thì thực ra nó chính là giá trị từ đầu tiên nó sẽ là số 1 đó cộng b chia cho w2. Rồi tương tự như vậy thì ở đây sẽ là W1 nhân cho 4 + B chia W2. Rồi bây giờ chúng ta sẽ pause nó một chút. plt.pause ở đây là 0.2. Rồi bây giờ chúng ta sẽ chạy thuật toán. Rồi chúng ta thấy là ở đây nó có cái sự dịch chuyển nhưng mà cái tốc độ của mình là nó khá là chậm. À cái tốc độ cập nhật của mình nó khá là chậm. Do đó thì chúng ta sẽ tăng tốc lên. Một là chúng ta sẽ cho cái alpha này tăng lên. Ví dụ như là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 20,
      "start_timestamp": "0:19:15",
      "end_timestamp": "0:19:33"
    }
  },
  {
    "page_content": "ta sẽ cho cái alpha này tăng lên. Ví dụ như là 0.5. Alpha tăng lên là 0.5. Rồi chúng ta thấy là cái đường thẳng của mình à nó đã tiến về và dần dần nó sẽ xoay để mà nó tách hai cái tập màu đỏ và màu xanh hoặc là hình tròn và dấu cộng ở đây ra làm hai phần. Rồi thì ở đây nếu mà cái biên trái chúng ta cho nó là 1 thì nó sẽ rõ ràng hơn ha. À là 0 thì nó vẽ tới bên bên trái luôn. Và bên trái của mình sẽ cho lên khoảng 3.5 là vừa. Rồi thì ở đây chúng ta sẽ thứ nhất là cho tốc độ của nó nhanh hơn.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 21,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "ta sẽ thứ nhất là cho tốc độ của nó nhanh hơn. Cái thứ hai đó là biên trái của mình là số 0. Biên phải của mình là khoảng 3.5 thì 0 ở đây chúng ta sẽ nhân vô 0. Còn ở đây sẽ là 3.5 5 rồi.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=d208XcqDgDI",
      "filename": "d208XcqDgDI",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 1)",
      "chunk_id": 22,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề, học sâu, machine learning, logistic regression, imageNet. User-User Collaborative Filtering Ví dụ như chúng ta muốn gợi ý danh sách mua sắm cho người A của người A cho người B thì chúng ta thấy rằng là người A và người B có sở thích giống nhau thì nếu A thích thì B có lẽ cũng thích thì ý tưởng của nó là như vậy tức là nếu chúng ta tìm thấy được A và B là đều cùng mua đều cùng mua các sản phẩm này đều cùng mua các sản phẩm này tức là chúng ta đang giả định rằng là A và B có sở thích giống",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DAFPEgXUqSc",
      "filename": "DAFPEgXUqSc",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 3",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:05"
    }
  },
  {
    "page_content": "ta đang giả định rằng là A và B có sở thích giống nhau A và B giống nhau thế thì nếu user A mà họ mua thêm 1 sản phẩm mới là sản phẩm này thì người ta sẽ khuyến nghị cho user B cũng sẽ mua sản phẩm này thì ở đây là By buy còn ở đây là recommend ta sẽ lấy cái sản phẩm này để đi recommend cho người B thì quy trình tổng quát đó là với cái người dùng x thì chúng ta sẽ có một tập các sở thích tập sở thích này nó sẽ dựa trên những lịch sử mua hàng hoặc là rating của người dùng và chúng ta sẽ đem tập",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DAFPEgXUqSc",
      "filename": "DAFPEgXUqSc",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 3",
      "chunk_id": 1,
      "start_timestamp": "0:01:02",
      "end_timestamp": "0:02:01"
    }
  },
  {
    "page_content": "là rating của người dùng và chúng ta sẽ đem tập sở thích này để đi so sánh với những sở thích của người khác thì chúng ta tìm ra được nhóm n người có sở thích tương tự ví dụ như nhóm n nhóm n này là chúng ta có các sản phẩm này và sản phẩm này tương tự với sở thích của người dùng x Thế thì nếu như chúng ta sẽ từ sở thích của nhóm n này hệ thống sẽ gợi ý cho người dùng x tức là từ database chúng ta biết được rằng là nhóm n này có những sở thích riêng là 2 cái sản phẩm này thì chúng ta sẽ đem 2",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DAFPEgXUqSc",
      "filename": "DAFPEgXUqSc",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 3",
      "chunk_id": 2,
      "start_timestamp": "0:01:53",
      "end_timestamp": "0:02:56"
    }
  },
  {
    "page_content": "riêng là 2 cái sản phẩm này thì chúng ta sẽ đem 2 cái sản phẩm này để đi gợi ý để đi gợi ý cho người dùng x thì đó chính là quy trình tổng quát và ý tưởng của thuật toán đầu tiên là neighborhood based collaborative filtering chúng ta sẽ dựa trên các độ đo, ví dụ như là độ đo Jaccard Similarity để so sánh tập item được đem ra để đánh giá thì vấn đề là với Jaccard Similarity thì nó lại đi bỏ qua các giá trị rating Cosine Similarity là độ đo tính góc giữa vector đánh giá của 2 người RX và RY Vấn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DAFPEgXUqSc",
      "filename": "DAFPEgXUqSc",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 3",
      "chunk_id": 3,
      "start_timestamp": "0:02:48",
      "end_timestamp": "0:03:42"
    }
  },
  {
    "page_content": "góc giữa vector đánh giá của 2 người RX và RY Vấn đề đó là coi cho chỗ thiếu của dữ liệu như là giá trị là bằng 0 những cái vùng, ví dụ như là cái item user và item những cái user và item mà không có được đánh giá tức là chưa có trong cái lịch sử chưa được đánh giá thì nó sẽ cho cái giá trị là bằng 0 thì cái chỗ này có thể là một cái vấn đề của của cái phương pháp tính cosine similarity thì similarity của a và của x và y thì nó sẽ được tính là bằng rx nhân ri chia cho norm của rx và norm của ri",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DAFPEgXUqSc",
      "filename": "DAFPEgXUqSc",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 3",
      "chunk_id": 4,
      "start_timestamp": "0:03:38",
      "end_timestamp": "0:04:24"
    }
  },
  {
    "page_content": "rx nhân ri chia cho norm của rx và norm của ri và cái độ đo cuối cùng đó là Pearson correlation thì chúng ta chỉ tính trên các cái item mà cả hai cùng đánh giá chỉ tính trên các cái item mà cả hai cùng đánh giá như vậy thì đối với những cái item mà chưa có được đánh giá thì nó sẽ bỏ qua và nó sẽ không xem xét thì đây là cái cách tính mà có vẻ là hợp lý nhất thế thì bây giờ chúng ta sẽ cùng lấy một cái ví dụ đó là cái ma trận tương đồng giữa user và item thì ví dụ như ở hàng, trên cái cột hàng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DAFPEgXUqSc",
      "filename": "DAFPEgXUqSc",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 3",
      "chunk_id": 5,
      "start_timestamp": "0:04:20",
      "end_timestamp": "0:05:03"
    }
  },
  {
    "page_content": "và item thì ví dụ như ở hàng, trên cái cột hàng này thì đây tính là cái danh sách các user còn ở cái cột này thì nó sẽ là các cái item Rồi, thì ở đây những cái chỗ đề trống đó là những cái mà chúng ta chưa có dữ liệu tương tác Vì nó chưa có dữ liệu nên nếu mình sử dụng các độ đo như là Jaccard mình xem nó như là con số 0 thì rõ ràng là cái phương pháp này nó không có ổn tại vì nó không có dữ liệu thì không có nghĩa là nó bằng 0 với độ đo Jaccard thì user similarity giữa A và B là bằng 1 phần 5",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DAFPEgXUqSc",
      "filename": "DAFPEgXUqSc",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 3",
      "chunk_id": 6,
      "start_timestamp": "0:04:58",
      "end_timestamp": "0:05:55"
    }
  },
  {
    "page_content": "thì user similarity giữa A và B là bằng 1 phần 5 và similarity giữa A và C là bằng 2 phần 4 thì chi tiết cách tính của nó như thế nào chúng ta sẽ xem trong slide tiếp theo thì cách này có vẻ là không hợp lý Cách tính về cosine similarity cũng tương tự như vậy, nó gần đúng hơn ở chỗ đó là nếu mà về mặt trực giác thì chúng ta thấy user A đánh giá item số x1 là 4 user B đánh giá item x1 là 5 thì trong khi đó đối với sản phẩm x4 thì A và C có cái quan điểm trái nhau A thì đánh giá rất là cao, C",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DAFPEgXUqSc",
      "filename": "DAFPEgXUqSc",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 3",
      "chunk_id": 7,
      "start_timestamp": "0:05:32",
      "end_timestamp": "0:06:35"
    }
  },
  {
    "page_content": "quan điểm trái nhau A thì đánh giá rất là cao, C đánh giá thấp rồi sản phẩm x5 thì A đánh giá thấp C là cao thì rõ ràng chúng ta thấy là A và C nó sẽ không có tương đồng bằng A và B tại vì A và B có cái điểm giống nhau đó là Cả hai đều cùng đánh giá cao cái X1 Còn các sản phẩm kia là nó không có thông tin nên nó phải bỏ qua Thì không có thông tin thì chúng ta không nên xem xét Không có thông tin chúng ta không nên xem xét Như vậy nếu mà xét về mặt trực giác Thì user A có vẻ sẽ giống user B hơn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DAFPEgXUqSc",
      "filename": "DAFPEgXUqSc",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 3",
      "chunk_id": 8,
      "start_timestamp": "0:06:33",
      "end_timestamp": "0:07:14"
    }
  },
  {
    "page_content": "trực giác Thì user A có vẻ sẽ giống user B hơn User A sẽ giống user B hơn Mặc dù với cái cách tính cosine similarity là 0.38 lớn hơn 0.32 tức là kết luận rằng A với B A gần B hơn, A giống B hơn so với giống C thì cũng đúng nhưng mà nó chỉ là gần đúng hơn thôi Còn cái cách tính Pearson thì chúng ta sẽ thấy là nó sẽ hợp lý nhất tại vì cái giá trị của nó là khác biệt nhiều nhất do đó thì giải pháp Pearson correlation sẽ là xử lý tốt hơn cái vấn đề về dữ liệu bị thiếu hoặc là thiên lệch để trung",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DAFPEgXUqSc",
      "filename": "DAFPEgXUqSc",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 3",
      "chunk_id": 9,
      "start_timestamp": "0:07:10",
      "end_timestamp": "0:08:04"
    }
  },
  {
    "page_content": "về dữ liệu bị thiếu hoặc là thiên lệch để trung bình dữ liệu bị thiếu có nghĩa là những tình huống này hoặc là tình huống này thì là dữ liệu bị thiếu như vậy thì chúng ta sẽ xem chi tiết cách tính cho từng cái độ đo đối với cái độ đo Jaccard thì nó sẽ lấy tập các item được đánh giá của A là bao gồm là 1,4 và 5 Các item được đánh giá của B sẽ là Các item được đánh giá của B đó là 1,4 và 5 là chỉ số của item Các item được đánh giá của B đó là chỉ số 1,2 và 3 Các item được đánh giá của C đó là 4,5",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DAFPEgXUqSc",
      "filename": "DAFPEgXUqSc",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 3",
      "chunk_id": 10,
      "start_timestamp": "0:08:01",
      "end_timestamp": "0:09:05"
    }
  },
  {
    "page_content": "1,2 và 3 Các item được đánh giá của C đó là 4,5 và 6 tức là tương ứng là các sản phẩm này similarity giữa A và B đó là 1,4,5 và 1,2,3 thì nó sẽ là bằng phần giao của hai cái này giao của hai thằng này sẽ là bằng 1 phần tử và phần hợp của nó sẽ là 1,2,3,4,5 tức là 5 phần tử Với cách tính của Jaccard, nó sẽ không xem xét đến giá trị rating nó không xem xét giá trị rating mà nó chỉ xem xét là có rate hay không thôi ví dụ chúng ta thấy là rõ ràng ở đây cái x5 cái item số 5 này thì nó được đánh giá",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DAFPEgXUqSc",
      "filename": "DAFPEgXUqSc",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 3",
      "chunk_id": 11,
      "start_timestamp": "0:09:03",
      "end_timestamp": "0:09:54"
    }
  },
  {
    "page_content": "đây cái x5 cái item số 5 này thì nó được đánh giá là bằng bằng 1, tức là đánh giá rất là thấp nhưng mà nó vẫn xem xét vô đây là item đã được A đánh giá chứ còn nó không xem xét là 1 và 4 là cao và 5 là thấp, nó chỉ có xem xét là có đánh giá hay không thôi, thì cái cách cái phương pháp Jaccard này thì nó sẽ không có ổn ở chỗ đó Còn đối với similarity, cosine similarity thì nó xem những cái item mà chưa được đánh giá là 0 Do đó thì cái vector rating của A sẽ là bằng 4005100 thì rõ ràng là các cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DAFPEgXUqSc",
      "filename": "DAFPEgXUqSc",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 3",
      "chunk_id": 12,
      "start_timestamp": "0:09:44",
      "end_timestamp": "0:10:30"
    }
  },
  {
    "page_content": "của A sẽ là bằng 4005100 thì rõ ràng là các cái con số 0 này nó không phản ánh đúng tức là người ta không đánh giá thì không có nghĩa là nó là điểm thấp thì vector rating A là nhiều đây vector rating B là nhiều đây và chúng ta tính cosine similarity thì lấy vector rating A nhân vector rating B là nhân vô hướng với nhau thì những chỗ nào bằng 0 thì chúng ta sẽ lại bỏ qua chia cho norm của vector rating A và norm của vector rating B thì ra là 0.38 và similarity giữa A và C sẽ là 0.32 similarity",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DAFPEgXUqSc",
      "filename": "DAFPEgXUqSc",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 3",
      "chunk_id": 13,
      "start_timestamp": "0:10:25",
      "end_timestamp": "0:11:09"
    }
  },
  {
    "page_content": "và similarity giữa A và C sẽ là 0.32 similarity giữa A và B là 0.38 thì cái kết quả nó nói lên là A nó sẽ tương đồng với B hơn so với lại với C nhưng mà 2 cái con số 0.38 và 0.32 nó không phản ánh đúng là Các quan điểm của người dùng A gần người dùng B nhiều hơn và khác biệt hoàn toàn so với người dùng C Tại vì chúng ta để ý vô giá trị của nó thì chúng ta thấy đối với sản phẩm số 5 A không thích nhưng mà C thì lại thích Đối với sản phẩm số 4 thì A thích và C thì lại không thích nó chưa có thể",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DAFPEgXUqSc",
      "filename": "DAFPEgXUqSc",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 3",
      "chunk_id": 14,
      "start_timestamp": "0:11:06",
      "end_timestamp": "0:11:53"
    }
  },
  {
    "page_content": "A thích và C thì lại không thích nó chưa có thể phản ánh rõ cho chuyện đấy Chúng ta sẽ đến với phương pháp Pearson correlation Chúng ta sẽ tính trung bình của a Chúng ta thấy là 4, 5 và 1 Chia 3 ra thì nó sẽ ra là 10 phần 3 Từ giá trị trung bình này Chúng ta sẽ đi tính từng giá trị Ví dụ như là cái phần tử a x1 Giá trị góc là 4, mình sẽ chuẩn hóa nó Đây là chuẩn hóa Rồi thì nó sẽ là bằng 4 trừ cho cái giá trị trung bình này 4 trừ cho giá trị trung bình này thì nó sẽ ra là 2 phần 3 và biến thành",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DAFPEgXUqSc",
      "filename": "DAFPEgXUqSc",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 3",
      "chunk_id": 15,
      "start_timestamp": "0:11:48",
      "end_timestamp": "0:12:32"
    }
  },
  {
    "page_content": "bình này thì nó sẽ ra là 2 phần 3 và biến thành cái con số này Đây là giá trị trung bình của 4 trên cái một cái hàng này Đó thì tương tự như vậy chúng ta sẽ ra được là x4 là bằng 5 phần 3 x5 là trừ 7 phần 3 tương tự như các cái giá trị ở đây Và đối với cái hàng cuối thì chúng ta thấy là trung bình của nó là 3 thì 3 trừ 3 là bằng 0 thì similarity của A và B sẽ là bằng cosine của các giá trị mà chúng ta đã chuẩn hóa Với cái cách làm này thì nó sẽ xem xét đến cái yếu tố là nếu mà giá trị dương thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DAFPEgXUqSc",
      "filename": "DAFPEgXUqSc",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 3",
      "chunk_id": 16,
      "start_timestamp": "0:12:25",
      "end_timestamp": "0:13:28"
    }
  },
  {
    "page_content": "xét đến cái yếu tố là nếu mà giá trị dương thì nó sẽ là thích và giá trị mà âm thì nó có xu hướng là không thích thì nó sẽ xem xét đến cái yếu tố là thích và không thích đó tốt hơn thì chúng ta thấy là chúng ta lấy cái hàng này chúng ta lấy cái hàng này chúng ta nhân với cái hàng này sau đó chia cho chuẩn hóa thì nó sẽ ra là 0.09 tức là có xu hướng là a và b là tương đồng với nhau cao tương đồng hơn Còn A và C nó ra là âm 0.559 tức là xu hướng đó là A và C không có cái sở thích giống nhau Tức",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DAFPEgXUqSc",
      "filename": "DAFPEgXUqSc",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 3",
      "chunk_id": 17,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "đó là A và C không có cái sở thích giống nhau Tức là nó trái ngược nhau Như vậy thì cái cách này nó cũng sẽ giống cái cách là Cosine similarity nhưng mà nó thể hiện được xu hướng là A và B tương đồng với nhau Và A với C là ngược hướng với nhau Hãy subscribe cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video hấp dẫn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DAFPEgXUqSc",
      "filename": "DAFPEgXUqSc",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 3",
      "chunk_id": 18,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Bạn có từng tự hỏi tại sao việc xây dựng một mô hình học máy tốt lại mất nhiều thời gian và công sức từ việc chọn lựa thuật toán, tinh chỉnh siêu tham số, xử lý dữ liệu đến đánh giá mô hình. Tất cả đều đòi hỏi kiến thức chuyên sâu và nhiều thử nghiệm. Đây chính là lý do AutoML Automatic Machine Learning ra đời giúp tự động hóa quá trình này, giảm bớt công sức và mở rộng khả năng áp dụng học máy đến nhiều lĩnh vực hơn. AutoML là công nghệ giúp tự động hóa quá trình xây dựng mô hình học máy từ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DoXWiAg6UAA",
      "filename": "DoXWiAg6UAA",
      "title": "[CS114 - Chương 2] Học máy tự động",
      "chunk_id": 0,
      "start_timestamp": "0:00:19",
      "end_timestamp": "0:01:00"
    }
  },
  {
    "page_content": "tự động hóa quá trình xây dựng mô hình học máy từ tiền xử lý dữ liệu, lựa chọn mô hình, tối ưu hóa siêu tham số đến đánh giá và triển khai mô hình. Mục tiêu chính của AutoML là giảm thiểu yêu cầu chuyên môn về ML, giúp các nhà khoa học dữ liệu, lập trình viên và cả những người không chuyên có thể dễ dàng áp dụng học máy vào các bài toán thực tế. AutoML quan trọng bởi vì nó giúp giải quyết hiệu quả các thách thức trong học máy truyền thống. Quá trình xây dựng một mô hình học máy trước đây yêu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DoXWiAg6UAA",
      "filename": "DoXWiAg6UAA",
      "title": "[CS114 - Chương 2] Học máy tự động",
      "chunk_id": 1,
      "start_timestamp": "0:00:54",
      "end_timestamp": "0:01:37"
    }
  },
  {
    "page_content": "trình xây dựng một mô hình học máy trước đây yêu cầu nhiều bước phức tạp. Thứ nhất là lựa chọn thuật toán học máy cần hiểu rõ và chọn giữa các thuật toán như SVM, Random Forest, Neural Network vân vân. Bước thứ hai là xử lý và tiền xử lý dữ liệu bao gồm việc chuẩn hóa, chọn đặc trưng quan trọng và làm sạch dữ liệu. Bước thứ ba là điều chỉnh siêu tham số, tìm ra giá trị tối ưu cho các tham số, ví dụ như là learning rate, số lượng layer trong các mạng Neural. Bước tiếp theo là đánh giá và triển",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DoXWiAg6UAA",
      "filename": "DoXWiAg6UAA",
      "title": "[CS114 - Chương 2] Học máy tự động",
      "chunk_id": 2,
      "start_timestamp": "0:01:33",
      "end_timestamp": "0:02:14"
    }
  },
  {
    "page_content": "mạng Neural. Bước tiếp theo là đánh giá và triển khai mô hình, kiểm tra hiệu suất của mô hình và triển khai vào hệ thống thực tế. Tất cả những công việc này đòi hỏi chuyên môn cao, tốn nhiều thời gian và dễ xảy ra sai sót. Điều này khiến cho học máy trở nên khó tiếp cận, đặc biệt đối với người không chuyên. AutoML giúp tự động hóa các bước trên và mang lại nhiều cái lợi ích quan trọng. Thứ nhất là tiết kiệm thời gian, giảm công sức thủ công trong việc thử nghiệm và tối ưu mô hình. Thứ hai là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DoXWiAg6UAA",
      "filename": "DoXWiAg6UAA",
      "title": "[CS114 - Chương 2] Học máy tự động",
      "chunk_id": 3,
      "start_timestamp": "0:02:09",
      "end_timestamp": "0:02:49"
    }
  },
  {
    "page_content": "việc thử nghiệm và tối ưu mô hình. Thứ hai là giảm thiểu sai sót, hạn chế lỗi do con người khi xử lý dữ liệu và điều chỉnh tham số. Thứ ba là tối ưu hóa hiệu suất của mô hình, tự động tìm kiếm các mô hình và siêu tham số tốt nhất và cuối cùng là tiếp cận dễ dàng hơn cho người không chuyên giúp những người không biết nhiều về machine learning mà vẫn có thể triển khai được các cái ứng dụng. Ờ AutoML không chỉ giúp các chuyên gia học máy mà còn hỗ trợ cho doanh nghiệp và người không chuyên tiếp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DoXWiAg6UAA",
      "filename": "DoXWiAg6UAA",
      "title": "[CS114 - Chương 2] Học máy tự động",
      "chunk_id": 4,
      "start_timestamp": "0:02:45",
      "end_timestamp": "0:03:30"
    }
  },
  {
    "page_content": "trợ cho doanh nghiệp và người không chuyên tiếp cận AI dễ dàng hơn. Ờ cụ thể đó là AutoML giúp giảm rào cản kỹ thuật. Các doanh nghiệp không cần phải chuyên gia học máy vẫn có thể sử dụng AI để giải quyết các bài toán thực tế. Thứ hai là tăng hiệu suất. AutoML giúp thử nghiệm nhiều mô hình nhanh chóng và tìm ra mô hình tối ưu trong một thời gian ngắn. Và cuối cùng đó là tích hợp rất là nhanh chóng các nền tảng AutoML như Google AutoML, H2O.ai cho phép triển khai mô hình dễ dàng vào hệ thống",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DoXWiAg6UAA",
      "filename": "DoXWiAg6UAA",
      "title": "[CS114 - Chương 2] Học máy tự động",
      "chunk_id": 5,
      "start_timestamp": "0:03:23",
      "end_timestamp": "0:04:03"
    }
  },
  {
    "page_content": "cho phép triển khai mô hình dễ dàng vào hệ thống thực tế. Thế thì các thành phần chính của AutoML bao gồm các bước quan trọng trong quy trình tự động hóa học máy. Chúng ta hãy nhìn vào à bảng sau để thấy các cái thành phần. Thành phần thứ nhất là tiền xử lý dữ liệu, data preprocessing. Làm sạch dữ liệu bao gồm loại bỏ dữ liệu nhiễu, dữ liệu bị thiếu, gồm chuẩn hóa dữ liệu, biến đổi dữ liệu về dạng phù hợp để mô hình có thể học hiệu quả. Chọn đặc trưng feature selection. Tự động xác định và chọn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DoXWiAg6UAA",
      "filename": "DoXWiAg6UAA",
      "title": "[CS114 - Chương 2] Học máy tự động",
      "chunk_id": 6,
      "start_timestamp": "0:03:57",
      "end_timestamp": "0:04:41"
    }
  },
  {
    "page_content": "trưng feature selection. Tự động xác định và chọn ra các cái đặc trưng quan trọng tốt nhất trong tập dữ liệu và giúp cải thiện hiệu suất mô hình và giảm độ phức tạp. Thứ ba là chọn mô hình model selection thì thử nghiệm nhiều thuật toán học máy khác nhau để tìm ra mô hình phù hợp nhất với dữ liệu. Tối ưu hóa siêu tham số hyper parameter tuning. À bước này thì tự động tìm kiếm và tinh chỉnh các cái siêu tham số nhằm tối ưu hóa hiệu suất của mô hình. Đánh giá mô hình model evaluation thì tính",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DoXWiAg6UAA",
      "filename": "DoXWiAg6UAA",
      "title": "[CS114 - Chương 2] Học máy tự động",
      "chunk_id": 7,
      "start_timestamp": "0:04:35",
      "end_timestamp": "0:05:16"
    }
  },
  {
    "page_content": "hình. Đánh giá mô hình model evaluation thì tính toán các chỉ số đánh giá như accuracy, precision, recall để xác định chất lượng của cái mô hình. Triển khai mô hình model deployment. Xác định mô hình, xuất cái mô hình dưới dạng API hoặc tích hợp vào hệ thống thực tế để sử dụng trong các ứng dụng. Như vậy AutoML giúp tự động hóa toàn bộ quy trình xây dựng mô hình học máy từ xử lý dữ liệu, lựa chọn thuật toán, tối ưu hóa đến đánh giá và triển khai mô hình. Điều này giúp giảm thời gian, công sức",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DoXWiAg6UAA",
      "filename": "DoXWiAg6UAA",
      "title": "[CS114 - Chương 2] Học máy tự động",
      "chunk_id": 8,
      "start_timestamp": "0:05:09",
      "end_timestamp": "0:05:54"
    }
  },
  {
    "page_content": "mô hình. Điều này giúp giảm thời gian, công sức và yêu cầu về chuyên môn trong lĩnh vực học máy. Các nền tảng AutoML phổ biến bao gồm Google Cloud AutoML. Đây là bộ công cụ giúp đào tạo mô hình à học máy chất lượng cao mà không cần chuyên môn sâu. Microsoft Azure Automated ML giúp tự động hóa việc phát triển, triển khai và mở rộng mô hình học máy dễ dàng. IBM Watson Studio thì hỗ trợ tự động hóa quy trình học máy giúp phát triển và triển khai mô hình một cách nhanh chóng. Amazon SageMaker dịch",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DoXWiAg6UAA",
      "filename": "DoXWiAg6UAA",
      "title": "[CS114 - Chương 2] Học máy tự động",
      "chunk_id": 9,
      "start_timestamp": "0:05:49",
      "end_timestamp": "0:06:37"
    }
  },
  {
    "page_content": "hình một cách nhanh chóng. Amazon SageMaker dịch vụ học máy được quản lý hoàn toàn giúp đào tạo, huấn luyện và triển khai mô hình dễ dàng. Và à H2O.ai là nền tảng mã nguồn mở hỗ trợ AutoML tự động thử nghiệm thuật toán và tối ưu siêu tham số. Các ứng dụng của AutoML bao gồm phân tích hình ảnh gồm có nhận diện đối tượng, kiểm tra chất lượng sản phẩm và chẩn đoán y tế, à xử lý văn bản, phân loại tài liệu, phân tích cảm xúc, chatbot và dịch thuật, dự báo chuỗi thời gian, dự báo doanh số, nhu cầu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DoXWiAg6UAA",
      "filename": "DoXWiAg6UAA",
      "title": "[CS114 - Chương 2] Học máy tự động",
      "chunk_id": 10,
      "start_timestamp": "0:06:30",
      "end_timestamp": "0:07:14"
    }
  },
  {
    "page_content": "dự báo chuỗi thời gian, dự báo doanh số, nhu cầu sản phẩm, giá trị tài chính, tiếp thị và khách hàng à phân khúc à khách hàng, dự đoán rời bỏ, cá nhân hóa chiến lược. Phát hiện gian lận bao gồm ngăn chặn gian lận tài chính, giám sát bất thường bảo mật mạng. Tối ưu logistics bao gồm dự báo tồn kho, tối ưu giao hàng, dự đoán bảo trì thiết bị. Và trong lĩnh vực y tế, giáo dục gồm có chẩn đoán bệnh, cá nhân hóa điều trị và hỗ trợ học tập. Chúng ta có thể thấy là các ứng dụng của AutoML rất là đa",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DoXWiAg6UAA",
      "filename": "DoXWiAg6UAA",
      "title": "[CS114 - Chương 2] Học máy tự động",
      "chunk_id": 11,
      "start_timestamp": "0:07:08",
      "end_timestamp": "0:07:49"
    }
  },
  {
    "page_content": "có thể thấy là các ứng dụng của AutoML rất là đa dạng. Mặc dù AutoML mang lại nhiều lợi ích, tuy nhiên nó vẫn có một số hạn chế. Thứ nhất đó là thiếu linh hoạt. À không thể tùy chỉnh mô hình như là cách truyền thống. Cái thứ hai là yêu cầu cái tài nguyên lớn. Một số công cụ như AutoML cần GPU hoặc cloud computing rất mạnh và không thể thay thế hoàn toàn chuyên gia học máy. AutoML giúp tự động hóa nhưng vẫn cần con người để phân tích kết quả và đưa ra quyết định cuối cùng. AutoML đang trở thành",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DoXWiAg6UAA",
      "filename": "DoXWiAg6UAA",
      "title": "[CS114 - Chương 2] Học máy tự động",
      "chunk_id": 12,
      "start_timestamp": "0:07:42",
      "end_timestamp": "0:08:28"
    }
  },
  {
    "page_content": "ra quyết định cuối cùng. AutoML đang trở thành xu hướng quan trọng trong AI và học máy. Một số hướng phát triển chính thì bao gồm kết hợp với Explainable AI để giải thích kết quả mô hình cách tự động; tích hợp nhiều hơn với MLOps để triển khai và giám sát các mô hình học máy liên tục; và tối ưu hóa trên các cái thiết bị IoT và Edge Computing (tính toán biên) giúp chạy mô hình AI trên các thiết bị nhỏ gọn. Khi nào thì nên sử dụng AutoML? Thì các trường hợp này ờ giúp cho người dùng phải xác định",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DoXWiAg6UAA",
      "filename": "DoXWiAg6UAA",
      "title": "[CS114 - Chương 2] Học máy tự động",
      "chunk_id": 13,
      "start_timestamp": "0:08:25",
      "end_timestamp": "0:09:01"
    }
  },
  {
    "page_content": "hợp này ờ giúp cho người dùng phải xác định à liệu AutoML có là giải pháp phù hợp với nhu cầu của họ hay không. Chúng ta hãy quan sát trên cái bảng. Thì ở đây ờ chi tiết các trường hợp sẽ được giải thích như sau. Thứ nhất là nếu mà không có nhiều kinh nghiệm về học máy thì à nó rất là phù hợp. AutoML giúp những người không chuyên về học máy dễ dàng xây dựng mô hình mà không cần hiểu sâu về thuật toán hoặc tối ưu hóa mô hình. Cái thứ hai là trong trường hợp dữ liệu nhỏ hoặc vừa thì có thể dùng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DoXWiAg6UAA",
      "filename": "DoXWiAg6UAA",
      "title": "[CS114 - Chương 2] Học máy tự động",
      "chunk_id": 14,
      "start_timestamp": "0:08:57",
      "end_timestamp": "0:09:33"
    }
  },
  {
    "page_content": "trường hợp dữ liệu nhỏ hoặc vừa thì có thể dùng bởi vì AutoML có thể hoạt động tốt với các tập dữ liệu nhỏ và vừa giúp nhanh chóng tạo ra mô hình mà không cần nhiều công sức. Trong trường hợp mà cần tối ưu hóa mô hình nhanh chóng thì đây là rất là hữu ích. Nếu dự án yêu cầu có mô hình nhanh mà không cần nghiên cứu sâu thì AutoML có thể giúp tự động hóa và tăng tốc quá trình thử nghiệm mô hình. Trong trường hợp dự án yêu cầu tùy chỉnh cao thì à không phù hợp. Bởi vì AutoML thường không cho phép",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DoXWiAg6UAA",
      "filename": "DoXWiAg6UAA",
      "title": "[CS114 - Chương 2] Học máy tự động",
      "chunk_id": 15,
      "start_timestamp": "0:09:29",
      "end_timestamp": "0:10:04"
    }
  },
  {
    "page_content": "phù hợp. Bởi vì AutoML thường không cho phép kiểm soát chi tiết từng khía cạnh của mô hình nên không phù hợp với các dự án cần tùy chỉnh thuật toán hoặc kiến trúc mô hình đặc biệt. Trong trường hợp cần kiểm soát từng bước của mô hình học máy thì cũng lời khuyên là nên dùng cách truyền thống. Nếu mà cần kiểm soát toàn bộ quy trình từ tiền xử lý dữ liệu, lựa chọn đặc trưng, tối ưu tham số đến đánh giá mô hình thì phương pháp truyền thống sẽ tốt hơn AutoML. AutoML thì phù hợp với những người mới",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DoXWiAg6UAA",
      "filename": "DoXWiAg6UAA",
      "title": "[CS114 - Chương 2] Học máy tự động",
      "chunk_id": 16,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "AutoML. AutoML thì phù hợp với những người mới tiếp cận ML, dự án có quy mô nhỏ đến vừa hoặc khi cần nhanh chóng có mô hình hoạt động tốt. Ok. Nghỉ tí.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=DoXWiAg6UAA",
      "filename": "DoXWiAg6UAA",
      "title": "[CS114 - Chương 2] Học máy tự động",
      "chunk_id": 17,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Tiếp theo thì chúng ta sẽ đến với cái câu hỏi so sánh cái hàm mất mát giữa việc tại sao chúng ta không sử dụng cái mean squared error của mô hình hồi quy tuyến tính mà ở đây chúng ta lại dùng cái hàm khá là phức tạp đó là binary cross-entropy. Đối với những bạn nào mà lần đầu thấy cái công thức này thì sẽ thấy đó là một cái hàm rất là à khoai vì nó có hàm log trong đó cũng như là cái số phép toán của mình nó thực hiện nhiều hơn nhiều. Thế thì cái L này á là sai số giữa y và cái xác suất P. Đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=EarBLLqMgLg",
      "filename": "EarBLLqMgLg",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 2 (Phần 1)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:51"
    }
  },
  {
    "page_content": "L này á là sai số giữa y và cái xác suất P. Đây chính là cái dự đoán của mình. Và nó có cái công thức như trên. Nhưng mà câu hỏi đó là tại sao chúng ta không sử dụng L(y, p) bằng (y - p) tất cả bình. Đây là công thức bình phương sai số. Thì tại sao chúng ta không sử dụng cái công thức này? Nó có xuất phát điểm từ cái miền giá trị của L và ở trong cái công thức là MSE với lại cái miền giá trị ở trong cái hàm binary cross- entropy. Thì ở đây chúng ta lấy một cái tình huống đó là y của mình là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=EarBLLqMgLg",
      "filename": "EarBLLqMgLg",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 2 (Phần 1)",
      "chunk_id": 1,
      "start_timestamp": "0:00:44",
      "end_timestamp": "0:01:41"
    }
  },
  {
    "page_content": "ta lấy một cái tình huống đó là y của mình là bằng 0 à và p của mình ví dụ như nó là bằng 1. Tức là chúng ta đang dự đoán sai. Thì điều gì sẽ xảy ra với cái tình huống này? Nếu chúng ta sử dụng MSE giống như ở đây thì cái sai số của mình lúc này đó là bằng (0 - 1) tất cả bình phương nó sẽ là bằng 1. Trong khi đó nếu chúng ta thế những cái con số này vào cái công thức ở trên thì cái sai số của mình nó sẽ là bằng cộng vô cùng. Một số ví dụ khác khi chúng ta không sử dụng các cái con số P nó mang",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=EarBLLqMgLg",
      "filename": "EarBLLqMgLg",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 2 (Phần 1)",
      "chunk_id": 2,
      "start_timestamp": "0:01:33",
      "end_timestamp": "0:02:25"
    }
  },
  {
    "page_content": "chúng ta không sử dụng các cái con số P nó mang tính chất tuyệt đối là 1 như thế này. Chúng ta cũng có thể thử những con số nhỏ hơn ví dụ như là 0.9 0.8 thì nó đều có một cái điểm chung. Đó là đối với cái hàm MSE thì cái sai số của mình nó khá là bé. Nó khá là bé. Mình lấy ví dụ y của mình là bằng 0 và p của mình là bằng 0.9. Thì khi đó cái công thức ở trên nó sẽ là bằng (0 - 0.9) tất cả bình phương. Mà một cái con số nhỏ hơn 1 lớn hơn 0 thì khi chúng ta nhân lên thì nó sẽ lùi xuống nó sẽ càng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=EarBLLqMgLg",
      "filename": "EarBLLqMgLg",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 2 (Phần 1)",
      "chunk_id": 3,
      "start_timestamp": "0:02:17",
      "end_timestamp": "0:03:07"
    }
  },
  {
    "page_content": "chúng ta nhân lên thì nó sẽ lùi xuống nó sẽ càng lúc càng giảm chứ nó không có tăng lên. Đó thì đó chính là à cái sự khác biệt về mặt giá trị. thì hàm binary cross-entropy nó sẽ cho cái sai số lớn hơn so với lại hàm MSE so với lại cái hàm MSE. Vậy thì cái câu hỏi đặt ra đó là tại sao nó lại phải làm chuyện đó.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=EarBLLqMgLg",
      "filename": "EarBLLqMgLg",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 2 (Phần 1)",
      "chunk_id": 4,
      "start_timestamp": "0:03:01",
      "end_timestamp": "0:03:26"
    }
  },
  {
    "page_content": "Bạn có bao giờ tự hỏi điều gì ảnh hưởng đến giá nhà không? Có phải diện tích lớn thì giá cao hơn hay là số phòng ngủ quyết định giá trị của căn nhà? Hồi quy tuyến tính Linear Regression là một trong những phương pháp đơn giản nhưng mạnh mẽ để giúp chúng ta hiểu và dự đoán những mối quan hệ như thế này. Vậy hồi quy tuyến tính hoạt động như thế nào? Hãy cùng khám phá ngay sau đây. Hồi quy tuyến tính là một phương pháp học có giám sát supervised learning. có nghĩa là chúng ta sẽ sử dụng dữ liệu có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 0,
      "start_timestamp": "0:00:21",
      "end_timestamp": "0:00:58"
    }
  },
  {
    "page_content": "có nghĩa là chúng ta sẽ sử dụng dữ liệu có nhãn để huấn luyện mô hình. Mục tiêu chính của mô hình này là tìm ra mối quan hệ tuyến tính giữa các biến đầu vào và biến đầu ra. Vậy biến đầu vào là gì? Đây là những yếu tố ảnh hưởng đến kết quả đầu ra. Ví dụ như trong bài toán dự đoán giá nhà, các đặc điểm như diện tích nhà, số phòng ngủ, vị trí địa lý chính là những biến đầu vào. Còn biến đầu ra thì sao? Đó chính là giá trị chúng ta muốn dự đoán trong trường hợp này là giá nhà. Công việc chính của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 1,
      "start_timestamp": "0:00:52",
      "end_timestamp": "0:01:43"
    }
  },
  {
    "page_content": "trường hợp này là giá nhà. Công việc chính của mô hình hồi quy tuyến tính là tìm ra đường thẳng phù hợp nhất với dữ liệu hoặc là siêu phẳng để từ đó giúp chúng ta dự đoán giá trị mới dựa trên dữ liệu đầu vào. Hồi quy tuyến tính được ứng dụng rộng rãi trong nhiều lĩnh vực khác nhau từ bất động sản, kinh doanh tài chính cho đến y tế và sức khỏe. Đối với lĩnh vực bất động sản, chắc hẳn là các bạn đã từng nghe đến việc dự đoán giá nhà. Hồi quy tuyến tính giúp ta ước tính giá trị bất động sản trên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 2,
      "start_timestamp": "0:01:36",
      "end_timestamp": "0:02:18"
    }
  },
  {
    "page_content": "tính giúp ta ước tính giá trị bất động sản trên các yếu tố như diện tích, vị trí, số phòng ngủ và nhiều yếu tố khác. Ngoài ra, nó cũng được sử dụng để ước tính mức giá thuê cho một căn hộ hoặc văn phòng dựa trên những đặc điểm cụ thể. Đối với lĩnh vực kinh doanh và tài chính, một lĩnh vực mà dữ liệu đóng vai trò vô cùng quan trọng. Hồi quy tuyến tính giúp các doanh nghiệp phân tích mối quan hệ giữa chi phí marketing và doanh thu, từ đó tối ưu hóa ngân sách quảng cáo. Không chỉ vậy, nó còn được",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 3,
      "start_timestamp": "0:02:13",
      "end_timestamp": "0:02:54"
    }
  },
  {
    "page_content": "ngân sách quảng cáo. Không chỉ vậy, nó còn được dùng để dự báo doanh số bán hàng theo mùa và đánh giá tác động của các yếu tố kinh tế vĩ mô giúp các công ty đưa ra quyết định kinh doanh chính xác hơn. Trong y tế và sức khỏe thì hồi quy tuyến tính cũng đóng vai trò quan trọng. Nó có thể phân tích mối liên hệ giữa chế độ ăn uống và các chỉ số sức khỏe giúp chúng ta hiểu rõ hơn về các tác động của thực phẩm đối với cơ thể. Ngoài ra, mô hình này cũng còn giúp ước tính nguy cơ bệnh dựa vào các chỉ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 4,
      "start_timestamp": "0:02:50",
      "end_timestamp": "0:03:26"
    }
  },
  {
    "page_content": "còn giúp ước tính nguy cơ bệnh dựa vào các chỉ số sinh học dựa trên nồng độ đường trong máu hay là chỉ số BMI. Hồi quy tuyến tính có nhiều ứng dụng thực tế nhưng có bao giờ chúng ta tự hỏi tại sao hồi quy tuyến tính lại quan trọng? Thứ nhất, hồi quy tuyến tính đơn giản và dễ hiểu. Đây là một trong những mô hình học máy cơ bản nhất, dễ dàng triển khai và trực quan. Điều này khiến nó trở thành một điểm khởi đầu lý tưởng cho những ai mới tìm hiểu về học máy. Thứ hai, mặc dù là đơn giản, hồi quy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 5,
      "start_timestamp": "0:03:22",
      "end_timestamp": "0:03:57"
    }
  },
  {
    "page_content": "về học máy. Thứ hai, mặc dù là đơn giản, hồi quy tuyến tính lại tỏ ra hiệu quả trong việc giải quyết nhiều bài toán thực tế như tài chính, y tế, kinh doanh. Điều này cho thấy sức mạnh của mô hình ngay cả khi so sánh với những phương pháp phức tạp hơn. Thứ ba là hồi quy tuyến tính có khả năng diễn giải cao. Các hệ số của mô hình không chỉ giúp dự đoán kết quả mà còn cho chúng ta biết mức độ ảnh hưởng của từng biến đầu vào lên biến đầu ra. Điều này là rất quan trọng đặc biệt trong các lĩnh vực",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 6,
      "start_timestamp": "0:03:52",
      "end_timestamp": "0:04:30"
    }
  },
  {
    "page_content": "này là rất quan trọng đặc biệt trong các lĩnh vực như kinh tế và y học. Nơi việc hiểu rõ mối quan hệ giữa các yếu tố là điều bắt buộc. Và cuối cùng hồi quy tuyến tính chính là nền tảng cho các mô hình phức tạp hơn. Khi bạn hiểu rõ về nó, bạn sẽ dễ dàng tiếp cận các phương pháp nâng cao như hồi quy logistic bài toán phân loại hoặc là hồi quy logistic hay thậm chí cả mạng nhân tạo. Tóm lại thì việc hồi quy tuyến tính không chỉ đơn giản mà còn rất mạnh mẽ và hữu ích. Đây là lý do tại sao nó là một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 7,
      "start_timestamp": "0:04:25",
      "end_timestamp": "0:05:06"
    }
  },
  {
    "page_content": "mẽ và hữu ích. Đây là lý do tại sao nó là một trong những mô hình đầu tiên mà bất kỳ ai học về học máy cũng nên nắm vững. Trong học máy thì chúng ta muốn sử dụng dữ liệu để đưa ra dự đoán hoặc tối ưu hóa một hệ thống. Nhưng làm thế nào để chuyển một vấn đề thực tế thành một bài toán mà máy có thể hiểu và giải quyết. Để làm được điều đó thì chúng ta cần tuân theo một cái quy trình tổng quát. Hãy cùng tìm hiểu các bước quan trọng trong việc xây dựng mô hình học máy theo quy trình này. Bước đầu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 8,
      "start_timestamp": "0:05:02",
      "end_timestamp": "0:05:41"
    }
  },
  {
    "page_content": "dựng mô hình học máy theo quy trình này. Bước đầu tiên là chúng ta chuyển đổi bài toán từ mô tả bằng lời sang mô hình toán học. Khi đối mặt với một bài toán thực tế, chúng ta thường mô tả nó bằng ngôn ngữ tự nhiên. Chẳng hạn như làm thế nào để dự đoán giá nhà dựa trên diện tích và vị trí. Mô tả bằng lời giúp chúng ta hiểu rõ bản chất và các yếu tố quan trọng của bài toán. Nhưng nó không thể giúp máy tính thực hiện các phép tính hay đưa ra kết quả một cách tự động. Vì vậy, chúng ta cần chuyển",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 9,
      "start_timestamp": "0:05:35",
      "end_timestamp": "0:06:14"
    }
  },
  {
    "page_content": "quả một cách tự động. Vì vậy, chúng ta cần chuyển đổi bài toán này thành một mô hình toán học, sử dụng các biến số, công thức và quy tắc rõ ràng. Sau khi có mô hình toán học, bước tiếp theo là xác định lời giải dựa trên mô hình này. Chúng ta sẽ áp dụng các phương pháp toán học để tìm ra kết quả mong muốn. Ví dụ nếu bài toán của chúng ta liên quan đến dự đoán giá nhà, ta có thể sử dụng hồi quy tuyến tính để tìm mối quan hệ giữa các biến. Nếu bài toán liên quan tới tối ưu hóa như tìm lộ trình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 10,
      "start_timestamp": "0:06:09",
      "end_timestamp": "0:06:50"
    }
  },
  {
    "page_content": "toán liên quan tới tối ưu hóa như tìm lộ trình giao hàng tối ưu thì ta có thể sử dụng thuật toán tìm đường đi ngắn nhất. Sau khi đã chuyển đổi bài toán thành mô hình toán học và xác định cách giải quyết, bước tiếp theo là chúng ta phát triển thuật toán và triển khai chương trình trên máy tính. Đây là một bước quan trọng vì nó giúp chúng ta hiện thực hóa mô hình và cho phép máy tính tự động tìm ra lời giải. Đầu tiên, chúng ta cần lựa chọn cái thuật toán phù hợp với bài toán cụ thể. Ví dụ nếu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 11,
      "start_timestamp": "0:06:46",
      "end_timestamp": "0:07:29"
    }
  },
  {
    "page_content": "thuật toán phù hợp với bài toán cụ thể. Ví dụ nếu liên quan đến dự đoán giá trị liên tục thì chúng ta có thể chọn hồi quy tuyến tính. Nếu bài toán phân loại các thuật toán như KNN hay là cây quyết định Decision Tree có thể là lựa chọn tốt. Sau khi chọn thuật toán thì chúng ta sẽ cài đặt nó bằng cách sử dụng các thư viện lập trình. Trong Python có rất nhiều thư viện hỗ trợ như NumPy, SciPy, Scikit-learn, TensorFlow. giúp chúng ta triển khai mô hình một cách nhanh chóng. Ngoài ra khi cần thiết",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 12,
      "start_timestamp": "0:07:24",
      "end_timestamp": "0:08:03"
    }
  },
  {
    "page_content": "hình một cách nhanh chóng. Ngoài ra khi cần thiết chúng ta cũng có thể viết thêm các hàm mới để tùy chỉnh thuật toán sao cho phù hợp với thực tế. Tuy nhiên việc xây dựng chương trình chỉ là một phần của quá trình. Chúng ta còn cần đánh giá và phân tích mô hình. Để đánh giá hiệu suất của mô hình, chúng ta sử dụng các tiêu chí như độ chính xác (accuracy), thời gian chạy (runtime), mức độ sử dụng tài nguyên (computational cost). Nếu mô hình không đạt yêu cầu, chúng ta cần phân tích nguyên nhân và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 13,
      "start_timestamp": "0:07:59",
      "end_timestamp": "0:08:41"
    }
  },
  {
    "page_content": "yêu cầu, chúng ta cần phân tích nguyên nhân và tìm cách cải tiến. Ví dụ, nếu mô hình chưa chính xác, chúng ta có thể tinh chỉnh siêu tham số (hyperparameter) hoặc thử nghiệm thuật toán khác. Nếu chương trình chạy quá chậm, ta có thể tối ưu code hoặc sử dụng thuật toán hiệu quả hơn. Tóm lại, quá trình phát triển thuật toán không chỉ dừng lại ở việc viết code mà còn là một vòng lặp liên tục giữa xây dựng, đánh giá và cải tiến để đạt kết quả tốt nhất. Khi bắt đầu tìm hiểu về hồi quy tuyến tính,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 14,
      "start_timestamp": "0:08:36",
      "end_timestamp": "0:09:16"
    }
  },
  {
    "page_content": "nhất. Khi bắt đầu tìm hiểu về hồi quy tuyến tính, tại sao chúng ta lại bắt đầu với hồi quy tuyến tính một biến thay vì nhiều biến cùng một lúc? Trên thực tế, việc học một mô hình đơn giản trước sẽ giúp chúng ta dễ dàng hiểu rõ bản chất của phương pháp này trước khi mở rộng sang các trường hợp phức tạp hơn. Thứ nhất, hồi quy tuyến tính một biến là cách tiếp cận đơn giản nhất. Nó cho phép chúng ta tập trung vào những khái niệm cốt lõi như mối quan hệ tuyến tính, hệ số hồi quy và cách tìm đường",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 15,
      "start_timestamp": "0:09:11",
      "end_timestamp": "0:09:57"
    }
  },
  {
    "page_content": "hệ tuyến tính, hệ số hồi quy và cách tìm đường thẳng tốt nhất để mô tả dữ liệu. Thứ hai, với chỉ một biến đầu vào X và một biến đầu ra Y, chúng ta có thể dễ dàng hình dung mối quan hệ này trên đồ thị. Khi vẽ dữ liệu trên mặt phẳng, ta có thể thấy một đường thẳng mô tả xu hướng. Điều này trực quan hơn rất nhiều so với hồi quy nhiều biến nơi việc biểu diễn dữ liệu trở nên phức tạp hơn. Cuối cùng việc nắm vững hồi quy một biến sẽ giúp chúng ta dễ dàng mở rộng sang hồi quy nhiều biến nơi chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 16,
      "start_timestamp": "0:09:52",
      "end_timestamp": "0:10:35"
    }
  },
  {
    "page_content": "dàng mở rộng sang hồi quy nhiều biến nơi chúng ta có thể sử dụng nhiều đặc trưng để dự đoán kết quả. Nếu hiểu chắc về mô hình đơn giản, chúng ta sẽ không gặp khó khăn khi làm việc với các mô hình phức tạp hơn. Giả sử bạn đang muốn mua một căn nhà và tự hỏi giá nhà sẽ thay đổi như thế nào khi diện tích thay đổi? Có vẻ như những ngôi nhà lớn hơn sẽ có giá cao hơn. Nhưng làm thế nào để định lượng mối quan hệ này và dự đoán giá một cách chính xác? Đây chính là một bài toán điển hình mà chúng ta có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 17,
      "start_timestamp": "0:10:31",
      "end_timestamp": "0:11:16"
    }
  },
  {
    "page_content": "chính là một bài toán điển hình mà chúng ta có thể giải quyết bằng hồi quy tuyến tính. Hãy cùng bắt đầu bằng mô tả bài toán này bằng ngôn ngữ tự nhiên. Bài toán của chúng ta như sau. Người dùng muốn dự đoán giá nhà dựa trên diện tích. Quan sát thực tế cho thấy những căn nhà có diện tích lớn hơn thường có giá cao hơn. Do đó chúng ta có thể giả định rằng có một mối quan hệ giữa diện tích và giá nhà. Và nhiệm vụ của chúng ta là tìm ra cái quy luật này. Mục tiêu của bài toán là xây dựng một phương",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 18,
      "start_timestamp": "0:11:10",
      "end_timestamp": "0:11:57"
    }
  },
  {
    "page_content": "này. Mục tiêu của bài toán là xây dựng một phương pháp giúp dự đoán giá nhà của một căn nhà mới chỉ cần biết diện tích của nó. Nếu biết có một cách để mô hình hóa mối quan hệ này, chúng ta có thể sử dụng nó để đưa ra các dự đoán chính xác. Tuy nhiên, có một vấn đề mô tả này mang tính định tính và chung chung. Chúng ta chỉ biết giá nhà tăng khi diện tích tăng nhưng không có công thức cụ thể, không có cách nào để tự động hóa việc dự đoán trên máy tính. Điều này khiến việc tính toán trở nên khó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 19,
      "start_timestamp": "0:11:52",
      "end_timestamp": "0:12:37"
    }
  },
  {
    "page_content": "tính. Điều này khiến việc tính toán trở nên khó khăn. Vậy làm thế nào để chuyển bài toán từ mô hình mô tả bằng lời sang mô hình toán học mà máy tính có thể xử lý? Chúng ta đã mô tả bài toán dự đoán giá nhà bằng lời nhưng để máy tính có thể xử lý thì chúng ta cần chuyển nó thành một mô hình toán học. Vậy mô hình nào phù hợp để giải quyết bài toán này? Vì giá nhà có xu hướng tăng khi diện tích tăng? Một cách tự nhiên chúng ta có thể giả định rằng giá nhà và diện tích có mối quan hệ tuyến tính.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 20,
      "start_timestamp": "0:12:32",
      "end_timestamp": "0:13:21"
    }
  },
  {
    "page_content": "giá nhà và diện tích có mối quan hệ tuyến tính. Điều này dẫn chúng ta đến mô hình hồi quy tuyến tính. Bài toán dự đoán giá nhà sẽ được giải quyết bằng cách tiếp cận học máy có giám sát Supervised Learning. Cụ thể thì supervised learning sẽ có quá trình này sẽ gồm có ba bước chính. Bước thứ nhất là thu thập dữ liệu huấn luyện. Training set sẽ bao gồm các cặp giá trị diện tích và giá nhà. Thứ hai là chúng ta huấn luyện mô hình sử dụng cái tập dữ liệu này để tìm ra các tham số sao cho mô hình có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 21,
      "start_timestamp": "0:13:17",
      "end_timestamp": "0:14:06"
    }
  },
  {
    "page_content": "liệu này để tìm ra các tham số sao cho mô hình có thể dự đoán giá nhà chính xác nhất. Và thứ ba là sau khi huấn luyện mô hình có thể nhận đầu vào là diện tích mới của một căn nhà và dự đoán giá bán của nó. Chúng ta có thể sử dụng hồi quy tuyến tính để ước lượng giá trị của một ngôi nhà dựa trên diện tích của nó. Công thức tổng quát của mô hình hồi quy tuyến tính một biến được biểu diễn như sau: y = wx + b. Trong đó x là biến đầu vào hay còn gọi là đặc trưng hay là còn gọi là biến độc lập, biến",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 22,
      "start_timestamp": "0:14:01",
      "end_timestamp": "0:14:47"
    }
  },
  {
    "page_content": "là đặc trưng hay là còn gọi là biến độc lập, biến dùng để dự đoán, ví dụ như diện tích nhà. Y là giá trị dự đoán hay còn gọi là biến phụ thuộc là kết quả cần dự đoán chẳng hạn như là giá nhà. W còn gọi là hệ số hồi quy hay trong trường hợp này gọi là slope thì là một tham số quan trọng thể hiện mức độ ảnh hưởng của X đến Y. Ví dụ như nếu W lớn hơn 0 thì giá nhà tăng khi diện tích tăng. Nếu W nhỏ hơn 0 thì giá nhà giảm khi diện tích giảm. Ví dụ như là chúng ta thấy là nếu mà hệ số W bằng 5000",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 23,
      "start_timestamp": "0:14:42",
      "end_timestamp": "0:15:24"
    }
  },
  {
    "page_content": "như là chúng ta thấy là nếu mà hệ số W bằng 5000 thì điều đó có nghĩa là cứ mỗi mét vuông diện tích tăng lên thì giá nhà sẽ tăng thêm 5000 đơn vị tiền tệ. Một tham số khác đó là B là hệ số chặn. Đây là giá trị của Y khi mà x = 0, tức là giá nhà dự đoán khi diện tích bằng 0. Hệ số này giúp mô hình phù hợp hơn với dữ liệu thực tế. Trong thực tế thì giá nhà không chỉ phụ thuộc vào diện tích mà còn ảnh hưởng bởi nhiều yếu tố khác như vị trí, số phòng, phòng ngủ, tiện ích xung quanh. Cho nên ví dụ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 24,
      "start_timestamp": "0:15:21",
      "end_timestamp": "0:16:00"
    }
  },
  {
    "page_content": "phòng ngủ, tiện ích xung quanh. Cho nên ví dụ như nếu B mà bằng 200 thì điều đó có ý nghĩa là khi diện tích bằng 0 giá nhà vẫn có giá trị dự đoán là 200 đơn vị tiền tệ. Hệ số chặn giúp điều chỉnh mô hình để phản ánh tốt hơn xu hướng của dữ liệu, tránh đưa ra các dự đoán phi thực tế ví dụ như là giá nhà âm. Khi xây dựng mô hình học máy, điều quan trọng đầu tiên là chúng ta cần có dữ liệu huấn luyện. Dữ liệu huấn luyện là một tập hợp các giá trị (x, y) được thu thập từ thực tế. Trong bài toán dự",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 25,
      "start_timestamp": "0:15:53",
      "end_timestamp": "0:16:39"
    }
  },
  {
    "page_content": "y) được thu thập từ thực tế. Trong bài toán dự đoán giá nhà, mỗi cặp giá trị này tương ứng với x là diện tích nhà, đơn vị tính là mét vuông. Đây là biến đầu vào hay còn gọi là feature. Y là giá nhà, ví dụ đơn vị tính là triệu Việt Nam đồng thì đây là giá trị đầu ra hay còn gọi là giá trị target. Ví dụ như là chúng ta có một cái bảng dữ liệu sau. Mỗi dòng đại diện cho một mẫu dữ liệu và mỗi mẫu dữ liệu thì chứa một cặp giá trị x_i, y_i. Trong đó x_i là diện tích của nhà của mẫu dữ liệu thứ i và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 26,
      "start_timestamp": "0:16:33",
      "end_timestamp": "0:17:21"
    }
  },
  {
    "page_content": "x_i là diện tích của nhà của mẫu dữ liệu thứ i và y_i là giá bán tương ứng của căn nhà đó. Tại sao dữ liệu huấn luyện quan trọng? Mô hình học từ các mẫu dữ liệu này để tìm ra mối quan hệ giữa diện tích nhà và giá nhà. Nếu dữ liệu huấn luyện chính xác và đủ lớn, mô hình sẽ có khả năng dự đoán tốt hơn khi gặp các mẫu dữ liệu mới. Chúng ta hãy cùng xem một minh họa trực quan về dữ liệu huấn luyện và hồi quy tuyến tính. Trên đồ thị, mỗi điểm màu xanh biểu diễn một cặp dữ liệu (X, Y) trong tập dữ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 27,
      "start_timestamp": "0:17:16",
      "end_timestamp": "0:17:56"
    }
  },
  {
    "page_content": "biểu diễn một cặp dữ liệu (X, Y) trong tập dữ liệu huấn luyện. Đây là những quan sát thực tế về diện tích nhà và giá bán tương ứng. Đường thẳng màu đỏ là đường hồi quy tuyến tính, mô hình mà chúng ta đang xây dựng. Nó thể hiện mối tương quan, mối quan hệ giữa diện tích và giá nhà mà mô hình học được. Mô hình này cố gắng tìm ra một cái đường thẳng tối ưu nhất để mô tả mối quan hệ giữa diện tích và giá nhà. Ngoài ra, các đường gạch chấm màu xanh lá cây biểu diễn độ sai lệch hay còn gọi là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 28,
      "start_timestamp": "0:17:50",
      "end_timestamp": "0:18:40"
    }
  },
  {
    "page_content": "xanh lá cây biểu diễn độ sai lệch hay còn gọi là residual. Chúng cho thấy sự chênh lệch giữa giá thực tế và giá dự đoán. Độ dài của các đường này càng lớn nghĩa là mô hình có sai số càng cao. Tuy nhiên dù chưa hoàn toàn chính xác mô hình này vẫn cố gắng mô tả xu hướng chung của dữ liệu. Thuật toán học máy thì sẽ tìm một cái đường thẳng tối ưu để mô tả mối quan hệ giữa diện tích và giá nhà. Vậy làm thế nào để tìm được đường thẳng tối ưu trong mô hình hồi quy tuyến tính? Chúng ta cần xác định hai",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 29,
      "start_timestamp": "0:18:34",
      "end_timestamp": "0:19:14"
    }
  },
  {
    "page_content": "hồi quy tuyến tính? Chúng ta cần xác định hai tham số quan trọng W là độ dốc và B là giao điểm với trục tung. Nhưng thế nào là tối ưu? Chúng ta cần một tiêu chí đo lường mức độ sai lệch giữa giá trị dự đoán và giá trị thực tế. Để làm điều này thì ta sử dụng hàm mất mát. Hàm này cho biết sai lệch giữa giá trị dự đoán và giá trị thực tế trên một mẫu dữ liệu. Một lựa chọn phổ biến là Mean Squared Error (MSE), đo bằng bình phương sai số giữa giá trị thực tế và giá trị dự đoán. Tuy nhiên, chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 30,
      "start_timestamp": "0:19:09",
      "end_timestamp": "0:19:57"
    }
  },
  {
    "page_content": "thực tế và giá trị dự đoán. Tuy nhiên, chúng ta không chỉ quan tâm đến một mẫu mà cần đánh giá trên toàn bộ tập dữ liệu huấn luyện. Vì vậy, ta sử dụng hàm chi phí là trung bình của tất cả các giá trị hàm mất mát. Mục tiêu của thuật toán là tìm ra W và B sao cho hàm chi phí J(W,B) đạt giá trị nhỏ nhất. Chúng ta hãy cùng quan sát đồ thị bên trái. Đây là một ví dụ về mô hình hồi quy tuyến tính với một giá trị cụ thể của W. Đường màu xanh thể hiện mô hình dự đoán, còn các điểm màu đỏ là dữ liệu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 31,
      "start_timestamp": "0:19:52",
      "end_timestamp": "0:20:37"
    }
  },
  {
    "page_content": "mô hình dự đoán, còn các điểm màu đỏ là dữ liệu thực tế. Bây giờ hãy nhìn vào cái đồ thị bên phải. Đây là hàm chi phí J(W). Trục ngang là giá trị của W, còn trục dọc là B. Và các đường cong trong đồ thị thể hiện các mức giá trị của J(W,B). Một điểm quan trọng là các điểm nằm trên cùng một đường sẽ có cùng giá trị J(W,B). Điều này có nghĩa là nếu chúng ta di chuyển dọc theo một đường ellipse, giá trị hàm chi phí này không đổi. Mục tiêu của chúng ta là tìm giá trị W và B sao cho J(W,B) đạt giá",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 32,
      "start_timestamp": "0:20:33",
      "end_timestamp": "0:21:09"
    }
  },
  {
    "page_content": "ta là tìm giá trị W và B sao cho J(W,B) đạt giá trị nhỏ nhất, tức là điểm nằm ở đáy của đường cong này. Trước đó thì chúng ta đã xem xét cách J(W) thay đổi theo W. Tuy nhiên, trong mô hình hồi quy tuyến tính thì cả hai tham số W và B đều ảnh hưởng đến giá trị của J(W,B). Hãy cùng quan sát hai đồ thị trên. Đồ thị bên trái cho thấy một đường hồi quy tuyến tính với các giá trị cụ thể của W là 50 và B là 2000. Đường màu xanh thể hiện mô hình dự đoán trong khi các điểm màu đỏ là dữ liệu thực tế. Đồ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 33,
      "start_timestamp": "0:21:04",
      "end_timestamp": "0:21:40"
    }
  },
  {
    "page_content": "trong khi các điểm màu đỏ là dữ liệu thực tế. Đồ thị bên phải là biểu đồ đường đồng mức của hàm J(W,B). Trục ngang là W, trục dọc là B. Và các đường cong trong đồ thị thể hiện các mức giá trị của J(W,B). Một điểm quan trọng là các điểm nằm trên cùng một đường sẽ có cùng giá trị J(W,B). Điều này có nghĩa là nếu chúng ta di chuyển dọc theo một đường ellipse, giá trị hàm chi phí này không đổi. Mục tiêu của chúng ta là tìm điểm thấp nhất trên đồ thị này tương ứng với J(W,B) nhỏ nhất. Đây chính là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 34,
      "start_timestamp": "0:21:35",
      "end_timestamp": "0:22:12"
    }
  },
  {
    "page_content": "này tương ứng với J(W,B) nhỏ nhất. Đây chính là giá trị tối ưu của W và B giúp mô hình có khả năng dự đoán tốt nhất. Gradient Descent là thuật toán tối ưu hóa giúp chúng ta tìm ra giá trị nhỏ nhất của hàm chi phí, tức là tìm ra các tham số W và B sao cho J(W,B) đạt giá trị thấp nhất. Ý tưởng chính của thuật toán này là thay vì thử hết mọi giá trị để tìm điểm tối ưu, ta sẽ lần lượt điều chỉnh tham số theo hướng làm giảm hàm chi phí nhanh nhất dựa vào độ dốc hiện tại của hàm. Các bạn hãy tưởng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 35,
      "start_timestamp": "0:22:07",
      "end_timestamp": "0:22:48"
    }
  },
  {
    "page_content": "vào độ dốc hiện tại của hàm. Các bạn hãy tưởng tượng là mình đang đứng trên sườn một thung lũng trong sương mù, không thể nhìn thấy toàn bộ địa hình. Nhiệm vụ của chúng ta là đi xuống điểm thấp nhất của thung lũng nhưng chỉ có thể cảm nhận được độ dốc dưới chân để xác định hướng đi. Tương tự như trong quá trình tối ưu thì chúng ta dựa vào độ dốc của hàm chi phí tại vị trí hiện tại để quyết định nên điều chỉnh các tham số theo hướng nào và với bước đi bao nhiêu. Nếu độ dốc lớn thì ta có thể đi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 36,
      "start_timestamp": "0:22:44",
      "end_timestamp": "0:23:17"
    }
  },
  {
    "page_content": "đi bao nhiêu. Nếu độ dốc lớn thì ta có thể đi nhanh hơn, bước dài hơn. Nếu độ dốc nhỏ thì ta bước đi nên ngắn để tránh bước qua điểm thấp nhất. Quá trình này được lặp lại nhiều lần. Sau mỗi bước ta lại kiểm tra độ dốc ở vị trí mới và tiếp tục điều chỉnh cho đến khi gần như không còn độ dốc nữa, tức là đã tiến gần đến điểm tối ưu mà chúng ta cần tìm. Sau khi hiểu được cái ý nghĩa trực quan, ý tưởng trực quan của Gradient Descent, bây giờ chúng ta sẽ cùng nhau chuyển sang cách mô tả thuật toán",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 37,
      "start_timestamp": "0:23:12",
      "end_timestamp": "0:23:51"
    }
  },
  {
    "page_content": "ta sẽ cùng nhau chuyển sang cách mô tả thuật toán này bằng ngôn ngữ toán học. Đầu tiên khi bắt đầu quá trình tối ưu các tham số W và B thường được khởi tạo với giá trị ngẫu nhiên hoặc đơn giản là bằng 0. Từ thời điểm xuất phát này thì chúng ta sẽ cập nhật các tham số từng bước một dựa vào gradient tức là độ dốc của hàm tại vị trí hiện tại. Gradient thì cho chúng ta biết hướng và độ lớn của sự thay đổi. Để giảm hàm chi phí ta sẽ điều chỉnh tham số ngược lại theo hướng của gradient. Độ lớn của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 38,
      "start_timestamp": "0:23:46",
      "end_timestamp": "0:24:26"
    }
  },
  {
    "page_content": "số ngược lại theo hướng của gradient. Độ lớn của mỗi bước điều chỉnh này được kiểm soát bởi một siêu tham số. gọi là learning rate (tốc độ học) ký hiệu là alpha. Learning rate là quyết định mỗi lần cập nhật chúng ta sẽ bước một đoạn lớn hay nhỏ trên con đường đi xuống thung lũng. Ở mỗi bước lặp thì chúng ta sẽ cập nhật cái giá trị của W và B bằng cách trừ đi một phần gradient nhân với learning rate và quá trình này cứ lặp đi lặp lại cho đến khi hàm chi phí không còn giảm đáng kể nữa, tức là nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 39,
      "start_timestamp": "0:24:22",
      "end_timestamp": "0:25:01"
    }
  },
  {
    "page_content": "hàm chi phí không còn giảm đáng kể nữa, tức là nó đã đạt đến hoặc tiến đến rất gần với điểm cực tiểu. Ở hình minh họa bên phải thì các bạn có thể quan sát quá trình dịch chuyển của điểm tham số từ vị trí ban đầu xuống đáy của hàm chi phí. Mỗi mũi tên thể hiện một lần cập nhật tham số, di chuyển từng bước một về phía cực tiểu. Khi các bước di chuyển càng nhỏ dần và điểm tham số này nằm gần đáy của thung lũng, tức là chúng ta đã gần như đạt đến điểm tối ưu nhất của mô hình. Việc lựa chọn learning",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 40,
      "start_timestamp": "0:24:58",
      "end_timestamp": "0:25:32"
    }
  },
  {
    "page_content": "tối ưu nhất của mô hình. Việc lựa chọn learning rate phù hợp là rất quan trọng. Nếu chọn quá lớn, mô hình có thể vượt qua điểm tối ưu hoặc dao động quanh đáy thung lũng mà không hội tụ. Nếu chọn quá nhỏ thì quá trình học sẽ rất chậm. Chúng ta sẽ tìm hiểu kỹ hơn về điều này ở các slide tiếp theo. Sau khi đã hiểu ý tưởng của mô hình toán học của Gradient Descent, tiếp theo là chúng ta sẽ tìm hiểu về các bước cụ thể để thực hiện thuật toán này. Đầu tiên là chúng ta khởi tạo các tham số của mô hình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 41,
      "start_timestamp": "0:25:26",
      "end_timestamp": "0:26:01"
    }
  },
  {
    "page_content": "tiên là chúng ta khởi tạo các tham số của mô hình thường là W và B với giá trị ban đầu tùy chọn có thể là zero hoặc là ngẫu nhiên. Sau đó thuật toán sẽ lặp lại các bước cập nhật tham số dựa trên gradient cho tới khi hội tụ. Cụ thể là ở mỗi vòng lặp ta sẽ tính toán giá trị đạo hàm của hàm chi phí theo từng tham số W và B. Dựa vào giá trị gradient này, chúng ta cập nhật W và B theo hướng ngược lại với chiều tăng của hàm chi phí với lượng điều chỉnh bằng learning rate đã chọn. Quá trình này được",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 42,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "bằng learning rate đã chọn. Quá trình này được lặp đi lặp lại nhiều lần. Sau mỗi lần cập nhật ta sẽ kiểm tra điều kiện dừng. Ví dụ như hàm chi phí không còn giảm đáng kể hoặc là đạt đến số vòng lặp tối đa. Khi đó thì các tham số W và B cuối cùng chính là nghiệm tối ưu mà chúng ta cần tìm. Như vậy Gradient Descent là một quá trình tối ưu lặp liên tục điều chỉnh các tham số để tiến dần đến điểm cực tiểu của hàm chi phí. Đây là nền tảng rất quan trọng không chỉ cho hồi quy tuyến tính mà còn cho",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 43,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "trọng không chỉ cho hồi quy tuyến tính mà còn cho nhiều thuật toán học máy khác. Đến đây thì chúng ta hãy cùng nhau tổng kết lại những kiến thức quan trọng đã học về hồi quy tuyến tính một biến. Đầu tiên hồi quy tuyến tính dùng công thức y = Wx + B để mô tả mối quan hệ giữa biến đầu vào và giá trị dự đoán. Để đánh giá mức độ sai lệch giữa giá trị dự đoán và giá trị thực tế thì chúng ta sử dụng hàm mất mát phổ biến nhất là Mean Squared Error (MSE) (bình phương sai số trung bình). Khi xét trên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 44,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "(bình phương sai số trung bình). Khi xét trên toàn bộ dữ liệu huấn luyện, chúng ta có hàm chi phí là trung bình của các giá trị mất mát. Mục tiêu của quá trình huấn luyện là tìm ra bộ tham số W và B sao cho giá trị của hàm chi phí này là nhỏ nhất. Học có giám sát hay còn gọi là supervised learning chính là quá trình sử dụng tập dữ liệu có nhãn để huấn luyện mô hình giúp máy tính tự động tìm ra bộ tham số tối ưu. Và cuối cùng bài toán tối ưu này thường được giải bằng thuật toán Gradient Descent",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 45,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "thường được giải bằng thuật toán Gradient Descent giúp mô hình từng bước điều chỉnh các tham số để tiến gần đến điểm cực tiểu toàn cục của hàm chi phí.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=fazKT_4josc",
      "filename": "fazKT_4josc",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 1)",
      "chunk_id": 46,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chúng ta sẽ cùng đến với một trong những kỹ thuật à cực kỳ quan trọng khi chúng ta sử dụng mô hình hồi quyến tính và cũng như là các cái mô hình mà dạng đơn giản như là tuyến tính đó chính là kỹ thuật feature engineering. Thế thì feature engineering là gì? Là một cái quá trình mà để chúng ta tạo ra hoặc là biến đổi các cái đặc trưng à đầu vào cho mô hình máy học. Thế thì chúng ta biết rằng cái mô hình dự đoán của mình thì y thì nó sẽ là bằng các cái W nhân với lại xy. Sau đó sẽ cộng với lại cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:55"
    }
  },
  {
    "page_content": "cái W nhân với lại xy. Sau đó sẽ cộng với lại cái bias. Thế thì cái vai trò của Xi này cực kỳ quan trọng đối với mô hình tuyến kính tuyến tính. Bản chất của một cái mô hình tuyến tính là nó không tìm ra đặc trưng mới mà nó chỉ là đánh giá xem cái vai trò của cái đặc trưng đó nó ảnh hưởng như thế nào đến cái giá trị đầu ra y đó. hay nói cái khác đó là nó đi tìm cái trọng số thể hiện cái sự quan trọng của cái đặc trưng xy à trong cái việc là đưa ra cái giá trị dự đoán. Thế thì à nếu như chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 1,
      "start_timestamp": "0:00:50",
      "end_timestamp": "0:01:33"
    }
  },
  {
    "page_content": "cái giá trị dự đoán. Thế thì à nếu như chúng ta đưa vào những cái đặc trưng xy này bằng những cái đặc trưng mà không quan trọng thì cái trọng số w i của mình nó sẽ được gán là bằng 0. Nó sẽ tiến về bằng 0. Nhưng mà như vậy thì rõ ràng nó sẽ không giúp cho cái quá trình dự đoán cái giá trị output. Muốn tìm được những cái đặc trưng thật sự có ích hữu ích cho cái việc là dự đoán cái giá trị target là I này nè thì nó sẽ cần có một cái bước gọi là feature engineering. Và feature engineering ở những",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 2,
      "start_timestamp": "0:01:27",
      "end_timestamp": "0:02:09"
    }
  },
  {
    "page_content": "engineering. Và feature engineering ở những cái giai đoạn đầu của các cái mô hình học máy đó chính là chúng ta sẽ phải dựa trên cái domain knowledge tức là cái tri thức một cái tên gọi khác đó chính là tri thức chuyên gia. domain knowledge thì có thể thực hiện cái việc feature engineering này bằng cách đó là chúng ta sẽ biến đổi trên cái feature góc à thì chúng ta có thể là lấy log bình phương hoặc là căn bậc ha đó lấy log lấy bình phương hoặc là lấy căn bậc ha thì đối với cái bình phương hoặc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 3,
      "start_timestamp": "0:02:03",
      "end_timestamp": "0:02:53"
    }
  },
  {
    "page_content": "lấy căn bậc ha thì đối với cái bình phương hoặc là lấy bậc lớn hơn thì ở trong cái slide sau chúng ta sẽ có cái kỹ thuật nó gọi là polynomioal regression. Rồi thì ngoài ra thì chúng ta có thể kết hợp nhiều cái đặc trưng góc lại với nhau. Kết hợp nhiều cái đặc trưng góc lại với nhau để tạo ra một cái đặc trưng mới, tạo ra cái feature mới. Ví dụ đặc trưng gốc của chúng ta chỉ có thuộc tính đó là à thông tin đó là giá nhà và chiều sâu của căn nhà. Thì chúng ta biết rằng là khi mua nhà thì cái việc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 4,
      "start_timestamp": "0:02:47",
      "end_timestamp": "0:03:26"
    }
  },
  {
    "page_content": "chúng ta biết rằng là khi mua nhà thì cái việc định giá một căn nhà nó có ngoài việc chiều dạ chiều chiều rộng và chiều sâu thì nó còn phụ thuộc vào cái yếu tố về diện tích. Đó thì cái diện tích này nè nó chính là cái domain knowledge và nó khai thác bằng cách là đưa vào cái thông tin chiều rộng và chiều sâu nhân lại với nhau thực hiện cái phép nhân để tính ra được. Rồi một số ví dụ khác ví dụ như là trong những cái bài toán mà có ký tính ứng dụng trong thực tế đó thì domain knowledge là một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 5,
      "start_timestamp": "0:03:21",
      "end_timestamp": "0:04:01"
    }
  },
  {
    "page_content": "dụng trong thực tế đó thì domain knowledge là một cái vai trò là cực kỳ quan trọng quyết định phần lớn đến cái độ chính xác của một cái mô hình hồi quyến tính hoặc là mô hình logistic regression về sau. Tại vì cái domain knowledge này á nó tạo ra cho chúng ta những cái đặc trưng thật sự hữu ích. Còn nếu như chúng ta chỉ lấy những cái đặc trưng X mà là những đặc trưng thô thì có thể nó sẽ không phản ánh được những cái quy luật trong tự nhiên. Đó. Do đó thì à hai cái kỹ thuật một đó là chuẩn hóa,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 6,
      "start_timestamp": "0:03:57",
      "end_timestamp": "0:04:33"
    }
  },
  {
    "page_content": "Do đó thì à hai cái kỹ thuật một đó là chuẩn hóa, lấy log, lấy bình phương, lấy căn bậc hai hoặc là cái kỹ thuật mà kết hợp các cái đặc trưng góc để tạo ra đặc trưng mới thì nó có cái vai trò rất là quan trọng trong cái việc là quyết định đến cái độ chính xác của một cái hệ thống ờ máy học. Và trong thậm chí trong nhiều trường hợp thì feature engineering còn quan trọng hơn cả cái việc lựa chọn mô hình. Tức là chúng ta chọn mô hình nào không quan trọng mà feature có tốt hay không nó mới là quan",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 7,
      "start_timestamp": "0:04:29",
      "end_timestamp": "0:05:05"
    }
  },
  {
    "page_content": "trọng mà feature có tốt hay không nó mới là quan trọng. Thì ý của cái câu này là như vậy. Thì một lần nữa chúng ta chỉ khẳng định lại cái vai trò của cái feature engineering này là rất là quan trọng. Và để có một cái feature engineering tốt thì nó phải có cái domain knowledge tức là phải có cái tri thức trong cái lĩnh vực. Thì vì chúng ta đang làm chúng ta là à trong lĩnh vực khoa học máy tính. Thế thì chúng ta sẽ ngay ban đầu chúng ta sẽ không có cái domain knowledge tức là cái tri thức chuyên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 8,
      "start_timestamp": "0:05:01",
      "end_timestamp": "0:05:38"
    }
  },
  {
    "page_content": "cái domain knowledge tức là cái tri thức chuyên môn. Như vậy thì để mà có thể hỗ trợ cho à các cái đối tác của chúng ta trong cái việc xây dựng mô hình một cách hiệu quả thì chúng ta cũng phải tích cực trong cái việc là tự học tập để mà bổ sung thêm cái domain knowledge này nhằm tạo ra các cái đặc trưng tốt hơn phục vụ cho cái việc mà dự đoán của mô hình nó chính xác hơn. Và các cái mô cái cái feature engineering này á thì bản chất của nó là gì? bản chất của nó là chúng ta đang đi tìm ra, đi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 9,
      "start_timestamp": "0:05:33",
      "end_timestamp": "0:06:11"
    }
  },
  {
    "page_content": "bản chất của nó là chúng ta đang đi tìm ra, đi khai thác ra các cái mối quan hệ phức tạp hoặc là cái mối quan hệ ẩn trong dữ liệu mà cái mô hình tuyến tính nó bỏ qua. Và như đã nói, thực ra bản chất của mô hình tuyến tính nó không phải là nó đi tìm ra đặc trưng mới mà bản chất của nó chỉ là đi đánh trọng số những cái đặc trưng đã có chứ không phải là tìm ra đặc trưng mới. Các cái mô hình học sâu hiện đại thì nó sẽ có cái phần là rút trích ra đặc trưng. Nhưng cái việc rút chích đặc trưng đó thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 10,
      "start_timestamp": "0:06:07",
      "end_timestamp": "0:06:52"
    }
  },
  {
    "page_content": "trưng. Nhưng cái việc rút chích đặc trưng đó thì nó cũng có tính chất là theo một cái quy luật nhất định. Còn đôi khi cái domain knowledge nó phải là một cái sự tích lũy kinh nghiệm của rất nhiều năm chúng ta mới có được à mới có thể tạo ra được những cái đặc trưng mà hiệu quả. Thì đó chính là cái ý tưởng của feature engineering. Thì một cái dạng mở rộng của mô hình linear regression và có cái sự tham gia của feature engineering đó chính là polynomial regression. Trong cái giả định của chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 11,
      "start_timestamp": "0:06:44",
      "end_timestamp": "0:07:28"
    }
  },
  {
    "page_content": "regression. Trong cái giả định của chúng ta thì cái y thì nó sẽ là phụ thuộc một cách tuyến tính với lại một cái đặc trưng x. Và ở đây là mối quan hệ tuyến tính. Nhưng thực tế thì chúng ta biết các cái mối quan hệ này nó không có bao giờ nó tuyến tính mà nó sẽ có một cái mối quan hệ phi tuyến nhất định. Đó. Vậy thì ờ làm sao chúng ta có thể hồi chúng ta có thể dự đoán được chính xác cái giá trị output đầu ra đối với những cái mối quan hệ mà phi tiến tính. Thế thì polino regression nó sẽ giúp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 12,
      "start_timestamp": "0:07:23",
      "end_timestamp": "0:08:03"
    }
  },
  {
    "page_content": "tiến tính. Thế thì polino regression nó sẽ giúp cho chúng ta mô tả cái mối quan hệ phi tiến tính đó, mô hình hóa cái mối quan hệ phi tiến tính đó bằng những cái đặc trưng dạng lũy thừa, bậc lũy thừa. Tức là thay vì chúng ta chỉ cung cấp là x thì bây giờ chúng ta sẽ cung cấp cái đặc trưng của chúng ta. là thêm là x bình phương, x lũ lý thừa 3 và x l x lũy thừ 4 vân vân. Đó thì đây là những cái đặc trưng bổ sung thêm à mà chúng ta đã nói trong cái bước cái cái slide trước đó đó là feature",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 13,
      "start_timestamp": "0:07:57",
      "end_timestamp": "0:08:42"
    }
  },
  {
    "page_content": "cái bước cái cái slide trước đó đó là feature engineering. Vậy thì tại sao chúng ta cần phải có cái polynom polynomial regression? Tức là bổ sung thêm các cái đặc trưng mà bật cao. Thì lý do đó là vì cái mô hình lini regression nó chỉ có phù hợp với những cái dữ liệu có xu hướng tiến tính thôi. Nhưng thực tế thì lại không có cái xu hướng đó mà nó có cái mối quan hệ phức tạp hơn. Đó. Cụ thể đó là dữ liệu thì nó sẽ có ở dạng đường cong phức tạp và linear regression thì nó sẽ không cho cái mối",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 14,
      "start_timestamp": "0:08:34",
      "end_timestamp": "0:09:18"
    }
  },
  {
    "page_content": "và linear regression thì nó sẽ không cho cái mối quan hệ phức tạp. Đó. Lấy ví dụ như à chúng ta có cái mối quan hệ là dạng đường cong như thế này. Đó thì cái trục ngang của mình là X và trục đứng sẽ là cái trục Y. Thì khi đó cái đường cong để mà fit được với cái dữ liệu của mình đó nó sẽ có một dạng như thế này. Và đây là một cái dạng bậc hai. Đó là một cái đường cong bậc hai. Do đó thì mô hình linear regression truyền thống với cái đặc trưng là xit được. Tại vì không có thể nào mà có một đường",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 15,
      "start_timestamp": "0:09:11",
      "end_timestamp": "0:09:59"
    }
  },
  {
    "page_content": "xit được. Tại vì không có thể nào mà có một đường thẳng. Nếu mà có một cái đường thẳng để mà fit qua thì đâu đó chỉ có thể là đường thẳng này. Nhưng mà rõ ràng sau này khi cái điểm dữ liệu của mình mà nó đi lên theo cái quy luật như thế này thì không thể nào mà nó fit được. Đó. Rồi và polynomial nó sẽ giúp cho chúng ta giải quyết được vấn đề này. Đó là mô hình hóa được cái mối quan hệ phi tuyến để fit vào cái đường trongg cho cái dữ liệu. Và thực chất nó là một cái một cái cách của feature",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 16,
      "start_timestamp": "0:09:46",
      "end_timestamp": "0:10:38"
    }
  },
  {
    "page_content": "thực chất nó là một cái một cái cách của feature engineering. Tức là nếu như đặc trưng của chúng ta chỉ đơn thuần chúng ta đưa vào là x thì bây giờ chúng ta sẽ bổ sung thêm là x bình phương x lũy thừa 3 và vân vân x lư thừa tô 4. Và khi đó thì chúng ta sẽ có cái bộ trọng số là W1 cho X rồi W2 cộng cho W2 của X bình cộng cho W3 của X l từ 3 vân vân. Thì bản chất đây chính là một cái mô hình hồi quy đa biến. Trong đó các cái biến mới thành phần tham gia vào đó là x bình phương và x từ 3. Và cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 17,
      "start_timestamp": "0:10:33",
      "end_timestamp": "0:11:09"
    }
  },
  {
    "page_content": "gia vào đó là x bình phương và x từ 3. Và cái việc sử dụng polynomial thì chúng ta lưu ý đó là vì các cái x lũy thừ 2 và x lũy thừ 3 á là cái bậc của nó rất là cao nên cái giá trị của mình nó tăng lên rất là nhanh. Mình lấy ví dụ x của mình là bằng 2 thì nếu lũy thừa 2 lên nó sẽ là 4, lũy thừa 3 lên là 8. Đó thì chúng ta thấy là cái giải giá trị của mình nó đã tăng lên cấp lũy thừa. Do đó thì một trong những công việc rất là quan trọng đó chính là phải thực hiện cái feature scaling. Tức là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 18,
      "start_timestamp": "0:11:03",
      "end_timestamp": "0:11:43"
    }
  },
  {
    "page_content": "là phải thực hiện cái feature scaling. Tức là chúng ta sẽ phải chuẩn hóa cho các cái đặc trưng này. Ngoài ra thì chúng ta cũng cần phải lựa chọn cái bậc nào cho phù hợp. Không phải là cái kỹ thuật này nó đơn giản là chúng ta cứ tăng bật lên là xong mà chúng ta phải có cái domain knowledge tức là chúng ta sẽ phải có một cái mối quan hệ à chúng ta sẽ phải có cái tri thức chuyên ngành để mà dự đoán được dự biết được là y của mình nó sẽ cần có cái thông tin x bậc mấy để chúng ta đưa vào cái bậc cho",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 19,
      "start_timestamp": "0:11:35",
      "end_timestamp": "0:12:19"
    }
  },
  {
    "page_content": "tin x bậc mấy để chúng ta đưa vào cái bậc cho phù hợp chứ không phải là chúng ta cứ nhồi nhét vào bậc 4 bậc 5 bậc 6 cho đến bậc 100 thì mô hình nó sẽ tự học ra thì không phải như vậy Khi mô hình của mình mà bổ sung thêm quá nhiều đặc trưng như vậy thì nó rất dễ bị cái hiện tượng là overfitting. Thế thì nhắc lại những cái hiện tượng over underfitting overfitting thì underfitting đó là cái đường một à một cong à đường hình màu đỏ này đúng không? Chúng ta thấy là nó là một cái bậc hai. Trong khi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 20,
      "start_timestamp": "0:12:15",
      "end_timestamp": "0:12:57"
    }
  },
  {
    "page_content": "Chúng ta thấy là nó là một cái bậc hai. Trong khi thực sự cái dữ liệu của mình à là các cái điểm chấm ở đây nó được tạo ra theo cái quy luật là một cái bậc ba. Thì cái bậc 3 này nó sẽ không fit với bậc hai. Và bậc hai này thì thường là dưới cơ so với bậc hai nên nó sẽ gây ra cái hiện tượng là underfitting. Còn overfitting đó là xảy ra khi cái mô hình của mình nó quá phức tạp. Ví dụ như đây là một cái fit, một cái à đối chiếu khớp với lại dữ liệu mà bậc đến 19. Trong khi thực tế dữ liệu của mình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 21,
      "start_timestamp": "0:12:51",
      "end_timestamp": "0:13:28"
    }
  },
  {
    "page_content": "mà bậc đến 19. Trong khi thực tế dữ liệu của mình nó chỉ có bậc 3 thôi. Thì cái bậc 19 này nó quá lớn so với cái bậc 3. Nên chúng ta thấy là cái đường mô hình của mình nó sẽ đi xuyên qua. Nó không phải là đi xuyên qua nữa mà là nó đi đến trực tiếp các cái điểm này luôn. nó nối các cái điểm này lại à nó nối trực tiếp các cái điểm này lại và dẫn đến là cái mô hình của mình mặc dù cái size số cực thấp các cái đường màu x màu màu vàng này size số rất thấp đó nhưng mà khi chúng ta test thì chắc chắn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 22,
      "start_timestamp": "0:13:24",
      "end_timestamp": "0:14:17"
    }
  },
  {
    "page_content": "thấp đó nhưng mà khi chúng ta test thì chắc chắn nó sẽ không có chính xác đó tại vì đi theo cái quy luật như thế này đó thì rõ ràng là nếu mà cái đường màu vàng này mà tiếp tục thì nó sẽ bị lệch ra khỏi cái các cái tập dữ liệu đúng của mình. Đó. Đây ví dụ như những cái điểm màu đỏ này sẽ là những cái điểm đúng. Thì cái đường màu vàng này vì nó quá khớp so với lại các cái mẫu dữ liệu và đặc biệt có những cái giá trị outlayer ví dụ như những giá trị này nó có thể khiến cho cái sự lệch lạc của mô",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 23,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "này nó có thể khiến cho cái sự lệch lạc của mô hình ngày càng cao hơn. đó thì đối với cái mô hình quá phức tạp nó sẽ rất là nhạy cảm với các cái điểm gọi là điểm nhiễu. Và cuối cùng đó là một cái good fit. Thì à cái đường bậc hai xin gọi cái đường bậc ba nó sẽ dùng một cái mô hình bởi cái mô hình bậc ba luôn. Thì hai cái đường bậc ba này nó sẽ giúp cho dữ liệu của chúng ta là nó à gọi là fit phù hợp và vừa đúng với lại cái rotus của mình. Thì hai cái đường này chúng ta thấy là nó đi khá là sát",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 24,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "cái đường này chúng ta thấy là nó đi khá là sát với nhau. Nó đi khá là sát với nhau và đi theo cái dạng đường cong đúng như là cái round trot ban đầu.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=F_EdKKp8Rb4",
      "filename": "F_EdKKp8Rb4",
      "title": "[CS114 - Chương 3] Feature Engineering",
      "chunk_id": 25,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Trong phần tiếp theo thì chúng ta sẽ cùng ôn tập những cái chủ đề mà chúng ta đã tìm hiểu trong môn học à CS114 học máy. Thì à ba cái chủ đề chính mà chúng ta đã cùng tìm hiểu đó chính là về supervised, tức là học có giám sát. Rồi unsupervised learning tức là học không có giám sát. Và cuối cùng đó là học tăng cường. Thế thì cái sự khác biệt giữa ba cái hình thức học này đó là gì? Đối với à học có giám sát thì chúng ta sẽ có cái dữ liệu đầu vào và cái dữ liệu đầu ra rõ ràng. Thì cái dữ liệu y này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gK8D6uEJdho",
      "filename": "gK8D6uEJdho",
      "title": "[CS114 - Chương 8] Tổng kết môn học",
      "chunk_id": 0,
      "start_timestamp": "0:00:14",
      "end_timestamp": "0:01:18"
    }
  },
  {
    "page_content": "cái dữ liệu đầu ra rõ ràng. Thì cái dữ liệu y này nè là cái dữ liệu mình gọi là nhãn là do một cái tập người dùng họ đã gán nhãn trước và có một cái quy trình kiểm tra rất là nghiêm ngặt. Thì cái y này là cái nhãn tương đối là sạch và có thể có nhiễu nhưng mà cái việc nhiễu này là chấp nhận được. Tại vì mô hình của mình nó sẽ phải có khả năng là chấp nhận những cái ngoại lệ hoặc là những cái điểm nhiễu. Nhưng mà nhìn chung đó là cái nhãn Y này là cái dữ liệu sạch. Còn đối với cái học không có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gK8D6uEJdho",
      "filename": "gK8D6uEJdho",
      "title": "[CS114 - Chương 8] Tổng kết môn học",
      "chunk_id": 1,
      "start_timestamp": "0:01:11",
      "end_timestamp": "0:01:57"
    }
  },
  {
    "page_content": "là cái dữ liệu sạch. Còn đối với cái học không có giám sát thì ở đây chúng ta không có cái nhãn Y mà chúng ta chỉ có cái dữ Thế thì mục tiêu của cái học có giám sát đó là chúng ta tìm một cái hàm à chúng ta sẽ phải ước lượng một cái hàm để ánh xạ từ x sang sang y này. Đó thì mục tiêu của mình chính là mapping là một cái hàm ánh xạ. Còn mục tiêu của ờ mô hình học không có giám sát đó là chúng ta sẽ đi học cái phân bố của dữ liệu và từ đó chúng ta sẽ gom nhóm tạo ra thành những cái phân lớp khác",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gK8D6uEJdho",
      "filename": "gK8D6uEJdho",
      "title": "[CS114 - Chương 8] Tổng kết môn học",
      "chunk_id": 2,
      "start_timestamp": "00:01:51",
      "end_timestamp": "0:02:43"
    }
  },
  {
    "page_content": "sẽ gom nhóm tạo ra thành những cái phân lớp khác nhau. Tuy nhiên ở đây sẽ là những cái phân lớp khúc khách hàng thì đó là do chúng ta tự đặt ra các cái class này. Đó. Còn đối với học tăng cường thì ở đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=gK8D6uEJdho",
      "filename": "gK8D6uEJdho",
      "title": "[CS114 - Chương 8] Tổng kết môn học",
      "chunk_id": 3,
      "start_timestamp": "00:02:36",
      "end_timestamp": "0:02:45"
    }
  },
  {
    "page_content": "Trong phần tiếp theo thì chúng ta sẽ cùng à tìm hiểu về à cây quyết định chúng ta sẽ cùng thực hành. Thì cây quyết định đó là một trong những cái mô hình à có tính chất rất là thú vị. Đó là cái tính dễ giải thích. Tại vì sao? Tại vì cái cây quyết định nó sẽ mô phỏng theo cái cách mà con người chúng ta suy nghĩ. Tức là thông thường khi chúng ta đưa ra một cái quyết định gì đó thì chúng ta sẽ chia ra làm nhiều các cái trường hợp, nhiều các cái tình huống khác nhau. Với trường hợp này thì chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:45"
    }
  },
  {
    "page_content": "huống khác nhau. Với trường hợp này thì chúng ta sẽ làm gì? Với trường hợp kia thì chúng ta sẽ làm cái gì? Thì nó gần gũi với cái cách con người chúng ta à suy nghĩ. Do đó thì cây quyết định là một trong những cái mô hình à thường được sử dụng trong cái tình huống đó là nó đòi hỏi cái tính dễ giải thích. Tức là nó sẽ không quá quan trọng cái việc là độ chính xác cao mà nó sẽ quan trọng cái tính dễ giải thích cho một cái người mà không có chuyên môn nhiều về các cái mô hình máy học cũng như là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 1,
      "start_timestamp": "0:00:42",
      "end_timestamp": "0:01:18"
    }
  },
  {
    "page_content": "môn nhiều về các cái mô hình máy học cũng như là các cái mô hình toán có thể hiểu được. Thì để thực hành với cái cây quyết định thì chúng ta sẽ à sử dụng thư viện là scikit-learn và chúng ta sẽ xây dựng cái cây quyết định. Sau đó thì chúng ta sẽ trực quan hóa cái cây quyết định này bằng hai cách. Một đó là chúng ta sẽ vẽ cái cấu trúc cây và hai đó là chúng ta sẽ vẽ cái đường bao tức là cái đường phân loại, đường phân loại đường phân lớp hay còn gọi là cái đường boundary thì chúng ta sẽ trực",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 2,
      "start_timestamp": "0:01:11",
      "end_timestamp": "0:01:58"
    }
  },
  {
    "page_content": "gọi là cái đường boundary thì chúng ta sẽ trực quan hóa bằng hai cách này. Và chúng ta sẽ sử dụng một cái phương pháp là pruning cắt tỉa cây quyết định để cho cái cây của mình nó có cái độ mượt nhưng mà đồng thời nó không quá phức tạp để giải quyết cái vấn đề về overfitting. Rồi và để thực hiện thực hành được cái bài tập này thì chúng ta sẽ sử dụng một cái tập dữ liệu Moons. Thì đây là một cái dữ liệu mô phỏng. Trong đó chúng ta thấy là nó bao gồm hai cái phân lớp là đỏ và xám. thì nó được sắp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 3,
      "start_timestamp": "0:01:52",
      "end_timestamp": "0:02:33"
    }
  },
  {
    "page_content": "hai cái phân lớp là đỏ và xám. thì nó được sắp theo cái hình là hai cái bán nguyệt nó lồng với nhau. Thế thì chúng ta thấy với cái à tập dữ liệu này chúng ta không thể phân tách nó ra làm hai phần bằng một cái đường thẳng được đúng không? Ví dụ như chúng ta chia như thế này hoặc chia như thế này thì không thể nào mà tách nó ra làm hai được. Thì đây là một cái à dữ liệu mà nó có cái tính chất đó là phi tuyến. Thế thì chúng ta sẽ cùng tìm hiểu xem là làm sao cái cây quyết định có thể à giải quyết",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 4,
      "start_timestamp": "0:02:29",
      "end_timestamp": "0:03:10"
    }
  },
  {
    "page_content": "là làm sao cái cây quyết định có thể à giải quyết được các cái bài toán phi tuyến tính nó đã giải quyết như thế nào. Và cái cấu trúc cây mà khi chúng ta trực quan hóa thì nó sẽ có cái dạng hình thù như thế này. Rồi thì chúng ta sẽ sử dụng Google Colab. Thì đối với Google Colab thì chúng ta sẽ khai thác cái thư viện là scikit-learn để xây dựng cây quyết định. À ở đây chúng ta sẽ dùng là sklearn.tree và import cái DecisionTreeClassifier. Ngoài ra thì chúng ta sẽ sử dụng cái plot_tree để vẽ cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 5,
      "start_timestamp": "0:03:04",
      "end_timestamp": "0:03:49"
    }
  },
  {
    "page_content": "thì chúng ta sẽ sử dụng cái plot_tree để vẽ cái cấu trúc cây của mình. Đối với cái tập dữ liệu thì chúng ta dùng sklearn.datasets và dùng cái phương thức đó là make_moons. Thì đầu tiên chúng ta sẽ tạo cái dataset với một cái noise là 0.1. Thì cái noise này á là để cho biết là cái điểm của mình nó dao động có nhiều hay không. Lấy ví dụ như noise ở đây đang là 0.1 thì chúng ta thấy đó là nó vẫn còn tương đối là có thể tách nhau ra được, nó không có giao thoa với nhau. Còn ví dụ như chúng ta cho",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 6,
      "start_timestamp": "0:03:45",
      "end_timestamp": "0:04:30"
    }
  },
  {
    "page_content": "có giao thoa với nhau. Còn ví dụ như chúng ta cho noise là bằng 0 thì chúng ta thấy là cái tập điểm này nó rất là hoàn hảo đúng không? Và nó đi theo các cái đường cong. Còn nếu như chúng ta cho cái noise này lớn hơn ví dụ như là 0.25 thì chúng ta thấy là hai cái tập này nó có cái sự giao thoa với nhau. Đó thì ở đây để đơn giản thì chúng ta sẽ sử dụng đó là khoảng 0.1, ngưỡng là khoảng 0.1 và với 300 điểm đó thì nó còn tương đối có thể tách ra được như thế này. Rồi à sau đó thì chúng ta sẽ gọi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 7,
      "start_timestamp": "0:04:23",
      "end_timestamp": "0:05:12"
    }
  },
  {
    "page_content": "như thế này. Rồi à sau đó thì chúng ta sẽ gọi cái tạo cái cây và với cái tham số mặc định Decision Tree chúng ta sẽ không truyền cái gì vào và chúng ta sẽ gọi hàm fit với cái dữ liệu xy mà chúng ta đã khởi tạo ở trên. Rồi thì sau khi chúng ta tạo và train cái mô hình Decision Tree xong thì ở đây chúng ta sẽ gọi cái hàm trực quan hóa. Thì cái cách trực quan hóa đầu tiên đó là chúng ta sẽ vẽ cái đường quyết định cái decision boundary bằng cách đó là chúng ta sẽ thử nghiệm trên một cái lưới các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 8,
      "start_timestamp": "0:05:06",
      "end_timestamp": "0:05:42"
    }
  },
  {
    "page_content": "là chúng ta sẽ thử nghiệm trên một cái lưới các cái điểm từ trong cái không gian của mình. Và với mỗi cái điểm đó chúng ta sẽ gọi cái hàm à classifier tức là cái hàm ở đây chúng ta sẽ gọi cái hàm tree đúng không? Chúng ta sẽ truyền cái tree này vào. Và với cái classifier này á thì chúng ta sẽ gọi trên toàn bộ cái lưới điểm và sau đó chúng ta vẽ lên. Thì những cái điểm nào mà nằm trong cái lớp đầu tiên thì vẽ bằng một màu, nằm trong cái lớp thứ hai vẽ bằng một cái màu khác. Thì ở đây chúng ta sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 9,
      "start_timestamp": "0:05:39",
      "end_timestamp": "0:06:27"
    }
  },
  {
    "page_content": "vẽ bằng một cái màu khác. Thì ở đây chúng ta sẽ xem. Rồi thì với cái ở đây chúng ta sẽ vẽ là cái decision boundary ha. Rồi thì ở đây chúng ta thấy là chúng ta đã chia ra thành một lưới từ -1 cho đến 2.5 và ở trục tung thì là khoảng -1 cho đến 1.5. Thì chúng ta với mỗi cái điểm trong cái lưới này chúng ta sẽ gọi cái classifier để dự đoán. Và từ cái giá trị dự đoán này thì với cái giá trị màu đó chúng ta sẽ tô màu lên. Đó thì màu đỏ chúng ta sẽ tô màu đỏ và màu xám chúng ta sẽ tô màu xám. Thì đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 10,
      "start_timestamp": "0:06:22",
      "end_timestamp": "0:06:59"
    }
  },
  {
    "page_content": "màu đỏ và màu xám chúng ta sẽ tô màu xám. Thì đây là cái cách mà để vẽ đường bao. Rồi sau đó thì chúng ta sẽ tiến hành là trực quan hóa cái cây của mình bằng cái phương thức đó là plot_tree. Thì trong cái thư viện scikit-learn nó sẽ có cái phương thức là plot_tree. Chúng ta sẽ truyền vào cái cây và các cái đặc trưng của mình. Thì vì mỗi một cái điểm trong cái không gian của mình nó sẽ bao gồm là trục x1 và trục x2 tức là trục hoành và trục tung. Nên chúng ta để cái feature_names ở đây là x1 và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 11,
      "start_timestamp": "0:06:53",
      "end_timestamp": "0:07:53"
    }
  },
  {
    "page_content": "Nên chúng ta để cái feature_names ở đây là x1 và x2. Và cái cái class_names của mình sẽ đặt tên đó là class 0 và class 1. Rồi thì ở đây chúng ta sẽ gọi à vẽ cái cây này ra. Thì ở cái nút gốc chúng ta sẽ có cái ờ quyết định đó là x2 tức là cái trục tung của mình có bé hơn 0.495 hay không. Thì ý đó là gì? Chúng ta sẽ có một cái đường để chia ra tại cái trục ở giữa này. Chúng ta thấy có một cái đường ở đây. Có một cái đường ở đây thì nó chia ra làm đôi. Nếu như x2 mà bé hơn à 0. 495 tức là nó nằm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 12,
      "start_timestamp": "0:07:46",
      "end_timestamp": "0:08:33"
    }
  },
  {
    "page_content": "đôi. Nếu như x2 mà bé hơn à 0. 495 tức là nó nằm ở nửa dưới thì chúng ta sẽ đi về phía bên tay trái. Còn nếu như sai tức là nó nằm ở phần nửa trên. nằm ở nửa trên thì chúng ta sẽ đi về bên tay phải. Thì nếu mà đi về tay phải thì chúng ta sẽ phải tiếp tục kiểm tra thêm một số cái điều kiện ràng buộc. Còn nếu đi về bên tay trái thì chúng ta lại kiểm tra xem là cái x1 của mình nó có bé hơn -0.513 hay không. Đó thì ở đây chúng ta sẽ xem là x1 của mình là đây. Tức là cái đường của mình nó chính là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 13,
      "start_timestamp": "0:08:28",
      "end_timestamp": "0:09:17"
    }
  },
  {
    "page_content": "là đây. Tức là cái đường của mình nó chính là cái đường này nè là -0.513 là đường này nó chia ra làm đôi. Thì nếu như bé hơn -0.513 tức là nó nằm về nửa phía bên đây. Tức là vừa dưới cái đường này. Vừa dưới cái đường này và về tay phía bên tay trái của cái đường này. Rồi thì nó lại tiếp tục kiểm tra xem là x2 có bé hơn -0.03 hay không. Tức là x2 của mình có bé hơn -0.03 không? Tức là cái ngưỡng này, ngưỡng này nếu mà nó bé hơn, nếu nó bé hơn thì nó lại tiếp tục kiểm tra xem x1 có bé hơn 0.53",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 14,
      "start_timestamp": "0:09:12",
      "end_timestamp": "0:09:55"
    }
  },
  {
    "page_content": "nó lại tiếp tục kiểm tra xem x1 có bé hơn 0.53 hay không. Thế thì chúng ta thấy là với mỗi cái lần kiểm tra x1, x2 này á thì nó sẽ tạo ra một cái lát cắt và tổ hợp của các cái lát cắt thì nó sẽ chia cái không gian đặc trưng của mình ra thành các cái không gian như thế này, thành các cái vùng như thế này. Thì ở đây chúng ta thấy là cái cây của mình nó rất là phức tạp. Mặc dù số vùng ở đây thì rất là ít nhưng mà cái cây của mình nó lại rất là phức tạp. Do đó thì chúng ta sẽ có cái kỹ thuật là để",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 15,
      "start_timestamp": "0:09:51",
      "end_timestamp": "0:10:34"
    }
  },
  {
    "page_content": "tạp. Do đó thì chúng ta sẽ có cái kỹ thuật là để gọi là cắt tỉa cái cây sao cho nó đơn giản nhất có thể. Nhưng mà trước khi đi qua cái kỹ thuật cắt tỉa cây thì chúng ta sẽ à sử dụng một cái kỹ thuật đó là chúng ta sẽ thử với rất nhiều những cái à thay đổi rất nhiều những cái max_depth tức là cái một trong những cái siêu tham số quan trọng trong cái thuật toán Decision Tree ha. Depth của mình á là được tính từ cái nút này nè là ví dụ như cái cây này nó sẽ có cái depth là bằng 6. Thế thì bây giờ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 16,
      "start_timestamp": "0:10:28",
      "end_timestamp": "0:11:07"
    }
  },
  {
    "page_content": "này nó sẽ có cái depth là bằng 6. Thế thì bây giờ chúng ta sẽ thử với các cái depth là 2, 4, 8 và None. None có nghĩa đó là nó sẽ không có cái ràng buộc max là bằng bao nhiêu ha. Thì chúng ta thử là vẽ cái decision boundary à tương ứng với từng cái depth nhau thì nó sẽ nhìn như thế nào. Thì với depth mà bằng 2 đúng không? thì chúng ta sẽ thấy là cái cây của mình nó đơn giản dẫn đến là cái phân vùng của mình được tách ra thành các vùng như thế này. Thì cái cách với cái depth thì rõ ràng là nó đã",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 17,
      "start_timestamp": "0:11:01",
      "end_timestamp": "0:11:46"
    }
  },
  {
    "page_content": "Thì cái cách với cái depth thì rõ ràng là nó đã loại bỏ nó đã bỏ sót những cái điểm màu đỏ này nó đã phân vùng sai. Tức là depth nếu mà chọn ít quá thì nó sẽ không thể phân loại được đủ tốt. Đó, chúng ta tăng cái depth lên bằng 4 thì chúng ta thấy đó là nó đã cái vùng của mình nó đã có cái tính chất phức tạp hơn. À nó đã có cái tính chất phức tạp hơn tuy nhiên nó lại bị sai ở chỗ khu vực này. Đó là cái điểm màu xám này nó lại được xếp vào cái vùng màu đỏ. Đó là sai. Chỗ này khi tăng lên depth",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 18,
      "start_timestamp": "0:11:41",
      "end_timestamp": "0:12:20"
    }
  },
  {
    "page_content": "màu đỏ. Đó là sai. Chỗ này khi tăng lên depth bằng 8 thì chúng ta thấy là nó đã phức tạp hơn và nó đã phân tách ra làm hai phần như thế này. Thì ở đây chúng ta có thể thử nghiệm thêm một số cái tình huống nữa ví dụ như là 5, 6 để xem xem là nó thay đổi như thế nào ha. Rồi thì với depth = 4, với depth bằng 5 thì chúng ta thấy là nó đã phân tách khá là tốt. Nó đã phân tách khá là tốt. Chia ra làm hai phần là màu đỏ và màu xám. Tuy nhiên thì nó vẫn còn một vài cái điểm màu xám ở đây. À nó vẫn còn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 19,
      "start_timestamp": "0:12:15",
      "end_timestamp": "0:12:54"
    }
  },
  {
    "page_content": "còn một vài cái điểm màu xám ở đây. À nó vẫn còn một vài điểm màu xám ở đây nó đã bị lọt sổ và cho cái à nằm trong cái vùng màu đỏ phân vùng màu đỏ. Rồi với depth = 6 thì chúng ta thấy đó là nó đã giải quyết được hai cái vấn đề này. Tuy nhiên nó đã tạo ra hai cái đường phân lớp à nó tạo ra hai cái đường phân lớp đặc biệt này chỉ để xử lý hai cái điểm màu xám ở đây thôi. Thế thì cái việc này là nó không đáng. Cái việc này là nó không đáng. Rồi như vậy thì làm sao chúng ta có thể cắt tỉa được cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 20,
      "start_timestamp": "0:12:49",
      "end_timestamp": "0:13:33"
    }
  },
  {
    "page_content": "vậy thì làm sao chúng ta có thể cắt tỉa được cái cây? Thì ý tưởng chúng ta sẽ sử dụng thuật toán đó là Cost-Complexity Pruning (CCP). Một cái cây quyết định á thì nó sẽ rất là phức tạp nếu mà nó có càng nhiều nhánh và càng nhiều nhánh á thì nó sẽ càng dễ bị overfit tại vì cái mô hình của mình nó cứ chăm chăm học những cái tình huống ngoại lệ. À ví dụ như trong cái sơ đồ đây chúng ta thấy có hai cái điểm này là hai cái điểm ngoại lệ nè. đó thì nó chăm chăm nó tăng thêm cái cây chỉ để học thêm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 21,
      "start_timestamp": "0:13:25",
      "end_timestamp": "0:14:09"
    }
  },
  {
    "page_content": "nó chăm chăm nó tăng thêm cái cây chỉ để học thêm hai cái điểm này là chuyện đó là nó không đáng tại vì nó giảm đi cái tính tổng quát của mô hình. Như vậy để cân bằng thì ta định nghĩa một cái hàm chi phí đó là dính đến tham số alpha là một cái siêu tham số. Trong đó RT chính là cái lỗi à của cái cây T của mình và trị tuyệt đối T chính là cái số lá của cái cây. Thế thì bình thường á nếu như alpha mà bằng 0 á tức là nó là cái tham số mặc định của mình. Đó chính là cái việc mà chúng ta khởi tạo",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 22,
      "start_timestamp": "0:14:05",
      "end_timestamp": "0:14:46"
    }
  },
  {
    "page_content": "mình. Đó chính là cái việc mà chúng ta khởi tạo một cái cây với cái tham số mặc định của mình. Đó là như thế này rồi. Nhưng nếu alpha tăng lên, alpha tăng lên thì nó sẽ đưa cái số nút lá này vào như là một cái hàm độ lỗi. Như vậy thì ngoài cái việc là nó giảm cái độ lỗi của cái việc mà phân loại à giảm cái độ lỗi của cái việc phân loại của cái cây thì nó đồng thời nó sẽ phải đưa thêm cái tính phức tạp của cái cây vào. Nếu như cái cây này mà quá phức tạp thì nó sẽ không tốt. Nó sẽ làm sao hài",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 23,
      "start_timestamp": "0:14:41",
      "end_timestamp": "0:15:40"
    }
  },
  {
    "page_content": "phức tạp thì nó sẽ không tốt. Nó sẽ làm sao hài hòa được cả hai yếu tố này. Tức là độ lỗi phân loại cũng phải thấp nhưng mà cái độ phức tạp của cái cây cũng phải thấp. Tức là cái T này cũng phải thấp. Đó thì cái tham số alpha này là cái tham số mà nó gọi là penalty. Tức là cái hình phạt cho cái độ phức tạp. Rồi. Thế thì ở đây chúng ta sẽ lấy ra các cái tham số alpha và cái impurity tức là cho biết cái độ lỗi của mình tương ứng với các alpha đó. Đó thì khi chúng ta chạy cái thuật toán này thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 24,
      "start_timestamp": "0:15:32",
      "end_timestamp": "0:16:12"
    }
  },
  {
    "page_content": "Đó thì khi chúng ta chạy cái thuật toán này thì chúng ta thấy đó là có bảy cái alpha khác nhau. Tức là chúng ta sẽ cho bảy cái hệ số alpha khác nhau từ chạy từ 0 ha. Thì ban đầu nó sẽ là bằng 0 rồi 0.003, 0.001, 0.004 và 0.009, rồi 0.23. Thì tương ứng với từng cái alpha này chúng ta tăng lên thì chúng ta sẽ thấy cái độ lỗi của mình sẽ như thế nào. Rồi thì bây giờ nó cũng sẽ tăng lên theo. Thế thì bây giờ chúng ta sẽ thử với từng cái alpha ha với từng cái alpha mà chúng ta đã tìm ra được ở đây.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 25,
      "start_timestamp": "0:16:06",
      "end_timestamp": "0:16:55"
    }
  },
  {
    "page_content": "từng cái alpha mà chúng ta đã tìm ra được ở đây. Và chúng ta sẽ gọi cái Decision Tree. Gọi cái Decision Tree. Rồi sau đó chúng ta fit dữ liệu vào. Và chúng ta sẽ tìm cách đó là chúng ta trực quan hóa. Rồi thì với cái alpha mà bằng 0, alpha bằng 0 tức là cái mô hình của mình à không có cái không có cái t ở đây. Tức là mô hình của mình chính là cái Decision Tree mặc định ha. Không có cắt tỉa gì hết. Thì chúng ta thấy cái độ chính xác rất là cao. Độ chính xác rất là cao. Gần như xấp xỉ là bằng 1",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 26,
      "start_timestamp": "0:16:45",
      "end_timestamp": "0:17:53"
    }
  },
  {
    "page_content": "Độ chính xác rất là cao. Gần như xấp xỉ là bằng 1 là vì cái mô hình của mình nó overfitting. Nó overfitting với lại cái những cái điểm ngoại lệ. Đó. Khi cái alpha này tăng lên, khi alpha này bắt đầu tăng lên thì chúng ta thấy là cái độ chính xác nó bắt đầu rớt xuống nhưng nó sẽ không có rớt nhiều. Đó. Thì chúng ta thấy là ở ba cái ngưỡng này là đến khoảng trước cái con số là 0.05 thì cái ngưỡng của mình, cái ngưỡng độ chính xác của mình là chấp nhận được nó giảm một chút. À nhưng bù lại khi nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 27,
      "start_timestamp": "0:17:47",
      "end_timestamp": "0:18:34"
    }
  },
  {
    "page_content": "nhận được nó giảm một chút. À nhưng bù lại khi nó giảm thì cái độ chính xác giảm thì cái tính tổng quát hóa của nó sẽ cao hơn. Đó nhưng mà nếu mà alpha tăng lên cao quá ví dụ như là bằng 0.1 ở đây thì nó sẽ giảm xuống cái độ chính xác của mình là còn khoảng 0.8 mươi mấy. Đó thì cái này lại đánh đổi nhiều quá. Do đó đẹp nhất là alpha của mình là khoảng ở mức số 3 ở đây. 0 1 2 3 tức là tương ứng 0 nè 1 2 3 tức là khoảng 0.012 là vừa đẹp. Rồi thì bây giờ chúng ta sẽ thử các cái alpha khác nhau.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 28,
      "start_timestamp": "0:18:26",
      "end_timestamp": "0:19:14"
    }
  },
  {
    "page_content": "bây giờ chúng ta sẽ thử các cái alpha khác nhau. Chúng ta sẽ thử các alpha khác nhau. Ví dụ như ở đây chúng ta sẽ lấy ra các cái giá trị đầu tiên rồi sau đó thì chúng ta sẽ vẽ cái cây. Thì với alpha bằng 0 tức là cái cây của mình là giống như là cái cây mặc định của mình nó sẽ chúng ta sẽ thấy có rất nhiều cái đường zigzag ở đây và nó rất là phức tạp đúng không? Nó chỉ có thêm những cái đường này chủ yếu là để bắt được hai cái điểm ngoại lệ ở đây. Nhưng mà khi alpha bằng 0.003 thì chúng ta thấy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 29,
      "start_timestamp": "0:19:08",
      "end_timestamp": "0:19:49"
    }
  },
  {
    "page_content": "Nhưng mà khi alpha bằng 0.003 thì chúng ta thấy là cái đường của mình nó đã mượt mà hơn, nó đã trơn hơn. Đó. Và khi tăng lên đó thì chúng ta thấy là alpha bằng 0.01 đó thì nó sẽ tạo ra và nó vẫn chia tách ra làm hai phần như thế này và cái đường đi của mình nó sẽ à đơn giản hơn. Đó thì khi alpha càng tăng, khi alpha càng tăng thì cái mô hình của mình càng đơn giản. Chúng ta thấy là nó chỉ có các cái đường lên nè, qua phải xuống qua phải lên qua phải. Còn khi alpha mà nhỏ thì nó sẽ càng phức",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 30,
      "start_timestamp": "0:19:45",
      "end_timestamp": "0:20:35"
    }
  },
  {
    "page_content": "phải. Còn khi alpha mà nhỏ thì nó sẽ càng phức tạp. Nó đi qua đây xuống rồi lên đây rồi qua đây. Đó một cái đường nhỏ như thế này thôi. Nó zích zắc nó đi ra một cái đường nhỏ như thế này thôi cũng là rất nhiều cái nhánh rồi. Rồi thì ở đây chúng ta thấy là với alpha khoảng 0.003 là nó sẽ ra một cái đường bao có vẻ là hợp lý. Đó, nó ra một cái đường bao có vẻ là hợp lý là vì nó sẽ giải quyết được các cái điểm ở đây. Còn với alpha bằng 0.012 thì nó sẽ bỏ sót các cái điểm ở đây. Đó thì đây là à ý",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 31,
      "start_timestamp": "0:20:27",
      "end_timestamp": "0:21:03"
    }
  },
  {
    "page_content": "sẽ bỏ sót các cái điểm ở đây. Đó thì đây là à ý nghĩa của cái việc là cắt tỉa nó sẽ tạo ra các cái đường bao đơn giản mà hiệu quả và tăng cái tính tổng quát của cái mô hình. Như vậy thì trong cái bài thực hành này thì chúng ta đã cùng à thử nghiệm về cái Decision Tree rồi ý nghĩa của cái max_depth. Thì cái tham số max_depth đó là cái độ sâu của mình. Nếu mà độ sâu của mình càng càng lớn, tức là nó càng nhiều nhánh thì cái mô hình của mình nó sẽ càng phức tạp và gây ra cái hiện tượng là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 32,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "nó sẽ càng phức tạp và gây ra cái hiện tượng là overfitting. Do đó, để giảm cái hiện tượng overfitting này thì hoặc là chúng ta chủ động giảm cái max_depth này xuống hoặc là chúng ta sẽ sử dụng một cái phương pháp regularization chính quy hóa. Chúng ta thêm một cái hệ số alpha, siêu tham số alpha ở đây để mà đưa cái số nhánh vào trong cái hàm loss của mình. Đó thì khi alpha của mình mà khoảng một cái giá trị ví dụ như là khoảng 0. 03 hoặc là 0.012 thì nó sẽ cho cái mô hình của mình vừa đủ đơn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 33,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "thì nó sẽ cho cái mô hình của mình vừa đủ đơn giản. nó sẽ cho một cái mô hình vừa đủ đơn giản nhưng mà nó đủ tổng quát và đủ độ chính xác cao trên cái tập điểm ờ phi tuyến tính của mình. Đó thì đây là một cái sự cân bằng giữa độ chính xác khi train. Nhưng mà lưu ý là khi chúng ta train mà độ chính xác cao thì không chắc là khi test chúng ta sẽ có cái độ chính xác cao ha. Tại vì nếu như chúng ta chăm chăm vào học những cái điểm ngoại lệ này nè thì đâu đó khi chúng ta test có thể chúng ta sẽ đánh",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 34,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "đâu đó khi chúng ta test có thể chúng ta sẽ đánh đổi cái độ chính xác trên cái tập test. Do đó, mô hình của mình càng đơn giản thì là càng tốt.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=i0v_PbwMiBs",
      "filename": "i0v_PbwMiBs",
      "title": "[CS114 - Tutorial] Decision Tree",
      "chunk_id": 35,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Câu hỏi tiếp theo đó chính là tại sao hàm mất mát của cái mô hình của mình chúng ta lại à sử dụng cái hàm mean squared error mà chúng ta không sử dụng những cái hàm bậc một hoặc là mae là min absolute error. Cụ thể hơn đối với cái hàm bậc một thì chúng ta có thể thay cái công thức ở trên bằng cái công thức đó là J của W B là bằng 1/N của tổng của y trừ cho WXY cộng B. Tức là đây là cái sai số giữa cái giá trị dự đoán và cái giá trị à thực tế. Thì đây chính là dự đoán nè, còn đây là thực tế nè.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=I2JXWweJm0g",
      "filename": "I2JXWweJm0g",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 2)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:01:15"
    }
  },
  {
    "page_content": "đây chính là dự đoán nè, còn đây là thực tế nè. Thì rõ ràng chúng ta dùng công thức này nó sẽ đơn giản hơn. Thì để giải thích cho việc này á thì chúng ta có hai cách. Cách thứ nhất đó là chúng ta cũng sử dụng lý thuyết về toán. Rồi thì nếu như chúng ta nhìn cái hàm này á ở một góc độ nào đó thì cái hàm JWB này đó là một cái hàm bậc nhất theo W và B. Là một cái hàm bậc nhất theo W và B. Mà trong lý thuyết về toán tối ưu chúng ta biết rồi. Một cái hàm bậc một đó là một hàm tuyến tính thì nó sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=I2JXWweJm0g",
      "filename": "I2JXWweJm0g",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 2)",
      "chunk_id": 1,
      "start_timestamp": "0:01:09",
      "end_timestamp": "0:01:55"
    }
  },
  {
    "page_content": "hàm bậc một đó là một hàm tuyến tính thì nó sẽ không có giá trị nhỏ nhất. à nó sẽ không có giá trị nhỏ nhất dẫn đến là chúng ta không thể huấn luyện cái mô hình này để tìm ra cái W và B sao cho cái giá trị này nhỏ nhất được tại vì nó vốn không có đó nếu chúng ta vẽ nó trên cái không gian giả sử như ở đây chúng ta xét một cái hàm tuyến tính mà một biến thôi ha thì W ở đây thì rõ ràng đây là một cái hàm nó tiến về trừ vô cùng và mình sẽ không có cái giá trị nhỏ nhất ở đây. Mặt khác nếu mà cái hàm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=I2JXWweJm0g",
      "filename": "I2JXWweJm0g",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 2)",
      "chunk_id": 2,
      "start_timestamp": "0:01:49",
      "end_timestamp": "0:02:31"
    }
  },
  {
    "page_content": "giá trị nhỏ nhất ở đây. Mặt khác nếu mà cái hàm của mình nó như thế này thì chúng ta thấy là chúng ta cũng sẽ không có được cái giá trị nhỏ nhất. À chúng ta không có được cái giá trị nhỏ nhất. Vậy thì với cái cách lý giải đầu tiên đó là sử dụng lý thuyết toán thì sử dụng hàm bậc một không thể giúp cho chúng ta tìm được cái giá trị nhỏ nhất đúng hơn đó là nó không có để mà tìm. Cái cách giải thích thứ hai đó là chúng ta có thể dựa trên cái lý thuyết về mặt thông tin. Thì nếu như chúng ta tính",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=I2JXWweJm0g",
      "filename": "I2JXWweJm0g",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 2)",
      "chunk_id": 3,
      "start_timestamp": "0:02:25",
      "end_timestamp": "0:03:11"
    }
  },
  {
    "page_content": "về mặt thông tin. Thì nếu như chúng ta tính tổng các cái sai số này thì nó sẽ có khả năng đó là những cái tình huống y dự đoán lớn hơn thực à y là là thực tế lớn hơn dự đoán thì lúc đó cái sai số của mình là dương. Lấy ví dụ như là dương 2 đi. Rồi nó cũng sẽ có cái tình huống đó là y nó nhỏ hơn wxy cộng cho b thì khi đó cái hiệu này nó sẽ là một con số âm giả sử như là -1. Thì ở đây chúng ta thấy là nó có một cái bất cập là hàm loss là cái hàm mà thể hiện cái sự sai số. Nhưng nếu đúng ra thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=I2JXWweJm0g",
      "filename": "I2JXWweJm0g",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 2)",
      "chunk_id": 4,
      "start_timestamp": "0:03:05",
      "end_timestamp": "0:03:47"
    }
  },
  {
    "page_content": "mà thể hiện cái sự sai số. Nhưng nếu đúng ra thì cái sai số ở đây nó nên là 3 hoặc hơn thế nữa. Còn ở đây chúng ta lại thấy 2 mà trừ đi cho 1 thì như vậy là còn còn 1. Tức là nó đã giảm bớt cái sai số của cái mẫu dữ liệu trước. Hay nói khác đó là các cái mẫu dữ liệu nó triệt tiêu lẫn nhau. Và đương nhiên cái tình huống hy hữu đó là ví dụ như mẫu dữ liệu đầu tiên thì là lớn hơn hai đơn vị, mẫu dữ liệu tiếp theo thì nhỏ hơn một đơn vị. Và mẫu tiếp theo là nhỏ hơn -1 đơn vị. Vậy thì tổng sai số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=I2JXWweJm0g",
      "filename": "I2JXWweJm0g",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 2)",
      "chunk_id": 5,
      "start_timestamp": "0:03:43",
      "end_timestamp": "0:04:23"
    }
  },
  {
    "page_content": "theo là nhỏ hơn -1 đơn vị. Vậy thì tổng sai số của mình này là bằng 0. Thì đó là một cái điều rất là vô lý vì à rõ ràng là cái dự đoán không khớp với lại giá trị thực tế nhưng mà tổng sai số lại thì nó lại là ra con số 0. Đó thì đó chính là cái bất cập. Và chúng ta sẽ xét đến cái một cái ví dụ à một cái tình huống thứ hai đó là tại sao chúng ta không sử dụng hàm min absolute error. Thì nếu chúng ta sử dụng hàm min absolute error thì cái J của WB nó sẽ được viết lại là bằng 1/n của tổng của y",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=I2JXWweJm0g",
      "filename": "I2JXWweJm0g",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 2)",
      "chunk_id": 6,
      "start_timestamp": "0:04:17",
      "end_timestamp": "0:05:21"
    }
  },
  {
    "page_content": "WB nó sẽ được viết lại là bằng 1/n của tổng của y chạy từ 1 cho đến n. thì ở đây nó sẽ là y trừ cho w xy cộng b. Đó thì với cái công thức này à x ở đây chúng ta sẽ có một cái dấu trị tuyệt đối ha. Dấu trị tuyệt đối. Đây mình sẽ đổi màu để cho các bạn hình dung. Rồi thì đối với cái hàm này nó đã khắc phục được cái vấn đề của hàm bậc một gây ra đó là nó bị triệt tiêu. Thì khi chúng ta đưa cái dấu trị tuyệt đối vào thì à ví dụ như là 2 rồi -1 -1 thì khi chúng ta cộng lại thì nó sẽ có trị tuyệt đối",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=I2JXWweJm0g",
      "filename": "I2JXWweJm0g",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 2)",
      "chunk_id": 7,
      "start_timestamp": "0:05:15",
      "end_timestamp": "0:06:02"
    }
  },
  {
    "page_content": "khi chúng ta cộng lại thì nó sẽ có trị tuyệt đối và tổng sai số lúc này của mình là 4. Và nếu chia bình quân nữa thì ở đây là chúng ta sẽ chia cho hai mẫu tức là bằng 2. Như vậy thì với cái sai số này chúng ta sẽ thấy đó là một cái sai số lớn và nó có một cái lý do để khiến cho chúng ta không sử dụng min absolute error một cách phổ biến. Đó chính là cái đạo hàm thì nó sẽ khó tính, tức là khó tính toán. Cụ thể đó là đạo hàm của hàm |x| thì nó sẽ là một cái dấu như thế này và nó sẽ bằng 1 nếu x",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=I2JXWweJm0g",
      "filename": "I2JXWweJm0g",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 2)",
      "chunk_id": 8,
      "start_timestamp": "0:05:56",
      "end_timestamp": "0:06:39"
    }
  },
  {
    "page_content": "là một cái dấu như thế này và nó sẽ bằng 1 nếu x dương và nó sẽ là bằng -1 nếu x của mình là con số âm. Thì đây là cái điều kiện à cho cái hàm à mất mát của mình đối với mô hình hồi quy tuyến tính. Và trong những phần tiếp theo thì chúng ta sẽ cùng tìm hiểu về những cái chủ đề nâng cao hơn của mô hình hồi quy tuyến tính.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=I2JXWweJm0g",
      "filename": "I2JXWweJm0g",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 2)",
      "chunk_id": 9,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chúng ta sẽ cùng đến với câu hỏi thảo luận tiếp theo đó là ý nghĩa hình học của mô hình của mình là gì? Thì trong cái công thức của chúng ta thì chúng ta thấy nó sẽ là hàm sigmoid của wx cộng cho b. Thế thì ý nghĩa của W và B ở đây sẽ là gì? Thì ở đây giả sử như chúng ta à xét cái đặc trưng đầu vào là trong một cái à một cái vector 2D. Có nghĩa là x của chúng ta là thuộc R2. Thì x lúc này sẽ bao gồm hai thành phần đó là x1 và x2. Thì x1 và x2 này sẽ là hai cái tọa độ là cái à giá trị tọa độ của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IReTdn_z-es",
      "filename": "IReTdn_z-es",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 3",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:01:05"
    }
  },
  {
    "page_content": "sẽ là hai cái tọa độ là cái à giá trị tọa độ của một cái điểm của mình. Đó thì một cái điểm trong cái không gian của mình nó sẽ là bao gồm x1 và x2. Thì các cái điểm này sẽ được tách ra làm hai phần. Và sau khi cái mô hình hồi quy logistic mà đã được huấn luyện xong thì chúng ta thấy nó đã tách ra à làm hai phần bằng một cái đường thẳng. Vậy thì cái ý nghĩa về mặt hình học của cái đường thẳng này đó chính là ờ WX cộng B bằng 0. Thì như chúng ta biết đó là phương trình của một đường thẳng thì nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IReTdn_z-es",
      "filename": "IReTdn_z-es",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 3",
      "chunk_id": 1,
      "start_timestamp": "0:00:59",
      "end_timestamp": "0:01:47"
    }
  },
  {
    "page_content": "đó là phương trình của một đường thẳng thì nó sẽ có dạng là một cái đẳng thức như thế này. Và nếu như chúng ta biểu diễn nó ra thành đầy dạng đầy đủ, tức là x của chúng ta gồm nhiều thành phần thì lúc này công thức của mình nó sẽ là w1x1 cộng cho w2x2 cộng cho b. Thì cái x của mình nó là một cái vector hai chiều thì W của mình tương ứng nó cũng sẽ có hai thành phần là hai chiều W1 W2 và cái này sẽ là bằng 0. Thì đây chính là cái dạng thức của cái phương trình đường thẳng dạng tổng quát mà chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IReTdn_z-es",
      "filename": "IReTdn_z-es",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 3",
      "chunk_id": 2,
      "start_timestamp": "0:01:41",
      "end_timestamp": "0:02:27"
    }
  },
  {
    "page_content": "phương trình đường thẳng dạng tổng quát mà chúng ta đã học ở chương trình phổ thông. Thế thì ý nghĩa của cái phương trình đường thẳng này đó là nó tách ra làm hai phần và những cái phần nào mà nằm ở phía trên cụ thể là các cái điểm màu xanh thì nó các cái điểm màu xanh này khi thế vào sẽ khi thế cái tọa độ đó khi thế tọa độ vào cái công thức là W1 x1 W2 X2 cộng cho B này á thì nó sẽ cùng dấu. Ví dụ như những cái điểm ở phía trên tức là màu xanh này khi chúng ta thế vào, chúng ta thế vào thì W1",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IReTdn_z-es",
      "filename": "IReTdn_z-es",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 3",
      "chunk_id": 3,
      "start_timestamp": "0:02:21",
      "end_timestamp": "0:03:08"
    }
  },
  {
    "page_content": "này khi chúng ta thế vào, chúng ta thế vào thì W1 x1 cộng cho W2 x2 cộng cho B thì nó sẽ là một con số lớn hơn 0. Và tất cả các cái con à cái tọa độ của à tất cả các cái tọa độ của thì nó đều cùng dấu, tức là dương. Ngược lại. Thế thì nếu như ở phía trên mà dương thì ở phía ngược lại thì nếu mà chúng ta lấy cái tọa độ của các cái điểm màu cam này nè thế vào phương trình thì w1x1 cộng cho w2x2 à nó sẽ là cộng b nó sẽ ra cùng dấu và nó sẽ là bé hơn 0. Mặt khác thì nó sẽ có thêm một cái tính chất",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IReTdn_z-es",
      "filename": "IReTdn_z-es",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 3",
      "chunk_id": 4,
      "start_timestamp": "0:03:03",
      "end_timestamp": "0:03:41"
    }
  },
  {
    "page_content": "0. Mặt khác thì nó sẽ có thêm một cái tính chất hình học nữa đó là những cái điểm mà ở càng xa à có cái khoảng cách càng xa so với lại cái đường thẳng này á thì khi chúng ta thế vào đối với những cái điểm mà ví dụ như điểm này chúng ta thế vào thì chúng ta thấy cái khoảng cách của mình đó là xa nhất. Thì khi thế vào cái công thức w1 x1 w2 x2 cộng b à cộng b thì nó sẽ ra một con số rất là lớn à nó sẽ lớn hơn so với lại cái con số mà khi chúng ta thế vô tại đây. Thì cái này là ừ nó tương đương",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IReTdn_z-es",
      "filename": "IReTdn_z-es",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 3",
      "chunk_id": 5,
      "start_timestamp": "0:03:36",
      "end_timestamp": "0:04:13"
    }
  },
  {
    "page_content": "thế vô tại đây. Thì cái này là ừ nó tương đương với lại một cái công thức về mặt khoảng cách điểm mà càng gần thì khi chúng ta thế vô cái công thức này thì nó sẽ ra một cái con số dương nhưng mà con số này gần gần bằng 0. Còn nếu như cái điểm mà xa hơn à khi thay thế vào công thức này thì nó cũng ra số dương nhưng mà cái con số của nó sẽ lớn hơn. Ở khía cạnh ngược lại thì ví dụ như cái điểm này thế vào thì chúng ta thấy là nó sẽ ra số âm nhưng mà nó sẽ gần số 0. Còn nếu chúng ta thế vô cái điểm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IReTdn_z-es",
      "filename": "IReTdn_z-es",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 3",
      "chunk_id": 6,
      "start_timestamp": "0:04:09",
      "end_timestamp": "0:04:51"
    }
  },
  {
    "page_content": "nó sẽ gần số 0. Còn nếu chúng ta thế vô cái điểm này à là cái điểm mà nó có cái khoảng cách lớn thì khi chúng ta thế vào công thức này nó cũng ra số âm nhưng mà cái trị tuyệt đối của nó nó lớn hơn. Thì đó chính là cái ý nghĩa về mặt hình học của cái mô hình hồi quy logistic một cách tóm gọn. Đó là cái bộ tham số W và B. B là bias. Nó chính là cái bộ hệ số của cái phương trình đường thẳng để tách ra làm hai phần. à tách cái không gian của cái điểm màu xanh và màu cam ở đây thì khoảng cách mà",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IReTdn_z-es",
      "filename": "IReTdn_z-es",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 3",
      "chunk_id": 7,
      "start_timestamp": "0:04:46",
      "end_timestamp": "0:05:28"
    }
  },
  {
    "page_content": "điểm màu xanh và màu cam ở đây thì khoảng cách mà càng gần đến cái đường phân lớp này thì khi thế vô cái giá trị tuyệt đối của nó sẽ là càng nhỏ và cái điểm càng xa thì cái trị tuyệt đối của nó sẽ càng xa. Thì đó chính là ý nghĩa hình học của mô hình.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=IReTdn_z-es",
      "filename": "IReTdn_z-es",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 3",
      "chunk_id": 8,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề, học sâu, machine learning, logistic regression, ImageNet. Chủ đề, học sâu, machine learning, ImageNet. Thì theo định nghĩa của được trích dẫn trong các tài liệu về lịch sử học máy của Arthur Samuel năm 1959, thì học máy là một lĩnh vực nghiên cứu mà trong đó máy tính có thể học mà không cần lập trình chi tiết từng bước. Ở một định nghĩa khác của Tom Mitchell năm 1997 thì Học máy là một chương trình máy tính được cho là học để thực hiện tác vụ T từ kinh nghiệm E nếu hiệu suất thực hiện",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=it7jk54r_N8",
      "filename": "it7jk54r_N8",
      "title": "[CS114 - Chương 2] Khái niệm học máy",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:05"
    }
  },
  {
    "page_content": "tác vụ T từ kinh nghiệm E nếu hiệu suất thực hiện của tác vụ T được đo bằng chỉ số P cải thiện nhờ vào kinh nghiệm E. Kevin Murphy năm 2012 cũng đưa ra một định nghĩa học máy. Học máy là một tập hợp các phương pháp có thể tự động phát hiện mẫu hình trong dữ liệu, sau đó sử dụng các mẫu hình đã khám phá để dự đoán dữ liệu tương lai hoặc thực hiện các loại ra quyết định khác trong điều kiện không chắc chắn, chẳng hạn như là kế hoạch hoặc là làm thêm dữ liệu. Thì đối với các cái định nghĩa trên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=it7jk54r_N8",
      "filename": "it7jk54r_N8",
      "title": "[CS114 - Chương 2] Khái niệm học máy",
      "chunk_id": 1,
      "start_timestamp": "0:01:00",
      "end_timestamp": "0:01:39"
    }
  },
  {
    "page_content": "thêm dữ liệu. Thì đối với các cái định nghĩa trên thì nó đều tập trung vào các khía cạnh quan trọng của học máy như là khả năng học hỏi từ dữ liệu, mức độ cải thiện hiệu suất qua kinh nghiệm và không yêu cầu lập trình chi tiết từng bước cho từng tác vụ. Định nghĩa của Samuel và Mitchell thì thiên về cái khái niệm học hỏi liên tục và nhấn mạnh vào khía cạnh tự động hóa, khả năng cải thiện hiệu suất nhờ tương tác với dữ liệu mà không tập trung chi tiết vào cái dữ liệu được xử lý hay là tác vụ cụ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=it7jk54r_N8",
      "filename": "it7jk54r_N8",
      "title": "[CS114 - Chương 2] Khái niệm học máy",
      "chunk_id": 2,
      "start_timestamp": "0:01:31",
      "end_timestamp": "0:02:07"
    }
  },
  {
    "page_content": "tiết vào cái dữ liệu được xử lý hay là tác vụ cụ thể. Trong khi đó thì định nghĩa của Kevin Murphy giải thích rõ hơn về tác vụ các thuật toán học máy bao gồm thứ nhất là khám phá mẫu hình từ dữ liệu, Thứ hai là sử dụng các mẫu hình đó để dự đoán hoặc hỗ trợ ra quyết định Thì so với các định nghĩa thứ nhất thì cách này đi sâu vào chi tiết của quá trình xử lý và ứng dụng dữ liệu Chúng ta có thể có một cái định nghĩa tổng hợp dựa trên các định nghĩa từ trước đây và các định nghĩa hiện đại Thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=it7jk54r_N8",
      "filename": "it7jk54r_N8",
      "title": "[CS114 - Chương 2] Khái niệm học máy",
      "chunk_id": 3,
      "start_timestamp": "0:02:02",
      "end_timestamp": "0:02:43"
    }
  },
  {
    "page_content": "nghĩa từ trước đây và các định nghĩa hiện đại Thì chúng ta có thể đưa ra một cái định nghĩa tổng hợp như sau Học máy là một cái lĩnh vực nghiên cứu và phát triển các thuật toán và hệ thống có khả năng tự động khám phá các mẫu hình từ dữ liệu mà không cần được lập trình chi tiết từng bước Cải thiện hiệu suất qua thời gian nhờ kinh nghiệm tích lũy và sử dụng các mẫu hình này để dự đoán dữ liệu chưa thấy hoặc thực hiện các tác vụ ra quyết định trong điều kiện không chắc chắn Định nghĩa này là sự",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=it7jk54r_N8",
      "filename": "it7jk54r_N8",
      "title": "[CS114 - Chương 2] Khái niệm học máy",
      "chunk_id": 4,
      "start_timestamp": "0:02:38",
      "end_timestamp": "0:03:12"
    }
  },
  {
    "page_content": "điều kiện không chắc chắn Định nghĩa này là sự kết hợp giữa việc nhấn mạnh khả năng tự động hóa, cải thiện hiệu suất qua thời gian và việc khám phá mẫu hình từ dữ liệu để thực hiện các tác vụ chuyên biệt Việc đưa hệ thống vào định nghĩa chỉ mở rộng phạm vi để phù hợp với thực tế hiện đại của học máy trong khi các định nghĩa kinh điển tập trung vào thuật toán và khía cạnh học thuật. Định nghĩa tổng hợp này bao gồm cả quá trình phát triển, tích hợp và ứng dụng học máy trong thế giới thực nơi các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=it7jk54r_N8",
      "filename": "it7jk54r_N8",
      "title": "[CS114 - Chương 2] Khái niệm học máy",
      "chunk_id": 5,
      "start_timestamp": "0:03:06",
      "end_timestamp": "0:03:39"
    }
  },
  {
    "page_content": "và ứng dụng học máy trong thế giới thực nơi các hệ thống đóng vai trò quan trọng cũng như các thuật toán. Ví dụ như hệ thống gợi ý nội dung như Netflix bao gồm cả thuật toán học máy học từ thói quen xem của người dùng nhưng hệ thống cũng cần xử lý dữ liệu từ người dùng. tối ưu hóa, hiển thị nội dung và phản hồi theo thời gian thực. Trong hình này, minh họa một quy trình học máy cơ bản gồm có hai bước chính, bước huấn luyện. Đây là bước mà mô hình tự động khám phá và trích xuất các mẫu hình từ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=it7jk54r_N8",
      "filename": "it7jk54r_N8",
      "title": "[CS114 - Chương 2] Khái niệm học máy",
      "chunk_id": 6,
      "start_timestamp": "0:03:35",
      "end_timestamp": "0:04:09"
    }
  },
  {
    "page_content": "tự động khám phá và trích xuất các mẫu hình từ dữ liệu đầu vào và giúp mô hình học cách biểu diễn các mối quan hệ trong dữ liệu. Bước thứ hai ở bên tay phải là bước đánh giá sau khi huấn luyện thì mô hình sẽ được sử dụng các mẫu hình đã học để dự đoán kết quả trên dữ liệu mới hoặc thực hiện các tác vụ cụ thể. Mô hình không chỉ là kết quả của quá trình huấn luyện, mà còn là thành phần chính được sử dụng trong giai đoạn đánh giá và triển khai thực tế. Ok, đây là các bài quiz.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=it7jk54r_N8",
      "filename": "it7jk54r_N8",
      "title": "[CS114 - Chương 2] Khái niệm học máy",
      "chunk_id": 7,
      "start_timestamp": "0:04:06",
      "end_timestamp": "0:04:18"
    }
  },
  {
    "page_content": "Trong phần tiếp theo thì để có thể hiểu rõ cái thực toán Ren đã chạy như thế nào thì chúng ta sẽ cùng mô phỏng với một cái dữ liệu như sau. Chúng ta sẽ có hai trục, trục X và trục Y. Thì trong đó X chính là cái đặc trưng của mình. đặc trưng đồ vào của mình và y chính là cái giá trị đầu ra chúng ta cần dự đoán. Thì à giả sử như x và y của mình nó có mối quan hệ tuyến tính và các cái điểm dữ liệu của mình nó xoay xung quanh một cái đường thẳng mà chúng ta biết trước đó là y ví dụ như là bằng 3x -",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:58"
    }
  },
  {
    "page_content": "ta biết trước đó là y ví dụ như là bằng 3x - 5. Đó thì đây là chúng ta đã biết trước trong đầu mình. Và khi huấn luyện mô cái mô hình xong thì W và B của mình liệu nó có tiến về các cái giá trị mà chúng ta đã biết trước hay không? Cụ thể đó là W có tiến về số 3 và B có tiến về -5 hay không thì chúng ta sẽ kết luận được là cái mô hình của mình có đang thực thi đúng hay không. Thế thì ở đây chúng ta sẽ sử dụng một cái code Python. Đầu tiên chúng ta sẽ import nump snp. Rồi trước tiên thì chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 1,
      "start_timestamp": "0:00:54",
      "end_timestamp": "0:01:44"
    }
  },
  {
    "page_content": "sẽ import nump snp. Rồi trước tiên thì chúng ta sẽ khởi tạo à các điểm dữ liệu thì x sẽ là bằng np. Và giá trị chúng ta lấy từ 1 cho đến 3 với bước nhảy. Ví dụ như ở đây chúng ta chọn là 0.2 2 đi rồi và y thì là bằng 3x - 5. Rồi sau đó chúng ta sẽ in ra x và in ra y để xem cái giá trị của mình nó nhìn như thế nào. Thì để chạy chúng ta sẽ nhấn Ctrl Shift P. Chúng ta sẽ chọn cái Python Interpreter và chọn ví dụ ở đây chúng ta chọn cái Python của Anaconda ha. Rồi để chạy thì chúng ta sẽ nhấn cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 2,
      "start_timestamp": "0:01:40",
      "end_timestamp": "0:02:30"
    }
  },
  {
    "page_content": "Anaconda ha. Rồi để chạy thì chúng ta sẽ nhấn cái nút ở đây. Rồi thì chúng ta thấy là x sẽ là bằng 1 1.2 1.4. Tức là cái bước nhảy của mình là 0.2 và lên đến 2.8. Tức là nó chưa chạm đến được cái số 3 là nó sẽ dừng. Rồi y thì sẽ là bằng ví dụ như x mà bằng 1 thì y sẽ là bằng 3 x 1 - 5. Tức là à 3 x 1 - 5 là bằng -2. Rồi ví dụ như nếu x của mình mà bằng 2 đi thì 2 x 3 là 6 - 1 - 5 là còn còn 1. Tức là cái giá trị của mình sẽ là 1 ở đây. Rồi tuy nhiên nếu mà chúng ta à để dữ liệu như thế này thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 3,
      "start_timestamp": "0:02:23",
      "end_timestamp": "0:03:17"
    }
  },
  {
    "page_content": "nếu mà chúng ta à để dữ liệu như thế này thì sẽ hơi khó để trực quan hóa. Do đó chúng ta sẽ dùng một cái công cụ đó là M plist để trực quan hóa rồi. Và ở đây sẽ là import MLI list. chpot và để trực quan hóa thì chúng ta sẽ dùng là hàm plt.S truyền vào x và y. Trong đó thì với mỗi một cái cặp giá trị x và y thì chúng ta sẽ dùng một cái dấu chấm màu đỏ. Dấu màu đỏ là rấu chấm sẽ là dùng số 0. Rồi. À rồi thì bây giờ à ở đây chúng ta sẽ dùng hàm là hàm plot ha. unlock. Và sau khi chúng ta thực hiện",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 4,
      "start_timestamp": "0:03:14",
      "end_timestamp": "0:04:04"
    }
  },
  {
    "page_content": "plot ha. unlock. Và sau khi chúng ta thực hiện xong thì là plt.show rồi chúng ta lưu lại và chạy. Tuy nhiên cái dữ liệu này thì chúng ta thấy là nó thẳng tóc à đúng như trên một cái đường thẳng nhưng mà thực tế thì cái dữ liệu của chúng ta nó sẽ dao động vì các cái yếu tố nhiễu trong cái quá trình lấy mẫu. Do đó chúng ta sẽ phải thêm vô một cái đại lượng nhiễu nữa là bằng np.random chn normal. Và normal chúng ta sẽ dùng là mi của nó là bằng 0 và sigma là bằng 1. Và cái số chiều của mình thì nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 5,
      "start_timestamp": "0:03:59",
      "end_timestamp": "0:04:48"
    }
  },
  {
    "page_content": "sigma là bằng 1. Và cái số chiều của mình thì nó cũng đúng bằng cái số chiều của x chấm sh kích thước của x. Rồi chúng ta sẽ lấy cái số phần tử của nó. Còn không thì chúng ta có thể lấy là lens của x thì cách nào cũng được. Rồi đó thì chúng ta thấy là nó đã có cái sự dao động. Muốn tăng cái độ dao động này lên chúng ta có thể nhân với cái hệ số ví dụ như là 2. Rồi đó chúng ta thấy là đã có cái sự dao động khá là nhiều rồi. Thì ở đây chúng ta thôi để một cái đại lượng nhiễu vừa phải là ví dụ như",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 6,
      "start_timestamp": "0:04:43",
      "end_timestamp": "0:05:38"
    }
  },
  {
    "page_content": "để một cái đại lượng nhiễu vừa phải là ví dụ như 1.5 thôi. Đó thì tiếp theo đó là chúng ta sẽ đi cài đặt cái mô hình của mình. Thì cái trong cái mô hình của mình thì ờ chúng ta sẽ phải cài cái hàm loss và chúng ta sẽ đi tính đạo hàm của nó. Thế thì ban đầu bước đầu tiên là chúng ta sẽ phải khởi tạo trước thì là W và B. Thì W đương nhiên chúng ta đã biết cái giá trị ở đây là 3 nhưng chúng ta không có gán nó bằng 3 mà chúng ta sẽ gán bằng một cái giá trị ngẫu nhiên nào đó. Ví dụ như B bằng à W là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 7,
      "start_timestamp": "0:05:33",
      "end_timestamp": "0:06:16"
    }
  },
  {
    "page_content": "trị ngẫu nhiên nào đó. Ví dụ như B bằng à W là bằng -10 đi. Rồi B chúng ta biết là nó sẽ tiến về -5 nhưng mà chúng ta không nên chọn B = -5 mà chúng ta chọn cái giá trị nào đó. Ví dụ như ờ 6 đi. Đó thì đây là các cái giá trị ngẫu nhiên. Rồi sau đó thì chúng ta sẽ ờ tiến hành đó là lập cho đến khi hội tụ. Thì để cài đặt cho vòng lập này thì chúng ta sẽ dùng vòng lập white. Nhưng mà hiện giờ chúng ta chưa biết là mình lập cho đến khi nào xong. Do đó chúng ta cứ để là true các cái hàm tính đạo",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 8,
      "start_timestamp": "0:06:11",
      "end_timestamp": "0:07:06"
    }
  },
  {
    "page_content": "Do đó chúng ta cứ để là true các cái hàm tính đạo hàm. Thì đạo hàm của hàm los theo W, đạo hàm của hàm loss theo B thì chúng ta sẽ có công thức đó là bằng trung bình cộng nè của wx cộng B trừ cho Y tất cả nhân với lại xy. Rồi thì chúng ta sẽ tính đạo hàm theo W trước đi ha. Thì derivative theo W sẽ là bằng wx. Ở đây chúng ta sẽ ghi với dạng công thức à giống như đại số trước cộng b rồi sau đó chúng ta sẽ trừ cho y là cái giá trị mà trú rồi tất cả nhân với lại x rồi chúng ta sẽ lấy tổng và chia",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 9,
      "start_timestamp": "0:06:58",
      "end_timestamp": "0:07:51"
    }
  },
  {
    "page_content": "nhân với lại x rồi chúng ta sẽ lấy tổng và chia bình quân thì tổng và chia bình quân chúng ta có thể dùng hàm np. min như thế này. Rồi thì W nhân với X cộng cho B rồi trừ cho Y rồi tất cả nhân với lại X. Thì đây là đạo hàm ờ của hàm ờ sai số của hàm mất mát theo cái biến W. Tương tự như vậy, chúng ta sẽ có cái công thức đạo hàm cho B. Thì đạo hàm cho B thì nó sẽ là cũng là bằng trung bình cộng. Nhưng có điều là chúng ta sẽ không có cái thành phần à xy ở đây. Chúng ta không có. Như vậy thì công",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 10,
      "start_timestamp": "0:07:46",
      "end_timestamp": "0:08:43"
    }
  },
  {
    "page_content": "à xy ở đây. Chúng ta không có. Như vậy thì công thức chúng ta cũng có thể kế thừa là bỏ đi cái thằng x này là xong. Vậy cái việc code này cũng khá là đơn giản. Tiếp theo thì trong cái bước của phòng lập nó sẽ là cập nhật W là bằng W trừ cho alpha nhân cho đạo hàm của W. Rồi tương tự như vậy, B là bằng B trừ cho alpha nhân cho đạo hàm theo B. Khi đó thì chúng ta thiếu một cái tham số alpha. Alpha giả sử ở đây chúng ta chọn alpha là bằng à ví dụ như 0.01 đi. Rồi thì khi đó chúng ta sẽ cập nhật",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 11,
      "start_timestamp": "0:08:32",
      "end_timestamp": "0:09:41"
    }
  },
  {
    "page_content": "như 0.01 đi. Rồi thì khi đó chúng ta sẽ cập nhật lại nè W và B. Ở đây là tính đạo hàm của hàm mất mát theo W và B. Rồi thì W là bằng W trừ cho alpha nhân cho đạo hàm theo W. Tương tự như vậy, chúng ta sẽ có B rồi. Và cái việc này nó sẽ lặp lại khi nào thì dừng. Khi cái đạo hàm theo W và B này đủ nhỏ thì chúng ta sẽ dừng. Như vậy thì cái điều kiện dừng đó là if ABS của đạo hàm theo W à đuổi nhỏ đủ nhỏ một cái ngưỡng epsilon. Thì giả sử như ở đây silon của mình là bằng 0.001. Đồng thời là đạo hàm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 12,
      "start_timestamp": "0:09:36",
      "end_timestamp": "0:10:39"
    }
  },
  {
    "page_content": "của mình là bằng 0.001. Đồng thời là đạo hàm của B. Đạo hàm theo B cũng đủ nhỏ thì chúng ta sẽ break. Và khi break thì chúng ta sẽ in ra là cái W à sau khi huấn luyện. Đó. Rồi chúng ta sẽ in ra W. Tương tự như vậy, chúng ta sẽ có tham số B sau khi huấn luyện. Thì bây giờ chúng ta lưu lại và chạy cái chương trình này. Rồi thì chúng ta thấy là W sau khi huấn luyện đó thì nó sẽ ra là bằng 2,69. Ở trên đây chúng ta thấy là cái phương trình góc của mình nó sẽ là y = 3x - 5 và w của mình thì nó đã",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 13,
      "start_timestamp": "0:10:33",
      "end_timestamp": "0:11:19"
    }
  },
  {
    "page_content": "mình nó sẽ là y = 3x - 5 và w của mình thì nó đã tiến đến là 2,69 tức là gần bằng 2,7 là cũng xắp xỉ với số 3 rồi. À cái hệ số b của mình đáng lẽ nó phải ra là -5 thì ở đây nó ra là -4. Thì nguyên nhân đó là do ở đây chúng ta có một cái đại lượng nhiễu khá là lớn. dẫn đến là cái W và B sau khi huấn luyện nó chưa chạm được đến cái giá trị 3 và 5. Thì cái điều này là cũng là dễ hiểu đó. Rồi tiếp theo thì chúng ta sẽ mô phỏng xem trong cái quá trình mà thực toán nó cập nhật thì nó đã thay đổi ra",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 14,
      "start_timestamp": "0:11:12",
      "end_timestamp": "0:12:14"
    }
  },
  {
    "page_content": "mà thực toán nó cập nhật thì nó đã thay đổi ra sao cái phương trình đường thẳng của mình. Thế thì ở đây chúng ta sẽ thêm một cái phần nữa đó là trực quan hóa. Ờ cái mô hình thì cũng tương tự chúng ta sẽ blog XY à ở đây chúng ta sẽ không show ở đây ha. Rồi chúng ta sẽ plot xy. Rồi sau đó chúng ta sẽ vẽ cái phương trình đường thẳng của mình. Thế thì để vẽ đường phương trình đường thẳng thì chúng ta sẽ lấy ra một cái cặp điểm. bắt đầu và điểm kết thúc. Thế thì điểm bắt đầu là với x là bằng 1 và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 15,
      "start_timestamp": "0:12:04",
      "end_timestamp": "0:13:01"
    }
  },
  {
    "page_content": "thúc. Thế thì điểm bắt đầu là với x là bằng 1 và điểm kết thúc của x sẽ là bằng 3. Do đó chúng ta sẽ cho cái cặp là 1 và 3. Còn tọa độ y thì sao? Tọa độ Y thì tương ứng nó sẽ là bằng W nhân với lại 1 cộng cho B. Rồi W lại à 3 cộng cho B. Tức là chúng ta đang vẽ một cái đường thẳng để nối cái điểm là à 1 3 đến cái điểm là à W1 + B rồi W3 + B. Đó. Rồi sau đó chúng ta sẽ dừng một chút xíu plt. F chúng ta dừng ví dụ như 0,5 giây. Rồi thì bây giờ chúng ta sẽ xem cái chương trình nó chạy ha. Đó thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 16,
      "start_timestamp": "0:12:55",
      "end_timestamp": "0:14:02"
    }
  },
  {
    "page_content": "ta sẽ xem cái chương trình nó chạy ha. Đó thì càng về sau chúng ta sẽ thấy là nó đang có xu hướng là à ở đây là chúng ta nhầm ha. Ở đây chúng ta nhầm tức là lẽ ra chúng ta phải đi theo một cặp tức là W B ờ WB ở đây nó sẽ phải đi với số 1. Rồi số 3 ở đây thì sẽ đi với lại W3 cộng B. Cái này mới đúng. Rồi bây giờ chúng ta sẽ chạy lại ha. Thì tại sao ở đây nó không có? Tại vì ban đầu khi khởi tạo thì các cái giá trị của mình là nó không có ở khu vực này. Rồi cái tốc độ hội tụ của nó hơi chậm. Tốc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 17,
      "start_timestamp": "0:13:58",
      "end_timestamp": "0:15:07"
    }
  },
  {
    "page_content": "này. Rồi cái tốc độ hội tụ của nó hơi chậm. Tốc độ hội tụ hơi chậm. Tại sao cái điểm này nó lại là cứ cố định là 13 ta? À hồi nãy là chúng ta đã đúng rồi nhưng mà chúng ta lại đi sửa lại nó thành sai ha. À nó sẽ đi theo cặp là x1 x2 trước. Nó sẽ đi theo cặp x1 x2 rồi y1 y2 rồi 1 3 và w nhân bà w x 3 + b. Cái này mới đúng. Rồi và ở đây thì để cho nhanh chúng ta sẽ giảm cái thời gian chờ xuống là còn khoảng 0.1 đó. Rồi thì chúng ta thấy cái sự dịch chuyển của cái mô hình của mình. Rồi thì cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 18,
      "start_timestamp": "0:14:55",
      "end_timestamp": "0:15:51"
    }
  },
  {
    "page_content": "dịch chuyển của cái mô hình của mình. Rồi thì cái bước nhảy ở đây chúng ta thấy là nó đi hơi chậm nên cái đường thẳng của mình nó sẽ không có xoay. Nó sẽ không có xoay à nhanh. Do đó chúng ta sẽ à tăng cái tốc độ hội tụ của mình lên. Tăng cái tốc độ huấn luyện lên bằng cách đó là 10 lần là alpha. Thay vì là để 0.01 chúng ta sẽ cho alpha bằng 0.1. Tức là cái tốc độ huấn luyện nó sẽ nhanh hơn. Đó thì chúng ta sẽ thấy là dần dần cái đường thẳng của mình nó sẽ xoay. Nó sẽ xoay và nó sẽ đi vào trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 19,
      "start_timestamp": "0:15:46",
      "end_timestamp": "0:16:11"
    }
  },
  {
    "page_content": "mình nó sẽ xoay. Nó sẽ xoay và nó sẽ đi vào trong tập các cái điểm dữ liệu màu đỏ của mình. Đó thì đây chính là cái à cách mà mô hình của mình nó cập nhật. Ban đầu thì là ở cái đường thẳng như thế này. Nó khá là xa đó, khá là xa. Sau đó thì nó sẽ nhảy một bước, nhảy một bước và càng về sau thì cái đường thẳng của mình nó sẽ càng tiến về cái khu vực có cái điểm dữ liệu. Rồi sau đó khi mà nó đã tận tiến đến đây xong rồi thì nó sẽ bắt đầu nó xoay. Nó sẽ xoay và xoay làm sao cho các cái đường thẳng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 20,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "sẽ xoay và xoay làm sao cho các cái đường thẳng của mình nó sẽ đi xuyên qua được các cái điểm sao cho tổng cái size số cái khoảng cách từ các cái điểm dữ liệu đến cái đường thẳng của mình là là nhỏ nhất. Thì đó chính là cái cách mà mô hình của mình nó huấn luyện với thực toán radian des.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=JqHpAp-xdu0",
      "filename": "JqHpAp-xdu0",
      "title": "[CS114 - Chương 3] Mô phỏng",
      "chunk_id": 21,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Học máy có thể được xem như là cách máy tính học hỏi từ dữ liệu. Tương tự như cách con người học từ kinh nghiệm. Nhưng cũng như cách con người có nhiều phương pháp học khác nhau, học qua quan sát, học qua thử nghiệm, học từ người hướng dẫn thì các thuật toán học máy cũng được chia thành nhiều nhóm khác nhau. Hãy cùng tìm hiểu về các loại thuật toán này. Xét theo phương thức học thì các thuật toán học máy có thể được chia thành ba loại chính: học có giám sát, học không giám sát và học tăng cường.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KJJ6eFW1nV0",
      "filename": "KJJ6eFW1nV0",
      "title": "[CS114 - Chương 2] Phân biệt một số loại mô hình học máy",
      "chunk_id": 0,
      "start_timestamp": "0:00:16",
      "end_timestamp": "0:00:52"
    }
  },
  {
    "page_content": "giám sát, học không giám sát và học tăng cường. mỗi loại có một cách tiếp cận và ứng dụng riêng. Cụ thể, đối với học có giám sát thì thuật toán học dữ liệu đã được gán nhãn, tức là chúng ta có các cặp đầu vào và đầu ra để dự đoán kết quả cho dữ liệu mới. Bao gồm bài toán phân loại classification là dự đoán các giá trị rời rạt. Ví dụ phân loại email là spam hoặc không spam. Ừ. Hồi quy, regression là dự đoán các giá trị liên tục, ví dụ như dự đoán giá nhà dựa trên diện tích và vị trí. Đối với học",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KJJ6eFW1nV0",
      "filename": "KJJ6eFW1nV0",
      "title": "[CS114 - Chương 2] Phân biệt một số loại mô hình học máy",
      "chunk_id": 1,
      "start_timestamp": "0:00:46",
      "end_timestamp": "0:01:31"
    }
  },
  {
    "page_content": "giá nhà dựa trên diện tích và vị trí. Đối với học không giám sát thì thuật toán sẽ khám phá dữ liệu không có nhãn để phát hiện các mẫu và cấu trúc ẩn. Thì bao gồm phân cục clustering là nhóm các cái điểm dữ liệu có đặc điểm tương tự vào cùng một cục. Ví dụ như phân nhóm khách hàng dựa trên hành vi mua sắm hay là giảm chiều dữ liệu. Dimensionality reduction giảm số lượng biến trong dữ liệu. Trong khi giữ lại thông tin quan trọng thì chúng ta có thể quan sát cái hình minh họa giữa clustering và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KJJ6eFW1nV0",
      "filename": "KJJ6eFW1nV0",
      "title": "[CS114 - Chương 2] Phân biệt một số loại mô hình học máy",
      "chunk_id": 2,
      "start_timestamp": "0:01:26",
      "end_timestamp": "0:02:07"
    }
  },
  {
    "page_content": "thể quan sát cái hình minh họa giữa clustering và dimensionality reduction trên cái hình ở bên phải. Đây là hình ảnh minh họa sự khác biệt giữa học có giám sát và học không giám sát. Thông qua ví dụ à phân loại về loài vật thì học có giám sát sẽ phân phân loại. Chúng ta xem cái hình bên trái. Thì trong học có giám sát thì mô hình được huấn luyện trên cái dữ liệu có nhãn. Ví dụ như trong hình, một tập dữ liệu bao gồm hình ảnh các con vật trong đó mỗi hình được gán nhãn là đất con vịt hoặc là not",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KJJ6eFW1nV0",
      "filename": "KJJ6eFW1nV0",
      "title": "[CS114 - Chương 2] Phân biệt một số loại mô hình học máy",
      "chunk_id": 3,
      "start_timestamp": "0:02:03",
      "end_timestamp": "0:02:42"
    }
  },
  {
    "page_content": "mỗi hình được gán nhãn là đất con vịt hoặc là not đất không phải là con vịt. Mô hình sẽ học từ dữ liệu này và tạo ra một cái hệ thống dự đoán. Khi có một hình ảnh mới thì mô hình có thể phân loại nó thành duck hoặc là not duck dựa trên những gì đã học được. Ở hình bên phải học không giám sát à bài toán phân cùm thì trong học không giám sát thì mô hình không có nhãn về dữ liệu trước. Ví dụ như trong hình thì tập dữ liệu nó gồm nhiều cái động vật khác nhau và không có cái nhãn nào. Thế mô hình sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KJJ6eFW1nV0",
      "filename": "KJJ6eFW1nV0",
      "title": "[CS114 - Chương 2] Phân biệt một số loại mô hình học máy",
      "chunk_id": 4,
      "start_timestamp": "0:02:37",
      "end_timestamp": "0:03:19"
    }
  },
  {
    "page_content": "nhau và không có cái nhãn nào. Thế mô hình sẽ tự động tìm kiếm các đặc điểm chung và nhóm các hình ảnh có đặc điểm tương tự vào cùng một cụm. Kết quả là các con vịt sẽ được nhóm vào một cụm, thở vào một cụm khác và nhím vào một cụm riêng biệt mà không cần biết trước chúng sẽ thuộc nhóm nào. Nói tóm lại thì học có giám sát sẽ yêu cầu dữ liệu có nhãn để huấn luyện mô hình. Nó có thể dự đoán chính xác. trên dữ liệu mới. Còn học không giám sát sẽ tự khám phá cấu trúc trong dữ liệu mà không cần",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KJJ6eFW1nV0",
      "filename": "KJJ6eFW1nV0",
      "title": "[CS114 - Chương 2] Phân biệt một số loại mô hình học máy",
      "chunk_id": 5,
      "start_timestamp": "0:03:14",
      "end_timestamp": "0:03:52"
    }
  },
  {
    "page_content": "tự khám phá cấu trúc trong dữ liệu mà không cần nhãn, giúp mô hình tìm ra các nhóm hoặc mẫu ẩn trong dữ liệu. Đối với học tăng cường thì thuật toán học thông qua thử nghiệm và sai sót, tương tác với môi trường và nhận phần thưởng hoặc là hình phạt. Kỹ thuật này được sử dụng trong các lĩnh vực như robot, học, trò chơi và giao dịch tài chính. Sau đây là các bài quiz. Bài quiz thứ nhất, hãy cho biết mục tiêu chính của thuậc toán hồi quy tuyến tính trong học có giám sát là gì? Phân loại dữ liệu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KJJ6eFW1nV0",
      "filename": "KJJ6eFW1nV0",
      "title": "[CS114 - Chương 2] Phân biệt một số loại mô hình học máy",
      "chunk_id": 6,
      "start_timestamp": "0:03:48",
      "end_timestamp": "0:04:31"
    }
  },
  {
    "page_content": "trong học có giám sát là gì? Phân loại dữ liệu thành các nhóm, dự đoán giá trị số liên tục, giảm số chiều của dữ liệu, phát hiện bất thường trong dữ liệu. Học tăng cường khác với học có giám sát ở điểm nào? Sử dụng nhãn dữ liệu để huấn luyện. Học từ thử nghiệm và sai lầm. thông qua phần thưởng và hình phạt không cần dữ liệu huấn luyện, sử dụng dữ liệu không được gắn nhãn. Loại thuật toán nào thường được sử dụng để dự đoán một giá trị liên tục? Phân loại, hồi quyến tính, phân cụm, giảm chiều.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KJJ6eFW1nV0",
      "filename": "KJJ6eFW1nV0",
      "title": "[CS114 - Chương 2] Phân biệt một số loại mô hình học máy",
      "chunk_id": 7,
      "start_timestamp": "0:04:25",
      "end_timestamp": "0:05:11"
    }
  },
  {
    "page_content": "Phân loại, hồi quyến tính, phân cụm, giảm chiều. Mục tiêu của thuậc toán phân cụm là gì? Dự đoán một giá trị liên tục, phân loại dữ liệu thành các nhóm riêng biệt. Tìm ra cấu trúc ẩn trong dữ liệu không được gán nhãn. Học một chính sách để tối đa hóa phần thưởng. Bạn cần xây dựng một mô hình để dự đoán điểm số của học sinh dựa trên thời gian học tập. Loại thuật toán nào là phù hợp nhất? Phân loại, hội quy tuyến tính, phân cục, giảm chiều. Bạn cần xây dựng một mô hình để dự đoán một bệnh nhân có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KJJ6eFW1nV0",
      "filename": "KJJ6eFW1nV0",
      "title": "[CS114 - Chương 2] Phân biệt một số loại mô hình học máy",
      "chunk_id": 8,
      "start_timestamp": "0:05:06",
      "end_timestamp": "0:05:52"
    }
  },
  {
    "page_content": "xây dựng một mô hình để dự đoán một bệnh nhân có mắc bệnh hay không dựa trên dữ liệu y tế của họ. Loại thức toán nào là phù hợp nhất? Phân loại, hội quy tuyến tính, phân cục hoặc tăng cường. Một công ty bảo hiểm muốn dự đoán mức bồi thường tối đa cho một khách hàng dựa trên lịch sử tai nạn, tuổi tác và loại xe. Loại thuật tán nào phù hợp nhất? Phân loại, hội quy tiến tính, phân công, giảm chiều. Một công ty thương mại điện tử muốn xác định các nhóm khách hàng có hành vi mua sắm tương tự nhau để",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KJJ6eFW1nV0",
      "filename": "KJJ6eFW1nV0",
      "title": "[CS114 - Chương 2] Phân biệt một số loại mô hình học máy",
      "chunk_id": 9,
      "start_timestamp": "0:05:49",
      "end_timestamp": "0:06:34"
    }
  },
  {
    "page_content": "khách hàng có hành vi mua sắm tương tự nhau để tối ưu hóa chiến dịch quảng cáo. Loại thuật tán nào phù hợp nhất? Phân loại, hồi quyến tính, phân cục hoặc tăng cường. Một công ty xe tự hành đang phát triển hệ thống AI giúp xe học cách tự lái an toàn qua giao lộ đung đúc, dựa trên thử nghiệm của nhiều lần và phản hồi từ môi trường. Loại thuục tán nào phù hợp nhất? Phân loại, hồi quy, học tăng cường, phân cục. Một ngân hàng muốn dự đoán khách hàng có khả năng vỡ nợ cao dựa trên lịch sử tín dụng,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KJJ6eFW1nV0",
      "filename": "KJJ6eFW1nV0",
      "title": "[CS114 - Chương 2] Phân biệt một số loại mô hình học máy",
      "chunk_id": 10,
      "start_timestamp": "0:06:29",
      "end_timestamp": "0:07:19"
    }
  },
  {
    "page_content": "có khả năng vỡ nợ cao dựa trên lịch sử tín dụng, thu nhập và số dư tài khoản. Loại thuật toán nào phù hợp nhất? phân loại hội quy tuyến tính phân cục giảm chịu. Một nền tảng phát nhạc trực tuyến muốn đề xuất danh sách cá nhân hóa cho từng người dùng dựa trên sở thích nghe nhạc trước đó. Loại thuật toán nào là phù hợp nhất? Phân loại: Hồi quy tuyến tính, phân cục hoặc tăng cường. Bạn đã từng tự hỏi tại sao việc xây dựng một mô hình học máy tốt lại mất nhiều thời gian và công sức từ viện chọt lựa",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KJJ6eFW1nV0",
      "filename": "KJJ6eFW1nV0",
      "title": "[CS114 - Chương 2] Phân biệt một số loại mô hình học máy",
      "chunk_id": 11,
      "start_timestamp": "0:07:12",
      "end_timestamp": "0:07:56"
    }
  },
  {
    "page_content": "mất nhiều thời gian và công sức từ viện chọt lựa thuật toán, tin chỉnh siêu tham số, xử lý dữ liệu đến đánh giá mô hình, tất cả đều đòi hỏi kiến thức chuyên sâu và nhiều thử nghiệm. Đó chính là lý do auto ml ra đời giúp tự động hóa quá trình này, giảm bớt công sức và mở rộng khả năng áp dụng học máy đến nhiều lĩnh vực hơn. Auto ML Automatic Machine Learning là một công nghệ giúp tự động hóa quá trình xây dựng mô hình machine learning từ tiền xử lý dữ liệu, lựa chọn mô hình, tối ưu hóa siêu tham",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KJJ6eFW1nV0",
      "filename": "KJJ6eFW1nV0",
      "title": "[CS114 - Chương 2] Phân biệt một số loại mô hình học máy",
      "chunk_id": 12,
      "start_timestamp": "0:07:51",
      "end_timestamp": "0:09:42"
    }
  },
  {
    "page_content": "dữ liệu, lựa chọn mô hình, tối ưu hóa siêu tham số đến đánh giá và triển khai mô hình. Mục tiêu chính của AutoML là giảm thiểu như thầy đắ bắt đầu slide mới đúng không? >> Ừ, >> lại đi cũng được. Kem ra Cái gì? Ok. Còn tới 126 slide. K chưa? Thôi hôm nay vậy thôi. Mai tiếp baby step sửa tiếp đi. Giờ đi nhậu đã xong. Không nên ok >> mỗi buổi chiều nửa tiếng để >> 3 tiếng rồi >> để em giờ cái phòng xuống dưới chỗ á dưới chỗ A13 gần thầy luôn không không cần không cần lên đây tôi thấy hết Dạ.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=KJJ6eFW1nV0",
      "filename": "KJJ6eFW1nV0",
      "title": "[CS114 - Chương 2] Phân biệt một số loại mô hình học máy",
      "chunk_id": 13,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chào mừng các bạn đến với khóa học học máy machine learning của trường Đại học Công nghệ Thông tin. Tôi tên là Lê Đình Duy, giảng viên khoa khoa học máy tính. Các bạn hẳn đã được nghe nhiều về học máy, một lĩnh vực đang có rất nhiều ứng dụng thực tiện trong cuộc sống ngày nay của chúng ta. Quang sát hình ảnh sau, chúng ta sẽ thấy xu hướng quan tâm đến học máy đã gia tăng vượt bật từ năm 2015 trở đi. Điều này không phải là ngẫu nhiên, bởi đây chính là thời kỳ mà các công nghệ liên quan tới học",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:46"
    }
  },
  {
    "page_content": "là thời kỳ mà các công nghệ liên quan tới học máy như là xe tự lái, trợ lý ảo như Siri và Google Assistant Và các hệ thống gợi ý như Netflix, Amazon đã bắt đầu trở nên phổ biến và có tác động mạnh mẽ đến đời sống. Đặc biệt, trong những năm gần đây, những bước tiến vượt bật của học máy đã trao ra đời các ứng dụng, ấn tượng như ChartGBT, một hệ thống có khả năng giao tiếp tự nhiên với con người, hay các mô hình AI hỗ trợ sáng tạo nội dung giúp nâng cao hiệu quả và trải nghiệm trong nhiều lĩnh",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 1,
      "start_timestamp": "0:00:40",
      "end_timestamp": "0:01:23"
    }
  },
  {
    "page_content": "nâng cao hiệu quả và trải nghiệm trong nhiều lĩnh vực. Những thành tựu này không chỉ làm nối bật tìm năng to lớn của học máy, mà còn khẳng định vai trò quan trọng của học máy trong việc định hình tương lai công nghệ. Vậy, điều gì khiến học máy trở thành một chủ đề nóng và thu hút nhiều sự quan tâm đến vậy? Chính sự ảnh hưởng sâu rộng và tìm năng to lớn của học máy trong việc giải quyết các vấn đề phức tạp đã biến nó thành một trong những lĩnh vực trọng tâm của nguyên số. Để hiểu rõ hơn về sự",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 2,
      "start_timestamp": "0:01:17",
      "end_timestamp": "0:01:58"
    }
  },
  {
    "page_content": "vực trọng tâm của nguyên số. Để hiểu rõ hơn về sự phát triển mạnh mẽ này, chúng ta cần khám phá cách học máy hoạt động, những yếu tố cốt loại làm nên sức mạnh của nó và những ứng dụng thực tiện đang thay đổi cuộc sống trong ta. Môn học này sẽ giới thiệu các khái niệm cơ bản, các thuật toán và các kỹ thuật cốt lõi của học máy. Xin viên sẽ được học cách áp dụng các phương pháp học máy để giải quyết các vấn đề thực tế thông qua việc phát triển các ứng dụng đơn giản. Môn học này cũng trang bị cho",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 3,
      "start_timestamp": "0:01:45",
      "end_timestamp": "0:02:30"
    }
  },
  {
    "page_content": "ứng dụng đơn giản. Môn học này cũng trang bị cho sin viên nền tảng kiến thức và kỹ năng cần thiết để nâng cao khả năng tư duy và giải quyết vấn đề bằng các công cụ học máy. Mục tiêu của môn học thì bao gồm phần kiến thức lý thuyết, sẽ cung cấp các khái niệm và nguyên lý cơ bản của học máy, giúp sinh viên hiểu rõ nền tảng lý thuyết và cách thức hoạt động của các thuật toán học máy. Trang bị kiến thức về các thuật toán học máy phổ biến như hồi quy tuyến tính, hồi quy logistic và toán phân loại,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 4,
      "start_timestamp": "0:02:26",
      "end_timestamp": "0:03:03"
    }
  },
  {
    "page_content": "tuyến tính, hồi quy logistic và toán phân loại, cây quyết định, phân cụm, hệ thống gợi ý, học tăng cường. Đối với kỹ năng thực hành thì sẽ trang bị cho sinh viên kỹ năng lập trình với Python và sử dụng các thư viện học máy như Scikit-learn, TensorFlow và Keras để xây dựng và triển khai mô hình. Phát triển khả năng tư duy phân tích và đánh giá mô hình giúp sinh viên hiểu cách phân tích dữ liệu, đánh giá các mô hình học máy và tối ưu hóa hiểu xúc của mô hình. Về ứng dụng thực tế thì khuyến khách",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 5,
      "start_timestamp": "0:02:55",
      "end_timestamp": "0:03:33"
    }
  },
  {
    "page_content": "của mô hình. Về ứng dụng thực tế thì khuyến khách ứng dụng các kiến thức qua các bài tập và dự án thực hành giúp sinh viên áp dụng các thuật toán học máy vào các bài toán thực tế. Kết quả mong đợi, sinh viên có thể giải thích được các khái niệm và nguyên lý cơ bản của học máy, bao gồm học có giám sát, học không giám sát và học tăng cường. Áp dụng các thuật toán học máy phổ biến như hội quy tiến tính, hội quy logistic, funcung và hệ thống người ý vào các bài toán thực tế. Sử dụng thành thạo các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 6,
      "start_timestamp": "0:03:23",
      "end_timestamp": "0:04:06"
    }
  },
  {
    "page_content": "vào các bài toán thực tế. Sử dụng thành thạo các thư viện Python như Scikit-learn, TensorFlow và Keras để xây dựng, huấn luyện và triển khai các mô hình học máy đơn giản. Phân tích đánh giá hiệu suất các mô hình học máy và lựa chọn mô hình phù hợp cho từng bài toán. Tối ưu hóa mô hình học máy thông qua việc điều chỉnh các siêu tham số và cải thiện hiệu suất mô hình cho từng bài toán cụ thể. Thiết kế và triển khai các ứng dụng học máy đơn giản từ bước tiền xử lý dữ liệu đến huấn luyện và triển",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 7,
      "start_timestamp": "0:03:58",
      "end_timestamp": "0:04:35"
    }
  },
  {
    "page_content": "bước tiền xử lý dữ liệu đến huấn luyện và triển khai mô hình trên các dữ liệu thực tế. Đối tượng học của môn học là các sinh viên thuộc các ngành liên quan đến các khoa học máy tính, kỹ thuật, kinh doanh và các ngành khác có nhu cầu ứng dụng học máy và công việc. Sinh viên có kiến thức cơ bản về lập trình, về cấu trúc dữ liệu, thuộc toán và toán học, ví dụ như đại số tiến tính, sát sát thống kê. Các bạn sinh viên có mong muốn tìm hiểu về các khái niệm thuộc toán và ứng dụng của học máy trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 8,
      "start_timestamp": "0:04:27",
      "end_timestamp": "0:05:03"
    }
  },
  {
    "page_content": "niệm thuộc toán và ứng dụng của học máy trong các lĩnh vực như phân tích dữ liệu, phát triển hệ thống trí tệ nhân tạo hoặc tối ưu hóa quy trình quân doanh. Học là các bạn sinh viên muốn phát triển kỹ năng lập trình bằng Python và sử dụng các thư viện học máy phổ biến như Scikit-learn, TensorFlow và Keras. Trạp điểm của khóa học là chú chọn kỹ năng thực hành, áp dụng các kiến thức đã học vào giải quyết các vấn đề bài toán thực tế, phát triển các kỹ năng lập trình Python và cách sử dụng công cụ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 9,
      "start_timestamp": "0:04:56",
      "end_timestamp": "0:05:37"
    }
  },
  {
    "page_content": "kỹ năng lập trình Python và cách sử dụng công cụ thư viện học máy để giải quyết các vấn đề cụ thể, phát triển kỹ năng giải quyết vấn đề và khuyến khích tìm tòa hình khám phá các phương pháp mới để tối ưu hóa mô hình và giải quyết các bài toán phức tạp thông qua nghiên cứu và thực hành. Để thực hành các kỹ thuật học máy thì chúng ta có thể sử dụng hai nền tảng phổ biến, đó là Google Collab và Cargo. Google Collab là một nền tảng trực tuyến hoàn toàn miễn phí và được thiết kế để hỗ trợ các bạn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 10,
      "start_timestamp": "0:05:34",
      "end_timestamp": "0:06:07"
    }
  },
  {
    "page_content": "toàn miễn phí và được thiết kế để hỗ trợ các bạn trong việc thực hành và phát triển các dự án Python, đặc biệt là trong lĩnh vực Machine Learning và Deep Learning. Với Google Collab thì bạn không cần phải cài đặt bất kỳ phần mềm nào, mà chỉ cần một chình dụy web để bắt đầu làm việc. Đền tảng này cung cấp khả năng sử dụng GPU và TPU miễn phí, giúp tăng tốc quá trình huấn luyện mô hình máy học và xử lý dữ liệu lớn. Ngoài ra thì Collab còn tích hợp sẵn với các thư viện phổ biến như NumPy, Pandas,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 11,
      "start_timestamp": "0:06:03",
      "end_timestamp": "0:06:37"
    }
  },
  {
    "page_content": "sẵn với các thư viện phổ biến như NumPy, Pandas, TensorFlow, PyTorch và nhiều công cụ khác giúp việc làm trình trở nên dễ dàng và hiệu quả hơn. Google Collab thì rất là đặc biệt và phù hợp cho sinh viên và các nhà nghiên cứu và các lập trình viên muốn thử nghiệm các ý tưởng hoặc học tập mà không cần đầu tư nhiều và phần cứng. Nền tảng thứ hai là Cargo. Cargo là một nền tảng nổi tiếng trong cộng đồng khoa học dữ liệu và học máy nơi tập trung hành triệu người dùng trên toàn thế giới. Cargo không",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 12,
      "start_timestamp": "0:06:34",
      "end_timestamp": "0:07:06"
    }
  },
  {
    "page_content": "triệu người dùng trên toàn thế giới. Cargo không chỉ cung cấp các bộ dữ liệu phong phú và miễn phí, mà còn là nơi lý tưởng để thực hành học hỏi và thử thách bản thân thông qua các cuộc thi AI và khoa học dữ liệu. Người dùng Cargo có thể truy cập vào các công cụ phân tích dữ liệu trực tuyến, chia sẻ mã nguồn học hỏi từ cộng đồng thông qua các notebook được xây dựng sản. Đặc biệt là Cargo cũng là một môi trường lý tưởng để nâng cao kỹ năng xử lý dữ liệu, xây dựng mô hình, học máy và thử nghiệm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 13,
      "start_timestamp": "0:06:58",
      "end_timestamp": "0:07:47"
    }
  },
  {
    "page_content": "dữ liệu, xây dựng mô hình, học máy và thử nghiệm với các bài toán dữ liệu thực tế. Đây cũng là nơi mà các công ty lớn tổ chức các cuộc thi để tìm kiếm giải pháp sáng tạp cho vấn đề của họ. Để tìm hiểu về học máy thì có rất nhiều tài liệu tham khả vụ ích. Dưới đây là giới thiệu về hai cuốn sách nổi bật cung cấp kiến thức từ cơ bản đến nâng cao, lý thuyết, đi kèm thực hành, giúp bạn bắt đầu và phát triển trong lĩnh vực này. đó là Introduction to Machine Learning with Python và Hang On Machine",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 14,
      "start_timestamp": "0:07:41",
      "end_timestamp": "0:08:12"
    }
  },
  {
    "page_content": "Machine Learning with Python và Hang On Machine Learning with Scikit-learn Keras and TensorFlow. Cuốn sách Introduction to Machine Learning with Python được viết bởi Andreas Müller và Sarah Gildow là một tài liệu phù hợp dành cho những người mới bắt đầu tìm hiểu về học máy. Cuốn sách này cung cấp các khái niệm cốt lõi các thuộc toán học máy quan trọng và khuyến dẫn cách triển khai chúng bằng ngôn ngữ Python thông qua thư biện Scikit Learn. Với cách trình bày rõ ràng và dễ hiểu, thì sách tập",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 15,
      "start_timestamp": "0:08:10",
      "end_timestamp": "0:08:43"
    }
  },
  {
    "page_content": "cách trình bày rõ ràng và dễ hiểu, thì sách tập trung vào việc giải thích cách chuẩn bị dữ liệu lựa chọn mô hình phù hợp, đánh giá hiệu quả của mô hình. Đặc biệt là sách nhấn mạnh đến tính thực hành với nhiều ví dụ thực tế và mãn nguồn minh họa, giúp người học không chỉ nắm vững lý thuyết mà còn tự tin áp dụng kiến thức vào các dự án cụ thể. Đây là tờ liệu hữu ích dành cho sinh viên, nhà nghiên cứu và những ai muốn bắt đầu quá trình học tập trong lĩnh vực học máy. Cuốn sách thứ hai là Hands-on",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 16,
      "start_timestamp": "0:08:37",
      "end_timestamp": "0:09:12"
    }
  },
  {
    "page_content": "lĩnh vực học máy. Cuốn sách thứ hai là Hands-on Machine Learning with Scikit-Learn Keras and TensorFlow. Cuốn sách này được viết bởi Aurelien Gieron là một trong những tài liệu hữu ích dành cho những ai muốn học và thực hành học máy một cách hệ thống và thực tiện. Nội dung sách tập trung vào việc cung cấp cả nền tảng kiến thức cơ bản, lẫn các kỹ thuật nâng cao trong lĩnh vực học máy. Đồng thời sách cũng hiến dẫn cách áp dụng các thuộc toán vào các bài toán thực tế sử dụng các thư viện phổ biến",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 17,
      "start_timestamp": "0:09:01",
      "end_timestamp": "0:09:43"
    }
  },
  {
    "page_content": "bài toán thực tế sử dụng các thư viện phổ biến như SiteKit Learn, Keras và TensorFlow. Người học sẽ được học cách xây dựng và triển khai các mô hình từ những bước cơ bản nhất đến các kỹ thuật hiện đại trong Deep Learning. Với sự hỗ trợ của các ví dụ minh họa rõ ràng, bài tắc thực hành và lời giải thích chi tiết, đây là tài liệu phù hợp cho cả người học mới và các chuyên gia muốn nâng cao các kỹ năng. Nếu các bạn tìm kiếm các khóa học trực tuyến chất lượng để bắt đầu hoặc nâng cao kiến thức về",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 18,
      "start_timestamp": "0:09:30",
      "end_timestamp": "0:10:16"
    }
  },
  {
    "page_content": "chất lượng để bắt đầu hoặc nâng cao kiến thức về học máy, thì hãy cùng tìm hiểu hai chương trình đào tạo trực tuyến nổi bật sau đây được thiết kế bởi các chuyên gia hàng đầu từ các trường đại học danh tiến như Đại học Stanford và Đại học Washington. Chổi các khóa học Machine Learning Specialization hợp tác giữa Đại học Stanford và Nib Learning AI được giảng dạy bởi giáo sư Andrew Inge, một trong những nhà tiên phong hàng đầu trong lĩnh vực AI. Giáo sư Andrew Inge đã dẫn dắt nhiều nghiên cứu độc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 19,
      "start_timestamp": "0:10:11",
      "end_timestamp": "0:10:46"
    }
  },
  {
    "page_content": "sư Andrew Inge đã dẫn dắt nhiều nghiên cứu độc phá tại Stanford, Google Brand, Baidu và Landing.ai. Chối 3 khóa học này là phiên bản cập nhập của khóa học Machine Learning nổi tiếng đã đạt hơn 4,8 triệu lật đăng ký kể từ khi ra mắc vào năm 2012. Các khóa học cung cấp kiến thức toàn diện về những nguyên lý và kỹ thuật học máy hiện đại bao gồm học có giám sát, hội quy tuyến tính, hội quy logistic, mạng, neuron và cây quyết định. Họ không giám sát gồm có phân cụm, giảm chiều dữ liệu và hệ thống",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 20,
      "start_timestamp": "0:10:41",
      "end_timestamp": "0:11:12"
    }
  },
  {
    "page_content": "gồm có phân cụm, giảm chiều dữ liệu và hệ thống gợi ý. cùng với kinh nghiệm thực chiến là các phương pháp đánh giá và tổ ưu hóa mô hình cùng với cách tiếp cận dựa trên dữ liệu để nâng cao hiệu sức. Thứ hai là chuỗi các khóa học Machine Learning Specialization của Đại học Washington được thiết kế bởi giáo sư Emily Fox và giáo sư Carlos Gretrin, những chuyên gia hàng đầu trong lĩnh vực họp máy. Chuỗi các khóa học này tập trung vào các tình huống thực tế, giúp cho bạn tích lụy kinh nghiệm, áp dụng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 21,
      "start_timestamp": "0:11:08",
      "end_timestamp": "0:11:45"
    }
  },
  {
    "page_content": "tế, giúp cho bạn tích lụy kinh nghiệm, áp dụng trong các lĩnh vực chính của học máy như dự đoán, phân loại, phân cụm, truy xúc thông tin. Người học sẽ được trang bị các kỹ năng phân tích dự lược lớn và phức tạp xây dựng các hệ thống có khả năng thích nghi và tối ưu hóa theo thời gian, đồng thời phát triển các ứng dụng thông minh có khả năng đưa ra dự đoán chính xác tự dĩ liệu. Python đã trở thành một trong những ngôn ngữ lập trình phổ biến nhất cho học máy. Vậy điều gì đã khiến Python trở nên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 22,
      "start_timestamp": "0:11:36",
      "end_timestamp": "0:12:20"
    }
  },
  {
    "page_content": "cho học máy. Vậy điều gì đã khiến Python trở nên phù hợp với lĩnh vực này? Hãy cùng tìm hiểu một số ưu điểm nổi bật của Python trong học máy. Thứ nhất, đó là cú pháp đơn giản và dễ học. Thích hợp, rất thích hợp cho người mới bắt đầu và giúp tập trung vào logic thay vì các cú pháp phức tạp. Thứ hai là hệ sinh thái, thư viện rất là phong phú, hỗ trợ mạnh mẽ với các thư viện như Numpy, Pandas, Scikit-learn, TensorFlow, PyTorch và Matplot. Cộng đồng lớn và hỗ trợ rất tốt, dễ dàng tìm kiếm hỗ trợ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 23,
      "start_timestamp": "0:12:15",
      "end_timestamp": "0:12:54"
    }
  },
  {
    "page_content": "lớn và hỗ trợ rất tốt, dễ dàng tìm kiếm hỗ trợ tài liệu, học tập, kỹ dụng như thực hành và các nguồn hỗ trợ khác sẵn có để giúp cho người học khi gặp khó khăn. Tính linh hoạt và khả năng mở rộng tích hợp cao có thể được sử dụng cho nhiều mục đích khác nhau từ phát triển ghếp cho đến phân tích dự liệu và tự động hóa. Ý khác nữa đó là Python miễn phí và mãn nguồn mở rất phổ biến trong công nghiệp. Python là một ngô ngữ lập trình biển phí và mã nguồn mở cho nên giúp giảm chi phí học tập đặc biệt",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 24,
      "start_timestamp": "0:12:50",
      "end_timestamp": "0:13:26"
    }
  },
  {
    "page_content": "mở cho nên giúp giảm chi phí học tập đặc biệt phù hợp với giáo dục. Ngoài ra, đây là ngô ngữ được sử dụng rộng rãi trong các công ty công nghệ lớn như Google, Facebook, Amazon. Hỗ trợ giảng dạy trực quan, Python tích hợp tốt với Google Notebook và Google Collab, cung cấp một môi trường thử nghiệm tương tác giúp trình bày rõ ràng và kết hợp mã nguồn văn bản cùng các hình ảnh bên ngoài. Trong bài giảng này, các bạn sẽ được học theo cách tiếp cận học tập ứng dụng. Cách tiếp cận này có đặc điểm là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 25,
      "start_timestamp": "0:13:24",
      "end_timestamp": "0:14:05"
    }
  },
  {
    "page_content": "tập ứng dụng. Cách tiếp cận này có đặc điểm là kết nối lý thuyết và thực tiện để giúp người học có thể áp dụng chiến thức vào các bài toán thực tế thay vì chỉ học kiếm thiết xuân. Chuẩn bị cho công việc thực tế rèn luyện kỹ năng xử lý giả lị, xây dựng và triển khai mô hình học máy xác với yêu cầu của ngành công nghiệp. Phát triển tư duy phản biệt, Người học được rèn luyện khả năng phân tích vấn đề và đưa ra các giải pháp sáng tạ và đồng thời tăng tính thực hành và hứng thú. Việc thực hiện các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 26,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "tính thực hành và hứng thú. Việc thực hiện các dự án thực tế sẽ giúp cho người học tham gia tích cực hơn và có động lực học tập cao hơn. Tánh giá môn học. Môn học này sẽ có điểm quá trình là 20% bao gồm tham dự và thảo luận trong lớp học, viết, note và mỗi buổi học. Điểm thực hành 30% Chủ yếu là thi lập trình Python trên hệ thống Wecode Kiểm tra quý kỳ gồm có điểm lý thuyết 20% và điểm đồ án 30% Sau đây là một số quiz để giúp các bạn ôn lại các ý đã được trình vầy trong vài học này Ví dụ, hãy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 27,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "ý đã được trình vầy trong vài học này Ví dụ, hãy cho biết lý do quan trọng nhất khiến Python trở thành ngôn ngữ phổ biến trong học máy. Yếu tố nào sau đây không phải là lý do chính khiến Python phổ biến trong học máy.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=lAsAS0ChePY",
      "filename": "lAsAS0ChePY",
      "title": "[CS114 - Chương 1] Giới thiệu môn học",
      "chunk_id": 28,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Sau đây thì chúng ta sẽ cùng thảo luận về một số vấn đề khi chúng ta huấn luyện với môi nồi Q đa biến. Thì vấn đề đầu tiên đó chính là vấn đề về feature scaling. Tức là là chúng ta chuẩn hóa các cái đặc trưng của mình. Nếu như trong mô hình hồi quy đơn biến thì chúng ta chỉ có duy nhất một biến x à và trọng số tương ứng của nó. Để thể hiện cái mối quan trọng của x tác động lên cái giá trị dự đoán thì đối với cái mô hình hồi quy đa biến thì nó không phải chỉ có 1x mà nó sẽ có nhiều hơn một giá",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:01:01"
    }
  },
  {
    "page_content": "phải chỉ có 1x mà nó sẽ có nhiều hơn một giá trị, một cái đặc trưng. Thì lúc này chúng ta sẽ có là x1, x2 vân vân cho đến xm à xn. Và tương ứng thì chúng ta sẽ có cái trọng số của nó đó là w1, w2 và wn. Rồi sau đó chúng ta sẽ cộng lại. À thì cái w nó sẽ thể hiện cái tầm quan trọng của à đặc trưng thứ y. Thì ở đây chúng ta sẽ xét một cái ví dụ là đặc trưng của mình là gồm các cái giá trị nhưng mà nó có giải giá trị rất là khác nhau. Có giải giá trị rất là khác nhau. Ví dụ như trong bài toán dự",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 1,
      "start_timestamp": "0:00:53",
      "end_timestamp": "0:01:40"
    }
  },
  {
    "page_content": "trị rất là khác nhau. Ví dụ như trong bài toán dự đoán giá nhà thì chúng ta sẽ có cái thông tin diện tích căn nhà thì có thể là từ 300 cho đến 2000 m². Tức là cái con số này rất là lớn. Trong khi đó chúng ta lại có một cái đặc trưng khác. Ví dụ như là số phòng ngủ thì nó chỉ có là từ 0 cho đến 5 phòng ngủ. Thì đây là một cái con số khá là bé. Thế thì điều gì xảy ra khi trong cái mô hình của mình nó có hai cái đặc trưng x1 và x2 và có cái giải giá trị chênh biệt chên lệch rất là lớn như thế này.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 2,
      "start_timestamp": "0:01:35",
      "end_timestamp": "0:02:18"
    }
  },
  {
    "page_content": "trị chênh biệt chên lệch rất là lớn như thế này. Thì cái hệ quả đầu tiên đó là cái trọng số lớn. À cái đặc trưng lớn thì thường là có cái trọng số nhỏ và cái đặc trưng nhỏ thì thường là có trọng số lớn. Điều này có nghĩa là gì? Khi chúng ta ở đây chúng ta xét cái ví dụ là x1 là một cái đặc trưng mà có cái giá trị lớn ha. Nó gọi ký hiệu à viết tắc là đặc trưng lớn. Còn x2 sẽ là cái đặc trưng nhỏ tức là cái giá trị nhỏ. thì cái trọng số cho cái W à X W1 cho cái X1 và W2 cho X2 thì thông thường là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 3,
      "start_timestamp": "0:02:12",
      "end_timestamp": "0:03:14"
    }
  },
  {
    "page_content": "X W1 cho cái X1 và W2 cho X2 thì thông thường là cái trọng số cho cái W1 sẽ nhỏ hơn cái trọng số cho W2 cho X2. Đó, tức là nếu mà x1 lớn x2 nhỏ thì thường là w1 sẽ nhỏ hơn nó sẽ nhỏ hơn W2 tức là nó sẽ nghịch đảo lại. Thế thì tại sao nó lại có vấn đề này? Tại vì khi chúng ta đưa ra cái mô hình dự đoán thì ờ nếu giả sử như cả x1 và x2 cùng có cái vai trò trong cái việc là dự đoán cái giá trị y thì cái giá trị lớn nó sẽ bị tác động nhiều hay ít thì thường nó sẽ bị tác động ít hơn. Lý do đó là vì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 4,
      "start_timestamp": "0:03:08",
      "end_timestamp": "0:03:53"
    }
  },
  {
    "page_content": "thường nó sẽ bị tác động ít hơn. Lý do đó là vì mỗi lần chúng ta thay đổi một đơn vị của W à W này chỉ cần chúng ta thay đổi một đơn vị thôi thì nó sẽ khiến cho cái giá trị Y này thay đổi rất là lớn. Mặt khác cái W2 tương ứng với lại cái đặc trưng nhỏ thì vì cái giá trị giải giá trị của x2 nó nhỏ nên khi chúng ta tăng một cái đơn vị của W2 thì nó sẽ làm cho cái Y này thay đổi rất là ít. Đó. Do đó và vì chúng ta giả định rằng là cả x1 và x2 đều có cái vai trò trong cái việc là dự đoán y thì để",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 5,
      "start_timestamp": "0:03:47",
      "end_timestamp": "0:04:36"
    }
  },
  {
    "page_content": "có cái vai trò trong cái việc là dự đoán y thì để cho nó cân bằng để nó cân bằng trở lại thì cái trọng số của W1 thường nó sẽ giảm xuống. Nó sẽ giảm xuống tại vì cái giá trị của x1 nó đã lớn rồi nên w1 nó phải giảm xuống để không có lấn ác cái x2. Ngược lại vì x2 nó quá nhỏ so với x1 nên cái để cân bằng lại thì cái w2 nó sẽ lớn hơn. để bù lại để cho cả hai cái x1 và x2 nó cùng tham gia vô cái quá trình dự đoán cái à giá trị y. Thì đó chính là cái lý do tại sao cái đặc trưng của à cái trọng số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 6,
      "start_timestamp": "0:04:26",
      "end_timestamp": "0:05:17"
    }
  },
  {
    "page_content": "lý do tại sao cái đặc trưng của à cái trọng số của các cái đặc trưng lớn thì thường là nhỏ và trọng số của cái đặc trưng nhỏ thì thường là lớn. Và chính vì cái sự bất cân xứng này nó sẽ khiến cho cái đường đồng mức của cái hàm chi phí của mình nó sẽ bị kéo dài và dẹt. Ví dụ đây là cái đường động mức của mình thôi. Rồi thì khi chúng ta huấn luyện cái mô hình thì nó sẽ khiến cho cái mô hình của mình nó sẽ dịch chuyển zíz đó. nó sẽ dịch chuyển zíz rồi sau đó nó mới à tiến về được cái giá trị tối",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 7,
      "start_timestamp": "0:05:11",
      "end_timestamp": "0:05:55"
    }
  },
  {
    "page_content": "rồi sau đó nó mới à tiến về được cái giá trị tối ưu toàn cục ở đây thì nó sẽ khiến cho cái việc mà hội tụ của mình nó bị chậm. Và với cái hệ quả như vậy thì chúng ta sẽ có những cái giải pháp gì? thì đương nhiên là nguyên nhân nào thì hệ thì cái giải pháp đó nguyên nhân đó là do cái x1 và x2 đó là những cái giá trị mà ờ có cái sự chênh lệch về giải giá trị lớn thì chúng ta sẽ kéo nó lại thành cái giải giá trị nó giống nhau. Thì cụ thể ở đây chúng ta sẽ đưa đặc trưng về cùng một giải giá trị.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 8,
      "start_timestamp": "0:05:50",
      "end_timestamp": "0:06:26"
    }
  },
  {
    "page_content": "ta sẽ đưa đặc trưng về cùng một giải giá trị. Đưa về cùng một giải giá trị. Và ví dụ như ở đây chúng ta chọn cái giải giá trị là 01. Nhưng mà đương nhiên đây không phải là một giải giá trị duy nhất mà chúng ta sẽ có những cái cách để mà normaliz chuẩn hóa về à khác nhau. Rồi thì khi cái cầm tu của chúng ta ờ cái đường đồng mức của mình á nó không còn ở dạng dài và dẹt như thế này nữa mà nó sẽ trở về cái dạng là giò tròn hơn. Và khi đường động mức mình nó tròn hơn thì cái radian của mình nó sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 9,
      "start_timestamp": "0:06:20",
      "end_timestamp": "0:07:06"
    }
  },
  {
    "page_content": "mình nó tròn hơn thì cái radian của mình nó sẽ hội tụ nhanh hơn. À nó sẽ hội tụ nhanh hơn. nó đi theo cái đường trực diện hơn. Thì đó chính là cái giải pháp để mà khắc chế trong cái trường hợp mà đặc trưng của mình nó có cái sự chênh lệch lớn về giải giá trị. Thì sau đây chúng ta sẽ có một vài cái phương pháp để mà à chuẩn hóa. Thì chuẩn hóa đầu tiên đó là mean normalization là một cái kỹ thuật đưa cái dữ liệu về quanh số 0 dựa vào cái mức độ trung bình và độ rộng của khoảng giá trị. Thì me",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 10,
      "start_timestamp": "0:07:00",
      "end_timestamp": "0:07:55"
    }
  },
  {
    "page_content": "trung bình và độ rộng của khoảng giá trị. Thì me normalization thì cái công thức của mình đó là lấy cái giá trị góc cái giá trị góc trừ cho cái trung bình là cái trung bình của mình đó là giá trị min của feature. Thì trong cái cột x của mình, trong cái cột đặc trưng của mình thì chúng ta sẽ trừ cho min của x. Mi của x là giá trị trung bình của cái các cái đặc trưng x. Sau rồi à chia cho cái khoảng max trừ min. Thì cái việc lấy x trừ cho mu của x nó sẽ đưa cái giá trị x của mình về cái một cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 11,
      "start_timestamp": "0:07:47",
      "end_timestamp": "0:08:34"
    }
  },
  {
    "page_content": "x nó sẽ đưa cái giá trị x của mình về cái một cái con số quanh số 0. và chia cho x tr x max trừ cho x min thì cái việc chia cho x max trừ x min á thì nó sẽ đưa cho cái khoảng giá trị của chúng ta về cùng một cái khoảng là từ 0 cho đến 1 hay nói cách khác đó là cái khoảng của mình nó sẽ nhỏ lại và nó chỉ khoảng là bằng một đơn vị thì khi đó cái khoảng cách giữa giá trị lớn nhất và giá trị nhỏ nhất của mình nó chỉ là bằng khoảng một đơn vị thôi. Rồi và cái cách mà chúng ta min normalization như",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 12,
      "start_timestamp": "0:08:28",
      "end_timestamp": "0:09:12"
    }
  },
  {
    "page_content": "Rồi và cái cách mà chúng ta min normalization như thế này á tức là chuẩn hóa theo kiểu trung bình thì nó sẽ có cái ưu điểm đó là feature mà sẽ về quanh số 0 và do đó thì nó sẽ giảm được cái độ lạch giảm cái bias do cái có cái sự khác biệt về mặt tỉ lệ à do có cái scale khác biệt giữa các cái đặc trưng với nhau. Thì ví dụ như chúng ta có x1 và x2 thì nếu như mà x1 mà đặc trưng này lớn hơn x2 thì nó sẽ có cái hiện tượng là bias vào cái x2. Tức là mô hình của mình nó sẽ chăm chăm nó ưu tiên để học",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 13,
      "start_timestamp": "0:09:07",
      "end_timestamp": "0:09:45"
    }
  },
  {
    "page_content": "hình của mình nó sẽ chăm chăm nó ưu tiên để học cho cái x1 chiều hơn. Lý do đó là vì cái việc mà thay đổi cái x1 nó sẽ ảnh hưởng lớn đến toàn bộ cái quá trình mà tính toán ra cái giá trị y. Đó thì cái việc mà chúng ta cân bằng x1 và x2 lại với nhau thì nó sẽ khiến cho chúng ta giảm cái sự chân nè, giảm cái sự phụ thuộc và giảm cái sự bias vào cái một cái đặc trưng nào đó. Nó sẽ giúp cho hai cái x1 và x2 nó sẽ cân bằng hơn. Cân bằng hơn. Và cái phương pháp scaling, feature scaling này thì nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 14,
      "start_timestamp": "0:09:40",
      "end_timestamp": "0:10:28"
    }
  },
  {
    "page_content": "phương pháp scaling, feature scaling này thì nó cũng khá là đơn giản và dễ dàng có thể tính toán được. công thức của nó rất là gọn và chúng ta nhiều khi chỉ cần tính toán trong nếu mà tính khéo thì chúng ta cũng chỉ có code một hai dòng code là xong. Và các cái thư vị hiện nay thì cũng có hỗ trợ các cái phép biến đổi ờ chuẩn hóa này. Cái dược điểm đó là vì cái công thức của chúng ta là có x trừ cho mu chia cho x max trừ cho x min. Thì những chính những cái giá trị max và min này á là nó có thể",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 15,
      "start_timestamp": "0:10:23",
      "end_timestamp": "0:11:11"
    }
  },
  {
    "page_content": "những cái giá trị max và min này á là nó có thể khả năng đó là nó nhận những cái outlayer tức là những cái giá trị mà rất khác biệt trong cái à không gian của mình. Đó thì trong cái phân bố của mình thì đâu đó nó sẽ có những cái giá trị là out layer nó sẽ nằm ở rất là xa. Nó sẽ nằm ở rất là xa. Thì những cái giá trị này nó sẽ khiến cho cái việc chuẩn hóa và tạo ra các cái đặc trưng mới mà nó cũng bị nhạy cảm với lại các cái giá trị outlayer. Thì nếu được thì chúng ta có thể chọn cái X max và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 16,
      "start_timestamp": "0:11:05",
      "end_timestamp": "0:11:46"
    }
  },
  {
    "page_content": "nếu được thì chúng ta có thể chọn cái X max và Xmin này thay bằng những cái giá trị khác mà nó không bị ảnh hưởng nhiều bởi các cái outlayer này. Ví dụ như chúng ta có thể chọn ra các cái ngưỡng là one 25% hoặc là one 75% ví dụ vậy để mà mình à giảm bớt cái sự phụ thuộc vào những cái giá trị mà ở nằm ở ngoài cận biên của cái giải giá trị. và nó sẽ không phù hợp à với cái dữ liệu mà có cái phân phối nó bị lệch. À thì rõ ràng cái cách cái cách mà chúng ta ờ chia ra như thế này đúng không? cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 17,
      "start_timestamp": "0:11:41",
      "end_timestamp": "0:12:23"
    }
  },
  {
    "page_content": "mà chúng ta ờ chia ra như thế này đúng không? cái cách mà chúng ta chia cho xap trừ x min thì đối với những cái dữ liệu mà nó có phân bố mà bị lệch mạnh ví dụ như là giải giá trị của mình tập trung phần nhiều là ở đây nhưng mà phần cuối thì lại kéo dài, kéo rất là dài thì những cái đuôi như thế này á nó sẽ khiến cho cái việc chuẩn hóa của chúng ta không có phù hợp, nó sẽ bị ảnh hưởng bởi những cái giá trị này rất là cao. Đó thì giải pháp cũng tương tự như vậy. chúng ta sẽ phải chọn cái ngưỡng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 18,
      "start_timestamp": "0:12:14",
      "end_timestamp": "0:13:11"
    }
  },
  {
    "page_content": "tự như vậy. chúng ta sẽ phải chọn cái ngưỡng để mà chúng ta scale lại cho nó phù hợp hơn. Vậy thì khi nào thì dùng min normalization? Đó là khi dữ liệu không có cái outayer quá lớn và khi chúng ta muốn dữ liệu nó về quay quanh xung số 0 nhưng mà vẫn giữ được cái thang đo tỷ lệ. Một cái kỹ thuật tiếp theo của feature scaling đó chính là score normalization. Thì trái ngược với lại cái kỹ thuật à x mean normalization thì ở đây thay vì chúng ta trừ cho mu và chia cho cái x max trừ cho x min. Thế",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 19,
      "start_timestamp": "0:13:06",
      "end_timestamp": "0:13:32"
    }
  },
  {
    "page_content": "cho mu và chia cho cái x max trừ cho x min. Thế thì cái các cái giá trị max và min này á thì nó sẽ rất dễ bị các cái giá trị outayer gọi là chi phối. Do đó thì để giảm bớt cái sự ảnh hưởng của các cái giá trị mà max và min mà tạo bởi các cái out layer thì chúng ta sẽ dùng cái độ lệch chuẩn. Và độ lịch chuẩn thì chúng ta sẽ ký hiệu bằng sigma. Thì cái công thức của mình đó sẽ là x' là bằng x trừ cho mi chia cho sigma. Và sau khi chuẩn hóa xong thì cái feature x' của mình nó sẽ có cái trung bình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 20,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "cái feature x' của mình nó sẽ có cái trung bình thì vẫn là bằng 0. À nhưng mà cái độ lệch của nó, độ lịch chuẩn của nó thì sẽ là bằng 1. Thì nó sẽ đưa về cái dạng mà gần với lại cái phân bố phân bố chuẩn trong đó min. là bằng 0 và standard deviation là bằng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m2XJJvGxll8",
      "filename": "m2XJJvGxll8",
      "title": "[CS114 - Chương 3] Feature Scaling",
      "chunk_id": 21,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề, học sâu, machine learning, logistic regression, imageNet. và một số cái phương pháp, một số cái giải pháp là H-Margin, S-Margin và bài toán đối ngẫu thì đây là cái bài toán tối ưu trong SVM cụ thể là ở trong cái sô đồ bên tay trái chúng ta thấy rằng là có cái đường phân lớp và có cái margin của mình thì ở đây chúng ta sẽ có cái công thức là W nhân VX cộng B là bằng 0 thì đây chính là cái phương trình đường thẳng của hyperplane, cái siêu phẳng của mình khi đó nới ra một chút, tức là wx",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:12"
    }
  },
  {
    "page_content": "phẳng của mình khi đó nới ra một chút, tức là wx cộng b mà trừ 1 thì nó sẽ ra phương trình của lề bên tay trái này và wx cộng b cộng 1, tức là chúng ta tỉnh tiến lên 1 đơn vị thì nó chính là phương trình của siêu phẳng của lề bên tay trái thì tất cả những cái điểm nào mà nằm về một phía bên đây nằm về một phía bên đây thì khi thế vào, cú ý ở đây x của mình x của mình chính là cái đặc trưng của cái điểm này thì khi thế vào cái wx cộng b thì nó sẽ lớn hơn cộng 1 là vì nó nằm về một phía còn những",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 1,
      "start_timestamp": "0:01:07",
      "end_timestamp": "0:02:00"
    }
  },
  {
    "page_content": "lớn hơn cộng 1 là vì nó nằm về một phía còn những cái điểm màu trắng những cái điểm x màu trắng khi chúng ta thế vào WX cộng B thì nó sẽ cho cái giá trị là bé hơn trường 1, tức là nó nằm về một phía thì cái bin này nó gọi là H-Margin tức là nó sẽ không cho phép những cái điểm màu đen nằm về phía bên đây và nó không cho phép cái điểm màu trắng nằm về phía bên đây Còn ở giữa này, khoảng không ở giữa này là không có điểm dữ liệu nào Thì đó, tính chất này rất là cứng Nó không cho phép có một điểm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 2,
      "start_timestamp": "0:01:53",
      "end_timestamp": "0:02:35"
    }
  },
  {
    "page_content": "này rất là cứng Nó không cho phép có một điểm nào nằm ở giữa Hoặc là không cho phép một điểm màu đen nào nằm phía bên đây Hoặc là điểm màu trắng nào nằm phía bên đây Thì công thức cho margin này, mô hình SVM cho tình huống H margin này nó sẽ tương đương với cái bài toán đó là chúng ta đi tìm cái W và B tìm cái W và B sao cho 1 phần 2 W bình là nhỏ nhất thì ở đây người ta bằng các công thức biến đổi toán là từ biên trái sang biên phải thì nó sẽ có cái giá trị là 2 tại vì wx cộng b là bằng 0 wx",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 3,
      "start_timestamp": "0:02:32",
      "end_timestamp": "0:03:27"
    }
  },
  {
    "page_content": "có cái giá trị là 2 tại vì wx cộng b là bằng 0 wx trừ 1 là biên trái wx cộng 1 là biên phải thì khi trừ ra cái khoảng cách của mình từ biên trái sang biên phải nó chính là 2 nhưng mà nó sẽ phải chia cho w thì đây chính là cái margin và khi chúng ta muốn nó liên bình phương và khi chúng ta muốn là cái bin này là lớn nhất chúng ta đang muốn maximize cái margin mà lớn nhất thì có phải tương đương chúng ta đang nghịch đạo lên, tức là chúng ta đi tìm giá trị nhỏ nhất của 1 phần 2 WB thì margin của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 4,
      "start_timestamp": "0:03:23",
      "end_timestamp": "0:04:25"
    }
  },
  {
    "page_content": "giá trị nhỏ nhất của 1 phần 2 WB thì margin của mình trong tình hu này công thức của nó sẽ là 2 trên W và tìm max tương đương với tìm min của công thức này và ngoài ra bên cạnh tìm bin lớn nhất này thì đồng thời nó phải thoải mạn các tính chất đó là tất cả những điểm mà mà màu đen tức là có những nhãn 1 phải nằm phía bên đây và nhãn trừ 1 nó phải nằm về phía bên đây thì tự lại y,y chính là cái nhãn và nhân với lại wx y cộng b thì rõ ràng là 2 cái này nó phải là cùng dấu y mà y,y mà dương thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 5,
      "start_timestamp": "0:04:22",
      "end_timestamp": "0:05:04"
    }
  },
  {
    "page_content": "cái này nó phải là cùng dấu y mà y,y mà dương thì thằng này nó cũng phải là lớn hơn 1 bằng 1 y,y mà âm bé hơn trừ 1 thì wx y cộng b cũng phải bé hơn trừ 1 thì khi nhân lại với nhau nó sẽ ra cái tính chất rất là đẹp cho dù điểm của mình là bên trái hay bên phải thì nó đều phải thoải mạng tính chất này đó là EI nhân W của XI cộng b lớn hơn hoặc bằng 1 thì đây là một cái công thức mà tìm giá trị nhỏ nhất thoải mạng một số cái ràng buộc Bình thường, trường đồng bình thường, bình thường, và bình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 6,
      "start_timestamp": "0:04:58",
      "end_timestamp": "0:05:48"
    }
  },
  {
    "page_content": "trường đồng bình thường, bình thường, và bình thường đối tống với bình thường Cái ràng buộc đầu tiên, xin lỗi, cái biểu thức đầu tiên này là thể hiện là biên của mình là lớn nhất Và đồng thời là các điểm dữ liệu mình phải được đặt đúng nơi Màu đen là phải nằm ở phía bên đây, màu trắng là phải nằm ở phía bên đây W và B chính là vétter pháp tuyến của siêu phẳng chính là vétter này W là vétter pháp tuyến còn B là cái độ dịch rồi thì mục tiêu hoa là tối đa hóa margin thì nó sẽ tương đương với lại",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 7,
      "start_timestamp": "0:05:45",
      "end_timestamp": "0:06:27"
    }
  },
  {
    "page_content": "tối đa hóa margin thì nó sẽ tương đương với lại cực tiểu hóa cái thằng WB trong trường hợp mà soft margin tức là chúng ta sẽ chấp nhận cái dữ liệu của mình nó bị nhiễu thì nó ra đời là để giải quyết cái nhật điểm của H margin HMGN không cho phép có những điểm nào nằm trong khu vực này không cho phép những điểm nào nằm trong khu vực này đồng thời không cho phép điểm màu đen và trắng bên đây thì SHORTMGN sẽ thả lỏng điều kiện nó ra bằng cách nó sẽ giới thiệu thêm một Slug Variable Đây là một cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 8,
      "start_timestamp": "0:06:22",
      "end_timestamp": "0:07:12"
    }
  },
  {
    "page_content": "giới thiệu thêm một Slug Variable Đây là một cái biến ZetaI lớn hơn 0 để cho phép một số cái vi phạm nhất định Chứ không phải vi phạm hết là chết Rồi, ở đây là một số cái vi phạm nhất định thôi Thì đây là cái phương pháp chuẩn thường dùng trong thực tế Khi nói về SVM thì chúng ta sẽ nói về SoftMargin SVM Tại vì bản chất của cái dữ liệu của mình lúc nào nó cũng sẽ có nhiễu và khi đó thì công thức của mình sẽ chuyển về là trong công thức lần trước là chúng ta chỉ đi tìm mean giá trị này thôi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 9,
      "start_timestamp": "0:07:06",
      "end_timestamp": "0:08:02"
    }
  },
  {
    "page_content": "là chúng ta chỉ đi tìm mean giá trị này thôi W-bin thôi, tức là cược đại hóa, cố đa hóa cái bin thôi nhưng chúng ta sẽ phải chấp nhận để có một số điểm dư liệu nhiễu thông qua cái thành phần này, thì đây nó gọi là ở đây là cái chấp nhận cho phép là vi phạm thì bình thường là y nhân chất Wxy cộng b lớn hơn 1 thì ở đây chúng ta sẽ giảm bớt giá trị 1 này xuống chúng ta sẽ giảm bớt giá trị 1 đại lượng là zeta là giảm bớt, nếu mà lớn hơn 1 thì đây là biên cứng khi chúng ta trừ đi đại lượng zeta này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 10,
      "start_timestamp": "0:07:57",
      "end_timestamp": "0:08:44"
    }
  },
  {
    "page_content": "biên cứng khi chúng ta trừ đi đại lượng zeta này thì nó sẽ mềm hơn nó sẽ mềm hơn rồi, và zeta chính là mức độ vi phạm mức độ vi phạm cái margin của mình và c là một cái hàng số lớn hơn không node là một cái tham số cân bằng giữa cái việc đó là tìm cái siêu phẳng sao cho có cái biên là rộng nhất nhưng đồng thời là cái lỗi giảm được cái lỗi phân loại của mình Mô hình SVM trong ngự cảnh là bài toán đối ngẩu hay đuo form là xuất phát từ HatchMachine hoặc SopMachine chúng ta sẽ viết lại nó thành bài",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 11,
      "start_timestamp": "0:08:37",
      "end_timestamp": "0:09:25"
    }
  },
  {
    "page_content": "hoặc SopMachine chúng ta sẽ viết lại nó thành bài toán đối ngẩu bằng phương pháp nhân tử lạc run Trong phần trước, chúng ta tìm giá trị nhỏ nhất của một hàm Nhưng mà kỹ thuật làm sao để có thể tìm được giá trị nhỏ nhất này thì chúng ta sẽ dùng phương pháp nhân tử Lagrange đã được học trong các tán giải tích cao cấp Thì cái lợi ích đó là nó sẽ bắt đầu xuất hiện các kernel tricks để cho chúng ta có thể thực hiện được trên dữ liệu phi tuyến Và khi này nó chỉ còn mô hình của mình Mô hình của mình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 12,
      "start_timestamp": "0:09:18",
      "end_timestamp": "0:10:06"
    }
  },
  {
    "page_content": "này nó chỉ còn mô hình của mình Mô hình của mình chỉ còn phụ thuộc vào những vector support khi dùng phương pháp nhân tử Lagrang, chúng ta sẽ giới thiệu thêm các cái giá trị đó là alpha i và alpha i này tương ứng với lại các mẫu dữ liệu thứ i ví dụ mẫu dữ liệu x i ở đây thì tương ứng với nó, nó sẽ có 1 cái alpha i tương ứng Và alpha y này nói một cách nona đó là cho biết cái điểm này nó có phải là có phải là một cái support vector tham gia vào cái việc xác định cái phương trình của cái siêu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 13,
      "start_timestamp": "0:10:02",
      "end_timestamp": "0:10:55"
    }
  },
  {
    "page_content": "cái việc xác định cái phương trình của cái siêu phản hệ không thì cái công thức của mình nó được đưa về cái dạng dual form đó là maximize của cái dạng như sau là tổng của các cái alpha y trừ cho 1 phần 2 tổng của alphaE, alphaZ, eeZ và kernel thì ở đây nó sẽ xuất hiện cái khái niệm là kernel và kernel này nó sẽ là những cái công thức đã được trình bày ở trong những cái slide trước bao gồm là linear kernel, polynomial kernel, rbf kernel, sigmoid kernel và cái điều kiện đó là alphaE nhân với lại",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 14,
      "start_timestamp": "0:10:48",
      "end_timestamp": "0:11:41"
    }
  },
  {
    "page_content": "kernel và cái điều kiện đó là alphaE nhân với lại ee thì nó phải là bằng 0 cái tổng này nó phải bằng 0 và alpha phải là những cái giá trị lớn hơn hoặc bằng 0 và bé hơn hoặc bằng c thì kxi, xi chính là kernel và ở đây những cái điểm alpha y nào mà không phải là support factor ví dụ như đây thì lúc đó alpha của mình nó sẽ là bằng 0 Tức là nó không tham gia vào việc cực đại hóa này Những alpha nào lớn hơn không thì nó mới tham gia vào công thức này Những alpha lớn hơn không thì nó tương ứng chính",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 15,
      "start_timestamp": "0:11:31",
      "end_timestamp": "0:12:38"
    }
  },
  {
    "page_content": "Những alpha lớn hơn không thì nó tương ứng chính là những điểm Support Vector Tức là những Vector hỗ trợ cho việc xác định phương trình đường thẳng của mình Vì vậy, đến đây chúng ta đã được tìm hiểu qua về thuật toán SVM trong việc phân loại dữ liệu của mình ra làm 2 phần Và lưu ý ở đây là chúng ta phân loại 2 phân Và như đã đề cập ở trong những phần trước thì SVM có thể phục vụ cho bài toán là phân loại đa lớp Multi Class SVM, tức là số lớp của mình nhiều hơn 2 1 vs Res 1 và tất cả lấy ví dụ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 16,
      "start_timestamp": "0:12:34",
      "end_timestamp": "0:13:32"
    }
  },
  {
    "page_content": "mình nhiều hơn 2 1 vs Res 1 và tất cả lấy ví dụ như ở đây chúng ta có 3 tập dữ liệu 3 loại phân loại tròn, tan giác và vuông 1 vs Res là nó sẽ đi phân lớp, tức là chúng ta sẽ dùng SVM như cái bản chất ban đầu của nó, đó là một cái máy phân lớp dị phân nhưng vậy chúng ta sẽ có 3 cái mô hình 3 cái mô hình SVM mô hình đầu tiên nó sẽ giúp cho chúng ta phân loại là tròn và không phải tròn vương và tan giác chính là không phải tròn mô hình thứ 2 đó là phân... ờ xin lỗi Mô hình thứ 2, đó là tròn và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 17,
      "start_timestamp": "0:13:28",
      "end_timestamp": "0:14:32"
    }
  },
  {
    "page_content": "là phân... ờ xin lỗi Mô hình thứ 2, đó là tròn và không phải tròn Mô hình thứ 2, đó là tam giác và không phải tam giác Mô hình thứ 3, đó là vuông và không phải vuông Với mô hình 1 vs Red, nó sẽ chia không gian ra như vậy Tuy nhiên, giải pháp 1 vs Red sẽ có vấn đề Nó sẽ có những khoảng như thế này Ví dụ tại đây, chúng ta sẽ không biết đó là thuộc về tròn hay tam giác Tại đây, nó nói rằng không phải là tròn, không phải tam giác, cũng không phải là vuông Vậy thì rút cuộc nó là cái gì? Và đó chính",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 18,
      "start_timestamp": "0:14:21",
      "end_timestamp": "0:15:14"
    }
  },
  {
    "page_content": "vuông Vậy thì rút cuộc nó là cái gì? Và đó chính là điểm yếu của mô hình 1 vs Red để giải quyết vấn đề này thì chúng ta sẽ có mô hình 1vs1 1vs1 là tổ hợp 2 của k phân lớp ví dụ như đây chúng ta sẽ có phân lớp là tròn và tam giác chúng ta sẽ học rồi tròn và vuông Trong trường hợp ca bằng 3, chúng ta thấy 2 mô hình đều có số lượng phân lớp giống nhau, đó là 3 Tuy nhiên khi mà k lớn hơn 3, k bằng 4 bằng 5 thì rõ ràng cái phương pháp này nó sẽ bùng nổ số mô hình Nó sẽ bùng nổ số mô hình Điểm yếu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 19,
      "start_timestamp": "0:15:09",
      "end_timestamp": "0:15:48"
    }
  },
  {
    "page_content": "nổ số mô hình Nó sẽ bùng nổ số mô hình Điểm yếu của phương pháp này là như vậy Nhưng mà bù lại thì nó sẽ không có cái khoảng 0 như thế này Ví dụ, chúng ta có một cái điểm ở đây, thì nó nói rằng là điểm của chúng ta là hình vuông Rồi tổng kết lại, đó là SVM là một cái thực toán rất là phổ biến khi mà đặc trưng của mình là đủ tốt Nó là một cái thực toán rất là phổ biến và nó sẽ thể hiện được ưu điểm của mình trong số Trong số đó là có cái việc là tính toán rất là hiệu quả trên các tập dữ liệu lớn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 20,
      "start_timestamp": "0:15:45",
      "end_timestamp": "0:16:45"
    }
  },
  {
    "page_content": "toán rất là hiệu quả trên các tập dữ liệu lớn Nó là phổ biến và nó hiệu quả trên tập dữ liệu lớn Và một số ưu điểm đó là có thể xử lý được trong không gian có số chiều cao Tức là đặc trưng X của mình, XI của mình thuộc RRD thì cái D này có thể là con số lớn và nó tiết kiệm bộ nhớ và tính linh hoạt của nó là cao là vì nó có thể phân lớp, thường là có thể phi tuyến thì chúng ta sẽ sử dụng một kênh nó mới Còn nếu trường hợp tuyến tính thì chúng ta có thể sử dụng linear kernel Như vậy là tuyến tính",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 21,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "thể sử dụng linear kernel Như vậy là tuyến tính hay phi tuyến cũng đều có thể giải quyết được Cái thứ 2 đó là Cái nhuận điểm của nó là trong trường hợp số thuộc tính của tập dữ liệu lớn hơn số lượng dữ liệu Tức là xy thì y sẽ chạy từ 1 cho đến n Nếu như tập dữ liệu này số thuộc tính đa đa lớn hơn n, tức là số mẫu dữ liệu thì SVM sẽ có kết quả kém và chậm khi dữ liệu lớn và khó chọn kernel phù hợp trong những tình huống như thế này Đó là một số điểm yếu của mô hình SVM Hãy subscribe cho kênh",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 22,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "điểm yếu của mô hình SVM Hãy subscribe cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video hấp dẫn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=mEMcmVX3hwg",
      "filename": "mEMcmVX3hwg",
      "title": "[CS114 - Chương 8] SVM (Part 3)",
      "chunk_id": 23,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chúng ta sẽ cùng đến với một số câu hỏi thảo luận cho mô hình hồi quy tuyến tính để giúp cho chúng ta hiểu rõ hơn về cái ý nghĩa của các cái công thức và tại sao chúng ta lại có những cái hàm biến đổi như vậy. Đầu tiên thì chúng ta sẽ xét đến cái hàm mô hình của mình. Thì trong cái hàm mô hình của mình nó sẽ à có một cái thành phần nó gọi là tham số B. Thì B này á còn có một cái tên gọi đó là bias. B là viết tắt của chữ bias. Thế thì vai trò của tham số B trong mô hình sau đây là gì? Tại sao",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m_jt8-LpwLs",
      "filename": "m_jt8-LpwLs",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 1",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:57"
    }
  },
  {
    "page_content": "tham số B trong mô hình sau đây là gì? Tại sao mình lại phải có cái tham số B này? Thì ở đây có rất nhiều cách giải thích khác nhau. Chúng ta sẽ có hai cách giải thích. Cách giải thích đầu tiên á đó là chúng ta dựa trên à lý thuyết về toán và cụ thể hơn đó là chúng ta sẽ dựa trên cái lý thuyết về à phương trình đường thẳng. À đây là một phần của toán giải tích. thích. Rồi thì ở đây chúng ta sẽ thấy là trong cái công thức này thì nó sẽ có thành phần W và thành phần B là hai cái tham số cần huấn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m_jt8-LpwLs",
      "filename": "m_jt8-LpwLs",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 1",
      "chunk_id": 1,
      "start_timestamp": "0:00:53",
      "end_timestamp": "0:01:47"
    }
  },
  {
    "page_content": "W và thành phần B là hai cái tham số cần huấn luyện và nó tương ứng chính là cái hệ số của cái phương trình đường thẳng trong cái không gian Oxy của mình. Ví dụ như đây là X và trục tung là Y. Thì rõ ràng hồi xưa trong chương trình toán phổ thông chúng ta đã từng học cái công thức về phương trình đường thẳng đó là y = ax + b. y = ax + b. Thế thì ở đây là cấp độ mà trong mô hình máy học à và ở bậc đại học thì cái tham số a và b ở đây nó sẽ được thay bằng W và B. Và rõ ràng nếu như chúng ta bỏ đi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m_jt8-LpwLs",
      "filename": "m_jt8-LpwLs",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 1",
      "chunk_id": 2,
      "start_timestamp": "0:01:39",
      "end_timestamp": "0:02:37"
    }
  },
  {
    "page_content": "bằng W và B. Và rõ ràng nếu như chúng ta bỏ đi cái thành phần B này thì lúc này phương trình của chúng ta chỉ còn là y = ax. mà trong lý thuyết chúng ta đã được học thì cái phương trình y = ax thì đó chỉ là một tập hợp các cái đường thẳng mà đi qua góc tọa độ này. Điều đó có nghĩa là chúng ta sẽ không thể tùy biến cái mô hình của mình để cho nó có thể đáp ứng được rất nhiều loại dữ liệu khác nhau. Ví dụ dữ liệu của chúng ta à đi như thế này đó thì rõ ràng là cái chúng ta cần sẽ là có một cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m_jt8-LpwLs",
      "filename": "m_jt8-LpwLs",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 1",
      "chunk_id": 3,
      "start_timestamp": "0:02:32",
      "end_timestamp": "0:03:15"
    }
  },
  {
    "page_content": "thì rõ ràng là cái chúng ta cần sẽ là có một cái đường không đi qua góc tọa độ thì nó mới có thể đi xuyên qua các cái điểm ở dữ liệu của mình để mà mình xấp xỉ nó bằng một cái hàm hồi quy tuyến tính. Thì nếu ràng buộc cái B à nếu bỏ cái ràng buộc B đi thì rõ ràng là chúng ta sẽ bị giới hạn cái đường thẳng các cái đường thẳng quay xung quanh góc tọa độ dẫn đến đó là nó không thể fit được vào cái dữ liệu bất kỳ của mình. Thì đó chính là cái ý đầu tiên. Và cái cách giải thích thứ hai đó là chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m_jt8-LpwLs",
      "filename": "m_jt8-LpwLs",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 1",
      "chunk_id": 4,
      "start_timestamp": "0:03:09",
      "end_timestamp": "0:04:03"
    }
  },
  {
    "page_content": "tiên. Và cái cách giải thích thứ hai đó là chúng ta có thể sử dụng lý thuyết về mặt thông tin. rồi. Thế thì lý thuyết về thông tin ờ thì vai trò của bias ở đây là gì? Rõ ràng trong các cái mô hình dự đoán, các cái giá trị đầu ra thì chúng ta phải giả định những cái tham số, những cái biến số đầu vào. Cụ thể trong trường hợp này đó chính là cái à đặc trưng x đầu vào. Nhưng điều gì xảy ra nếu như à cái y ngã của mình tức là cái giá trị dự đoán của mình ở đây nó không có phụ thuộc vào x mà nó có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m_jt8-LpwLs",
      "filename": "m_jt8-LpwLs",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 1",
      "chunk_id": 5,
      "start_timestamp": "0:03:57",
      "end_timestamp": "0:04:44"
    }
  },
  {
    "page_content": "mình ở đây nó không có phụ thuộc vào x mà nó có thể phụ thuộc vào nhiều yếu tố khác. Lấy ví dụ trong bài toán dự đoán giá nhà, chúng ta thấy là một cái căn nhà thì nó không chỉ nó phụ thuộc vào cái thông tin về mặt diện tích mà nó còn có thể phụ thuộc vào cái các cái thông tin khác mà chúng ta không biết được. Ví dụ như lộ giới trước nhà đúng không? Hoặc có những cái mà chúng ta không thể đo lường được. Ví dụ như là à hàng xóm của mình có vui vẻ hay không, có gần cái nơi an ninh trật tự hay",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m_jt8-LpwLs",
      "filename": "m_jt8-LpwLs",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 1",
      "chunk_id": 6,
      "start_timestamp": "0:04:38",
      "end_timestamp": "0:05:40"
    }
  },
  {
    "page_content": "vẻ hay không, có gần cái nơi an ninh trật tự hay không. Đó thì cái B này nó sẽ đại diện cho những cái đặc trưng mà chúng ta không biết hoặc là chúng ta không kiểm soát được. nó sẽ đại diện cho cái thông tin mà không có à kiểm soát bởi X. Đó thì như vậy là chúng ta đã xem xét à nó ở hai cái góc độ. Đó là lý thuyết về ờ toán đúng không? Là cái phương trình đường thẳng của mình nếu bỏ đi cái thành phần B thì nó sẽ bị giới hạn là chỉ xoay xung quanh góc tọa độ và nó sẽ không thể à xấp xỉ được với",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m_jt8-LpwLs",
      "filename": "m_jt8-LpwLs",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 1",
      "chunk_id": 7,
      "start_timestamp": "0:05:35",
      "end_timestamp": "0:06:17"
    }
  },
  {
    "page_content": "góc tọa độ và nó sẽ không thể à xấp xỉ được với một cách tổng quát là không không thể. Và khi chúng ta cho thêm cái thành phần B vô thì cái phương trình đường thẳng của mình nó sẽ tùy biến hơn, nó sẽ linh động hơn để có thể fit với lại những cái dữ liệu mà không đi qua góc tọa độ. Cách giải thích thứ hai đó là chúng ta dựa trên lý thuyết về mặt thông tin. Tức là à cái thành phần bias này nó sẽ đại diện có được kiểm soát bởi cái X đầu vào thì nó sẽ dồn hết vào trong cái B. Và đương nhiên chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m_jt8-LpwLs",
      "filename": "m_jt8-LpwLs",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 1",
      "chunk_id": 8,
      "start_timestamp": "00:06:12",
      "end_timestamp": "0:06:52"
    }
  },
  {
    "page_content": "sẽ dồn hết vào trong cái B. Và đương nhiên chúng ta sẽ giả định rằng là B trong trường hợp này sau khi chúng ta huấn luyện xong thì đó là một cái à tham số cố định nó không có biến động. Trong khi x của mình đó là cái đặc trưng đầu ta thay đổi thì nó sẽ đưa đến cái giá trị y ngã của chúng ta là cái giá trị dự đoán của mình cũng sẽ thay đổi. Đó thì vì cái các cái thông tin mà chúng ta không kiểm soát được á nên chúng ta sẽ cho nó neo lại nó cố định. Còn đương nhiên về thực tế thì khi chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m_jt8-LpwLs",
      "filename": "m_jt8-LpwLs",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 1",
      "chunk_id": 9,
      "start_timestamp": "00:06:47",
      "end_timestamp": "0:07:27"
    }
  },
  {
    "page_content": "định. Còn đương nhiên về thực tế thì khi chúng ta huấn luyện với những cái tình huống phức tạp thì có thể chúng ta sẽ phải update lại cái mô hình của mình nhiều lần để cho nó có thể à giải quyết được cái vấn đề đó là à tham số X và B thì B nó có thể cập nhật được theo cái xu hướng mới. B có thể cập nhật theo cái xu hướng mới. Đối với những cái biến mà có tác động không à gọi là chúng ta không kiểm soát nó. Ừ, chúng ta không biết trước đó là cái gì nhưng nó vẫn có tác động đến cái giá của chính",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m_jt8-LpwLs",
      "filename": "m_jt8-LpwLs",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 1",
      "chunk_id": 10,
      "start_timestamp": "00:07:22",
      "end_timestamp": "0:07:46"
    }
  },
  {
    "page_content": "gì nhưng nó vẫn có tác động đến cái giá của chính là cái câu hỏi thảo luận đầu tiên về vai trò của tham số B.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=m_jt8-LpwLs",
      "filename": "m_jt8-LpwLs",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 1",
      "chunk_id": 11,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Tiếp theo sẽ là những cái nhược điểm của PCA. Thì à nó sẽ kém về cái à tính toán khi số chiều dữ liệu lớn. Thì do cái thời gian chạy của PCA là tăng rất là nhanh, tức là một cái hàm bậc 3. à thì theo cái số chiều của dữ liệu. Do đó nếu như cái chiều của dữ liệu mà của mình mà lớn tức là D mà lớn thì lúc đó cái chi phí tính toán rất là lớn. Rồi các cái thành phần chính mới thì khó diễn giải thì ở cái trong cái không gian ờ của cái dữ liệu ban đầu của mình nó được thu thập từ dữ liệu thô. Và việc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=N3ulTwujJH4",
      "filename": "N3ulTwujJH4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 5",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:55"
    }
  },
  {
    "page_content": "của mình nó được thu thập từ dữ liệu thô. Và việc từ thu thập từ dữ liệu thô thì nó sẽ gần với lại cái cách giải thích của con người. Nhưng mà khi chúng ta đưa nó về một cái không gian mới thì các cái chiều dữ liệu mới nó đã mất đi cái ý nghĩa về mặt vật lý à khiến cho chúng ta khó giải thích cái kết quả của mình. Rồi nó chỉ hoạt động tốt với các cái dữ liệu tuyến tính. PCA là một cái kỹ thuật tuyến tính do đó nó sẽ rất là kém hiệu quả khi làm việc trên những cái dữ liệu mà có cái mối quan hệ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=N3ulTwujJH4",
      "filename": "N3ulTwujJH4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 5",
      "chunk_id": 1,
      "start_timestamp": "0:00:50",
      "end_timestamp": "0:01:25"
    }
  },
  {
    "page_content": "việc trên những cái dữ liệu mà có cái mối quan hệ phi tuyến tính hay là nonlinear. Và nó sẽ có một cái khuyết điểm nữa đó là nó sẽ nhạy cảm với lại các cái giá trị ngoại lai outlier. Khi chúng ta tối ưu hóa cái phương sai thì các cái giá trị ngoại lai có thể bị sai lệch, bị sai hướng của các cái thành phần chính vì những cái giá trị ngoại lai này. Đó ví dụ như khi chúng ta à có các điểm này thì bỗng nhiên có một cái giá trị outlier ở đây. nó có thể là kéo cái thành phần chính của mình bị lệch",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=N3ulTwujJH4",
      "filename": "N3ulTwujJH4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 5",
      "chunk_id": 2,
      "start_timestamp": "0:01:18",
      "end_timestamp": "0:02:04"
    }
  },
  {
    "page_content": "thể là kéo cái thành phần chính của mình bị lệch xuống thay vì là nó phải là như thế này đó thì nó lại bị kéo lệch về phía này và do cái thời gian có hạn thì chúng ta sẽ chỉ nói về thuật toán PCA ngoài ra thì chúng ta cũng còn rất nhiều những cái phương pháp những cái kỹ thuật để giảm chiều dữ liệu khác ví dụ như đối với cái thuật toán mà về tuyến tính thì chúng ta sẽ có Principal Component Analysis tức là PCA rồi Independent Component Analysis tức là ICA rồi Factor Analysis. Còn đối với cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=N3ulTwujJH4",
      "filename": "N3ulTwujJH4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 5",
      "chunk_id": 3,
      "start_timestamp": "0:01:58",
      "end_timestamp": "0:02:39"
    }
  },
  {
    "page_content": "tức là ICA rồi Factor Analysis. Còn đối với cái nhóm thuật toán phi tuyến tính thì nó sẽ nằm trong cái projection-based. Và đặc biệt là t-SNE thì đây là một trong những cái phương pháp giảm chiều phi tuyến tính được sử dụng rất là nhiều cho cái việc là trực quan hóa dữ liệu rất là phổ biến. Đó thì khi nếu muốn trực quan hóa dữ liệu đa chiều thì chúng ta có thể sử dụng cái t-SNE. Ngoài ra thì chúng ta có thể sử dụng các cái kỹ thuật về feature selection ví dụ như là missing ratio, low variance",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=N3ulTwujJH4",
      "filename": "N3ulTwujJH4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 5",
      "chunk_id": 4,
      "start_timestamp": "0:02:33",
      "end_timestamp": "0:03:17"
    }
  },
  {
    "page_content": "ví dụ như là missing ratio, low variance filter, random forest. Thì đây là một số cái mô hình để giúp cho chúng ta xác định xem cái đặc trưng nào là cái đặc trưng quan trọng.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=N3ulTwujJH4",
      "filename": "N3ulTwujJH4",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 5",
      "chunk_id": 5,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chúng ta sẽ đến với cái mô hình khuyến nghị đầu tiên đó là content-based Recommendation system. Thì cái ý tưởng của cái gợi ý dựa trên nội dung đó là chúng ta sẽ gợi ý cái sản phẩm có cái nội dung tương tự với cái sản phẩm mà người dùng đã thích. Ví dụ à gợi ý quần áo có cùng phong cách hoặc là màu sắc. Thì đối với user khi chúng ta thực hiện cái hành vi mua một cái sản phẩm nào đó chúng ta sẽ dựa trên cái thuộc tính đã được mô tả thì cái thuộc tính này nó sẽ được cung cấp bởi cái nhà à cung cấp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=n8MHFQuTKMU",
      "filename": "n8MHFQuTKMU",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 2",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:56"
    }
  },
  {
    "page_content": "này nó sẽ được cung cấp bởi cái nhà à cung cấp cái sản phẩm này hoặc là cái trang thương mại điện tử họ sẽ nhập cái dữ liệu về cái thuộc tính. Ví dụ như đây là một cái sản phẩm mà có cái à phong cách thời trang mới với cái bộ màu sắc là màu xanh. Rồi phong cách này là theo cái phong cách là cổ trang ví dụ vậy. Thì các cái thuộc tính này sẽ được ghi nhận à như là một cái nội dung. Và chúng ta sẽ dựa trên các cái thông tin thuộc tính này để đi tìm cái similar style, tức là những cái sản phẩm có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=n8MHFQuTKMU",
      "filename": "n8MHFQuTKMU",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 2",
      "chunk_id": 1,
      "start_timestamp": "00:00:51",
      "end_timestamp": "0:01:31"
    }
  },
  {
    "page_content": "cái similar style, tức là những cái sản phẩm có cái phong cách tương tự với những cái thuộc tính của sản phẩm đã mua. Và từ đó chúng ta sẽ gợi ý cho à gợi ý cho người dùng cái sản phẩm có tính chất tương tự này. Thì cái quy trình thực hiện một cách tổng quát đó là à với cái một cái user X khi chúng ta thu thập những cái ờ đánh giá hoặc là cái đánh giá này nó có thể là cái số like của người dùng à trên mạng xã hội hoặc là cái đơn hàng mà người này đã mua đó thì chúng ta sẽ ra được cái item",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=n8MHFQuTKMU",
      "filename": "n8MHFQuTKMU",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 2",
      "chunk_id": 2,
      "start_timestamp": "00:01:26",
      "end_timestamp": "0:02:16"
    }
  },
  {
    "page_content": "này đã mua đó thì chúng ta sẽ ra được cái item profile. Thì cái like này nó có thể là à rating. Nó có thể thay bằng rating, tức là cái đánh giá hoặc là view, cái số lượt xem hoặc là cái à lượt mua, mua hàng chính thức là xuống tiền. Đó thì chúng ta sẽ ra được cái item profile là chứa những cái sản phẩm mà cái người user này họ có cái mối quan tâm cao. User X có quan tâm. Rồi sau đó thì chúng ta sẽ tiến hành xây dựng à một cái user profile dựa trên các cái thuộc tính của cái item profile. Ví dụ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=n8MHFQuTKMU",
      "filename": "n8MHFQuTKMU",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 2",
      "chunk_id": 3,
      "start_timestamp": "00:02:07",
      "end_timestamp": "0:03:05"
    }
  },
  {
    "page_content": "các cái thuộc tính của cái item profile. Ví dụ như là người này có sở thích là màu xanh, màu hồng vân vân. đó thì à các cái phong cách thời trang thì từ đó màu sắc và phong cách thời trang nó sẽ đi so khớp với lại các cái sản phẩm mà khác ở trong cái kho hàng của mình để từ đó đi recommend đi khuyến nghị cho cái người dùng X này. Thì đây là cái quy trình tổng quát. Và cái việc xây dựng cái item profile thì với mỗi một cái sản phẩm hay item thì ta sẽ có các cái hồ sơ đặc trưng. Và hồ sơ này là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=n8MHFQuTKMU",
      "filename": "n8MHFQuTKMU",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 2",
      "chunk_id": 4,
      "start_timestamp": "00:02:58",
      "end_timestamp": "0:03:45"
    }
  },
  {
    "page_content": "ta sẽ có các cái hồ sơ đặc trưng. Và hồ sơ này là một tập vector à một tập hợp hay còn gọi là một vector các cái đặc điểm nổi bật. À thì ví dụ đối với phim chúng ta sẽ có các cái đặc trưng ví dụ như là tác giả của cái bộ phim này đó tiêu đề của cái bộ phim này, diễn viên đạo diễn. Tức là đôi khi chúng ta có những cái danh sách các cái item profile này nó thể hiện rằng là cái cái user X này họ rất thích cái diễn viên ví dụ như thật Tom Cruise chẳng hạn. Tại vì tất cả những cái bộ phim mà người",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=n8MHFQuTKMU",
      "filename": "n8MHFQuTKMU",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 2",
      "chunk_id": 5,
      "start_timestamp": "00:03:37",
      "end_timestamp": "0:04:19"
    }
  },
  {
    "page_content": "hạn. Tại vì tất cả những cái bộ phim mà người này coi đều có Tom Cruise xuất hiện trong đó. Hoặc là à một cái bộ phim có cái tiêu đề chủ đề về hành động chẳng hạn. Đó thì người này thích xem cái chủ đề về hành động hoặc là người này thích xem cái chủ đề về tình cảm thì đây sẽ là những cái đặc trưng ờ hồ sơ đặc trưng đại diện cho cái user của mình. Rồi đối với văn bản thì nó sẽ có thể là tập hợp các cái từ quan trọng trong cái tài liệu của mình. Ví dụ như chúng ta có một cái đoạn văn bản về mô",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=n8MHFQuTKMU",
      "filename": "n8MHFQuTKMU",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 2",
      "chunk_id": 6,
      "start_timestamp": "00:04:12",
      "end_timestamp": "0:04:49"
    }
  },
  {
    "page_content": "Ví dụ như chúng ta có một cái đoạn văn bản về mô tả một cái bộ phim hoặc là tóm tắt về cái chức năng của một cái sản phẩm nào đó thì chúng ta sẽ có những từ không quan trọng và những từ quan trọng. Thì đối với những cái từ không quan trọng nó còn gọi là stopwords thì chúng ta sẽ loại bỏ đi. Chúng ta tìm cách là loại bỏ đi và chỉ chừa lại những cái từ quan trọng. Thì để có thể làm được cái chuyện này thì chúng ta có thể sử dụng cái mô hình à đó là TF IDF là term frequency à nhân với lại inverse",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=n8MHFQuTKMU",
      "filename": "n8MHFQuTKMU",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 2",
      "chunk_id": 7,
      "start_timestamp": "00:04:45",
      "end_timestamp": "0:05:23"
    }
  },
  {
    "page_content": "TF IDF là term frequency à nhân với lại inverse document frequency. Thì đây là một cái kỹ thuật để biến một cái văn bản của mình thành một cái vector và vector này thì nó sẽ có đánh trọng số những cái từ quan trọng. Và đối với cái hồ sơ người dùng user profile thì chúng ta có thể dựa trên cái trung bình trọng số của các cái sản phẩm mà chúng ta đã đánh giá. Rồi chúng ta có thể điều chỉnh theo cái độ lệch so với cái mức độ trung bình của người dùng. Ví dụ một cách trung bình thì người này đánh",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=n8MHFQuTKMU",
      "filename": "n8MHFQuTKMU",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 2",
      "chunk_id": 8,
      "start_timestamp": "00:05:18",
      "end_timestamp": "0:05:55"
    }
  },
  {
    "page_content": "Ví dụ một cách trung bình thì người này đánh là ba sao. Thì khi đó những cái sản phẩm, những cái bộ phim mà được đánh giá là năm sao, tức là trên trung bình rất là nhiều thì cho thấy là nó thể hiện được cái sở thích đặc biệt của người này. Thì chúng ta có thể dựa trên cái thông tin về cái việc đánh giá sản phẩm này giúp cho chúng ta biết được là cái xu hướng, cái sở thích đặc biệt của người dùng. Rồi sau đó thì chúng ta sẽ dùng cái cosine similarity giữa user profile và các cái item profile.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=n8MHFQuTKMU",
      "filename": "n8MHFQuTKMU",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 2",
      "chunk_id": 9,
      "start_timestamp": "00:05:50",
      "end_timestamp": "0:06:50"
    }
  },
  {
    "page_content": "giữa user profile và các cái item profile. User profile được xây dựng từ một cái tập hợp các cái đặc trưng của các cái item mà họ đã mua hoặc là thích hoặc là quan tâm. Đó, dựa trên cái user profile này chúng ta sẽ đi tính toán cosine similarity tính cái độ tương đồng cosine với các item profile khác và sau đó chúng ta sẽ sắp xếp theo thứ tự giảm dần để từ đó recommend cho người dùng. Thì cái ưu điểm của cái content-based filtering đó là chúng ta sẽ không cần cái dữ liệu của người khác. Tức là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=n8MHFQuTKMU",
      "filename": "n8MHFQuTKMU",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 2",
      "chunk_id": 10,
      "start_timestamp": "00:06:46",
      "end_timestamp": "0:07:21"
    }
  },
  {
    "page_content": "sẽ không cần cái dữ liệu của người khác. Tức là với mỗi người dùng, với mỗi một cái user, chúng ta chỉ quan tâm đến cái hành vi của user đó thôi, không quan tâm đến những hành vi của người dùng khác. Để tránh cái vấn đề là à thưa thớt dữ liệu. Rồi nó sẽ phù hợp cho những cái người mà có cái gu riêng. Tức là chúng ta ví dụ như chúng ta có hàng trăm hàng ngàn khách hàng nhưng mà sẽ có những cái người mà có cái gu riêng hoặc là những người mà mới lần đầu hoặc là mới tiếp cận đến cái cửa hàng của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=n8MHFQuTKMU",
      "filename": "n8MHFQuTKMU",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 2",
      "chunk_id": 11,
      "start_timestamp": "00:07:15",
      "end_timestamp": "0:07:54"
    }
  },
  {
    "page_content": "lần đầu hoặc là mới tiếp cận đến cái cửa hàng của mình thì mình sẽ sẽ khó mà có thể có đủ thông tin của người này thì nó sẽ không có đi theo cái số đông và chúng ta có thể gợi được các cái sản phẩm mới cũng như là ít phổ biến. Tại vì với những cái sản phẩm mới và cái sản phẩm ít phổ biến thì nó sẽ không đi theo cái số đông. Vì nó mới nên chưa có nhiều người tiếp cận. Chưa có nhiều người tiếp cận thì thì chúng ta sẽ không bị bias bởi những cái nhóm người dùng đông đảo và có thể giải thích được",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=n8MHFQuTKMU",
      "filename": "n8MHFQuTKMU",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 2",
      "chunk_id": 12,
      "start_timestamp": "00:07:50",
      "end_timestamp": "0:08:32"
    }
  },
  {
    "page_content": "người dùng đông đảo và có thể giải thích được cái gợi ý này của mình. Tức là các cái hệ thống khuyến nghị của mình thì không phải là chỉ cho cái độ chính xác cao mà đôi khi chúng ta còn phải có thể giải thích được à cái gợi ý. Tức là khi chúng ta gợi ý cái sản phẩm đó thì nó có thể giải thích là tại sao nó chọn sản phẩm đó để gợi ý. Ví dụ nó gợi cái sản phẩm A vì cái sản phẩm A này nó có những cái thuộc tính giống à nó có những cái thuộc tính giống với lại những cái sản phẩm mà cái user X này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=n8MHFQuTKMU",
      "filename": "n8MHFQuTKMU",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 2",
      "chunk_id": 13,
      "start_timestamp": "00:08:27",
      "end_timestamp": "0:09:04"
    }
  },
  {
    "page_content": "với lại những cái sản phẩm mà cái user X này họ đã từng mua trước đây. Nhược điểm của cái hướng tiếp cận content-based filtering đó chính là khó xác định được cái đặc điểm là phù hợp. Lấy ví dụ như đối với những cái bộ phim thì ngoài cái caption thì nội dung phim như là video thì chúng ta cũng không có khai thác được cái cái nội dung bên trong đó. Nhưng mà đương nhiên với thời điểm hiện nay với cái thành tựu của Deep Learning thì từ bộ phim, bản nhạc hoặc hình ảnh chúng ta có thể rút trích ra",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=n8MHFQuTKMU",
      "filename": "n8MHFQuTKMU",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 2",
      "chunk_id": 14,
      "start_timestamp": "00:08:58",
      "end_timestamp": "0:09:42"
    }
  },
  {
    "page_content": "nhạc hoặc hình ảnh chúng ta có thể rút trích ra được cái nội dung chính của mình à rút trích ra được cái đặc trưng một cách à dễ dàng hơn. Thay vì là trước đây chúng ta cần phải được cung cấp bởi một cái bảng tóm tắt tiêu đề à do cái nhà cung cấp phim thì bây giờ toàn bộ bộ phim có thể được à rút trích đặc trưng nhờ các cái học sâu những cái đặc trưng học sâu. Rồi đối với người dùng mới thì chúng ta sẽ khó tạo hồ sơ khi chưa có dữ liệu. Thì cái này cũng quá rõ ràng khi chúng ta mới lần đầu à",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=n8MHFQuTKMU",
      "filename": "n8MHFQuTKMU",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 2",
      "chunk_id": 15,
      "start_timestamp": "00:09:37",
      "end_timestamp": "0:10:10"
    }
  },
  {
    "page_content": "này cũng quá rõ ràng khi chúng ta mới lần đầu à người dùng mới tạo tài khoản chúng ta gần như là không có thông tin gì về người này thì đôi khi à đối với người dùng mới chúng ta có thể gợi cho họ đó là các cái sản phẩm mà thuộc à cộng đồng có tính chất cộng đồng tức là top 10 những cái sản phẩm mà được mua bán gần nhất trong cái thời gian gần đây tại vì chúng ta không có thông tin và cái cách làm content-based này thì vô hình chung nó đã khóa chặt cái gu hiện tại. Tức là nó cứ gợi ý những cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=n8MHFQuTKMU",
      "filename": "n8MHFQuTKMU",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 2",
      "chunk_id": 16,
      "start_timestamp": "00:10:07",
      "end_timestamp": "0:10:25"
    }
  },
  {
    "page_content": "cái gu hiện tại. Tức là nó cứ gợi ý những cái sản phẩm mà người này đã từng mua, đã từng quan tâm. Nó sẽ không khai phóng, nó sẽ không thoát ra được cái gu của cái người đó. Đó chỉ gợi ý quanh quẩn những cái gì mà người ta đã thích, không mở rộng sang lĩnh vực mới, không tận dụng được cái đánh giá hoặc là cái chất lượng từ cộng đồng. Thì vì cái ưu điểm cũng là cái khuyết điểm ha. Đối với cái ưu điểm thì nó không cần dữ liệu của người khác nhưng cái khuyết điểm đó là nó không khai thác được à nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=n8MHFQuTKMU",
      "filename": "n8MHFQuTKMU",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 2",
      "chunk_id": 17,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "khuyết điểm đó là nó không khai thác được à nó không tận dụng được cái đánh giá từ cộng đồng.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=n8MHFQuTKMU",
      "filename": "n8MHFQuTKMU",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 2",
      "chunk_id": 18,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chúng ta sẽ cùng đến với câu hỏi thảo luận cho mô hình à hồi quy logistic logistic. Thì ở đây chúng ta sẽ có một số câu hỏi để thảo luận. Đầu tiên đó là hàm mô hình của mình. Tại sao chúng ta không sử dụng những cái hàm đơn giản như là hàm hồi quy tuyến tính mà chúng ta lại sử dụng cái hàm khá là phức tạp như thế này là sigma của wx + b. Thế thì khi chúng ta đưa cái công thức sigma của wx + b thì cái sigma nó là một cái hàm đặc biệt. Nó đặc biệt là vì thứ nhất đó là một cái hàm để ép cái giải",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NCSIAbS9gWs",
      "filename": "NCSIAbS9gWs",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 1)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:01:03"
    }
  },
  {
    "page_content": "là vì thứ nhất đó là một cái hàm để ép cái giải giá trị của mình từ trừ vô cùng cộng vô cùng sang cái giải giá trị là từ 0 cho đến 1. À chúng ta biết rằng là wx + b thì ban đầu của mình á là nó sẽ thuộc r tức là tập số thực à từ trừ vô cùng cho đến cộng vô cùng. Nhưng mà cái việc đó thì nó sẽ không giúp cho chúng ta phân định được là cái mẫu dữ liệu của mình nó thuộc về cái lớp nào. Do đó thì chúng ta sẽ đưa qua một cái hàm sigmoid ở đây để giúp chúng ta ép cái giá trị từ trừ vô cùng cộng vô",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NCSIAbS9gWs",
      "filename": "NCSIAbS9gWs",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 1)",
      "chunk_id": 1,
      "start_timestamp": "0:00:57",
      "end_timestamp": "0:01:47"
    }
  },
  {
    "page_content": "chúng ta ép cái giá trị từ trừ vô cùng cộng vô cùng về cái đoạn giá trị từ 0 cho đến 1. Đó. Và với cái giá trị mà trả ra từ 0 cho đến 1 thì chúng ta ờ tương đồng với lại cái giá trị xác suất tương đồng với cái giá trị xác suất. Nghĩa là nó sẽ thuộc cái giải là từ 0 cho đến 1 và nó càng lớn thì nó sẽ xác suất nó thuộc về cái lớp positive là càng cao. Còn ngược lại thì nó sẽ thuộc về cái lớp âm tính, lớp negative. Trong cái công thức của cái hàm mô hình ở đây thì chúng ta sẽ thấy nó còn một cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NCSIAbS9gWs",
      "filename": "NCSIAbS9gWs",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 1)",
      "chunk_id": 2,
      "start_timestamp": "0:01:40",
      "end_timestamp": "0:02:32"
    }
  },
  {
    "page_content": "mô hình ở đây thì chúng ta sẽ thấy nó còn một cái tính chất nữa đó chính là sigmoid là một cái hàm tính đạo hàm rất là dễ và đẹp. Đó cụ thể đó là chúng ta có thể chứng minh được đạo hàm của sigmoid thì sẽ là bằng sigmoid nhân với lại 1 trừ sigmoid. Thì tại sao chúng ta có được cái công thức đẹp như thế này? Đó là nhờ cái hàm e mũ. E mũ là một cái hàm đa thức mà có thể được đảm bảo là có thể tính được cái giải giá trị từ đảm bảo được cái giải giá trị từ -1 cho đến à từ 0 cho đến 1 để đưa về cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NCSIAbS9gWs",
      "filename": "NCSIAbS9gWs",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 1)",
      "chunk_id": 3,
      "start_timestamp": "0:02:27",
      "end_timestamp": "0:03:16"
    }
  },
  {
    "page_content": "trị từ -1 cho đến à từ 0 cho đến 1 để đưa về cái không gian à xác suất. Thế thì ở đây chúng ta sẽ phân tích xem là với cái sigmoid như thế này thì nó sẽ có những cái yếu điểm gì hay không. Thì trong cái công thức này á đó là nó sẽ bé hơn hoặc bằng tổng của hai cái này thì sigmoid nhân với 1 trừ sigmoid thì nó sẽ là bằng 1 tất cả bình phương chia cho 4. Đó. Như vậy thì cái đây ví dụ như đây chính là a ha, đây là a, đây là b thì a nhân b sẽ bé hơn hoặc bằng a + b tất cả bình chia 4 thì sigmoid",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NCSIAbS9gWs",
      "filename": "NCSIAbS9gWs",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 1)",
      "chunk_id": 4,
      "start_timestamp": "0:03:09",
      "end_timestamp": "0:03:58"
    }
  },
  {
    "page_content": "hoặc bằng a + b tất cả bình chia 4 thì sigmoid cộng cho 1 - sigmoid nó chính là bằng bằng 1. Thì cái à giá trị lớn nhất của mình cho cái hàm sigmoid này đó chính là à xin lỗi đạo hàm của sigmoid này đó chính là 0.25. Và nếu như sau này các cái mô hình của mình học sâu á mà có nhiều cái hàm kích hoạt này thì có thể sẽ gây ra cái hiện tượng nó gọi là vanishing gradient. Vanishing gradient thì đây là một cái vấn đề rất là khó khăn à trong cái việc mà huấn luyện cái mô hình của mình nó có thể thực",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NCSIAbS9gWs",
      "filename": "NCSIAbS9gWs",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 1)",
      "chunk_id": 5,
      "start_timestamp": "0:03:52",
      "end_timestamp": "0:04:27"
    }
  },
  {
    "page_content": "mà huấn luyện cái mô hình của mình nó có thể thực hiện được cái bước cập nhật tham số một cách nhanh chóng thì gradient descent à nó sẽ cản trở cái việc đó. Và một trong những cái cản trở đó chính là đến từ cái hàm kích hoạt của mình.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NCSIAbS9gWs",
      "filename": "NCSIAbS9gWs",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 1)",
      "chunk_id": 6,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Trong phần thực hành đầu tiên, các bạn sẽ được giới thiệu, làm quen và sử dụng một công cụ gọi là Google Colab. Thì Google Colab là một công cụ có thể nói là cực kỳ phổ biến đối với những người vừa mới bắt đầu học Máy học (Machine Learning). Google Colab có địa chỉ ở trang web collab.research.google.com. À tuy nhiên những người làm research chuyên nghiệp hoặc là những người dùng chuyên nghiệp ừ có thể sử dụng Machine Learning để tạo ra sản phẩm á thì thường là họ không sử dụng Google Colab nhiều",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Npf5IvqeXH8",
      "filename": "Npf5IvqeXH8",
      "title": "[CS114 - Tutorial] Google Colab (Phần 1)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:59"
    }
  },
  {
    "page_content": "thì thường là họ không sử dụng Google Colab nhiều mà họ có sẽ có những công học thì Google Colab là một công cụ rất là hữu ích đối với các bạn ha. Thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Npf5IvqeXH8",
      "filename": "Npf5IvqeXH8",
      "title": "[CS114 - Tutorial] Google Colab (Phần 1)",
      "chunk_id": 1,
      "start_timestamp": "00:00:55",
      "end_timestamp": "0:01:09"
    }
  },
  {
    "page_content": "Như vậy thì ở đây chúng ta sẽ có các cái biến à viết tắt là bias ha là sẽ là bằng bias của cái neuron thứ index. Rồi cái thành phần là param 1 thì sẽ là ờ param và nó là cái chỉ số 0 rồi nhưng mà cái hidden thứ index cái neuron thứ index tương tự như vậy cho cái param 2 là param 1 và cái chỉ số của neuron là index rồi. Rồi thì cái công thức ở đây chúng ta sẽ plt.plot -1 1. Và ở đây chúng ta sẽ dùng cái công thức mà chúng ta đã đề cập hồi nãy đó là trừ của param 1 là p1 trừ cho bias. tất cả chia",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NQkR51-HyOg",
      "filename": "NQkR51-HyOg",
      "title": "[CS114 - Tutorial] Neural Network (Phần 2)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:01:33"
    }
  },
  {
    "page_content": "trừ của param 1 là p1 trừ cho bias. tất cả chia cho param 2. param 1 là chúng ta sẽ nhân với x. Rồi thế thì khi x là bằng -1, khi x là bằng -1 thì chúng ta thế qua đây thì nó sẽ là -1. Do đó thì đảo dấu này lại sẽ là bằng (p1 - B) chia cho p2. Rồi khi chúng ta thế x là bằng 1 vào thì ở đây chúng ta sẽ trừ p1 nhân với lại 1 rồi à trừ cho B là B ha. Bias ở đây là B tất cả chia cho p2 đó thì nhân với 1 thì chúng ta không cần để ở đây. Như vậy thì khi chúng ta vẽ lên trên cái biểu đồ thì nó sẽ ra",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NQkR51-HyOg",
      "filename": "NQkR51-HyOg",
      "title": "[CS114 - Tutorial] Neural Network (Phần 2)",
      "chunk_id": 1,
      "start_timestamp": "0:01:24",
      "end_timestamp": "0:02:31"
    }
  },
  {
    "page_content": "khi chúng ta vẽ lên trên cái biểu đồ thì nó sẽ ra là cái đường thẳng của mình. đó thì chúng ta thấy là các cái đường mà có cái trọng số cao á à các cái neuron mà có trọng số cao thì khi chúng ta vẽ lên chúng ta thấy là nó tạo ra các cái đường thẳng mà trong đó những cái điểm mà nằm trong cái vùng được khoanh bởi năm cái đường thẳng này thì chúng ta thấy nó là những cái điểm màu xanh lá. Còn những cái điểm mà nằm phía bên ngoài thì đó sẽ là những cái điểm màu đỏ. Đó, phía bên này là sao màu đỏ.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NQkR51-HyOg",
      "filename": "NQkR51-HyOg",
      "title": "[CS114 - Tutorial] Neural Network (Phần 2)",
      "chunk_id": 2,
      "start_timestamp": "0:02:23",
      "end_timestamp": "0:03:06"
    }
  },
  {
    "page_content": "cái điểm màu đỏ. Đó, phía bên này là sao màu đỏ. Thì ở đây cái ý nghĩa của nó đó là gì? Với mỗi một cái đường thẳng ở đây, ví dụ như cái đường màu tím ở đây thì nó là một cái bộ phân loại yếu là một cái weak classifier. Trong đó nó phân loại ở phía nửa phía trên bên trái đó là những điểm màu đỏ. Còn những cái điểm mà nằm phía bên dưới thì đó là vừa đỏ vừa xanh. Thế thì nếu như nó kết luận rằng cái weak classifier này nó kết luận rằng đó là màu đỏ thì đó chắc chắn là màu đỏ. Nhưng ngược lại nếu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NQkR51-HyOg",
      "filename": "NQkR51-HyOg",
      "title": "[CS114 - Tutorial] Neural Network (Phần 2)",
      "chunk_id": 3,
      "start_timestamp": "0:03:02",
      "end_timestamp": "0:03:43"
    }
  },
  {
    "page_content": "thì đó chắc chắn là màu đỏ. Nhưng ngược lại nếu nó không phải là màu đỏ thì nó sẽ phải cần dựa thêm cái thông tin của các cái weak classifier khác. Ví dụ như ở đây là cái đường màu à màu xanh lá. Màu xanh lá là phía trên bên tay phải là màu đỏ. Thì nếu nó kết luận như vậy thì chắc chắn nó là màu đỏ. Nhưng ngược lại thì mình sẽ không chắc nó là màu đỏ hay màu xanh. Thế thì để kết luận được nó là màu xanh á thì nó sẽ phải là hội đủ của cả năm cái weak classifier này. Đó thì nếu như nó nằm phía",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NQkR51-HyOg",
      "filename": "NQkR51-HyOg",
      "title": "[CS114 - Tutorial] Neural Network (Phần 2)",
      "chunk_id": 4,
      "start_timestamp": "0:03:37",
      "end_timestamp": "0:04:20"
    }
  },
  {
    "page_content": "weak classifier này. Đó thì nếu như nó nằm phía dưới của đường màu tím, nó nằm phía dưới của màu xanh lá, nó nằm bên tay trái của cái đường màu đỏ này, nó nằm phía trên của cái đường màu cam này, nó nằm phía trên của cái đường màu xanh lá này thì đó chính là các cái điểm màu xanh lá. Đó. Thì cái ý nghĩa của mỗi một cái neuron đó là một cái bộ phân loại yếu nhưng mà kết hợp nhiều cái bộ phân loại yếu này lại với nhau thì nó sẽ tạo ra một cái bộ phân loại mạnh. Đó thì cái output của mình nó sẽ là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NQkR51-HyOg",
      "filename": "NQkR51-HyOg",
      "title": "[CS114 - Tutorial] Neural Network (Phần 2)",
      "chunk_id": 5,
      "start_timestamp": "0:04:15",
      "end_timestamp": "0:05:03"
    }
  },
  {
    "page_content": "loại mạnh. Đó thì cái output của mình nó sẽ là một cái phân loại mạnh. Bây giờ chúng ta sẽ thử xem vẽ xem một cái neuron mà có cái trọng số thấp thì nhìn như thế nào. Thì ví dụ như ở đây chúng ta à thử với cái neuron là rồi ví dụ ở đây là 0 nè 1 2 3 4. Chúng ta sẽ thử cái neuron số 4. Đó thì chúng ta thấy đó là cái đường này nó sẽ không đi ra giữa. Nó sẽ không đi ra cái giữa khoảng không này mà nó nằm ở tuốt phía trên. Thì đây là một cái weak classifier nhưng mà nó không tốt tại vì nó vẫn còn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NQkR51-HyOg",
      "filename": "NQkR51-HyOg",
      "title": "[CS114 - Tutorial] Neural Network (Phần 2)",
      "chunk_id": 6,
      "start_timestamp": "0:04:59",
      "end_timestamp": "0:05:38"
    }
  },
  {
    "page_content": "nhưng mà nó không tốt tại vì nó vẫn còn để cái điểm màu đỏ ở đây nè. Đó để thừa một cái điểm màu đỏ các cái điểm màu đỏ ở đây. Lẽ ra nó phải kéo từ đây sang thì nó sẽ tận dụng được tốt hơn, nó sẽ phân loại tốt hơn các cái điểm màu đỏ. Rồi bây giờ chúng ta sẽ thử nghiệm bằng cách đó là cho cái à số lớp neuron này, số neuron ở lớp ẩn này giảm xuống. Ví dụ như chúng ta giảm xuống còn khoảng 4 neuron thôi thì xem điều gì sẽ xảy ra. Rồi sau đó thì chúng ta sẽ chạy lại các cái đoạn code của mình. Rồi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NQkR51-HyOg",
      "filename": "NQkR51-HyOg",
      "title": "[CS114 - Tutorial] Neural Network (Phần 2)",
      "chunk_id": 7,
      "start_timestamp": "0:05:34",
      "end_timestamp": "0:06:49"
    }
  },
  {
    "page_content": "ta sẽ chạy lại các cái đoạn code của mình. Rồi chúng ta thấy là à loss sẽ bắt đầu giảm nè. Đó còn 0.4 0.3 đó nó bắt đầu giảm nhanh hơn 0.2 0.1 0.06. Rồi chúng ta thấy là cái đường đi của nó nó khá là không có đều đúng không? Nhưng mà miễn sao là nó đi xuống là được. Rồi thì sau khi huấn luyện xong thì chúng ta sẽ xem cái trọng số của cái lớp à cuối của mình. Thì ở đây chúng ta thấy đó là các cái trọng số cao sẽ là bao gồm ba cái neuron cuối. Còn trọng số thấp là cái neuron đầu tiên. Lưu ý là ở",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NQkR51-HyOg",
      "filename": "NQkR51-HyOg",
      "title": "[CS114 - Tutorial] Neural Network (Phần 2)",
      "chunk_id": 8,
      "start_timestamp": "0:06:43",
      "end_timestamp": "0:07:28"
    }
  },
  {
    "page_content": "trọng số thấp là cái neuron đầu tiên. Lưu ý là ở đây cao hay thấp á chúng ta phải dựa trên cái giá trị tuyệt đối chứ chúng ta không có dựa trên cái con số là âm dương ha. Rồi thì ba cái thằng cuối chúng ta sẽ trực quan là 1 2 3 đó thì nó sẽ ra cái đường như thế này đúng như là nó đúng như chúng ta đã lập luận ha tức là nó những cái điểm màu xanh lá thì nó sẽ nằm ở phủ bên trong đây. Còn cái đường neuron số 0 là trọng số thấp nhất đó thì cũng được 0 1 2 3 thì có vẻ hai cái đường này là nó hơi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NQkR51-HyOg",
      "filename": "NQkR51-HyOg",
      "title": "[CS114 - Tutorial] Neural Network (Phần 2)",
      "chunk_id": 9,
      "start_timestamp": "0:07:18",
      "end_timestamp": "0:08:06"
    }
  },
  {
    "page_content": "0 1 2 3 thì có vẻ hai cái đường này là nó hơi thừa nó trùng cái vai trò của nhau thì nó chỉ cần lấy ra một cái trọng số của một cái thôi. Thì ví dụ như cái -2 này nó sẽ cho trọng số thấp để bù lại cho cái -9 hoặc là cái -10 chẳng hạn. Rồi như vậy thì chúng ta đã kết thúc cái bài hướng dẫn thực hành cho mạng Neural Network. Thì hi vọng rằng các bạn đã hiểu được cái nguyên lý tại sao mạng Neural Network có thể giải quyết được các các bài toán phức tạp phi tuyến tính. Đó là vì nó kết hợp nhiều cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NQkR51-HyOg",
      "filename": "NQkR51-HyOg",
      "title": "[CS114 - Tutorial] Neural Network (Phần 2)",
      "chunk_id": 10,
      "start_timestamp": "0:08:00",
      "end_timestamp": "0:08:23"
    }
  },
  {
    "page_content": "tạp phi tuyến tính. Đó là vì nó kết hợp nhiều cái bộ phân loại yếu để tạo ra một cái bộ phân loại mạnh hơn.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=NQkR51-HyOg",
      "filename": "NQkR51-HyOg",
      "title": "[CS114 - Tutorial] Neural Network (Phần 2)",
      "chunk_id": 11,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chúng ta sẽ cùng đến với cái ví dụ trực quan để minh họa cho thuật toán gom cụm. Thì ở bước đầu tiên đó là chúng ta sẽ có một tập hợp các cái điểm và chúng ta sẽ chọn ngẫu nhiên K cụm. Thì lưu ý đó là à ở trong cái ví dụ này thì nó đã bỏ sót cái hình ngôi sao đầu tiên ha. Tức là cái cụm đầu tiên sẽ là chọn ngẫu nhiên là hai cái điểm này, hai cái ngôi sao. Và hai điểm này cũng chính là cái mẫu dữ liệu của mình. Rồi sau đó thì chúng ta sẽ sang cái bước B đó là gán các cái điểm vào à các cái cụm.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=o7R9x8C9SSo",
      "filename": "o7R9x8C9SSo",
      "title": "[CS114 - Chương 5] Cluster - Phần 3",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:49"
    }
  },
  {
    "page_content": "bước B đó là gán các cái điểm vào à các cái cụm. Thì chúng ta thấy là cái điểm này gần cái ngôi sao này, điểm này gần ngôi sao này nhất, điểm này gần ngôi sao này nhất, điểm này gần ngôi sao này nhất. Nên cả năm cái điểm này sẽ được gom vào một cụm số 1. Rồi các cái điểm này mặc dù chúng ta thấy là rất là xa nhưng mà nếu xét về khoảng cách từ điểm này đến cái ngôi sao này và từ điểm này đến ngôi sao này hay là cái tâm cụm thứ hai thì chúng ta thấy là đến đây nó sẽ gần hơn. Do đó, điểm này sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=o7R9x8C9SSo",
      "filename": "o7R9x8C9SSo",
      "title": "[CS114 - Chương 5] Cluster - Phần 3",
      "chunk_id": 1,
      "start_timestamp": "0:00:44",
      "end_timestamp": "0:01:21"
    }
  },
  {
    "page_content": "thấy là đến đây nó sẽ gần hơn. Do đó, điểm này sẽ được gán vào cái cụm của ngôi sao thứ hai. Điểm này sẽ gần ngôi sao thứ hai hơn so với ngôi sao thứ nhất. Các cái điểm ở khu vực này thì gần với lại cái ngôi sao này hơn. Do đó, tất cả các cụm này sẽ được gán nhãn bởi số 2. Còn đây là số 1. Và khi chúng ta gán nhãn xong thì chúng ta sẽ tiến hành cập nhật lại cái tâm cụm. Thì với cái cụm số 1, chúng ta thấy là các cái điểm của mình nó xu hướng lệch về bên phải hơn một chút so với cái ngôi sao",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=o7R9x8C9SSo",
      "filename": "o7R9x8C9SSo",
      "title": "[CS114 - Chương 5] Cluster - Phần 3",
      "chunk_id": 2,
      "start_timestamp": "0:01:16",
      "end_timestamp": "0:01:56"
    }
  },
  {
    "page_content": "lệch về bên phải hơn một chút so với cái ngôi sao này. Do đó thì ở đây chúng ta sẽ dời lên đây. Và cụm số hai là rõ ràng nhất. Chúng ta thấy rất nhiều điểm nó nằm lệch về phía tay phải, chỉ có hai điểm nằm bên tay trái. Do đó cái tâm cụm mới thì nó sẽ tiến về đây. Đó, nó sẽ tiến vào đây. Đó. Thì ở đây là một cái hình minh họa là tâm cụm cũ và tâm cụm mới. Đây là tâm cụm mới. Đây là tâm cụm mới. Và sau khi chúng ta cập nhật cái tâm cụm mới thì chúng ta sẽ đi gán nhãn lại cho các điểm. Thì hồi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=o7R9x8C9SSo",
      "filename": "o7R9x8C9SSo",
      "title": "[CS114 - Chương 5] Cluster - Phần 3",
      "chunk_id": 3,
      "start_timestamp": "0:01:52",
      "end_timestamp": "0:02:43"
    }
  },
  {
    "page_content": "chúng ta sẽ đi gán nhãn lại cho các điểm. Thì hồi nãy á là hai cái điểm này à trong cái slide trước thì hai cái điểm này là nằm trong cái cụm số 2. Thì sang cái ờ lần cập nhật tiếp theo chúng ta thấy là hai cái điểm này rõ ràng là nó gần cái cụm này hơn. Điểm này sẽ gần cái cụm này hơn so với lại điểm này, so với lại cụm này. Điểm này sẽ gần cái cụm thứ nhất hơn so với lại cụm thứ hai. Do đó điểm hai cái điểm này sẽ được gán nhãn lại và nó sẽ đưa về cái cụm số một. Còn cái điểm cụm số hai nó sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=o7R9x8C9SSo",
      "filename": "o7R9x8C9SSo",
      "title": "[CS114 - Chương 5] Cluster - Phần 3",
      "chunk_id": 4,
      "start_timestamp": "0:02:39",
      "end_timestamp": "0:03:17"
    }
  },
  {
    "page_content": "về cái cụm số một. Còn cái điểm cụm số hai nó sẽ bị mất đi hai cái điểm này. Sau đó chúng ta sẽ đi cập nhật lại tâm. thì chúng ta thấy là nó sẽ dịch chuyển về bên tay phải một chút xíu. Nó sẽ dịch chuyển về đây. Và vì cái cụm số hai nó mất hai điểm này nên cái tâm nó sẽ đổ về đây. Nó sẽ đổ vào bên trong. Và sang cái bước tiếp theo là chúng ta sẽ gán à à chúng ta tính lại tâm rồi thì chúng ta gán nhãn lại. Thì chúng ta thấy là hai cái điểm này cũng không hề thay đổi cái cụm. nó vẫn tiếp tục gán",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=o7R9x8C9SSo",
      "filename": "o7R9x8C9SSo",
      "title": "[CS114 - Chương 5] Cluster - Phần 3",
      "chunk_id": 5,
      "start_timestamp": "0:03:12",
      "end_timestamp": "0:03:51"
    }
  },
  {
    "page_content": "không hề thay đổi cái cụm. nó vẫn tiếp tục gán vô cái cụm số 1. Còn các điểm ở đây thì đương nhiên là nó vẫn giữ nguyên rồi. Các cái điểm ở đây cũng không có cập nhật gì. Còn hai điểm này nè là hai cái điểm giao nó vẫn tiếp tục giữ gần với lại cái cụm thứ nhất. Như vậy thì ở vòng lặp số ba là các cái nhãn của mình nó không hề thay đổi. Chúng ta nhìn cái hình hai hình bên đây là hai cái cấu trúc cụm này không thay đổi. Tức là nó đã đạt được cái điểm dừng. à nó đã đạt được cái điểm dừng thứ nhất.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=o7R9x8C9SSo",
      "filename": "o7R9x8C9SSo",
      "title": "[CS114 - Chương 5] Cluster - Phần 3",
      "chunk_id": 6,
      "start_timestamp": "0:03:46",
      "end_timestamp": "0:04:29"
    }
  },
  {
    "page_content": "dừng. à nó đã đạt được cái điểm dừng thứ nhất. Còn đối với cái tiêu chí điểm dừng thứ hai á là chúng ta sẽ đi tính lại cái tâm cụm. Sau đó chúng ta đi so với lại cái tâm cụm trước đó. Thì chúng ta thấy là cái tâm cụm này với tâm cụm này không thay đổi. Tâm cụm này với tâm cụm này không thay đổi vị trí. Tức là nó đạt được cái điểm dừng. Cái điều kiện điểm dừng. số 2. Còn đây là điểm dừng à điều kiện dừng à số 1. Còn như đã đề cập thì cái điều kiện dừng số 3 à tức là tối ưu toàn cục cái min đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=o7R9x8C9SSo",
      "filename": "o7R9x8C9SSo",
      "title": "[CS114 - Chương 5] Cluster - Phần 3",
      "chunk_id": 7,
      "start_timestamp": "0:04:22",
      "end_timestamp": "0:05:08"
    }
  },
  {
    "page_content": "dừng số 3 à tức là tối ưu toàn cục cái min đó thì chúng ta sẽ phải chứng minh bằng công thức toán mà ở đây thì chúng ta chạy lặp đi lặp lại bằng thực nghiệm không chắc là cái kết quả này sẽ là một cái kết quả tối ưu nhất. Và ở đây thì chúng ta sẽ có cái khái niệm đó là cái độ đo khoảng cách. thì mặc dù đây là một cái độ đo rất là kinh điển nhưng mà chúng ta cũng nên nhắc lại một chút xíu thì trong cái thuật toán K-Means thì trong cái không gian Euclid trong cái không gian Euclid thì giá trị",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=o7R9x8C9SSo",
      "filename": "o7R9x8C9SSo",
      "title": "[CS114 - Chương 5] Cluster - Phần 3",
      "chunk_id": 8,
      "start_timestamp": "0:05:04",
      "end_timestamp": "0:05:50"
    }
  },
  {
    "page_content": "Euclid trong cái không gian Euclid thì giá trị trung bình của một cụm sẽ được tính bằng công thức à tức là chúng ta đang xác định cái tâm cụm nè đây là cái tâm cụm của c sẽ được tính bằng trung bình cộng nè, số phần tử trong CJ nè nhân cho tổng của các cái XY với XY là thuộc CJ. Tức là chúng ta sẽ lấy tổng tất cả các cái điểm ờ XY nằm trong cụm này thôi, nằm trong cụm này thôi chứ không lấy những cái cụm khác. Sau đó chúng ta chia trung bình. Thì đây chính là cái điểm đại diện cho ờ cái tập hợp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=o7R9x8C9SSo",
      "filename": "o7R9x8C9SSo",
      "title": "[CS114 - Chương 5] Cluster - Phần 3",
      "chunk_id": 9,
      "start_timestamp": "0:05:43",
      "end_timestamp": "0:06:31"
    }
  },
  {
    "page_content": "đây chính là cái điểm đại diện cho ờ cái tập hợp là CJ. Trong đó CJ trị tuyệt đối là số lượng điểm trong dữ liệu của cụm nè. Rồi cái công thức để tính khoảng cách giữa một điểm với một cái cụm để từ đó chúng ta so sánh xem là điểm đó gần với cụm nào nhất đó mà thì chúng ta sẽ dùng công thức này lấy một điểm với một cái cụm tâm cụm và nó sẽ được tính bằng căn của xy thành phần thứ nhất trừ cho mj thành phần thứ nhất tất cả bình. Rồi thành phần thứ hai hiệu bình phương, thành phần thứ D bình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=o7R9x8C9SSo",
      "filename": "o7R9x8C9SSo",
      "title": "[CS114 - Chương 5] Cluster - Phần 3",
      "chunk_id": 10,
      "start_timestamp": "0:06:22",
      "end_timestamp": "0:07:11"
    }
  },
  {
    "page_content": "thứ hai hiệu bình phương, thành phần thứ D bình phương và xy sẽ được gán vào cái cụm nào mà cái tâm cụm có cái khoảng cách tâm cụm của nó là gần nhất. Và ưu điểm của thuật toán K-Means đó là nó rất là đơn giản, dễ hiểu và dễ triển khai và cũng tương đối là hiệu quả khi cái độ phức tạp về mặt thời gian của nó đó là bằng T nhân K nhân N. Trong đó N là cái à số điểm dữ liệu của mình. Thật ra trong cái hệ thống này thì chúng ta dùng đó là à ban đầu thì chúng ta sẽ dùng là m đúng không? Rồi rồi thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=o7R9x8C9SSo",
      "filename": "o7R9x8C9SSo",
      "title": "[CS114 - Chương 5] Cluster - Phần 3",
      "chunk_id": 11,
      "start_timestamp": "0:07:06",
      "end_timestamp": "0:08:01"
    }
  },
  {
    "page_content": "thì chúng ta sẽ dùng là m đúng không? Rồi rồi thì ở đây là N số điểm dữ liệu. Còn ở trong các ký hiệu trước thì nó là m rồi nhưng mà thôi ở đây là mình tại cái thời điểm nào mình ký hiệu đến đâu thì mình sẽ dùng cái cái cái ký hiệu đó. K là cái số cụm được người dùng cung cấp ban đầu và T tức là cái số lần mình lặp cập nhật lại cái cái tâm cụm. Đó thì đây là cái số lần lặp để mà cập nhật cái cụm mới. Thì T này mà lặp càng nhiều tức là độ phức tạp càng cao. K cụm càng nhiều thì độ phức tạp càng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=o7R9x8C9SSo",
      "filename": "o7R9x8C9SSo",
      "title": "[CS114 - Chương 5] Cluster - Phần 3",
      "chunk_id": 12,
      "start_timestamp": "0:07:53",
      "end_timestamp": "0:08:35"
    }
  },
  {
    "page_content": "càng cao. K cụm càng nhiều thì độ phức tạp càng cao và số điểm dữ liệu của mình càng nhiều thì độ phức tạp càng cao. Tuy nhiên thì vì K và T là đều nhỏ. Tức là nếu không ở đây là nếu chúng ta chọn nha, nếu chúng ta chọn à nếu chúng ta chọn K và T đủ nhỏ thì thuật toán K-Means có thể được coi như là một thuật toán tuyến tính. Tuy nhiên thực tế khi chúng ta áp dụng K-Means trong những cái tình huống dữ liệu bây giờ thì chúng ta thấy là dữ liệu của mình cực kỳ lớn. Số cụm của mình có thể là lên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=o7R9x8C9SSo",
      "filename": "o7R9x8C9SSo",
      "title": "[CS114 - Chương 5] Cluster - Phần 3",
      "chunk_id": 13,
      "start_timestamp": "0:08:31",
      "end_timestamp": "0:09:13"
    }
  },
  {
    "page_content": "mình cực kỳ lớn. Số cụm của mình có thể là lên đến hàng ngàn. Rồi số mẫu dữ liệu có thể lên đến hàng trăm triệu mẫu dữ liệu. đó thì cái độ phức tạp của nó cũng rất là cao. Thì sau đó nó cũng đã có những cái thuật toán cải tiến của K-Means. Ví dụ như là AKM là à Approximate K-Means. Approximate K-Means thì cái thuật toán này nó sẽ nhanh hơn cái thuật toán K-Means rất là nhiều. Và K-Means như đã đề cập trước đây nó là một thuật toán phân cụm rất là phổ biến vì cái tính đơn giản và dễ dùng của nó.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=o7R9x8C9SSo",
      "filename": "o7R9x8C9SSo",
      "title": "[CS114 - Chương 5] Cluster - Phần 3",
      "chunk_id": 14,
      "start_timestamp": "0:09:07",
      "end_timestamp": "0:09:49"
    }
  },
  {
    "page_content": "phổ biến vì cái tính đơn giản và dễ dùng của nó. À tuy nhiên thì à cái cái cách mà phân cụm của nó thì nó là tối ưu cục bộ. Nó không chắc là chúng ta có thể tìm được cái điểm tối ưu toàn cục do cái tính phức tạp của cái dữ liệu, phân bố của dữ liệu. Thì nhược điểm của K-Means đó là cái nhược điểm lớn nhất chúng ta có thể thấy ngay đó là chúng ta phải chỉ định K. Làm sao chúng ta có thể biết được là trong cái dữ liệu này nó có bao nhiêu cụm? Thì cái này nó sẽ phải được dựa trên cái kinh nghiệm.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=o7R9x8C9SSo",
      "filename": "o7R9x8C9SSo",
      "title": "[CS114 - Chương 5] Cluster - Phần 3",
      "chunk_id": 15,
      "start_timestamp": "0:09:43",
      "end_timestamp": "0:10:26"
    }
  },
  {
    "page_content": "cái này nó sẽ phải được dựa trên cái kinh nghiệm. của à cái người lập trình hoặc là cái nhà khoa học à dữ liệu. Rồi thuật toán này thì chỉ được áp dụng nếu cái giá trị trung bình được xác định. Tức là đối với cái việc mà đối với dữ liệu dạng phân loại thì cái cách thức K biểu diễn bằng giá trị xuất hiện thường xuyên nhất. Tức là cụm của mình nó sẽ được tâm cụm của mình nó sẽ đại diện cho những cái điểm mà xuất hiện thường xuyên. Và thuật toán này thì nó khá là nhạy cảm với lại cái giá trị ngoại",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=o7R9x8C9SSo",
      "filename": "o7R9x8C9SSo",
      "title": "[CS114 - Chương 5] Cluster - Phần 3",
      "chunk_id": 16,
      "start_timestamp": "0:10:19",
      "end_timestamp": "0:11:03"
    }
  },
  {
    "page_content": "thì nó khá là nhạy cảm với lại cái giá trị ngoại lai giá trị outlier. Tại vì tưởng tượng là chúng ta có các cái điểm dữ liệu ở đây nhưng mà chỉ vì có một cái điểm outlier nằm ngoài đây thì nó sẽ kéo thay vì tâm cụm nó nằm ở đây thì nó sẽ kéo cái tâm cụm về đây hướng về cái hướng này dẫn đến là nó lệch khỏi những cái điểm mà có xuất hiện những cái điểm mà có cái mật độ dày đặc. Và thuật toán này thì nhạy cảm với lại cái điểm khởi tạo ban đầu. Tức là ban đầu nếu chúng ta khởi tạo à tốt thì khởi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=o7R9x8C9SSo",
      "filename": "o7R9x8C9SSo",
      "title": "[CS114 - Chương 5] Cluster - Phần 3",
      "chunk_id": 17,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "là ban đầu nếu chúng ta khởi tạo à tốt thì khởi tạo cụm tốt thì cái cụm của chúng ta nó sẽ rải đều ra. Nhưng mà nếu chúng ta khởi tạo không tốt thì nó có thể bị hiện tượng đó là nó bị kẹt, nó không thoát ra được cái cực tiểu cục bộ đó.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=o7R9x8C9SSo",
      "filename": "o7R9x8C9SSo",
      "title": "[CS114 - Chương 5] Cluster - Phần 3",
      "chunk_id": 18,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chào mừng các bạn đến với môn CS114 học máy. Hôm nay chúng ta sẽ cùng đến với một mô hình máy học rất là nổi tiếng, phổ biến và mạnh mẽ trong những khoảng thời gian trước đây là **thuật toán Support Vector Machine** hay SVM. hay là SVM, thì **Support Vector Machine** từ 2012 trở về trước trước khi Deep Learning trở nên nổi bật thì được sử dụng rất nhiều trong các nghiên cứu là vì **tính** mô hình của nó tìm ra được cái nghiệm toàn cục nó không phải là cái nghiệm cục bộ vì nó đưa về những lý",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:52"
    }
  },
  {
    "page_content": "phải là cái nghiệm cục bộ vì nó đưa về những lý thuyết toán mà trong đó việc giải và tìm ra được mô hình của mình sẽ là mô hình tốt nhất với giả định của mình **Support Vector Machine** đã tạo nên 1 trào lưu trong giai đoạn từ năm 2000 cho đến năm 2012. Từ năm 2012 trở về sau Deep Learning đã tạo ra 1 sự thay đổi trong việc mô hình có thể vừa kết hợp giữa việc rút trích đặc trưng tốt mà đồng thời là có thể phân biệt được đặc trưng đó Còn trước năm 2012 thì các đặc trưng thường là Handcrafted",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 1,
      "start_timestamp": "0:00:48",
      "end_timestamp": "0:01:38"
    }
  },
  {
    "page_content": "năm 2012 thì các đặc trưng thường là Handcrafted Feature tức là những đặc trưng mà đã được rút trích đầy đủ và có nhiều thông tin quan trọng Thì Handcrafted Feature nó sẽ chỉ cần có một bộ máy phân lớp đủ tốt Svm luôn là lựa chọn cho bộ máy đó, cho thuật toán đó Svm sẽ kết hợp với 1 handcrafted feature đủ tốt thì nó sẽ cho 1 mô hình **máy học phân loại** rất là mạnh Vậy thì điều gì đã làm nên Svm trở nên phổ biến trong 1 giai đoạn như vậy Cho đến thời điểm hiện nay Svm còn được sử dụng nhiều",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 2,
      "start_timestamp": "0:01:31",
      "end_timestamp": "0:02:22"
    }
  },
  {
    "page_content": "đến thời điểm hiện nay Svm còn được sử dụng nhiều hay không thì chúng ta sẽ cùng trả lời trong những phần tiếp theo của bài học này Đầu tiên đó là giới thiệu về SVM SVM là một thuật toán học có giám sát và được sử dụng cho bài toán phân loại nhị phân tức là chúng ta sẽ phân chia tập dữ liệu mình ra làm 2 phần ví dụ như là 0 hoặc là 1 ý tưởng chính của nó là chúng ta sẽ tìm ra 1 siêu phẳng trong trường hợp này đường màu đen này chính là 1 siêu phẳng Thì cái khái niệm siêu phẳng đó là gì thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 3,
      "start_timestamp": "0:02:19",
      "end_timestamp": "0:03:08"
    }
  },
  {
    "page_content": "phẳng Thì cái khái niệm siêu phẳng đó là gì thì chúng ta sẽ cùng đến trong những slide tiếp theo Và cái siêu phẳng này nó sẽ phân tách dữ liệu của mình ra thành các lớp Svm có rất nhiều ứng dụng trong thực tế trong những khoảng thời gian trước đây và thậm chí bây giờ nếu như cái đặc trưng của mình được sử dụng để phân loại mà đủ tốt thì SVM vẫn tỏ ra rất là hiệu quả một số cái tình huống sử dụng thuật toán SVM, ví dụ như là trong bài toán phân loại văn bản lọc email là spam hay không phải là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 4,
      "start_timestamp": "0:02:56",
      "end_timestamp": "0:03:44"
    }
  },
  {
    "page_content": "loại văn bản lọc email là spam hay không phải là spam, có hại hay không có hại phân loại xem cái chủ đề tin tức của cái tài liệu của mình là thuộc về chủ đề nào Ví dụ như thuộc chủ đề về chính trị, văn hóa, xã hội, khoa học, v.v. Rồi **nhận diện** chữ viết tay, phân loại chó mèo, v.v. Thì đây là những ứng dụng của thuật toán SVM khi mà chúng ta đã có được những đặc trưng đủ tốt. Và ở đây chúng ta sẽ đến với những khái niệm cơ bản trong thuật toán SVM. Đầu tiên đó chính là cái khái niệm siêu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 5,
      "start_timestamp": "0:03:40",
      "end_timestamp": "0:04:25"
    }
  },
  {
    "page_content": "toán SVM. Đầu tiên đó chính là cái khái niệm siêu phẳng. thì đây là một cái không gian con có nhiều, có số chiều nhỏ hơn 1 so với lại không gian đặc trưng chứa dữ liệu ví dụ như trong không gian đặc trưng của mình là có 2D là không gian 2 chiều thì siêu phẳng của mình lúc này nó sẽ là một cái đường thẳng tại vì nó chỉ có 1 chiều là một cái đường thẳng và không gian của mình nếu là không gian 3 chiều, thì siêu phẳng của chúng ta lúc này sẽ là một cái mặt phẳng và tổng quát lên trong không gian",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 6,
      "start_timestamp": "0:04:18",
      "end_timestamp": "0:05:15"
    }
  },
  {
    "page_content": "cái mặt phẳng và tổng quát lên trong không gian mà n chiều thì siêu phẳng của mình sẽ có n trừ 1 chiều Mục tiêu của SVM là tìm ra một siêu phẳng tối ưu **sao** cho khoảng cách từ siêu phẳng đến điểm gần nhất của hai lớp dữ liệu Biên của mình là lớn nhất, tức là khoảng cách từ siêu phẳng đến điểm gần nhất là lớn nhất **Chi tiết** chúng ta sẽ được tìm hiểu trong những slide tiếp theo Khái niệm tiếp theo trong SVM chính là khái niệm về lề, là khoảng lề hay là margin là cái khoảng cách giữa siêu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 7,
      "start_timestamp": "0:05:07",
      "end_timestamp": "0:05:49"
    }
  },
  {
    "page_content": "lề hay là margin là cái khoảng cách giữa siêu phẳng phân tách các điểm dữ liệu gần nhất thuộc 2 lớp Lấy ví dụ như ở đây chúng ta có một cái siêu phẳng tối ưu là cái đường ở **giữa** thì cái khoảng cách giữa cái điểm gần nhất đây là hai cái điểm gần nhất đối với cái siêu phẳng này đây là cái điểm gần nhất đối với cái siêu phẳng này thì khoảng cách từ cái lề bên trái sang cái lề bên phải nó chính là cái margin và mục tiêu của SVM đó là làm sao để có thể tìm được cái margin này là lớn nhất Tại sao",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 8,
      "start_timestamp": "0:05:46",
      "end_timestamp": "0:06:27"
    }
  },
  {
    "page_content": "thể tìm được cái margin này là lớn nhất Tại sao margin lớn nhất? Chúng ta sẽ cùng tìm hiểu trong phần tiếp theo Nhưng mà cái lý do, một cách ngắn gọn đó là khi margin lớn thì mô hình của mình nó sẽ có khả năng tổng quát hóa cao hay là **tính generalization** Và **tính** tổng quát hóa cao này sẽ giúp chúng ta giảm nguy cơ bị hiện tượng đó là quá khớp dữ liệu hay gọi là overfitting Rồi, chúng ta sẽ cùng đến với cái khái niệm nữa cũng rất quan trọng đó chính là Support Vector, tức là những",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 9,
      "start_timestamp": "0:06:23",
      "end_timestamp": "0:07:03"
    }
  },
  {
    "page_content": "trọng đó chính là Support Vector, tức là những **Vector hỗ trợ** thì đây là những điểm dữ liệu đây là những điểm dữ liệu mà nó nằm gần nó nằm gần cái siêu phẳng phân tách ví dụ như ở đây là cái siêu phẳng để phân tách thì các điểm mà nằm gần cái siêu phẳng này đó chính là những điểm này thì đây là những cái điểm Support Vector hay là những **Vector hỗ trợ** tại sao được gọi nó là những **Vector hỗ trợ**? tại vì những cái điểm này nó sẽ có cái vai trò rất là quan trọng trong việc là ảnh hưởng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 10,
      "start_timestamp": "0:06:55",
      "end_timestamp": "0:07:32"
    }
  },
  {
    "page_content": "vai trò rất là quan trọng trong việc là ảnh hưởng trực tiếp đến cái mô hình của mình nếu như cái điểm này mà dịch chuyển đi lên hoặc đi xuống thì nó sẽ khiến cho cái việc là cập nhật lại cái **Decision Boundary** tức là cái siêu phẳng này dịch chuyển theo còn những cái điểm mà không phải là Support Vector ví dụ như những cái điểm ở bên ngoài như thế này thì cho dù nó có dịch chuyển lên xuống, dịch chuyển đi đâu trong cái không gian này đi chăng nữa thì nó cũng không ảnh hưởng đến cái siêu phẳng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 11,
      "start_timestamp": "0:07:27",
      "end_timestamp": "0:08:03"
    }
  },
  {
    "page_content": "thì nó cũng không ảnh hưởng đến cái siêu phẳng này còn những cái điểm mà nằm ở sát với lại cái **Boundary** này đó là 3 cái điểm này thì chỉ cần nó dịch chuyển vào trong hoặc là nó đi ra ngoài thì lập tức là cái đường bao này cũng sẽ được cập nhật theo do đó cái vai trò của những cái **Support Vector**, những cái điểm dữ liệu hỗ trợ những cái **Vector hỗ trợ** rất là quan trọng và đó chính là cái lý do tại sao chúng ta có cái tên đó là **Support Vector Machine** tức là cái máy mà được tạo bởi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 12,
      "start_timestamp": "0:07:58",
      "end_timestamp": "0:08:47"
    }
  },
  {
    "page_content": "Vector Machine** tức là cái máy mà được tạo bởi các cái **support vector** Thế thì chúng ta sẽ cùng minh họa cả 3 cái khái niệm mà chúng ta đã nói ở trên trong cùng một cái tấm hình thì cái siêu phẳng phân lớp siêu phẳng phân lớp chính là cái đường ở giữa ở đây và margin margin chính là cái khoảng cách từ cái biên trái sang cái biên phải trong đó biên trái sẽ là những cái biên gần nhất mà cái siêu phẳng nó chạm vào cái điểm màu đỏ Bên phải là siêu phẳng gần nhất **song song** với siêu phẳng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 13,
      "start_timestamp": "0:08:39",
      "end_timestamp": "0:09:24"
    }
  },
  {
    "page_content": "siêu phẳng gần nhất **song song** với siêu phẳng **phân lớp** ở đây và nó chạm vào những điểm màu xanh thì khoảng cách từ biên trái sang biên phải sẽ gọi là margin và các điểm nằm trên các biên trái và biên phải này là support vector thì đây là hình ảnh để minh họa cho cả 3 khái niệm mà chúng ta đã nói ở trên thế thì đối với dữ liệu, chúng ta sẽ xem xét một tình huống đơn giản trước đó là dữ liệu của mình, nó có một mối quan hệ tuyến tính hay là Linear Data thì đây là dữ liệu có thể phân tách",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 14,
      "start_timestamp": "0:09:18",
      "end_timestamp": "0:09:58"
    }
  },
  {
    "page_content": "Linear Data thì đây là dữ liệu có thể phân tách được bởi 1 siêu phẳng thì chúng ta nhìn cái hình này chúng ta thấy một cách trực quan thì chúng ta thấy là có thể phân chia được ra làm 2 phần bằng 1 đường thẳng thì phân lớp SVM trong dữ liệu **tuyến tính** cho tập dữ liệu gồm 2 lớp ví dụ như ở đây chúng ta có 2 điểm dữ liệu là màu xanh và màu đỏ thì thuật toán phân lớp SVM trong dữ liệu **tuyến tính** này là nó sẽ đi tìm 1 siêu phẳng để có thể **tách** nó ra làm 2 tức là chúng ta sẽ phải đi tối",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 15,
      "start_timestamp": "0:09:55",
      "end_timestamp": "0:10:32"
    }
  },
  {
    "page_content": "nó ra làm 2 tức là chúng ta sẽ phải đi tối ưu để làm sao tìm ra được một cái đường phân lớp ra làm hai phần một cái mặt phẳng, một cái siêu phẳng để phân cái tập điểm này ra làm hai phần thế thì ở đây chúng ta sẽ có một số cái ý tưởng trong cái việc là chọn cái đường để có thể phân tách ra làm hai thì chúng ta sẽ có cái giải pháp A cái giải pháp A này là một cái giải pháp mà chúng ta có thể dễ dàng thấy được rằng là nó có thể **tách** tập điểm màu xanh và màu đỏ ra làm 2 một cách dễ dàng và với",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 16,
      "start_timestamp": "0:10:29",
      "end_timestamp": "0:11:03"
    }
  },
  {
    "page_content": "xanh và màu đỏ ra làm 2 một cách dễ dàng và với cái giải pháp này thì chúng ta thấy là nó hoàn toàn có thể chia ra rất là tốt, có thể phân loại các điểm màu đỏ và màu xanh rất là tốt tuy nhiên, cái việc kết luận một cái giải pháp là tốt hay xấu thì sự khác biệt nó sẽ nằm sự khác biệt của nó là nó nằm ở cái mẫu **dữ liệu** mới chứ nó không phải nằm trên những mẫu **dữ liệu** cũ Tại vì nó sẽ kiểm tra xem cái giải pháp của chúng ta nó có tính **tổng quát** hay không, ổn định hay không Thì bây giờ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 17,
      "start_timestamp": "0:10:59",
      "end_timestamp": "0:11:45"
    }
  },
  {
    "page_content": "quát** hay không, ổn định hay không Thì bây giờ chúng ta sẽ có một cái mẫu dữ liệu mới, đây là cái điểm dữ liệu mới Vậy thì cái điểm dữ liệu mới này với cái giải pháp A thì nó sẽ phân vào cái lớp nào Thì với giải pháp A nó sẽ xếp cái dữ liệu này là màu đỏ tại vì nó nằm cùng phía Nó nằm về cùng một phía với các điểm màu **đỏ** trong cái tập dữ liệu **Train** Đây chính là các tập dữ liệu huấn luyện và đây là mẫu dữ liệu mới Bây giờ chúng ta sẽ cùng xem xét đến một giải pháp tiếp theo, đó là giải",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 18,
      "start_timestamp": "0:11:40",
      "end_timestamp": "0:12:26"
    }
  },
  {
    "page_content": "xem xét đến một giải pháp tiếp theo, đó là giải pháp B như thế này Và chúng ta cũng dễ dàng thấy là cái giải pháp B cũng giúp cho chúng ta chia hai tập màu xanh và màu đỏ ra làm 2 phần Và cũng với cái mẫu dữ liệu mới mà chúng ta đã đề cập trong những slide trước là cái điểm này trong quan điểm của **giải pháp** B, dữ liệu mới này sẽ được phân ra là màu xanh tại vì nó sẽ nằm về cùng phía với tập điểm màu xanh ở bên này Vậy thì giữa 2 **giải pháp** A và B, chúng ta đặt 2 **giải pháp** A và B nằm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 19,
      "start_timestamp": "0:12:22",
      "end_timestamp": "0:13:08"
    }
  },
  {
    "page_content": "A và B, chúng ta đặt 2 **giải pháp** A và B nằm chung với nhau chúng ta sẽ thấy 2 **giải pháp** này **giải pháp** nào là hiệu quả hơn Đó là 2 giải pháp A và B Giải pháp nào hiệu quả hơn Nó sẽ dựa trên việc phân loại điểm **dữ liệu** mới của mình có hiệu quả hay không Bằng một cách trực quan và trực giác, chúng ta thấy là điểm mới này nằm gần phía sau những điểm màu đỏ hơn Nằm gần phía sau những điểm màu đỏ hơn do đó thì một cách trực quan và trực giác thì cái điểm này lẽ ra nó nên là điểm màu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 20,
      "start_timestamp": "0:12:58",
      "end_timestamp": "0:13:43"
    }
  },
  {
    "page_content": "giác thì cái điểm này lẽ ra nó nên là điểm màu đỏ lẽ ra nó nên là cái điểm màu đỏ thì nó sẽ phù hợp hơn tại vì nó nằm về gần với lại các điểm này thì rõ ràng giữa 2 cái giải pháp A và B chúng ta thấy giải pháp A nó giúp cho chúng ta phân loại cái điểm này thành cái điểm màu đỏ do đó một cách trực quan, một cách trực giác thì giải pháp A là một cái giải pháp tốt và b là giải pháp không tốt Vậy thì tiêu chí nào để giúp chúng ta chọn lựa được đường phân tách tốt hơn so với giải pháp còn lại thì ở",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 21,
      "start_timestamp": "0:13:39",
      "end_timestamp": "0:14:37"
    }
  },
  {
    "page_content": "phân tách tốt hơn so với giải pháp còn lại thì ở đây chúng ta sẽ sử dụng khái niệm Margin là đường biên Thế thì chúng ta sẽ cùng xem xét lại khái niệm Margin trong giải pháp a này đối với giải pháp A này thì margin của chúng ta sẽ là từ **biên** là trái sang **biên** là phải giống như trên hình đây thì tại sao cái này là **biên**? tại vì chúng ta thấy từ giải pháp A tức là đường hyperplane A này chúng ta nới ra 2 bên và khi chúng ta chạm đến 1 điểm dữ liệu đầu tiên thì đó chính là **biên** và ở",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 22,
      "start_timestamp": "0:14:32",
      "end_timestamp": "0:15:25"
    }
  },
  {
    "page_content": "dữ liệu đầu tiên thì đó chính là **biên** và ở đây nó chạm đến điểm màu đỏ này đầu tiên đối xứng đối hình với **đường này** là bên phải và đối xứng bên tương ứng là bên trái đối xứng qua lại là margin của giải pháp A là khoảng cách màu xanh này chúng ta sẽ cùng xem xét giải pháp B chúng ta sẽ đi **song song**, **tịnh tiến song song** siêu **phẳng** này về 2 phía khi nó chạm đến điểm **dữ liệu** đầu tiên đó chính là một cái **biên** của **giải pháp** B lấy đối xứng qua, chúng ta sẽ tính được",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 23,
      "start_timestamp": "0:15:20",
      "end_timestamp": "0:16:08"
    }
  },
  {
    "page_content": "pháp** B lấy đối xứng qua, chúng ta sẽ tính được khoảng cách màu xanh lá này rõ ràng chúng ta thấy khoảng cách xanh lá này rất là bé tức là cái **biên** này rất là nhỏ, rất là hẹp Vậy thì đối chiếu giữa 2 giải pháp A và giải pháp B thì chúng ta thấy giải pháp A cho biên lớn hơn giải pháp B Như vậy, hồi nãy chúng ta đã kết luận rằng A tốt hơn B thì tương ứng biên của nó cũng sẽ lớn hơn **so** với B Như vậy, một cách **tổng quát** là làm sao chúng ta có thể tìm được một đường phân tách để cho nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 24,
      "start_timestamp": "0:16:03",
      "end_timestamp": "0:16:23"
    }
  },
  {
    "page_content": "ta có thể tìm được một đường phân tách để cho nó có margin lớn nhất Đây là một ví dụ cho tình huống tối ưu nhất của mình Khi đó, cái **biên** của mình là cực đại Và các điểm mà chúng ta vừa tiếp xúc đến hai lề trái và lề phải nó chính là các support vector mà chúng ta đã đề cập trong những slide trước thì chúng ta để 2 giải pháp A và B khi chúng ta nới ra khi nó chạm đến điểm đầu tiên thì hình như nó chỉ chạm về 1 phía là hoặc màu đỏ hoặc là màu xanh ví dụ như trong tình huống này nó đã chạm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 25,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "xanh ví dụ như trong tình huống này nó đã chạm đến điểm màu đỏ trước còn lề trái nó không chạm đến điểm màu xanh Tức là nó chưa thật sự tối ưu Còn cái đường phân tách tối ưu của mình đó là khi chúng ta nới ra hai bên Thì nó vừa chạm được đến cái điểm màu đỏ thì đồng thời nó cũng sẽ chạm được đến cái điểm màu xanh Nó chạm đến cái điểm này Thì nó cũng đồng thời nó sẽ chạm được đến các cái điểm này Đây là một cái mẹo để cho chúng ta biết là đường **phân tách** đã tối ưu hay chưa",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OKUHDwcQuJo",
      "filename": "OKUHDwcQuJo",
      "title": "[CS114 - Chương 8] SVM (Part 1)",
      "chunk_id": 26,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Cuối cùng đó là chúng ta sẽ khai thác cái nhãn chúng ta đã có. Tuy nhiên cái việc khai thác này nó không phục vụ cho việc huấn luyện mà chỉ để là một cái tài liệu là một cái thông tin để chúng ta có thể tham khảo cho cái việc là gom nhóm nó hiệu quả như thế nào. Thì ở đây chúng ta sẽ vẽ đó là sử dụng à nhãn của nhãn i ha. để xem coi là cái nhãn đúng của nó thì nó có xấp xỉ với nhau, có gần nhau ở trong cái không gian mà đã giảm chiều hay không. Đương nhiên là chúng ta sẽ không thể kỳ vọng là tất",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OlCGL3uIcn8",
      "filename": "OlCGL3uIcn8",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 4)",
      "chunk_id": 0,
      "start_timestamp": "0:00:14",
      "end_timestamp": "0:01:19"
    }
  },
  {
    "page_content": "nhiên là chúng ta sẽ không thể kỳ vọng là tất cả các cái mẫu dữ liệu này của mình nó đều đúng được. đó tại vì chúng ta là học trên không có giám sát rồi. Rồi thì trong cái cái chương trình này thì chúng ta thấy nè cái X của t-SNE à X của t-SNE chúng ta sẽ lấy hai cái thành phần tọa độ là thành phần thứ 0 và thứ 1 tương ứng là trục x và trục y chúng ta vẽ lên. Rồi màu á thì chúng ta sẽ dùng cái nhãn gốc tức là cái cái ground của mình để chúng ta vẽ lên. Thì chúng ta thấy là qua cái thuật toán",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OlCGL3uIcn8",
      "filename": "OlCGL3uIcn8",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 4)",
      "chunk_id": 1,
      "start_timestamp": "0:01:10",
      "end_timestamp": "0:02:11"
    }
  },
  {
    "page_content": "vẽ lên. Thì chúng ta thấy là qua cái thuật toán t-SNE thì những cái điểm nào mà có cùng một cái ờ cùng một cái nhãn thì nó sẽ nằm ở gần nhau. Chứng tỏ đó là thuật toán t-SNE nó đã giảm chiều rất là tốt và khi đưa lên cái không gian hai chiều này nó vẫn giữ được cái tính chất đó là những cái điểm nào mà có cùng một cái label thì sẽ có cùng màu và nó sẽ nằm trong gần một cái cụm. Đâu đó chúng ta vẫn sẽ thấy là những cái điểm trong cái cụm màu hồng nè thì có những cái điểm màu xanh, xanh dương,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OlCGL3uIcn8",
      "filename": "OlCGL3uIcn8",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 4)",
      "chunk_id": 2,
      "start_timestamp": "0:02:04",
      "end_timestamp": "0:02:46"
    }
  },
  {
    "page_content": "nè thì có những cái điểm màu xanh, xanh dương, xanh lá, xanh màu da xanh da trời vân vân thì nó lọt vào đây. Nhưng cái số đó khá là ít. Còn đâu đó là các cái điểm của mình đại đa số vẫn sẽ là những cái điểm có cùng màu thì nó cũng phản ánh được cái việc đó là các cái thuật toán gom cụm của mình nó đã tỏ ra hiệu quả. trong cái việc là đưa những cái ảnh mẫu ảnh nào mà có cùng một cái tính chất vào cùng một cái cụm ví dụ như là cụm số 5 chúng ta thấy là khá là giống nhau đó. Rồi cụm số 6 cũng vậy.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OlCGL3uIcn8",
      "filename": "OlCGL3uIcn8",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 4)",
      "chunk_id": 3,
      "start_timestamp": "0:02:39",
      "end_timestamp": "0:03:25"
    }
  },
  {
    "page_content": "là khá là giống nhau đó. Rồi cụm số 6 cũng vậy. Nó có cái con số 7 này thì nó viết bị lỗi nên nhìn nó vẫn có thể là giống con số 4 hoặc cũng có thể là giống con số 9. Thì mặc dù là nó sai nhưng mà có thể là cái sai này là chấp nhận được và dễ hiểu.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=OlCGL3uIcn8",
      "filename": "OlCGL3uIcn8",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 4)",
      "chunk_id": 4,
      "start_timestamp": "0:03:21",
      "end_timestamp": "0:03:27"
    }
  },
  {
    "page_content": "Chủ đề, học sâu, machine learning, logistic regression, CNN, ImageNet. Bỏ lỡ đi một cơ hội để cây quyết định có thể có một số nút sâu hơn sẽ giảm giá trị lỗi nhiều hơn. Như vậy, chúng ta có thể kết luận là chỉ tiêu chí dừng có thể không tạo ra các cây có hiệu suất cao. Một tình huống cụ thể đó là việc chúng ta dừng phân chia các vùng khi giá trị residual sum square, hay còn gọi là giá trị lỗi, nó không giảm nhiều trong một bước tuy nhiên, các bước tiếp theo có thể giảm nhiều một cách đột ngột và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:12"
    }
  },
  {
    "page_content": "tiếp theo có thể giảm nhiều một cách đột ngột và việc chúng ta dừng sớm đôi khi sẽ bỏ lỡ việc chúng ta sẽ cải thiện hiệu suất phân loại hoặc là hồi quy thì như vậy chúng ta có thể có một ý tưởng đó là chúng ta cứ phát triển một cây T0 nào đó rất là lớn đi Và sau đó chúng ta sẽ tiến hành cắt tỉa nó để thu được một cây con Thì ở đây chúng ta có thể hiểu đơn giản là cái việc phân chia các vùng chúng ta chỉ đơn giản là đi tìm các cây thôi Và khi chúng ta xây dựng cây thì chúng ta có thể có nhiều",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 1,
      "start_timestamp": "0:01:07",
      "end_timestamp": "0:01:51"
    }
  },
  {
    "page_content": "ta xây dựng cây thì chúng ta có thể có nhiều cách mà Đầu tiên thì chúng ta có thể xây dựng và chúng ta dừng và chúng ta dừng hoặc là chúng ta cứ xây dựng cho nó sâu đây và chúng ta sẽ cắt sau Tiếp theo, chúng ta sẽ tích hợp cái ý tưởng của việc cắt tỉa cây vào cái mục tiêu gốc của chúng ta đã đề cập ở các slide trước Ở đây, chúng ta sẽ có số nút lá trong một cây hay còn gọi là các vùng quyết định trong một cây ứng với từng điểm dữ liệu trong một nút lá hay cái vùng đó thì chúng ta sẽ tính toán",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 2,
      "start_timestamp": "0:01:49",
      "end_timestamp": "0:02:37"
    }
  },
  {
    "page_content": "nút lá hay cái vùng đó thì chúng ta sẽ tính toán độ lỗi để mà chúng ta trừng phạt số lượng nút lá trong một cây càng nhiều thì độ lỗi càng cao thì chúng ta sẽ có một cái siêu tham số alpha ở đây alpha bằng 0 của chúng ta có nghĩa là nó sẽ giống như là cái độ lỗi của bài toán ban đầu Trường hợp ngược lại, alpha là một con số rất là lớn, thì có nghĩa là cái cây của chúng ta khả năng sẽ có rất ít nút lá, nó sẽ tạo ra một cái cây đơn giản Nếu như vậy, ý tưởng tìm ra cây con ở đây thì chúng ta có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 3,
      "start_timestamp": "0:02:29",
      "end_timestamp": "0:03:33"
    }
  },
  {
    "page_content": "vậy, ý tưởng tìm ra cây con ở đây thì chúng ta có thể có nhiều cây con Mỗi cây con như vậy thì chúng ta sẽ đi đánh giá hiệu suất hồi quy của nó bằng phương pháp kiểm định chéo (cross-validation) Chúng ta sẽ có một minh họa về việc chọn kích thước của cây Chúng ta sẽ có tree size là số lượng nút lá trong một cây Chúng ta sẽ có giá trị tree size bằng 3 Vì vậy thì chúng ta thực sự không biết là bao nhiêu nút lá là tốt ha Cho nên là chúng ta sẽ tiến hành thử nghiệm Ở đây thì chúng ta sẽ có cái cây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 4,
      "start_timestamp": "0:03:16",
      "end_timestamp": "0:04:42"
    }
  },
  {
    "page_content": "hành thử nghiệm Ở đây thì chúng ta sẽ có cái cây quyết định khá là phức tạp Ở bên trái có rất là nhiều nút lá Và ứng với mỗi cây con như vậy á Chẳng hạn chúng ta sẽ có một cái cây có kích thước là 2 đây, đây là 3 này, đây là 4 này Thì chúng ta có thể quan sát hiệu suất của kiểm định chéo như sau Thì chúng ta sẽ thấy kiểm định chéo ở đây có hiệu suất mean square error, giảm dần, đáng kể mà số nút lá bằng 3 thì hiệu suất kiểm định chéo không còn được cải thiện nữa cho nên chúng ta có thể coi cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 5,
      "start_timestamp": "0:04:35",
      "end_timestamp": "0:05:39"
    }
  },
  {
    "page_content": "cải thiện nữa cho nên chúng ta có thể coi cái cây mà có số nút lá là 3 ở đây là một cái cây tốt nó đơn giản để có hiệu suất tốt như vậy là sau khi quá trình thực nghiệm thì chúng ta có được một cái cây hồi quy đơn giản nhưng mà lại có hiệu suất kiểm định chéo cao Trong phần trước, chúng ta đã tìm hiểu về cách xây dựng cây quyết định cho bài toán hồi quy Tiếp theo, chúng ta sẽ tìm hiểu cách xây dựng cây quyết định cho bài toán phân loại Tương tự với cây hồi quy, thì đầu tiên chúng ta sẽ chia",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 6,
      "start_timestamp": "0:05:32",
      "end_timestamp": "0:06:30"
    }
  },
  {
    "page_content": "tự với cây hồi quy, thì đầu tiên chúng ta sẽ chia không gian đặc trưng ban đầu thành các vùng riêng biệt, cụ thể là thành z vùng riêng biệt hay nói cái khác là chúng ta tìm ra một cây quyết định có z nút lá Sau khi chúng ta tìm ra được cây quyết định, thì đối với mỗi mẫu dữ liệu có thể thuộc tập huấn luyện hay không, một cái vùng nút nào đó, thì chúng ta sẽ đưa ra quyết định bằng cách là chúng ta sẽ đi tìm ra cái lớp mà xuất hiện nhiều nhất trong cái vùng đó. thì cụ thể hơn chúng ta có thể tính",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 7,
      "start_timestamp": "0:06:06",
      "end_timestamp": "0:07:14"
    }
  },
  {
    "page_content": "cái vùng đó. thì cụ thể hơn chúng ta có thể tính xác suất xuất hiện của một lớp thuộc về vùng đó. Cụ thể hơn thì cái việc phân tách không gian đặc trưng của chúng ta ứng với bài toán hồi quy, thì chúng ta dựa trên độ lỗi là residual sum square cho bài toán hồi quy tuyến tính. Đó là một tiêu chí để chúng ta phân chia thành các vùng tốt hơn Tuy nhiên, đối với lại bài toán phân loại thì chúng ta sẽ có 2 chỉ số khác đó là chỉ số Gini và chỉ số entropy 2 chỉ số này có thể đánh giá được một cây có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 8,
      "start_timestamp": "0:07:07",
      "end_timestamp": "0:08:17"
    }
  },
  {
    "page_content": "2 chỉ số này có thể đánh giá được một cây có phân loại tốt hay là không Cụ thể hơn, chỉ số Gini được định nghĩa bằng công thức sau Giả sử chúng ta sẽ có một vùng dữ liệu cố định Thì ở đây chúng ta có thể hình dung là vùng dữ liệu đó bao gồm 3 điểm dữ liệu ứng với lại bài toán phân loại X và O Các hạng mục này là 2 chẳng hạn, chúng ta sẽ có 2 lớp Khi thế vào công thức Gini, chúng ta sẽ có lớp thứ nhất là lớp O Xác suất xuất hiện của nó là 1 chẳng hạn, thành phần thứ nhất là 0 Lớp thứ 2 là lớp X,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 9,
      "start_timestamp": "0:08:10",
      "end_timestamp": "0:09:13"
    }
  },
  {
    "page_content": "hạn, thành phần thứ nhất là 0 Lớp thứ 2 là lớp X, chúng ta sẽ thay thế xác suất xuất hiện của nó vào Tương tự như vậy, chúng ta sẽ có giá trị nguyên cái thành phần này sẽ là 0 Và Gini của chúng ta sẽ có giá trị là 0 Không ở đây, nó sẽ phản ánh vùng dữ liệu này là thuần khiết lớp O thuần khiết ở đây thì chúng ta cũng có thể coi như là độ tinh khiết của một nút một nút ở đây thì phản ánh một vùng dữ liệu Tương tự như với ví dụ khác, chúng ta sẽ có chỉ số Gini để đo lường sự bất bình đẳng về thu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 10,
      "start_timestamp": "0:09:01",
      "end_timestamp": "0:10:10"
    }
  },
  {
    "page_content": "chỉ số Gini để đo lường sự bất bình đẳng về thu nhập của các quốc gia trên thế giới Trong trường hợp lý tưởng nhất, mọi người trên thế giới này sẽ có mức thu nhập như nhau, có nghĩa là mọi người đều giống nhau Cho nên chỉ số bất bình đẳng ở đây là 0 Ngoài Gini, chúng ta sẽ có một chỉ số phổ biến khác để đo lường độ hỗn loạn của thông tin trong một tập dữ liệu Chúng ta sẽ có xác suất xuất hiện của một giá trị Nó sẽ thuộc về lớp 1 hay lớp 0 Giả sử ứng với lại lớp 1 đi Nếu như xác suất xảy ra của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 11,
      "start_timestamp": "0:10:06",
      "end_timestamp": "0:11:20"
    }
  },
  {
    "page_content": "ứng với lại lớp 1 đi Nếu như xác suất xảy ra của nó là 1 thì entropy của chúng ta sẽ là 0 và tương tự như vậy cũng là 0 luôn và trong trường hợp xác suất xảy ra của các lớp bằng 0.5 hết, có nghĩa là tập dữ liệu của chúng ta đang bị hỗn loạn ví dụ chúng ta sẽ có trường hợp hỗn loạn thì trong tập dữ liệu đó chúng ta sẽ có hai lớp Nhưng mà mỗi lớp như vậy sẽ có giá trị xác suất xuất hiện là như nhau Có nghĩa là khả năng xảy ra của các lớp trong này bằng nhau Cứ như vậy thì chúng ta sẽ thế vào công",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 12,
      "start_timestamp": "0:11:00",
      "end_timestamp": "0:12:10"
    }
  },
  {
    "page_content": "bằng nhau Cứ như vậy thì chúng ta sẽ thế vào công thức Chúng ta sẽ có giá trị entropy như hình Trong thực tế thì, việc chúng ta sử dụng Gini hay entropy thì hiệu suất bài toán phân loại đều có giá trị như nhau, đều có hiệu suất tốt như nhau Thông thường thì trong thư viện Scikit-learn, người ta sẽ sử dụng Gini là tiêu chí mặc định cho việc phân chia cây Ở đây, chúng ta sẽ có một ví dụ về một cây phân loại Mục tiêu của cây này sẽ đi dự đoán sự hiện diện của bệnh tim là có xảy ra bệnh tim hay là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 13,
      "start_timestamp": "0:12:01",
      "end_timestamp": "0:13:10"
    }
  },
  {
    "page_content": "diện của bệnh tim là có xảy ra bệnh tim hay là không xảy ra bệnh tim Trong cây quyết định này, chúng ta có thể để ý một số điểm đặc biệt như sau đó là tại mỗi nút lá, cả nhánh trái và nhánh phải của một vùng phân loại thì đều có giá trị quyết định là NO thì ở đây chúng ta sẽ giải thích tình trạng này là như thế nào thực ra thì nó không phải là tại vì trong vùng dữ liệu này toàn NO đâu nhưng mà phần lớn giá trị sẽ là NO chỉ có một cái YES chẳng hạn, còn lại là NO hết và tương tự như vậy Vùng dữ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 14,
      "start_timestamp": "0:12:55",
      "end_timestamp": "0:13:55"
    }
  },
  {
    "page_content": "còn lại là NO hết và tương tự như vậy Vùng dữ liệu nhánh phải này của chúng ta thì lại là chứa YES nhiều. Có thể là chứa toàn bộ YES luôn. Như vậy, ở đây chúng ta có thể nghĩ lại tình trạng mà tất cả các giá trị quyết định đều như nhau trong phần tiêu chí dừng. Vì vậy, ở đây chúng ta có thể dừng việc phân chia dữ liệu thành các vùng Chúng ta có thể thay thế nguyên cái vùng này bằng một quyết định là NO thôi Đây là một ví dụ của cắt tỉa cây Chúng ta coi nguyên cái vùng này hoặc là nguyên cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 15,
      "start_timestamp": "0:13:49",
      "end_timestamp": "0:14:42"
    }
  },
  {
    "page_content": "ta coi nguyên cái vùng này hoặc là nguyên cái vùng lớn luôn là YES thôi Như vậy, chúng ta đã cùng nhau tìm hiểu về các thành phần trong một cây quyết định, cách mà chúng ta xây dựng cây quyết định cho bài toán hồi quy hoặc là bài toán phân loại. Trong phần này, chúng ta sẽ coi lại một số ưu điểm của cây quyết định. Về ưu điểm thì nó có thể giải thích được, giải thích tốt hơn các thuật toán khác chẳng hạn như Neural Network Và nó có thể nắm bắt được sự tương tác giữa các đặc trưng Đối với biến",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 16,
      "start_timestamp": "0:14:36",
      "end_timestamp": "0:15:26"
    }
  },
  {
    "page_content": "được sự tương tác giữa các đặc trưng Đối với biến định tính đó thì giả sử chúng ta sẽ có các giá trị là cao, thấp, trung bình Trong một cột dữ liệu đầu vào, chúng ta không cần phải chuyển thành biến định lượng như cách sử dụng cho Logistic Regression hoặc Neural Network. Về nhược điểm, cây quyết định có thể không ổn định. Trong trường hợp phương sai cao, đối với một tập dữ liệu, chẳng hạn như chúng ta sẽ có một tập dữ liệu như thế này, về bài toán hồi quy đó, nó tồn tại một vài cái điểm như thế",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 17,
      "start_timestamp": "0:15:23",
      "end_timestamp": "0:16:20"
    }
  },
  {
    "page_content": "hồi quy đó, nó tồn tại một vài cái điểm như thế này, thì mấy cái điểm này có thể gây không ổn định cho cây quyết định Tiếp theo thì nó sẽ có cái nhược điểm đó là thiếu tính mượt mà Tại vì đơn thuần cái việc chúng ta phân chia các vùng nó chỉ dựa trên đường thẳng mà thôi Và cái nhược điểm cuối cùng đó là nó khó nắm bắt tính cộng Đây là một cái điểm quan trọng Thuật toán cây quyết định nó khác nhiều so với thuật toán Linear Regression Trong thuật toán Linear Regression, chúng ta sẽ có hai đặc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 18,
      "start_timestamp": "0:16:10",
      "end_timestamp": "0:17:03"
    }
  },
  {
    "page_content": "toán Linear Regression, chúng ta sẽ có hai đặc trưng đầu vào, chẳng hạn như x1 và x2 đi ha? Và nó sẽ có các hệ số hồi quy, chẳng hạn như là beta 1 và beta 2 đi, là các tham số của mô hình. Thì bản thân của mô hình Linear Regression nó có sự cộng giữa hai đặc trưng. Còn đối với thuật toán cây quyết định thì không Như vậy, đây là một điểm đáng lưu ý Để sau này các bạn sẽ gặp một tình huống đó Cây quyết định sẽ cho hiệu suất hồi quy thấp hơn so với thuật toán Linear Regression Mặc dù là cây quyết",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 19,
      "start_timestamp": "0:16:56",
      "end_timestamp": "0:17:53"
    }
  },
  {
    "page_content": "thuật toán Linear Regression Mặc dù là cây quyết định có thể giải quyết trường hợp phi tuyến khá là tốt, tuy nhiên đối với một số trường hợp thuần tuyến tính thì nó có thể tệ hơn so với thuật toán Linear Regression Tiếp theo, chúng ta sẽ có một minh họa về tình huống mà cây quyết định đưa ra kết quả không ổn định khi chúng ta chỉ cần thay đổi một chút xíu trong tập dữ liệu thôi. Nó khác với cách chúng ta đã làm trong môn học nhập môn lập trình. Giả sử trong môn học đó các bạn có sự thay đổi về",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 20,
      "start_timestamp": "0:17:42",
      "end_timestamp": "0:18:14"
    }
  },
  {
    "page_content": "Giả sử trong môn học đó các bạn có sự thay đổi về dữ liệu hoặc dữ kiện thì thông thường chúng ta chỉ cần thay đổi một vài nhánh nhỏ thôi. Chúng ta kiểm soát rất là dễ nhưng đối với cây quyết định thì không. Trong trường hợp này thì chúng ta có thể thấy hai cấu trúc cây nó đã được biến đổi đi khá là khác nhau mặc dù là dữ liệu chỉ thay đổi một chút xíu mà thôi Như vậy, chúng ta đã vừa tìm hiểu xong về cây quyết định cho bài toán hồi quy và phân loại Đây là nền tảng để các bạn tiếp tục tìm hiểu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 21,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "loại Đây là nền tảng để các bạn tiếp tục tìm hiểu các thuật toán nâng cao khác dựa trên cây chẳng hạn như Random Forest Nâng cao hơn nữa có thể là XGBoost, LightGBM và CatBoost Nội dung bài giảng này được xây dựng dựa trên tài liệu đến từ đại học Stanford, Hoa Kỳ Chúc mừng mọi người.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=onVEt5FPeWc",
      "filename": "onVEt5FPeWc",
      "title": "[CS114 - Chương 8] Decision Tree (Part 3)",
      "chunk_id": 22,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chúng ta sẽ cùng tìm hiểu về bài toán phân loại classification, một trong những phương pháp học có giám sát cơ bản nhất trong học máy. Ở đây nhiệm vụ của mô hình là dự đoán lớp, nhãn hoặc là loại của một đối tượng dựa trên dữ liệu đầu vào. Cụ thể, mô hình sẽ nhận vào một hoặc nhiều biến số độc lập hay còn gọi là các đặc trưng features, ví dụ như chiều cao, cân nặng, màu sắc, đặc trưng hình ảnh. Kết quả đầu ra là một biến phân loại rời rạc, tức là một giá trị thuộc về một nhóm hoặc một nhãn xác",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 0,
      "start_timestamp": "0:00:28",
      "end_timestamp": "0:01:10"
    }
  },
  {
    "page_content": "một giá trị thuộc về một nhóm hoặc một nhãn xác định. Ví dụ, dự đoán loại hoa là cẩm chướng hồng hay cúc, dự đoán bệnh có hay không hoặc xác định email là spam hay không spam. Mục tiêu chính của bài toán phân loại là tìm ra một ranh giới quyết định hay còn gọi là decision boundary tối ưu trên không gian đặc trưng. Nhờ đó mà mô hình có thể gán nhãn chính xác cho các dữ liệu mới dựa vào các đặc trưng đầu vào. Hồi quy logistic là một trong những mô hình phân loại nhị phân phổ biến nhất và được ứng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 1,
      "start_timestamp": "0:01:05",
      "end_timestamp": "0:01:50"
    }
  },
  {
    "page_content": "hình phân loại nhị phân phổ biến nhất và được ứng dụng rộng rãi trong nhiều lĩnh vực thực tiễn. Đối với lĩnh vực tài chính và kinh doanh, các ngân hàng và doanh nghiệp thường sử dụng hồi quy logistic để dự đoán khả năng khách hàng rời bỏ dịch vụ hay còn gọi là churn. Ngoài ra, mô hình này còn được dùng để phát hiện các giao dịch thẻ tín dụng có dấu hiệu gian lận hay không hoặc phân loại hồ sơ vay vốn để xác định khách hàng có nguy cơ vỡ nợ hay không. Trong lĩnh vực y tế và sức khỏe, hồi quy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 2,
      "start_timestamp": "0:01:43",
      "end_timestamp": "0:02:24"
    }
  },
  {
    "page_content": "không. Trong lĩnh vực y tế và sức khỏe, hồi quy logistic cũng đóng vai trò then chốt. Ví dụ, mô hình này giúp các bác sĩ chẩn đoán bệnh dựa trên kết quả xét nghiệm. Một bài toán phân loại điển hình bệnh nhân có mắc bệnh hay không. Logistic regression cũng còn được sử dụng để dự đoán nguy cơ tái nhập viện của bệnh nhân hoặc phân loại mẫu bệnh phẩm lành tính hay ác tính, hỗ trợ quá trình ra quyết định lâm sàng. Trong lĩnh vực công nghệ và đời sống, hồi quy logistic xuất hiện trong nhiều ứng dụng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 3,
      "start_timestamp": "0:02:20",
      "end_timestamp": "0:02:59"
    }
  },
  {
    "page_content": "hồi quy logistic xuất hiện trong nhiều ứng dụng quen thuộc với chúng ta. Chẳng hạn các hệ thống email sử dụng hồi quy logistic để phân loại thư rác hay phân tích cảm xúc văn bản, ví dụ như phân loại một bình luận là tích cực hay tiêu cực. Như vậy thì dù ở bất kỳ lĩnh vực nào hồi quy logistic đều phát huy vai trò quan trọng trong việc giải quyết các bài toán phân loại. Vì sao phân loại? Bài toán phân loại lại quan trọng bởi vì nó là phổ biến và thực tế. Bài toán phân loại xuất hiện ở hầu hết các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 4,
      "start_timestamp": "0:02:54",
      "end_timestamp": "0:03:35"
    }
  },
  {
    "page_content": "tế. Bài toán phân loại xuất hiện ở hầu hết các lĩnh vực tài chính, y tế, thương mại, điện tử, truyền thông xã hội vân vân. Nhiều quyết định quan trọng như cho vay, chẩn đoán bệnh, sàng lọc hồ sơ, phát hiện gian lận đều là các bài toán phân loại. Dễ hiểu và dễ triển khai. Hồi quy logistic là mô hình phân loại nhị phân đơn giản nhưng hiệu quả và dễ triển khai. Được sử dụng rộng rãi như một baseline trước khi thử các thuật toán phức tạp hơn. Khả năng diễn giải, ý nghĩa của từng hệ số. Trọng số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 5,
      "start_timestamp": "0:03:28",
      "end_timestamp": "0:04:07"
    }
  },
  {
    "page_content": "năng diễn giải, ý nghĩa của từng hệ số. Trọng số trong mô hình cho phép giải thích tác động của các đặc trưng đầu vào đến kết quả phân loại phù hợp cho các ứng dụng cần minh bạch và giải thích được. Ví dụ như y tế và tài chính. Khả năng mở rộng và ứng dụng. Hồi quy logistic là nền tảng cho các mô hình nâng cao như regularized logistic regression, multiclass classification, softmax regression, bài toán phân loại đa lớp và nhiều mô hình sâu hơn. Chúng ta hãy cùng tìm hiểu cái mô hình toán học của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 6,
      "start_timestamp": "0:04:03",
      "end_timestamp": "0:04:46"
    }
  },
  {
    "page_content": "ta hãy cùng tìm hiểu cái mô hình toán học của hồi quy logistic. Trong ờ ví dụ này thì chúng ta sử dụng hồi quy logistic để dự đoán nguy cơ một bệnh nhân sẽ tái nhập viện dựa trên số ngày nằm viện ở lần nhập viện trước đó. Chúng ta có công thức như sau. Thì ở đây chúng ta sẽ thấy là X là biến đầu vào hay là đặc trưng độc lập. Biến này sử dụng để dự đoán. Ví dụ như dự đoán số ngày nằm viện ở lần à nhập viện gần nhất của bệnh nhân. Y ở đây là biến mục tiêu hay là biến phụ thuộc là kết quả cần phân",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 7,
      "start_timestamp": "0:04:41",
      "end_timestamp": "0:05:32"
    }
  },
  {
    "page_content": "tiêu hay là biến phụ thuộc là kết quả cần phân loại. Ví dụ như y = 1 thì bệnh nhân sẽ tái nhập viện trong 30 ngày tới và y = 0 là không tái nhập viện. Tiếp theo thì chúng ta có hai tham số quan trọng của mô hình. Thứ nhất là W tức là trọng số hay là weight. Tham số này cho biết mức độ ảnh hưởng của số ngày nằm viện x đến xác suất tái nhập viện. Nếu W lớn hơn 0 thì mỗi ngày nằm viện sẽ tăng thêm, sẽ làm tăng xác suất bệnh nhân tái nhập viện. Điều này phản ánh thực tế là những bệnh nhân phải nằm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 8,
      "start_timestamp": "0:05:25",
      "end_timestamp": "0:06:10"
    }
  },
  {
    "page_content": "này phản ánh thực tế là những bệnh nhân phải nằm viện lâu thường có tình trạng bệnh nặng hoặc hồi phục kém nên họ có nguy cơ tái nhập viện cao hơn. Nếu mà W nhỏ hơn 0 thì mỗi ngày nằm viện tăng thêm thì sẽ làm giảm xác suất tái nhập viện. Nếu W = 0 thì số ngày nằm viện không ảnh hưởng đến xác suất tái nhập viện. Ví dụ như W mà bằng 0.15 thì mỗi ngày nằm viện tăng lên sẽ làm tăng xác suất tái nhập viện. Hệ số thứ hai đó là hệ số chặn B. Đây là xác suất cơ bản khi số ngày nằm viện bằng 0 giúp mô",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 9,
      "start_timestamp": "0:06:03",
      "end_timestamp": "0:06:52"
    }
  },
  {
    "page_content": "suất cơ bản khi số ngày nằm viện bằng 0 giúp mô hình phù hợp hơn với dữ liệu thực tế. Hệ số chặn này điều chỉnh mô hình để phản ánh tốt hơn xu hướng của dữ liệu, tránh dự đoán xác suất phi thực tế. Thứ ba đó là hàm sigmoid được dùng chuyển đổi tổng w x + b thành giá trị xác suất nằm trong khoảng 01. Đây là điểm khác biệt then chốt của hồi quy logistic so với hồi quy tuyến tính. Đây cách sử dụng mô hình thì sau khi chúng ta tính được xác suất P ta sẽ đặt một ngưỡng thường là 0.5. Nếu xác suất",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 10,
      "start_timestamp": "0:06:46",
      "end_timestamp": "0:07:28"
    }
  },
  {
    "page_content": "ta sẽ đặt một ngưỡng thường là 0.5. Nếu xác suất lớn hơn hoặc bằng 0.5 dự đoán bệnh nhân sẽ tái nhập viện. Ngược lại, nếu xác suất nhỏ hơn 0.5 thì dự đoán bệnh nhân sẽ không tái nhập viện. Chúng ta sẽ định nghĩa hàm mất mát loss function theo như cái thuật toán mà chúng ta giải trong các cái bài toán máy học. Thì hàm mất mát ở đây dùng để đo lường sự khác biệt giữa xác suất dự đoán P và nhãn thực tế Y của từng mẫu. Mục tiêu của mô hình là làm cho sự khác biệt này càng nhỏ càng tốt. Khi có sự",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 11,
      "start_timestamp": "0:07:24",
      "end_timestamp": "0:08:10"
    }
  },
  {
    "page_content": "cho sự khác biệt này càng nhỏ càng tốt. Khi có sự khác biệt nhỏ có nghĩa là xác suất mô hình dự đoán ra rất gần với kết quả thực tế, chứng tỏ mô hình hoạt động tốt. Để đo lường sự khác biệt này, hồi quy logistic sử dụng hàm binary cross entropy à hay còn gọi là log loss thì chúng ta sẽ thấy cái công thức như trong hình vẽ. Ý nghĩa ở đây là nếu mà y là nhãn thực tế thì 0 hoặc 1 thì chúng ta sẽ có p là xác suất dự đoán mà mô hình đưa ra cũng là một con số từ 0 tới 1. Nếu y = 1 thì p càng gần 1",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 12,
      "start_timestamp": "0:08:07",
      "end_timestamp": "0:08:57"
    }
  },
  {
    "page_content": "một con số từ 0 tới 1. Nếu y = 1 thì p càng gần 1 thì lại càng tốt. Nhưng mà nếu y = 0 thì p càng gần 0 thì càng tốt. Công thức này đo lường sự khác biệt một cách rất thông minh, không phải bằng phép trừ đơn giản mà bằng cách phạt nặng những dự đoán sai. Chúng ta có thể thấy ở đây là ví dụ như là nhắc lại giống như hồi nãy thì chúng ta sẽ thấy y mà bằng 1 thì công thức trở thành là à trừ log p. Như vậy thì sự khác biệt được đo bằng trừ log p mà nếu dự đoán p gần với 1 thì sự khác biệt là nhỏ.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 13,
      "start_timestamp": "0:08:52",
      "end_timestamp": "0:09:32"
    }
  },
  {
    "page_content": "nếu dự đoán p gần với 1 thì sự khác biệt là nhỏ. Ví dụ như p = 0.99. Nhưng mà nếu đoán p xa với 1 ví dụ như p = 0.1 thì giá trị à sẽ rất là lớn. Như vậy thì cái trừ log p là đo chính xác mức độ lệch của dự đoán p so với giá trị đúng là 1. Còn trong trường hợp tương tự thì với nhãn thực tế là y = 0 thì công thức nó sẽ trở thành là loss sẽ bằng trừ log của 1 - p. Thế thì lúc này sự khác biệt sẽ được đo bằng trừ log của 1 - p. Và nếu dự đoán p gần với 0 ví dụ như p = 0.01 thì sự khác biệt này là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 14,
      "start_timestamp": "0:09:27",
      "end_timestamp": "0:10:07"
    }
  },
  {
    "page_content": "với 0 ví dụ như p = 0.01 thì sự khác biệt này là nhỏ. Nếu dự đoán p xa với 0 ví dụ như p = 0.9 thì sự khác biệt này là lớn. Như vậy thì chúng ta sẽ thấy là một cách tương tự thì trừ log của 1 - p nó đo chính xác cái mức độ lệch của dự đoán p so với giá trị đúng là 0. Thì bằng cách tối thiểu hóa hàm mất mát này thì chúng ta đang huấn luyện mô hình sao cho nó có thể đưa ra xác suất dự đoán P khớp nhất với nhãn thực tế Y dựa trên dữ liệu huấn luyện. Tương tự thì chúng ta có hàm chi phí. Hàm chi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 15,
      "start_timestamp": "0:10:02",
      "end_timestamp": "0:10:40"
    }
  },
  {
    "page_content": "Tương tự thì chúng ta có hàm chi phí. Hàm chi phí thì đơn giản đó là trung bình loss của tất cả các mẫu trong tập huấn luyện theo như công thức. Trong đó m là số lượng mẫu và P_i ở trên là xác suất dự đoán cho mẫu thứ i. Mục tiêu của quá trình huấn luyện là tìm ra tham số WB để cost function có giá trị nhỏ nhất à tương ứng với việc mô hình dự đoán tốt nhất. Cũng tương tự như trên thì chúng ta sẽ qua quá trình huấn luyện để tìm ra tham số WB để cost function nhỏ nhất bằng thuật toán gradient",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 16,
      "start_timestamp": "0:10:36",
      "end_timestamp": "0:11:19"
    }
  },
  {
    "page_content": "cost function nhỏ nhất bằng thuật toán gradient descent. ở trên là xác suất dự đoán cho mẫu thứ Thì bước đầu tiên là chúng ta khởi tạo các tham số của mô hình cụ thể là W và B với giá trị ban đầu có thể là zero hoặc là một con số ngẫu nhiên nhỏ. Tiếp theo thì nó là một quá trình lặp của quá trình cập nhật các tham số này dựa trên cái gradient cho tới khi hội tụ. Thì ở mỗi vòng lặp thì chúng ta sẽ tính toán đạo hàm của hàm chi phí đối với từng tham số W và B như trên màn hình. Thì giá trị",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 17,
      "start_timestamp": "0:11:14",
      "end_timestamp": "0:11:52"
    }
  },
  {
    "page_content": "tham số W và B như trên màn hình. Thì giá trị gradient này cho biết hướng thay đổi của hàm chi phí nếu chúng ta điều chỉnh các tham số. Thế thì để giảm hàm chi phí thì ta cập nhật W và B theo hướng ngược lại. Với chiều tăng của gradient mỗi lần một lượng nhỏ tương ứng với Learning Rate đã chọn cái thuật toán này thì nó cũng tương tự như là hồi quy tuyến tính mà thôi. thì chúng ta sẽ cứ mỗi lần cập nhật thì W và B sẽ được điều chỉnh dựa trên trung bình sai số giữa xác suất dự đoán và nhãn dữ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 18,
      "start_timestamp": "0:11:48",
      "end_timestamp": "0:12:09"
    }
  },
  {
    "page_content": "bình sai số giữa xác suất dự đoán và nhãn dữ liệu thực tế của toàn bộ dữ liệu huấn luyện. Quá trình này được lặp lại nhiều lần và sau mỗi lần cập nhật ta sẽ kiểm tra điều kiện dừng. Ví dụ như khi hàm chi phí không còn giảm đáng kể hoặc đã đạt đến số vòng lặp tối đa thì khi thuật toán kết thúc các giá trị cuối cùng của W và B chính là bộ tham số tối ưu giúp mô hình dự đoán xác suất gần với thực tế nhất. Như vậy thì chúng ta sẽ thấy gradient descent là một quá trình tối ưu lặp đi lặp lại liên tục",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 19,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "là một quá trình tối ưu lặp đi lặp lại liên tục điều chỉnh các tham số dựa trên dữ liệu giúp logistic regression học ra được mô hình dự đoán tốt nhất. Tiếp theo là cái phần quiz.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=oxjqoqQ79O4",
      "filename": "oxjqoqQ79O4",
      "title": "[CS114 - Chương 4] Mô hình phân lớp",
      "chunk_id": 20,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề, học sâu, machine learning, logistic regression, imageNet. Chủ đề, học sâu, machine learning, imageNet. tức là môi trường hay là thế giới mà Agent sẽ tương tác đến và tương tác ở đây sẽ là tương tác 2 chiều Agent sẽ quan sát trạng thái ST lưu ý là ở đây chúng ta sẽ có ngoài cái biến S hồi nãy chúng ta sẽ có thêm một cái chỉ số nữa đó là chỉ số T T này tương ứng là cái thời gian nó ám chỉ cái thời gian là tại cái thời điểm nào đó ví dụ như T là tại vị trí hiện tại T cộng 1 là thời điểm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:00"
    }
  },
  {
    "page_content": "T là tại vị trí hiện tại T cộng 1 là thời điểm tiếp theo Agent sẽ quan sát trạng thái từ môi trường và thực hiện các hành động AT, action tại thời điểm T và Environment sẽ trả về trạng thái mới tức là ứng với action này, thì môi trường sẽ trả về trạng thái mới và cộng với lại cái phần thưởng cho cái trạng thái của ST cộng với cái phần thưởng của cái trạng thái trước đó ví dụ khi chúng ta chơi game thì chúng ta muốn có một cái Agent Agent mà chơi game và nó sẽ tương tác với cái môi trường thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 1,
      "start_timestamp": "0:00:56",
      "end_timestamp": "0:01:39"
    }
  },
  {
    "page_content": "game và nó sẽ tương tác với cái môi trường thì cái môi trường này nó chính là cái setup của cái game của mình và khi chúng ta tương tác thì nó sẽ có các cái phần thưởng hoặc là hình phạt Ví dụ như nếu mà chúng ta tương tác và chúng ta ăn được một cái đối tượng trong game chẳng hạn chúng ta ăn được một cái tiền trong game thì cái reward của mình nó sẽ được tăng lên hoặc là nếu chúng ta rớt xuống dưới một cái bầu vật thì lúc đó reward nó sẽ tiến về âm vô cùng Rồi trong xe tự lái thì cái agent xe",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 2,
      "start_timestamp": "0:01:35",
      "end_timestamp": "0:02:20"
    }
  },
  {
    "page_content": "âm vô cùng Rồi trong xe tự lái thì cái agent xe tự lái nó sẽ tương tác với cái môi trường chính là con đường, người bộ hành, xe xung quanh là đường phố, môi trường là đường phố khái niệm tiếp theo cũng rất quan trọng mà chúng ta cần phải tìm hiểu là trạng thái trạng thái là nó sẽ miêu tả tình huống hiện tại của môi trường của mình cung cấp thông tin để Agent có thể đưa ra quyết định hành động tức là nó, Agent này muốn đưa ra được cái hành động tiếp theo tại thời điểm t, thì chúng ta sẽ phải có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 3,
      "start_timestamp": "0:02:10",
      "end_timestamp": "0:03:01"
    }
  },
  {
    "page_content": "theo tại thời điểm t, thì chúng ta sẽ phải có được thông tin từ môi trường chứ nó không có thông tin để ra quyết định và đặc điểm của trạng thái của mình là nó sẽ từ đơn giản là ví dụ như nó có thể là một trạng thái của mê cung hoặc đến phức tạp hơn Ví dụ như nó là một hình ảnh camera trong xe tự lái Thì cái trạng thái này Nếu mà nói về đơn giản thì có thể là cái trạng thái của các bunker trong game Hoặc là cái vị trí trong mekong, v.v. Rồi phức tạp thì nó có thể là hình ảnh hoặc là các dữ liệu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 4,
      "start_timestamp": "0:02:57",
      "end_timestamp": "0:03:42"
    }
  },
  {
    "page_content": "tạp thì nó có thể là hình ảnh hoặc là các dữ liệu từ sensor mà chúng ta thu thập được Trong cái xe tự lái Thì rõ ràng là các dữ liệu phức tạp thì thường là các dữ liệu thô Các dữ liệu thôi Còn các dữ liệu đơn giản thì thường nó sẽ là có cái khái niệm và cái concept nó rất là rõ ràng Và thường thì cả dữ liệu đơn giản hay là dữ liệu phức tạp thì nó cũng sẽ đều được biểu diễn bằng vector hoặc là cái dạng ma trận 2D hình ảnh hoặc là các cái thông tin rời ràng Nhưng mà thường đơn giản nhất là nó sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 5,
      "start_timestamp": "0:03:35",
      "end_timestamp": "0:04:17"
    }
  },
  {
    "page_content": "rời ràng Nhưng mà thường đơn giản nhất là nó sẽ biểu diễn dưới dạng là vector Tại vì chúng ta có một cái quy tắc đó là mọi dữ liệu đều có thể chuyển về được cái định dạng Vector Thì cái bàn gờ vua này nó sẽ đưa cái bốt cục của cái bàn gờ để đưa cho cái con Agent Thì đây là cái trạng thái, bốt cục của bàn gờ nó chính là cái trạng thái S tại cái thời điểm T của mình Hoặc là cái con robot, cái con robot nó bước đi trên cái mặt đất, đúng không? Vị trí của con robot, góc quay của con robot, vận tốc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 6,
      "start_timestamp": "0:04:10",
      "end_timestamp": "0:04:47"
    }
  },
  {
    "page_content": "của con robot, góc quay của con robot, vận tốc của con robot nó chính là cái trạng thái nó sẽ cung cấp cho con agent biết con robot đang ở trong cái trạng thái nào để nó có thể thực thi cái hành động tiếp theo Khá niệm cơ bản tiếp theo mà chúng ta sẽ cùng tìm hiểu đó chính là Hành động Action Thì đây là những gì mà agent có thể làm có thể làm trong một cái trạng thái có thể làm trong một cái trạng thái thì quyết định tương lai của Agent trong môi trường này thì với cái Action tại một cái trạng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 7,
      "start_timestamp": "0:04:43",
      "end_timestamp": "0:05:22"
    }
  },
  {
    "page_content": "trường này thì với cái Action tại một cái trạng thái, nó sẽ cho biết tương lai của Agent trong môi trường này nó sẽ như thế nào và đặc điểm của mình, đầu tiên đó là Discrete tức là đây là các lựa chọn rời rạt hành động của chúng ta sẽ là một cái hành động rời rạt một cái lựa chọn rời rạt nó sẽ di chuyển ví dụ như chúng ta có thể di chuyển theo các cái hướng rồi có thể cái hành động của mình là nhảy hay là đi bộ hoặc là phanh xe thì đây chính là những cái ví dụ về về hành động mà dạng rời ràng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 8,
      "start_timestamp": "0:05:18",
      "end_timestamp": "0:06:00"
    }
  },
  {
    "page_content": "những cái ví dụ về về hành động mà dạng rời ràng nhưng cái hành động của mình nó cũng có thể là ở dạng liên tục continuous là các cái giá trị liên tục ví dụ như là một cái góc quay của một cái bánh xe thì cái góc quay này nó có thể là một cái con số lạng ví dụ như là 3.15 độ hoặc là kinh lực tác động, ví dụ như là 3.61 Nm hoặc là kinh lực của cái fan thì đây là những cái action mà nó là liên tục còn cái action mà dạng rời rạc thì cho biết đó là mình sẽ đi theo hướng nào đi hướng lên trên, đi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 9,
      "start_timestamp": "0:05:58",
      "end_timestamp": "0:06:37"
    }
  },
  {
    "page_content": "mình sẽ đi theo hướng nào đi hướng lên trên, đi hướng rẻ trái hoặc là chúng ta đứng yên nhảy hoặc là đi bộ fan xe ở đây là fan hay không fan tức là chúng ta cho biết hành động của chúng ta là có phanh hay không phanh chiếc xe thì ví dụ như trong game Mario thì chúng ta sẽ có các cái hành động là rời đạt là đi về bên tay trái, đi về tay phải, hoặc là nhảy còn trong cái xe tự lái, ở trong môi trường thực tế thì action của chúng ta sẽ là những cái giá trị liên tục ví dụ như là tốc độ của chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 10,
      "start_timestamp": "0:06:33",
      "end_timestamp": "0:07:21"
    }
  },
  {
    "page_content": "giá trị liên tục ví dụ như là tốc độ của chúng ta hoặc là lực chúng ta sẽ đạp ga, lực chúng ta sẽ đạp phanh là gì hoặc là các action rời rạt, ví dụ như là quyết định xem là tăng tốc hay là giảm tốc rồi rẽ trái hay là rẽ phải nhưng mà đi kèm với các action rời rạt này thì nó sẽ có các action liên tục ví dụ như là muốn tăng tốc thì tăng tốc bao nhiêu rồi phải đạp ga là bao nhiêu rồi rẽ trái rẽ phải thì chúng ta sẽ xoay vô lăng là bao nhiêu độ như vậy và cái khái niệm quan trọng, cơ bản tiếp theo",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 11,
      "start_timestamp": "0:07:11",
      "end_timestamp": "0:07:52"
    }
  },
  {
    "page_content": "vậy và cái khái niệm quan trọng, cơ bản tiếp theo đó chính là Reward là R, thì đây là tín hiệu từ môi trường để đánh giá xem hành động của mình là tốt hay không Và giá trị của mình có thể nhận là giá trị số từ dương, âm hoặc số 0 Đặc điểm như immediate reward, tức là phản hồi ngay sau khi hành động của mình xảy ra thì đó là ngay lập tức nhưng mà nó cũng có thể là 1 cái delay reward tức là có những cái hành động mà không phải trả ra tức thời mà phải chờ 1 khoảng thời gian sau nó mới có thể đưa",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 12,
      "start_timestamp": "0:07:50",
      "end_timestamp": "0:08:29"
    }
  },
  {
    "page_content": "phải chờ 1 khoảng thời gian sau nó mới có thể đưa ra được cái reward thì đây là 2 loại reward khác nhau là immediate hoặc là delay thì đối với cái delay là cái mà có lẽ là xuất hiện phổ biến nhất là vì tại 1 cái action hiện tại thì nhiều khi chúng ta sẽ không thấy được ngay cái reward của mình là gì mà phải chờ sau rất nhiều bước tiếp theo thì chúng ta mới có thể thấy rõ được phần thưởng trả về ví dụ như là reward của mình đó là thắng cả ván cờ thì phải sau rất nhiều bước chúng ta mới thấy được",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 13,
      "start_timestamp": "0:08:23",
      "end_timestamp": "0:09:05"
    }
  },
  {
    "page_content": "phải sau rất nhiều bước chúng ta mới thấy được cái reward này thì nếu như chúng ta thắng ván cờ thì rõ ràng cái điểm của mình nó sẽ rất là cao là cộng 100 điểm rồi robot mà va chạm vô một cái chứng ngại vật nào đó thì đó sẽ là bị trừ điểm là điểm ong hoặc là xe đi đúng làng thì chúng ta sẽ có cộng một nhưng nếu mà lỡ chạm vào làng lấn làng thì chúng ta sẽ có điểm trường một đây là một số ví dụ về giá trị của reward của mình để cho biết môi trường đã tương tác với mình như thế nào và cái mà mình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 14,
      "start_timestamp": "0:08:56",
      "end_timestamp": "0:09:49"
    }
  },
  {
    "page_content": "đã tương tác với mình như thế nào và cái mà mình nhận được khi thực thi hành động trước đó là gì từ các khái niệm cơ bản này Bên bảng này thì chúng ta sẽ có hình dung về cái flow của các trạng thái hành động và reward Đầu tiên đó là môi trường Tại thời điểm t, cái agent sẽ nhận được trạng thái st từ môi trường Agent sẽ nhận được trạng thái để nó biết nó đang định vị nó đang ở đâu Sau đó thì cái agent từ trạng thái đó sẽ đưa ra action là at để thực hiện hành động AT để tương tác trở lại với môi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 15,
      "start_timestamp": "0:09:42",
      "end_timestamp": "0:10:40"
    }
  },
  {
    "page_content": "hiện hành động AT để tương tác trở lại với môi trường này sau khi trạng thái ST agent thực hiện hành động AT thì môi trường sẽ phản hồi lại phần thưởng là RT thì nó sẽ phản hồi lại RT là gì và sau đó thì môi trường sẽ cho biết trạng thái tiếp theo của chúng ta là gì Ngoài RT là Reward của ActionAT thì nó sẽ cho biết trạng thái tiếp theo của môi trường sẽ là cái gì Cứ như vậy thì chúng ta sẽ lập đi lập lại cho đến khi nào mà kết thúc và một số ví dụ về bài toán học tăng cường Carpone, chế cây và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 16,
      "start_timestamp": "0:10:35",
      "end_timestamp": "0:11:20"
    }
  },
  {
    "page_content": "dụ về bài toán học tăng cường Carpone, chế cây và cái xào Mục tiêu đó là chúng ta sẽ cân bằng cái xe đẩy Mục tiêu là cân bằng cây xào trên xe đẩy Giống như chúng ta làm shift với cây, làm sao để giữ cho cây này được thăng bằng, không bị ngã xuống dưới thì ở đây nó sẽ phải sử dụng rất nhiều những kiến thức về mặt vật lý chiếc xe, lực tương tác như thế nào, lực trọng trường ra sao góc nghi như thế này thì nó sẽ ảnh hưởng như thế nào đến việc ngã xuống của chiếc cây trọng lượng của cái vật ở bên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 17,
      "start_timestamp": "0:11:16",
      "end_timestamp": "0:12:12"
    }
  },
  {
    "page_content": "xuống của chiếc cây trọng lượng của cái vật ở bên trên đây nó sẽ là như thế nào thì ở đây nó sẽ dùng rất nhiều những kiến thức về mặt vật lý và với mục tiêu đó là để cân bằng cây xào trên xe đẩy này thì chúng ta sẽ có các trạng thái S trạng thái này nó sẽ cho biết là tại vị trí hiện tại thì cây xào đã tạo một góc theta một góc theta là bao nhiêu so với trục đứng này Mục tiêu là hướng đến để cho theta là 0 để cho cây xào đứng ở giữa Và tốc độ góc và tọa độ xi và vận tốc ngang là trạng thái của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 18,
      "start_timestamp": "0:12:08",
      "end_timestamp": "0:13:01"
    }
  },
  {
    "page_content": "và tọa độ xi và vận tốc ngang là trạng thái của cây xào rồi hành động của chúng ta đó là chúng ta sẽ tương tác một cái lực nằm ngang tác động vào cái xe đẩy như thế nào thì đây chính là cái action lưu ý là ở đây chúng ta nhầm đây là action A và cái phần thưởng đó là gì? Cộng 1, nếu tại thời điểm cộng 1, tại mỗi thời điểm cây xào mà thẳng đứng tức là nếu như cây xào đạt được đến trạng thái mà thẳng đứng như thế này thì chúng ta sẽ được 1 điểm như vậy thì chúng ta sẽ dịch chuyển trái phải cái xe",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 19,
      "start_timestamp": "0:12:56",
      "end_timestamp": "0:13:29"
    }
  },
  {
    "page_content": "vậy thì chúng ta sẽ dịch chuyển trái phải cái xe đẩy như thế nào đấy để cho cây xào này nó sẽ đứng yên và nó đứng ở giữa, nó không có bị ngả xuống hẳn về một phía nào đấy Đây chính là ví dụ về học tăng cường cho xe đẩy và cây sao Điều khiển chuyển động của robot Mục tiêu đó là làm cho robot có thể di chuyển về phía trước mà không ngã robot di chuyển về phía trước Nếu chúng ta để robot đi như thế này thì có thể nó sẽ di chuyển tùm lum hướng Trong khi đó nếu như chúng ta điều khiển cái con robot",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 20,
      "start_timestamp": "0:13:26",
      "end_timestamp": "0:14:16"
    }
  },
  {
    "page_content": "khi đó nếu như chúng ta điều khiển cái con robot này mà đúng thì chúng ta sẽ thấy là cái con robot nó sẽ di chuyển về một hướng thôi và đồng thời nó sẽ không có bị ngã Còn cái con robot này chúng ta thấy là một hồi sau chúng ta thấy là nó bị lật hướng và có xu hướng là muốn lật ngã xuống Thế thì trạng thái của chúng ta đó là góc và vị trí của các cái khớp và hành động của chúng ta đó là tác động lực điều khiển của các cái khớp tắc động lực của điều kiện của cái khớp, ví dụ như ở đây là 1 cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 21,
      "start_timestamp": "0:14:09",
      "end_timestamp": "0:14:29"
    }
  },
  {
    "page_content": "điều kiện của cái khớp, ví dụ như ở đây là 1 cái cánh tay thì cái lực nó sẽ cho biết chúng ta sẽ nhấp cái cánh tay này lên hay là gặp cái cánh tay này xuống hay là giữ nguyên thì đây là 1 cái khớp tay đây là 1 cái khớp tay và phần thưởng đó là cộng 1 tại mỗi lần đứng thẳng và không bị ngã chò chơi Atari thì mục tiêu đó là hoàn thành chò chơi để điểm số của mình là cao nhất và trạng thái của mình là S là dữ liệu pixel thô của trạng thái của trò chơi và hành động của chúng ta là điều khiển trò",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 22,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "chơi và hành động của chúng ta là điều khiển trò chơi, ví dụ như là đi qua trái, phải, lên, xuống và phần thưởng của mình là điểm tăng giảm ở mỗi bước của mỗi thời gian Hãy subscribe cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video hấp dẫn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=pTsVNxlbtbY",
      "filename": "pTsVNxlbtbY",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 2",
      "chunk_id": 23,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Thì ở đây chúng ta sẽ có một cái ví dụ về giảm chiều dữ liệu tuyến tính là cho một cái tập dữ liệu gồm có năm điểm trong cái không gian 2D như hình vẽ. Và chúng ta hãy giảm chiều dữ liệu về à cái không gian một chiều 1D. sao cho các cái điểm nó vẫn có cái sự phân biệt tốt. Thì ở đây chúng ta sẽ thử một vài cái phương án. Phương án đầu tiên đó là chúng ta sẽ chọn à cái không gian một chiều là một cái trục số như thế này, là một cái đường thẳng như thế này. Và tất cả các cái điểm trong cái không",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PtsZWpdkPrE",
      "filename": "PtsZWpdkPrE",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 2",
      "chunk_id": 0,
      "start_timestamp": "0:00:14",
      "end_timestamp": "0:00:51"
    }
  },
  {
    "page_content": "thế này. Và tất cả các cái điểm trong cái không gian hai chiều ban đầu chúng ta sẽ thực hiện cái phép chiếu xuống. Tại vì bản chất của giảm chiều tiếng tính nó chính là cái phép chiếu. Phép chiếu thì cái điểm này sẽ được chiếu về cái điểm này. Điểm này thì sẽ được chiếu về cái điểm này và điểm này thì sẽ chiếu về điểm này. Vân vân. Thì ở đây chúng ta sẽ để ý là cái à với cái việc mà chúng ta chiếu lên trên cái đường thẳng này thì chúng ta thấy là cái cái hình chiếu của nó nó sẽ dao động trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PtsZWpdkPrE",
      "filename": "PtsZWpdkPrE",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 2",
      "chunk_id": 1,
      "start_timestamp": "0:00:48",
      "end_timestamp": "0:01:27"
    }
  },
  {
    "page_content": "là cái cái hình chiếu của nó nó sẽ dao động trong một cái khoảng à khá là hẹp. khá là hẹp. Hay nói một cách khác đó là cái phương sai của mình nó nhỏ. Mà cái phương sai nó nhỏ thì nó sẽ làm giảm cái khả năng phân biệt giữa các cái điểm với nhau. Chúng ta sẽ khó phân biệt được điểm này với điểm này tại vì nó cứ co cụm về à nó cứ co cụm về một cái khu vực rất là nhỏ giống như là nó rất là chật chội á. dùng một cái từ nó hơi mang tính chất không gian đó là nó chật chội. Thì cái việc chật chội này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PtsZWpdkPrE",
      "filename": "PtsZWpdkPrE",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 2",
      "chunk_id": 2,
      "start_timestamp": "0:01:22",
      "end_timestamp": "0:01:58"
    }
  },
  {
    "page_content": "đó là nó chật chội. Thì cái việc chật chội này nó sẽ là làm cho các cái điểm dữ liệu sau khi chúng ta chiếu xong nó sẽ mất đi cái tính phân biệt. Trong khi đó chúng ta sẽ chọn một cái không gian 1D khác đó là cái đường thẳng này. Thì khi chúng ta chiếu xuống dưới à chiếu cái điểm này xuống thì chúng ta thấy là các cái điểm sau khi chúng ta chiếu lên trên cái không gian một chiều này nó rất là thưa. Đúng không? từ trái sang phải chúng ta thấy rằng rất là thưa. Tức là nó sẽ tạo ra cho chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PtsZWpdkPrE",
      "filename": "PtsZWpdkPrE",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 2",
      "chunk_id": 3,
      "start_timestamp": "0:01:51",
      "end_timestamp": "0:02:34"
    }
  },
  {
    "page_content": "rất là thưa. Tức là nó sẽ tạo ra cho chúng ta một cái dư địa để giúp cho chúng ta có thể dễ dàng phân biệt được cái điểm này với cái điểm kia. Trong khi đó với cái phương án số một chúng ta thấy là các cái điểm nó cứ chi chích đó có thể là trùng lắp. Ví dụ chúng ta thấy là hai cái điểm này nè là nó gần trùng lắp với nhau. Hai điểm này thì gần chạm với nhau. Trong khi đó khi chiếu lên cái không gian à mới này thì cái lựa chọn số hai này chúng ta thấy các cái điểm nó rất là phân biệt nhau. Và kết",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PtsZWpdkPrE",
      "filename": "PtsZWpdkPrE",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 2",
      "chunk_id": 4,
      "start_timestamp": "0:02:30",
      "end_timestamp": "0:03:07"
    }
  },
  {
    "page_content": "các cái điểm nó rất là phân biệt nhau. Và kết luận đó là chúng ta sẽ cần phải xác định cái hướng chiếu. Cần xác định hướng chiếu sao cho các cái điểm dữ liệu trên cái không gian mới nó có cái phương size lớn. Và à chúng ta sẽ có một cái thuục toán đầu tiên đó chính là thuậc toán PCA là viết tắt của chữ principal components analysis analysis tức là phân tích các cái thành phần chính. Thì đây là một cái kỹ thuật để à giảm chiều tuyến tính với cái mục tiêu đó là chuyển đổi các cái đặc trưng góc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PtsZWpdkPrE",
      "filename": "PtsZWpdkPrE",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 2",
      "chunk_id": 5,
      "start_timestamp": "0:03:02",
      "end_timestamp": "0:03:40"
    }
  },
  {
    "page_content": "mục tiêu đó là chuyển đổi các cái đặc trưng góc thành các cái đặc trưng mới. À thành các cái đặc trưng mới sao cho các cái đặc trưng mới này không trồng chéo lên nhau. Nó sẽ không trồng chéo lên nhau và giữ lại hầu hết nó sẽ giữ lại hầu hết các cái khác biệt cái sự khác biệt trong cái dữ liệu góc của mình. Đó thì đây là cái tính chất mà thuật toán PCA nó giúp cho chúng ta sau khi chúng ta đã giảm chiều dữ liệu. Và PCA thì nó sẽ sử dụng cái công cụ đó là đại số tiếng tính để biến đổi dữ liệu của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PtsZWpdkPrE",
      "filename": "PtsZWpdkPrE",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 2",
      "chunk_id": 6,
      "start_timestamp": "0:03:35",
      "end_timestamp": "0:04:15"
    }
  },
  {
    "page_content": "đó là đại số tiếng tính để biến đổi dữ liệu của mình thành các cái thành phần à trực giao thành các cái tập hợp các cái thành phần trực giao hay còn gọi là các cái thành phần chính. Và các cái đặc trưng nó sẽ tìm ra các cái đặc trưng này bằng cách đó là tính toán tính toán các cái vecơ riêng. à vecơ riêng để chỉ hướng và các cái giá trị riêng để thể hiện cái tầm quan trọng của các cái dữ liệu trên cái từng cái thành phần trực giao này từ cái ma trận hiệp phương sai và PCA thì là chọn các cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PtsZWpdkPrE",
      "filename": "PtsZWpdkPrE",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 2",
      "chunk_id": 7,
      "start_timestamp": "0:04:09",
      "end_timestamp": "0:04:53"
    }
  },
  {
    "page_content": "trận hiệp phương sai và PCA thì là chọn các cái thành phần hàng đầu chọn các thành phần hàng đầu có cái giá trị trị riêng cao nhất tức là chúng ta sau khi chúng ta phân tích bằng cái ma trận hiệp phương sai để tìm ra các cái giá trị trị riêng ví dụ vậy thì chúng ta sẽ sắp xếp theo thứ tự là giảm dần và chúng ta sẽ lấy à k thành phần đầu tiên mà có cái trị riêng cao nhất thì đó là những cái thành phần tương ứng với lại các cái vecơ mà cho cái à cái sự phân biệt các cái điểm sau khi chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PtsZWpdkPrE",
      "filename": "PtsZWpdkPrE",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 2",
      "chunk_id": 8,
      "start_timestamp": "0:04:49",
      "end_timestamp": "0:05:31"
    }
  },
  {
    "page_content": "à cái sự phân biệt các cái điểm sau khi chúng ta chiếu là cao nhất. Đó, còn những cái phần mà ở phía dưới thì nó có cái trị riêng thấp, tức là nó sẽ không giúp cho chúng ta phân biệt các cái điểm dữ liệu một cách dễ dàng thì chúng ta sẽ loại bỏ đi. Thì đây là cái ý tưởng à đây chúng ta sẽ giữ lại nè, còn đây chúng ta sẽ bỏ đi nè. thì thành phần nào mà lớn thì nó sẽ giữ, còn thành phần nào mà nhỏ thì chúng ta sẽ loại bỏ đi. Và đây là một cái ví dụ ở trong cái không gian góc là area là radius,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PtsZWpdkPrE",
      "filename": "PtsZWpdkPrE",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 2",
      "chunk_id": 9,
      "start_timestamp": "0:05:25",
      "end_timestamp": "0:06:17"
    }
  },
  {
    "page_content": "dụ ở trong cái không gian góc là area là radius, tức là à diện tích và bán kính. Đó thì chúng ta sẽ có các cái mẫu dữ liệu sau. Rồi sau khi chúng ta dùng cái thuật toán PC à vector, hai cái thành phần chính đó là PC1, principle component 1, PC2 là hai cái thành phần này nó trực giao với nhau. Và khi chúng ta lấy các cái điểm dữ liệu chiếu lên trên cái không gian các cái thành phần chính là PC1, PC2 thì chúng ta sẽ thấy rằng là à đối với cái thành phần chính số 2 khi chúng ta chiếu lên thì cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PtsZWpdkPrE",
      "filename": "PtsZWpdkPrE",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 2",
      "chunk_id": 10,
      "start_timestamp": "0:06:12",
      "end_timestamp": "0:06:52"
    }
  },
  {
    "page_content": "phần chính số 2 khi chúng ta chiếu lên thì cái variance tức là cái độ dao động của nó rất là bé. Trong khi đó chiếu lên thành phần số 1 thì đây là chúng ta thấy là rất là lớn. Do đó thì chúng ta có thể cân nhắc để loại bỏ đi cái thành phần PC2. chỉ cần chừa lại cái thành phần PC1 thì khi đó một cái điểm dữ liệu góc ban đầu của mình là nó có hai chiều, nó đã giảm xuống còn một chiều. Đó và chúng ta sẽ loại bỏ PC2 chỉ giữ lại PC1. Lý do đó là vì cái sự dao động cái variance của à cái điểm dữ liệu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PtsZWpdkPrE",
      "filename": "PtsZWpdkPrE",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 2",
      "chunk_id": 11,
      "start_timestamp": "0:06:48",
      "end_timestamp": "0:07:19"
    }
  },
  {
    "page_content": "sự dao động cái variance của à cái điểm dữ liệu khi mà chúng ta chiếu lên cái thành phần PC1 nó lớn nó giúp cho cái tính phân biệt nó cao hơn. Thì PCA đã chuyển đổi cái tập dữ liệu 2D thành cái dữ liệu một cái biểu diễn 1D trong khi mà vẫn giữ nguyên được cái độ biến thiên nhiều nhất có thể. Ah",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PtsZWpdkPrE",
      "filename": "PtsZWpdkPrE",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 2",
      "chunk_id": 12,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Sau đây thì chúng ta sẽ đến với một trong những cái thuật toán nổi tiếng đó là à để mà phân cụm đó chính là thuật toán Camin. Và Camin là một trong những cái thuật toán mà được bình chọn là top 10 thuật toán trong lĩnh vực về à khai thác dữ liệu, data mining và được sử dụng rất là phổ biến. Thì camin là một thực toán phân cụm theo kiểu là phân hoạch. Và chúng ta ký hiệu gọi cái tập dữ liệu điểm của mình là D. Và D này là bao gồm M m mẫu dữ liệu X1, X2 cho đến XM. Thì trong đó xy là một cái vecơ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PXfS6tZJIOI",
      "filename": "PXfS6tZJIOI",
      "title": "[CS114 - Chương 5] Cluster - Phần 2",
      "chunk_id": 0,
      "start_timestamp": "0:00:14",
      "end_timestamp": "0:00:58"
    }
  },
  {
    "page_content": "X2 cho đến XM. Thì trong đó xy là một cái vecơ trong không gian cái giá trị thực và xy này thì thuộc R. Trong đó R là à R này chính là số thuộc tính hoặc là số chiều của cái dữ liệu của mình. Tức là XY này nè là một cái vecơ có R chiều. Thì thuật toán K min sẽ phân chia dữ liệu của mình đã cho thành K cụm. Và lưu ý K này là cho trước. K này là được cho trước bởi người dùng và mỗi một cụm sẽ có một cái tâm cụm hay còn gọi là centroid. Mỗi một cụm sẽ có một cái tâm cụm. Thì cái ý tưởng của cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PXfS6tZJIOI",
      "filename": "PXfS6tZJIOI",
      "title": "[CS114 - Chương 5] Cluster - Phần 2",
      "chunk_id": 1,
      "start_timestamp": "0:00:51",
      "end_timestamp": "0:01:44"
    }
  },
  {
    "page_content": "sẽ có một cái tâm cụm. Thì cái ý tưởng của cái thuật toán Kamin này đó là với K thì thực toán Kamin sẽ hoạt động đó là bước một á là chúng ta sẽ cọn chọn ngẫu nhiên K điểm dữ liệu hay còn gọi là SH là hạt nhân. Đó, đây còn gọi là hạt nhân của cụm. Rồi và nhiệm vụ của chúng ta đó là à lấy kểm dữ liệu này nó sẽ làm các cái centroid tức là các cái tâm cụm ban đầu hay còn gọi là bước khởi tạo. Rồi và sang bước số hai đó là sau khi chúng ta đã khởi tạo được K cụm này xong thì chúng ta sẽ đi tiến",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PXfS6tZJIOI",
      "filename": "PXfS6tZJIOI",
      "title": "[CS114 - Chương 5] Cluster - Phần 2",
      "chunk_id": 2,
      "start_timestamp": "0:01:39",
      "end_timestamp": "0:02:40"
    }
  },
  {
    "page_content": "tạo được K cụm này xong thì chúng ta sẽ đi tiến hành gán nhãn. Mỗi một cái điểm dữ liệu trong cái tập dữ liệu D của mình á sẽ được gán vào một cái centroid gần nhất. Và khi nó được gán vào cái centroid gần nhất thì nó sẽ lấy cái nhãn của cái centroid đó luôn. Tức là cái chỉ số của cái cụm. Sau đó thì chúng ta sau khi chúng ta đã gán các cái điểm dữ liệu vào các cái centroid gần nhất thì chúng ta sẽ đi tiến hành cập nhật lại cái centroid. Tại vì ban đầu sen ờ cái centroid của mình là được lấy từ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PXfS6tZJIOI",
      "filename": "PXfS6tZJIOI",
      "title": "[CS114 - Chương 5] Cluster - Phần 2",
      "chunk_id": 3,
      "start_timestamp": "0:02:34",
      "end_timestamp": "0:03:13"
    }
  },
  {
    "page_content": "đầu sen ờ cái centroid của mình là được lấy từ các cái điểm dữ liệu à làm tâm cụm. Nhưng mà sau khi chúng ta đã phân bổ các cái điểm dữ liệu mới vào bên trong cái cụm của mình thì cái tâm cụm chắc chắn nó sẽ dịch chuyển. Và chúng ta sẽ tính lại cái các cái centroid dựa trên các cái thành viên cụm hiện tại mà mình đã gán ở cái bước số hai. Đã gán được ở bước số hai. Và nếu như cái việc mà tính toán cái centroid này nó chưa hội tụ thì ở đây cái khái niệm chưa hội tụ là gì thì sang cái slide sau",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PXfS6tZJIOI",
      "filename": "PXfS6tZJIOI",
      "title": "[CS114 - Chương 5] Cluster - Phần 2",
      "chunk_id": 4,
      "start_timestamp": "0:03:08",
      "end_timestamp": "0:03:47"
    }
  },
  {
    "page_content": "niệm chưa hội tụ là gì thì sang cái slide sau chúng ta sẽ sẽ đề cập. Nhưng mà nếu cái việc này chưa hồi tụ thì chúng ta sẽ quay trở lại bước số 2i và cứ lặp đi lặp lại 2 3 4 rồi lại quay lại 2 nếu như cái việc mà xác định các cái tâm cụm này là chưa hội tụ và biểu diễn hình thức hơn cho cái thuật toán cam min à đầu vào chúng ta sẽ có k loại thì đây là do người dùng cung cấp dàn bộ cái dữ liệu của mình là bao gồm là X à 1, X2 vân vân cho đến XM. Rồi và chọn K điểm dữ liệu này làm centroid ban",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PXfS6tZJIOI",
      "filename": "PXfS6tZJIOI",
      "title": "[CS114 - Chương 5] Cluster - Phần 2",
      "chunk_id": 5,
      "start_timestamp": "0:03:42",
      "end_timestamp": "0:04:36"
    }
  },
  {
    "page_content": "Rồi và chọn K điểm dữ liệu này làm centroid ban đầu. Thì đây chính là cái bước khởi tạo của mình. Sau đó nè, for chúng ta sẽ viết một cái vòng for for với mỗi một cái điểm dữ liệu XY thuộc D thì chúng ta sẽ tính khoảng cách từ XY này nè đến cái centroid. Thì ở đây chúng ta có K điểm tương ứng là Kroid đến từng cái centroid và gán cái XY này vào cái cụm có cái centroid gần nhất. Và chúng ta sẽ thực hiện đi thực hiện lại cái việc này cho duyệt qua hết tất cả các cái điểm. Chúng ta sẽ duyệt qua",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PXfS6tZJIOI",
      "filename": "PXfS6tZJIOI",
      "title": "[CS114 - Chương 5] Cluster - Phần 2",
      "chunk_id": 6,
      "start_timestamp": "0:04:30",
      "end_timestamp": "0:05:09"
    }
  },
  {
    "page_content": "hết tất cả các cái điểm. Chúng ta sẽ duyệt qua hết tất cả cái điểm trong cái bộ data của mình và cập nhật lại cái centroid dựa trên cái cụm hiện tại. Tức là khi chúng ta gán cái xi này vào cái centro gần nhất thì chúng ta sẽ đi tính lại cái tâm cộng mới bằng cách đó là chúng ta sẽ cộng trung bình các cái giá trị xy này lại của từng cụm. Và việc này sẽ lặp đi lặp lại cho đến khi nào cái điều kiện dừng được thỏa mãn. Thì cái tiêu chí dừng hay là tiêu chí tiêu chí hội tụ chúng ta có thể dựa trên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PXfS6tZJIOI",
      "filename": "PXfS6tZJIOI",
      "title": "[CS114 - Chương 5] Cluster - Phần 2",
      "chunk_id": 7,
      "start_timestamp": "0:05:04",
      "end_timestamp": "0:05:41"
    }
  },
  {
    "page_content": "tiêu chí tiêu chí hội tụ chúng ta có thể dựa trên một trong các cái tiêu chí sau. Thứ nhất đó là không hoặc là rất ít cái việc gán nhãn lại điểm dữ liệu. Tức là khi chúng ta gán một cái điểm dữ liệu cho một cái nhãn, ví dụ nhãn một rồi sang cái lần sau à cái điểm này nó vẫn tiếp tục là một. Sang lần sau điểm này tiếp tục là một. Tức là nó không cập nhật nhãn. Chúng ta sẽ lấy một ví dụ có nhiều hơn điểm ha. 1 rồi 2 2 và sang cái vòng lập sau chúng ta thấy cái này vẫn là nhãn 1 nhãn 1 nhãn 1 2 2",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PXfS6tZJIOI",
      "filename": "PXfS6tZJIOI",
      "title": "[CS114 - Chương 5] Cluster - Phần 2",
      "chunk_id": 8,
      "start_timestamp": "0:05:36",
      "end_timestamp": "0:06:21"
    }
  },
  {
    "page_content": "ta thấy cái này vẫn là nhãn 1 nhãn 1 nhãn 1 2 2 tức là không có cái không hoặc là rất ít ha. Tức là nếu mà có một cái điểm nào đó thay đổi nhãn thì cũng được gọi là rất ít. Thì không hoặc là rất ít nhãn à gán lại tại điểm dữ liệu thì nó được gọi là hội tụ. cái tiêu chí dừng hoặc là hội tụ thứ hai đó là không hoặc là rất ít sự thay đổi của centroid. Thì ở đây nghĩa là sao? Ví dụ như chúng ta có ba cái điểm này thì chúng ta sẽ tạo ra thành một cái centroid. Ba điểm này thì chúng ta sẽ tạo thành",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PXfS6tZJIOI",
      "filename": "PXfS6tZJIOI",
      "title": "[CS114 - Chương 5] Cluster - Phần 2",
      "chunk_id": 9,
      "start_timestamp": "0:06:17",
      "end_timestamp": "0:07:05"
    }
  },
  {
    "page_content": "centroid. Ba điểm này thì chúng ta sẽ tạo thành một cái centroid. Và sang cái vòng lập sau chúng ta thấy là là hai cái centroid này nó lại tiếp tục đứng yên. Tức là nó không có di chuyển hoặc là nó không hoặc là dịch chuyển rất là ít. Thì đây là một cái tiêu chí. Và ba đó là ờ giảm thiểu à trong tổng cái size số bình phương. Tức là à hàm SSE ở đây ha là sum square là tổng các cái khoảng cách của cái mẫu dữ liệu thứ y à đến cái MJ bình phương. Trong đó MJ chính là cái tâm à đây chính là cái tâm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PXfS6tZJIOI",
      "filename": "PXfS6tZJIOI",
      "title": "[CS114 - Chương 5] Cluster - Phần 2",
      "chunk_id": 10,
      "start_timestamp": "0:06:59",
      "end_timestamp": "0:08:05"
    }
  },
  {
    "page_content": "đó MJ chính là cái tâm à đây chính là cái tâm cụm thứ. Rồi thì cái size số này, cái size số này là nhỏ nhất. Và để mà à đạt được cái tiêu chí số 3 này thì đây là một cái bài toán khó là vì nó phải tối ưu toàn cục. Nó phải là tối ưu toàn cục. Tức là chúng ta sẽ tìm cái cách để phân hoạch cái tập điểm này làm sao đó để khoảng cách giữa xy đến một cái tâm cụm thứ j là à tổng của nó tổng hết tất cả các điểm thôi nha là nhỏ nhất. Đó. Rồi t chí thứ tư đó là C là cụm thứ J và CJ là cụm thứ J và MJ là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PXfS6tZJIOI",
      "filename": "PXfS6tZJIOI",
      "title": "[CS114 - Chương 5] Cluster - Phần 2",
      "chunk_id": 11,
      "start_timestamp": "0:07:59",
      "end_timestamp": "0:08:47"
    }
  },
  {
    "page_content": "đó là C là cụm thứ J và CJ là cụm thứ J và MJ là cái tâm của cái cụm đó. À lưu ý là C đây nó sẽ là một cái tập hợp đại diện cho một cụm. À đây là một cái tập hợp. Đây là một tập hợp đại diện. Còn MJ là cái giá trị trung bình nó cộng trung bình tất cả các điểm trong cái tập hợp này là cái điểm nằm ngay chính giữa đó là điểm ngay nằm trung tâm của cái cụm thứ CJ. Rồi và distance này chính là cái khoảng cách giữa hai cái điểm XY và MJ. Thì lưu ý là ở đây là chúng ta có ba cái tiêu chí. Trong đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PXfS6tZJIOI",
      "filename": "PXfS6tZJIOI",
      "title": "[CS114 - Chương 5] Cluster - Phần 2",
      "chunk_id": 12,
      "start_timestamp": "0:08:41",
      "end_timestamp": "0:09:24"
    }
  },
  {
    "page_content": "là ở đây là chúng ta có ba cái tiêu chí. Trong đó cái ở dưới đây là cái mô tả cái định nghĩa cho cái ở phía trên thôi. Tức là cái CJ là gì, MJ là gì, distance của XY, MJ là gì. Nó đang mô tả cho cái công thức ở phía trên thôi. Như vậy thì với ba tiêu chí này thì đây là cái tiêu chí toàn cục. và khó khó có thể tìm ra được. Trong khi đó hai tiêu chí ở trên thì nó sẽ rất là dễ để cài đặt. Chúng ta chỉ cần lặp đi lặp lại thuật toán. Khi thấy cái điểm dữ liệu nó không gán nhãn nữa thì chúng ta dừng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PXfS6tZJIOI",
      "filename": "PXfS6tZJIOI",
      "title": "[CS114 - Chương 5] Cluster - Phần 2",
      "chunk_id": 13,
      "start_timestamp": "0:09:19",
      "end_timestamp": "0:09:44"
    }
  },
  {
    "page_content": "dữ liệu nó không gán nhãn nữa thì chúng ta dừng hoặc là tâm cụm của mình nó không có dịch chuyển nhiều thì nó sẽ là dừng. Còn tiêu chí thứ tư à tiêu chí thứ ba thì không chắc lúc nào chúng ta cũng có thể khi mà thuật toán không gán nhãn lại hoặc là tâm cụng không di chuyển thì nó đã là cái điểm tối ưu toàn cục cái đó không chắc chắn.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=PXfS6tZJIOI",
      "filename": "PXfS6tZJIOI",
      "title": "[CS114 - Chương 5] Cluster - Phần 2",
      "chunk_id": 14,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chúng ta sẽ cùng đến với cái mô hình toán học của nó dướ dạng là vectơ. Thế thì ở trong cái công thức đằng trước á thì cái hàm nếu mà viết theo cái kiểu WB như vậy á thì nó sẽ khá là dài dòng à là bằng W1x1 cộng cho W2X2 cộng chấm chấm chấm đó thì nó sẽ dài dòng. Bây giờ chúng ta quy tụ W là thành W1, W2 cho đến WN và X sẽ là X1, X2 cho đến Xn. Thì khi đó cái hàm mô hình của mình sẽ viết gọn lại dướ dạng như sau. Đó là FWBX thì sẽ là bằng vectơ W nhân cho X. Thì khi đó chúng ta sẽ nhân phân phối",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qFLltTMPxvU",
      "filename": "qFLltTMPxvU",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 3)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:01:06"
    }
  },
  {
    "page_content": "nhân cho X. Thì khi đó chúng ta sẽ nhân phân phối vô từng phần tử W1X1, W2X2 và WN Xn. Rồi cuối cùng đó là chúng ta sẽ cộng cho thành phần bias. Thì đây là cái hàm mô hình viết với dạng là vectơ hóa và W nhân với X ở đây thì đây chính là cái ký hiệu của cái phép tích vô hướng của hai cái vecơ. Thì chúng ta chú ý cái công thức này. Và khi chúng ta đã có cái hàm mô hình rồi thì chúng ta sẽ đi tính cái hàm chi phí hay là hàm mất mát. Ờ thì ở đây cái công thức của mình nó sẽ là f wb xy trừ cho cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qFLltTMPxvU",
      "filename": "qFLltTMPxvU",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 3)",
      "chunk_id": 1,
      "start_timestamp": "0:00:58",
      "end_timestamp": "0:01:54"
    }
  },
  {
    "page_content": "công thức của mình nó sẽ là f wb xy trừ cho cái giá trị thực tế của mẫu dữ liệu thứ y tất cả bình phương. Và chúng ta sẽ lấy tổng size số à trung bình cộng của size số. Chúng ta lưu ý là nó sẽ có một thêm cái con số hai ở đây. Thì mục tiêu của nó đó là để cho khi chúng ta tính đạo hàm thì nó khử được cái con số 2 ở mũ của cái công thức này. Thì mục tiêu của nó là như vậy. Khi tính đạo hàm viết ra cái công thức của mình nó sẽ đẹp. Và W thì vẫn là bằng W1 cho đến WM là vecơ trọng số cần được huấn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qFLltTMPxvU",
      "filename": "qFLltTMPxvU",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 3)",
      "chunk_id": 2,
      "start_timestamp": "0:01:48",
      "end_timestamp": "0:02:33"
    }
  },
  {
    "page_content": "bằng W1 cho đến WM là vecơ trọng số cần được huấn luyện của mô hình. B là hệ số chặn cũng là cái hệ số bias cần phải xác định của mình. Đương nhiên nếu mà V mà nó không giống à nó có các cái giá trị của mình nó độc lập thì nó sẽ thể hiện được là W1 sẽ có cái trọng số nhất định với cái X1. W2 nó sẽ có một cái trọng số tin tưởng nhất định đối với cái W đối với X2. Thì đây là những cái hệ số huấn luyện được. Rồi X thì nhìn có vẻ giống như là à biến số nhưng thật ra nó là hằng số hay là cái giá trị",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qFLltTMPxvU",
      "filename": "qFLltTMPxvU",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 3)",
      "chunk_id": 3,
      "start_timestamp": "0:02:25",
      "end_timestamp": "0:03:23"
    }
  },
  {
    "page_content": "số nhưng thật ra nó là hằng số hay là cái giá trị của mẫu dữ liệu mà mình thu thập được. à đưa cái này cho à cái cái việc chúng ta sẽ đưa cái mô hình này để huấn luyện nó và phục vụ cho cái mô hình của cái hàm loss này. Trong cái công thức của hàm Jacobi à hàm hàm J thì chúng ta thấy chỉ có biến số là W và B. Còn X Y và Y của mình thì nó tương ứng chính là cái dữ liệu chúng ta đưa vào à dữ liệu huấn luyện. Rồi m ở đây sẽ là cái số lượng mẫu dữ liệu huấn luyện của mình. Thì khi đó cái công thức",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qFLltTMPxvU",
      "filename": "qFLltTMPxvU",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 3)",
      "chunk_id": 4,
      "start_timestamp": "0:03:16",
      "end_timestamp": "0:04:03"
    }
  },
  {
    "page_content": "huấn luyện của mình. Thì khi đó cái công thức cập nhật cho thuộc toán raden cũng hoàn toàn tương tự là w = w trừ cho alpha nhân đạo hàm. À có điều để ở đây là hàm của chúng ta là một cái hàm đa biến. Đó thì chúng ta sẽ ký hiệu như thế này. Đạo hàm của hàm đa biến. B cũng như vậy thì b sẽ là bằng b tr- alpha nhân cho đạo hàm của chi theo theo b. Ở đây chúng ta sẽ có cái tham số là learning ray là cái hệ số học hoặc là tốc độ học. Nhưng mà mình có một cái từ khác có thể là ừ nó sẽ gần gũi hơn đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qFLltTMPxvU",
      "filename": "qFLltTMPxvU",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 3)",
      "chunk_id": 5,
      "start_timestamp": "0:03:58",
      "end_timestamp": "0:04:52"
    }
  },
  {
    "page_content": "một cái từ khác có thể là ừ nó sẽ gần gũi hơn đó là hệ số dò hay còn gọi là dò dẫm. Thì nếu như cái đạo hàm này á nó chỉ hướng cho chúng ta là đi hướng này đó thì chúng ta sẽ đi theo cái hướng ngược lại. Chúng ta sẽ đi theo cái hướng ngược lại thì chắc chắn nó sẽ hướng đến cái điểm cực tiểu. Và nếu chúng ta đi theo cái hướng cực lại không thì nó sẽ có cái tình huống là đạo hàm của mình nó sẽ có giá trị rất là lớn. mình sẽ không biết là đi như thế nào. Thì cách đi của chúng ta đó là chúng ta sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qFLltTMPxvU",
      "filename": "qFLltTMPxvU",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 3)",
      "chunk_id": 6,
      "start_timestamp": "0:04:48",
      "end_timestamp": "0:05:35"
    }
  },
  {
    "page_content": "nào. Thì cách đi của chúng ta đó là chúng ta sẽ dò dẫm. À chúng ta sẽ dò dẫm chúng ta thêm vô một cái đại lượng alpha thì khi chúng ta đi thì chúng ta sẽ đi từng bước nhỏ thay vì chúng ta đi một cái bước lớn như thế này. Thì đó chính là cái ý nghĩa của à cái à hệ số alpha. Và tổng hợp lại thì chúng ta sẽ có đạo hàm riêng của hàm chi phí J theo từng cái WJ thì nó sẽ là bằng công thức này. Đối với W đối với WJ thì nó sẽ là bằng 1/m của tổng. hay nói cách khác đây chính là cái trung bình của cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qFLltTMPxvU",
      "filename": "qFLltTMPxvU",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 3)",
      "chunk_id": 7,
      "start_timestamp": "0:05:28",
      "end_timestamp": "0:06:33"
    }
  },
  {
    "page_content": "nói cách khác đây chính là cái trung bình của cái sai số giữa giá trị dự đoán và giá trị thực tế và chưa hết chúng ta sẽ còn nhân với lại cái thành phần xy j và khi chúng ta cập nhật cái cho trọng số wj và b thì chúng ta sẽ cài đặt bằng cái hai cái công thức này. đó thì W và và B đã được cập nhật lại. Thế thì cái chi tiết cho từng bước của thực toán của chúng ta thì bước một đó là chúng ta sẽ khởi tạo các cái tham số của mô hình bao gồm là W1 cho đến WM cộng với lại cái hệ số hệ số chặn hay còn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qFLltTMPxvU",
      "filename": "qFLltTMPxvU",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 3)",
      "chunk_id": 8,
      "start_timestamp": "0:06:26",
      "end_timestamp": "0:07:08"
    }
  },
  {
    "page_content": "đến WM cộng với lại cái hệ số hệ số chặn hay còn gọi là bias đó thì thường chúng ta để cho đơn giản để suy nghĩ chúng ta sẽ cho nó là một con số ngẫu nhiên còn cái việc mà cho giá trị nó bằng 0 hết thì nó sẽ tìm mẫn rất nhiều rủi ro đó thì cái cách này chúng ta hạn chế sử dụng mà ưu tiên là chọn ra các cái ờ giá trị ngẫu nhiên nhưng mà nó vừa phải. Tiếp theo đó là chúng ta sẽ lập cho đến khi hội tụ và chúng ta sẽ làm hai công việc đó là tính đạo hàm của hàm lỗi theo à W và tính đạo hàm của hàm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qFLltTMPxvU",
      "filename": "qFLltTMPxvU",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 3)",
      "chunk_id": 9,
      "start_timestamp": "0:07:01",
      "end_timestamp": "0:07:44"
    }
  },
  {
    "page_content": "hàm của hàm lỗi theo à W và tính đạo hàm của hàm lỗi theo B. Và khi chúng ta đã có được cái công thức của nó rồi thì chúng ta hoàn toàn có thể thế vào những cái phần phía sau. Ví dụ như là W cập nhật lại là bằng W trừ cho alpha nhân J chia cho đạo hàm theo WJ. Thì à cái công thức này thì hoàn toàn có thể tính tự động được. Nhưng mà cái đạo hàm này thì sao? Có tính được tự động hay không? Thì câu trả lời là có. Đạo hàm chúng ta có thể tính được cái ờ hàm số đạo hàm của một hàm số bất kỳ. Và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qFLltTMPxvU",
      "filename": "qFLltTMPxvU",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 3)",
      "chunk_id": 10,
      "start_timestamp": "0:07:38",
      "end_timestamp": "0:07:52"
    }
  },
  {
    "page_content": "cái ờ hàm số đạo hàm của một hàm số bất kỳ. Và chúng ta sẽ dừng cái thuật toán này khi thuậc toán của mình nó đã hội tụ. Nó đã hội tụ.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=qFLltTMPxvU",
      "filename": "qFLltTMPxvU",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 3)",
      "chunk_id": 11,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề tập trung vào môn học CS114, học mấy, tăng cường, reinforcement learning. Chúng ta sẽ so sánh học tăng cường với một số lĩnh vực học khác trong máy học Đầu tiên đó là học có giám sát Học tăng cường sẽ khác với học có giám sát Đối với học có giám sát, dữ liệu để chúng ta hướng luyện sẽ bao gồm dữ liệu đồ vào x và nhãn tương ứng y dữ liệu đồ vào này có thể hình ảnh video, văn bảng, hoặc là âm thanh Mục tiêu của học có giám sát là chúng ta sẽ học hoặc là ước lượng để tìm ra được 1 hàm ảnh xạ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Ql0n4t39rAU",
      "filename": "Ql0n4t39rAU",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 1",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:07"
    }
  },
  {
    "page_content": "học hoặc là ước lượng để tìm ra được 1 hàm ảnh xạ F từ x đến y Sau cho việc dự đoán Fx là xóc xỉ với y, Fx chính là ra trị dự đoán Còn y chính là giá trị thực tế Mục tiêu của học có giám sát là đi tìm hoặc ướp lượng hàm F này Một số ví dụ liên quan đến học có giám sát là chúng ta phân lớp dữ liệu đồ vào hoặc bài toán dự đoán giá trị liên tục, bài toán hồi quy phát hiện đối tượng, phân loại đối tượng, nhận diện dọng nói trong hình ở trên đây là một số ví dụ về học có giám sát đầu vào là tấm ảnh",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Ql0n4t39rAU",
      "filename": "Ql0n4t39rAU",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 1",
      "chunk_id": 1,
      "start_timestamp": "0:01:01",
      "end_timestamp": "0:01:56"
    }
  },
  {
    "page_content": "số ví dụ về học có giám sát đầu vào là tấm ảnh và đầu ra y của chúng ta nếu như y của chúng ta là nhãn của đối tượng ví dụ đây là bear, tức là gấu thì đây là bài toán phân loại đối tượng nhưng nếu y của chúng ta là mask cho biết có những đối tượng nằm ở vị trí nào Vị trí nào thì đây là bài toán phân đoạn đối tượng Còn nếu như chúng ta chỉ cần cái Y của chúng ta mà là cái Bounding Box kèm theo cái tên của đối tượng đó là gì thì đó là bài toán phát hiện đối tượng thì tùy vào cái Y của mình nó là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Ql0n4t39rAU",
      "filename": "Ql0n4t39rAU",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 1",
      "chunk_id": 2,
      "start_timestamp": "0:01:53",
      "end_timestamp": "0:02:33"
    }
  },
  {
    "page_content": "hiện đối tượng thì tùy vào cái Y của mình nó là cái gì thì chúng ta sẽ có cái loại bài toán tương ứng trong tương quan với học không có giám sát thì đối với mô hình học không có giám sát thì dữ liệu đầu vào chỉ có dữ liệu x đầu vào thôi và chúng ta sẽ không có nhãn tức là chúng ta sẽ không có y và mục tiêu của mình đó là sẽ khám phá cấu trúc ẩn bên trong dữ liệu tìm cách biểu diễn mới hoặc là dựa trên phân bố của dữ liệu mình có thể gom cộng các dữ liệu vậy với nhau ví dụ như ở đây chúng ta có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Ql0n4t39rAU",
      "filename": "Ql0n4t39rAU",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 1",
      "chunk_id": 3,
      "start_timestamp": "0:02:29",
      "end_timestamp": "0:03:12"
    }
  },
  {
    "page_content": "dữ liệu vậy với nhau ví dụ như ở đây chúng ta có dữ kiện đầu vào là các điểm của mình không có nhãn nhưng sau khi thực hiện xong, chúng ta quan sát và gom nhóm được thì chúng ta sẽ chia ra làm 3 cầm như thế này nó sẽ dựa trên phân bố của dữ liệu và ví dụ của thập không có giám sát về các thể loại bài toán đó là bài toán phân cầm, ví dụ như đây hoặc là bài toán giảm chiều dữ liệu, ví dụ như đây dựa trên phân bố của dữ liệu này thì chúng ta có thể chia tắt nó ra chiếu nó xuống một cái không gian",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Ql0n4t39rAU",
      "filename": "Ql0n4t39rAU",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 1",
      "chunk_id": 4,
      "start_timestamp": "0:03:10",
      "end_timestamp": "0:03:50"
    }
  },
  {
    "page_content": "chia tắt nó ra chiếu nó xuống một cái không gian và khi đó thì dữ liệu của mình nó giảm từ ví dụ như là 3 chiều giảm xuống chỉ còn 1 chiều nó đã giúp cho chúng ta tiết kiệm cái không gian lưu trữ thì đây là 2 cái thể loại bài toán phổ biến nhất trong những vật về học không có giám sát và đối với cái khái niệm mà chúng ta sẽ tìm hiểu ngày hôm nay đó chính là học tăng cường thì học tăng cường là chúng ta sẽ có các cái dữ liệu có liên quan đến cái thể loại học này đó chính là bao gồm trạng thái là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Ql0n4t39rAU",
      "filename": "Ql0n4t39rAU",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 1",
      "chunk_id": 5,
      "start_timestamp": "0:03:44",
      "end_timestamp": "0:04:32"
    }
  },
  {
    "page_content": "loại học này đó chính là bao gồm trạng thái là S hay còn gọi là viết tắc là Stay Action là hành động là viết tắc của chữ Action là A Phần thưởng là Reward là viết tắc là chữ R thế thì trạng thái là cái thông tin môi trường ở một cái thời điểm ví dụ như tại một cái thời điểm đó thì chúng ta sẽ biết được cái môi trường nó đang có cái trạng thái như thế nào Ví dụ như là trạng thái này thì nó có thể ở dạng hình ảnh, có thể là ở dạng vị trí hoặc là có thể là cái thông tin cảm biến mà chúng ta thu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Ql0n4t39rAU",
      "filename": "Ql0n4t39rAU",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 1",
      "chunk_id": 6,
      "start_timestamp": "0:04:27",
      "end_timestamp": "0:05:05"
    }
  },
  {
    "page_content": "có thể là cái thông tin cảm biến mà chúng ta thu thập được từ các cái cảm biến đem về Còn hành động tức Action là tập hợp các cái hành động mà Agent mà một cái tác nhân của cái học tăng cường nó có thể chọn lựa để thực hiện tức là tại một cái trạng thái chúng ta có thể đưa ra cái Action như thế nào Phần thưởng là tín hiệu phản hồi từ môi trường sau khi thực hiện hành động sau khi thực hiện hành động đó thì chúng ta sẽ nhận được cái gì Mục tiêu của học tăng cường là học chính sách hay policy nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Ql0n4t39rAU",
      "filename": "Ql0n4t39rAU",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 1",
      "chunk_id": 7,
      "start_timestamp": "0:05:00",
      "end_timestamp": "0:05:45"
    }
  },
  {
    "page_content": "học tăng cường là học chính sách hay policy nó từ không gian trạng thái sang action tức là với đầu vào là một trạng thái thì cho biết action tiếp theo chúng ta làm là gì thì cái này nó gọi là policy là hàm p và sao cho cái tổng phần thưởng kỳ vọng trong dài hạn là lớn nhất lưu ý ở đây là cái kỳ vọng, cái phần thưởng kỳ vọng trong dài hạn nó không phải dựa trên cái yếu tố ngắn hạn mà nó phải dựa trên cái dài hạn và cái kỳ vọng đó là lớn nhất ví dụ như chúng ta chơi game cờ vây, cờ bua hoặc là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Ql0n4t39rAU",
      "filename": "Ql0n4t39rAU",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 1",
      "chunk_id": 8,
      "start_timestamp": "0:05:41",
      "end_timestamp": "0:06:28"
    }
  },
  {
    "page_content": "dụ như chúng ta chơi game cờ vây, cờ bua hoặc là Atari thì đây là những cái bài toán trong game điều khiển robot, xe tự hành, v.v. Đây là một số thể loại bài toán sử dụng hợp tăng cường Ví dụ ở trên sơ đồ bên đây, chúng ta thấy chơi game thì trạng thái của mình sẽ là vị trí tại các ô chúng ta đang đặt những quân cờ nào rồi action của chúng ta, chúng ta sẽ đi quân cờ nào tiếp theo và phần thưởng cho chúng ta sẽ là cái phản hồi từ cái môi trường đó là cái phần thưởng mà chúng ta đạt được nếu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Ql0n4t39rAU",
      "filename": "Ql0n4t39rAU",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 1",
      "chunk_id": 9,
      "start_timestamp": "0:06:22",
      "end_timestamp": "0:07:00"
    }
  },
  {
    "page_content": "đó là cái phần thưởng mà chúng ta đạt được nếu chúng ta đi với cái hành động đó là gì thì đây là cái game cầu vua trong điều khiển robot thì chúng ta sẽ cho biết là cái hành động của các con robot nó sẽ phải làm gì tiếp theo để mà có thể đạt được cái mục tiêu của mình ví dụ như trong cái ví dụ ở đây chúng ta thấy là chúng ta sẽ phải điều khiển các cái hoạt động của cái cánh tay robot để sao cho phần thưởng có thể nhất được quả bóng đi lên và đặt đến một vị trí khác cho trước Trong xe tự hành",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Ql0n4t39rAU",
      "filename": "Ql0n4t39rAU",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 1",
      "chunk_id": 10,
      "start_timestamp": "0:06:51",
      "end_timestamp": "0:07:40"
    }
  },
  {
    "page_content": "đến một vị trí khác cho trước Trong xe tự hành thì cũng như vậy tức là chúng ta sẽ có các trạng thái điều kiện xe các action là chúng ta sẽ đánh vô lăng về tay trái, tay phải với góc là bao nhiêu đạp thắng hay không, để chúng ta có thể đạt được mục tiêu, phần thưởng đếm đích đến an toàn nhất và nhanh nhất và vì sao chúng ta cần phải có hợp tâm tăng cường? là vì chúng ta sẽ lấy một ví dụ đó là con người của chúng ta trong quá trình con người học thì nó sẽ dùng cơ chế đó là thử và sai và bằng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Ql0n4t39rAU",
      "filename": "Ql0n4t39rAU",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 1",
      "chunk_id": 11,
      "start_timestamp": "0:07:37",
      "end_timestamp": "0:08:21"
    }
  },
  {
    "page_content": "thì nó sẽ dùng cơ chế đó là thử và sai và bằng cách đó là tương tác với môi trường Ví dụ như ngay khi chúng ta còn nhỏ, chúng ta đã thử sai trong công việc liên quan đến tập đi Chúng ta thử những bước chân đầu tiên và đi chúng ta bị té Từ đó nó sẽ dạy cho bộ não chúng ta cách thức để chúng ta có thể giữ thăng bằng Cách thức để chúng ta có thể di chuyển làm sao đó không bị té hoặc đi nhanh mà đạt được đến đích đến và đó là cơ chế của con người, một cách tự nhiên của con người, đó là tương tác",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Ql0n4t39rAU",
      "filename": "Ql0n4t39rAU",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 1",
      "chunk_id": 12,
      "start_timestamp": "0:08:13",
      "end_timestamp": "0:08:57"
    }
  },
  {
    "page_content": "một cách tự nhiên của con người, đó là tương tác với môi trường thông qua cơ chế thử và sai và không chỉ ghi dữ liệu mà chúng ta còn có ra các quyết định và nhận phản hồi không chỉ là ghi nhớ dữ liệu, mà chúng ta phải đưa ra các quyết định và nhận phản hồi từ môi trường ví dụ như nếu chúng ta thực thi hành động đó, thì môi trường sẽ phản hồi lại như thế nào Ví dụ khi chúng ta bước vào một khu vực bị thấp xuống, bị trũng xuống thì có thể chúng ta sẽ bị mất thăng bằng thì phản hồi của môi trường",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Ql0n4t39rAU",
      "filename": "Ql0n4t39rAU",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 1",
      "chunk_id": 13,
      "start_timestamp": "0:08:47",
      "end_timestamp": "0:09:29"
    }
  },
  {
    "page_content": "sẽ bị mất thăng bằng thì phản hồi của môi trường sẽ làm cho chúng ta mất thăng bằng Đó là một vài ví dụ để minh họa cho cách thức học tăng cường đã bắt trước hoạt động học tập của con người chúng ta như thế nào Và AI cần cơ chế tương tự như vậy để có thể thích ứng với môi trường khi môi trường của mình nó thay đổi lưu ý là trong học tăng cường thì mọi thứ nó đều có tính thay đổi và môi trường của mình cũng như vậy rồi tự khám phá các chiến lược mới chúng ta sẽ có rất nhiều những lần thử và sai",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Ql0n4t39rAU",
      "filename": "Ql0n4t39rAU",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 1",
      "chunk_id": 14,
      "start_timestamp": "0:09:24",
      "end_timestamp": "0:09:41"
    }
  },
  {
    "page_content": "mới chúng ta sẽ có rất nhiều những lần thử và sai và ứng bắt chích những lần thử và sai chúng ta nhận được phản hồi nhận được cái reward từ môi trường thì từ đó chúng ta sẽ đúc kết được các kinh nghiệm và trong những lần tiếp theo chúng ta sẽ thay đổi chiến thuật cho nó phù hợp nhất Hãy subscribe cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video hấp dẫn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Ql0n4t39rAU",
      "filename": "Ql0n4t39rAU",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 1",
      "chunk_id": 15,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chào mừng các bạn đến với khóa học CS114, học máy. Chúng ta sẽ cùng đến với nội dung về giảm chiều dữ liệu. Tên tiếng Anh đó là Dimensionality Reduction Thì nói về khái niệm của giảm chiều dữ liệu Thì đây là một cái phương pháp để mà biểu diễn một tập các dữ liệu nhất định Lu ý là ở đây chúng ta sẽ có cái từ là nhất định Tức là nó sẽ giảm chiều cho những cái loại dữ liệu cho trước Chứ không phải là dữ liệu nào cũng có thể giảm chiều được Và đây là một cái phương pháp mà sử dụng ít đặc trưng hơn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RGEA9a14dBs",
      "filename": "RGEA9a14dBs",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 1",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:51"
    }
  },
  {
    "page_content": "một cái phương pháp mà sử dụng ít đặc trưng hơn tức là số chiều ít hơn so với lại đặc trưng ban đầu trong khi mà nó vẫn nắm bắt được các thuộc tính có ý nghĩa của dữ liệu góc thì ở đây chúng ta sẽ có một ví dụ minh họa cho việc giảm chiều dữ liệu rất là kinh điển và phổ thông đó là chúng ta dùng phép chiếu kỹ thuật thì ở đây sẽ là dữ liệu ở trong không gian góc Vì vậy, chúng ta muốn biểu diễn dữ liệu của chiếc ly này ở trong không gian ít chiều hơn thì chúng ta sẽ thực hiện phép chiếu và khi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RGEA9a14dBs",
      "filename": "RGEA9a14dBs",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 1",
      "chunk_id": 1,
      "start_timestamp": "0:00:47",
      "end_timestamp": "0:01:47"
    }
  },
  {
    "page_content": "hơn thì chúng ta sẽ thực hiện phép chiếu và khi chúng ta chiếu xuống dưới một cái mặt phẳng thì nó đã từ không gian 3 chiều nó giảm xuống còn một cái không gian đó là 2 chiều như vậy thì nó đã giúp cho chúng ta tiết kiệm được cái chi phí về mặt nưu trữ và về mặt kỹ thuật thì khi chúng ta chiếu ở 3 cái góc, 3 cái mặt phẳng khác nhau trực giao ở đây là chiếu ngang, chiếu xuống dưới và chiếu qua tay phải thì chúng ta có thể khôi phục ngược trở lại cái chữ liệu trong không gian gốc 3 chiều thế thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RGEA9a14dBs",
      "filename": "RGEA9a14dBs",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 1",
      "chunk_id": 2,
      "start_timestamp": "0:01:40",
      "end_timestamp": "0:02:25"
    }
  },
  {
    "page_content": "cái chữ liệu trong không gian gốc 3 chiều thế thì cái phép chiếu này nó đã giúp chúng ta có cung cấp một cái góc nhìn khác của cái đối tượng ở một cái mặt phẳng và mặt phẳng này là mặt phẳng 2 chiều và từ 3 cái mặt phẳng 2 chiều này chúng ta có thể hình dung được cái hình dạng của đối tượng khi ở trong không gian 3 chiều thế thì cái việc chiếu xuống này nó vẫn giúp cho chúng ta giữ được những cái ý nghĩa của cái dữ liệu góc mà chúng ta không có bị mất đi thế thì tại sao chúng ta cần phải giảm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RGEA9a14dBs",
      "filename": "RGEA9a14dBs",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 1",
      "chunk_id": 3,
      "start_timestamp": "0:02:21",
      "end_timestamp": "0:03:01"
    }
  },
  {
    "page_content": "bị mất đi thế thì tại sao chúng ta cần phải giảm chiều dữ liệu thì có rất nhiều lý do lý do đầu tiên đó là việc giảm chiều dữ liệu thì khi chúng ta sử dụng cái dữ liệu ở trong cái không gian ít chiều hơn thì thời gian huấn luyện mô hình sẽ giảm xuống Cái này là đương nhiên đúng không ạ? Và nó sẽ yêu cầu tài nguyên tính toán về mặt phần cứng là CPU, GPU cũng sẽ thấp hơn vì số chiều dữ liệu ít hơn thì chi phí tính toán cũng sẽ thấp hơn Và đôi khi trong một số thực toán thì nó sẽ chỉ hiệu quả khi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RGEA9a14dBs",
      "filename": "RGEA9a14dBs",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 1",
      "chunk_id": 4,
      "start_timestamp": "0:02:57",
      "end_timestamp": "0:03:36"
    }
  },
  {
    "page_content": "trong một số thực toán thì nó sẽ chỉ hiệu quả khi chúng ta làm việc trên chiều dữ liệu thấp Còn nếu chúng ta làm trên những dữ liệu trong không gian dày đặt nhiều chiều thì nó sẽ không hiệu quả Trong khi đó không gian mà thư thớt thì nó lại hiệu quả hơn Và nó lại giúp chúng ta ngăn ngừa hiện tượng overfitting Thì hiện tượng overfitting là một hiện tượng trong học máy trong đó mô hình của mình quá phức tạp Khi chúng ta giảm nhiều dữ liệu xuống thì chúng ta đã giảm độ phức tạp của mô hình Các dạm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RGEA9a14dBs",
      "filename": "RGEA9a14dBs",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 1",
      "chunk_id": 5,
      "start_timestamp": "0:03:33",
      "end_timestamp": "0:04:24"
    }
  },
  {
    "page_content": "chúng ta đã giảm độ phức tạp của mô hình Các dạm độ phức tạp này đã giúp chúng ta ngầm tránh được hiện tượng Overfitting Do nó loại bỏ được các kế thuật tấm thừa và phụ thuộc lại nhau Rồi nó cho phép chúng ta có thể biểu diễn dữ liệu ở trong một khuôn dạng phức tạp là không gian 2 chiều hoặc là 3 chiều để giúp chúng ta có thể khám phá và trực quan phân tích và khám phá được thì cái này nó phục vụ cho cái việc đầu tiên đó là chúng ta sẽ trực quan hóa dữ liệu tại vì trong không gian mà nhiều hơn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RGEA9a14dBs",
      "filename": "RGEA9a14dBs",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 1",
      "chunk_id": 6,
      "start_timestamp": "0:04:15",
      "end_timestamp": "0:04:53"
    }
  },
  {
    "page_content": "hóa dữ liệu tại vì trong không gian mà nhiều hơn 3 chiều chúng ta sẽ rất khó mà có thể tưởng tượng được nhưng mà khi chúng ta chiếu xuống cái không gian 2 chiều và 3 chiều đó là 2 cái không gian mà chúng ta có thể dễ dàng vẽ ở bên trong máy tính thì việc trực quan hóa dữ liệu sẽ giúp chúng ta hiểu dữ liệu hơn và từ việc hiểu dữ liệu hơn thì chúng ta sẽ dễ dàng khám phá dữ liệu của mình hơn Và một ví dụ về giảm chiều dữ liệu, đó là chúng ta sẽ xét một tập dữ liệu x bao gồm các phần tử xy trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RGEA9a14dBs",
      "filename": "RGEA9a14dBs",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 1",
      "chunk_id": 7,
      "start_timestamp": "0:04:47",
      "end_timestamp": "0:05:27"
    }
  },
  {
    "page_content": "một tập dữ liệu x bao gồm các phần tử xy trong đó thì y sẽ chạy từ 1 đến n, tức là chúng ta có n phần tử về các xe máy và được đặc trưng bởi một tập hợp các kế thuật tính và n các kế thuật tính thì bao gồm là kích thước, màu sắc, tốc độ tối đa v.v. thì một chiếc xe máy sẽ được biểu diễn bởi n kế thuật tính này và giả sử có hai kế thuật tính có kế thuật tính tương quan chặt chẽ với nhau ví dụ như là xej là tốc độ theo dặm giờ và xeq là tốc độ theo kmh thì ta có thể loại bỏ 1 trong 2 thuộc tính",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RGEA9a14dBs",
      "filename": "RGEA9a14dBs",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 1",
      "chunk_id": 8,
      "start_timestamp": "0:05:23",
      "end_timestamp": "0:06:16"
    }
  },
  {
    "page_content": "kmh thì ta có thể loại bỏ 1 trong 2 thuộc tính này tại vì thuộc tính Xiz và Xik tức là mẫu dữ liệu thứ y nhưng trường thông tin thứ chi và ka có tính tương tự nhau 1 cái thì tính theo dặm, 1 cái thì tính theo km thì 2 thuộc tính này hoàn toàn tương tự nhau và chúng ta hoàn toàn có thể loại bỏ ra khỏi bảng dữ liệu giúp chúng ta tiết kiệm chi phí tính toán khi chúng ta lại bỏ 1 trong 2 thuộc tính này thì nó đã giúp chúng ta giảm số chiều dữ liệu xuống chỉ còn là n-1 thôi lưu ý cái này là chống",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RGEA9a14dBs",
      "filename": "RGEA9a14dBs",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 1",
      "chunk_id": 9,
      "start_timestamp": "0:06:13",
      "end_timestamp": "0:07:03"
    }
  },
  {
    "page_content": "xuống chỉ còn là n-1 thôi lưu ý cái này là chống chấm thang chứ không phải là cái phép gọi là dài thừa một cái ứng dụng khác đó là giảm chiều dữ liệu giúp chúng ta khám phá được cấu trúc tập dữ liệu MNIST MNIST là một bộ data set gồm các chữ số, chữ cái từ 0 cho đến 9 và khi chúng ta chiếu ma trận địa mảnh ví dụ ở đây chúng ta có số 3 chẳng hạn chúng ta chiếu ma trận địa mảnh này xuống không gian thì chúng ta thấy nó sẽ nằm ở trong khu vực như thế này và chúng ta sẽ thấy những điểm nào mà cục",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RGEA9a14dBs",
      "filename": "RGEA9a14dBs",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 1",
      "chunk_id": 10,
      "start_timestamp": "0:06:55",
      "end_timestamp": "0:07:42"
    }
  },
  {
    "page_content": "thế này và chúng ta sẽ thấy những điểm nào mà cục cùng một con số thì nó sẽ nằm trong cùng một khu vực giống nhau Và một ví dụ khác đó là giúp cho chúng ta có sự liên kết giữa các thực thể trong dữ liệu của mình Ví dụ như đây khi chúng ta trực quan hóa dữ liệu DNA của các quốc gia ở châu Âu thì người ta thấy rằng có sự tương quan về mặt vị trí địa lý trong thực tế thì những quốc gia nào nằm gần nhau thì sẽ có cái cấu trúc, có cái cấu tạo của DNA nó sẽ giống nhau và nó sẽ nằm gần nhau rồi một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RGEA9a14dBs",
      "filename": "RGEA9a14dBs",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 1",
      "chunk_id": 11,
      "start_timestamp": "0:07:33",
      "end_timestamp": "0:08:23"
    }
  },
  {
    "page_content": "nó sẽ giống nhau và nó sẽ nằm gần nhau rồi một cái ứng dụng khác của giảm chiều dữ liệu đó là giúp chúng ta có thể nén hình ảnh ví dụ như đây là một cái ảnh gốc, ảnh Z-Pax thì nó có cái kích thước là hơn 800 kilobyte và khi chúng ta nén mà có mất mát và chấp nhận đó là mất mát khoảng 50% thì nó giảm từ 800 chỉ còn khoảng 76kb tức là đã giảm hơn 10 lần mặc dù là cái mức độ mất mát về mặt thông tin chỉ là 50% nhưng về mặt thị giác nếu chúng ta để ý kỹ thì 2 cái tấm hình này gần như là giống nhau",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RGEA9a14dBs",
      "filename": "RGEA9a14dBs",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 1",
      "chunk_id": 12,
      "start_timestamp": "0:08:14",
      "end_timestamp": "0:08:53"
    }
  },
  {
    "page_content": "ý kỹ thì 2 cái tấm hình này gần như là giống nhau Nếu như chúng ta không để ý kỹ, nếu như chúng ta xem vô cấu tạo từng pixel thì chúng ta thấy nó có sự khác biệt, nó sẽ có sự không mực mà ở đây Nhưng nếu chúng ta nhìn lướt qua thì chúng ta sẽ thấy là không có sự thay đổi đáng kể về mặt thị giác Như vậy thì chúng ta hoàn toàn có thể chấp nhận, đánh đổi cái việc là có thể nó sẽ không giống với dữ liệu ban đầu nhưng mà khối lượng dữ liệu của mình sẽ được giảm đi một cách đáng kể Và để mô hình hóa",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RGEA9a14dBs",
      "filename": "RGEA9a14dBs",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 1",
      "chunk_id": 13,
      "start_timestamp": "0:08:47",
      "end_timestamp": "0:09:37"
    }
  },
  {
    "page_content": "được giảm đi một cách đáng kể Và để mô hình hóa cho cái bài toán này thì chúng ta sẽ sử dụng cái mô hình như sau đó là giảm chiều dữ liệu, đó là một cái thuật toán học không giám sát để học từ phân bố của một cái dữ liệu nhất định để giúp cho chúng ta chuyển dữ liệu đó từ không gian nhiều chiều từ không gian nhiều chiều ban đầu sang cái không gian ít chiều hơn thì bản chất nó chính là một cái hàm mapping Và map từ không gian của x về z thì z là một tập hợp biểu diễn các số chiều thấp hơn của dữ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RGEA9a14dBs",
      "filename": "RGEA9a14dBs",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 1",
      "chunk_id": 14,
      "start_timestamp": "0:09:31",
      "end_timestamp": "0:10:38"
    }
  },
  {
    "page_content": "tập hợp biểu diễn các số chiều thấp hơn của dữ liệu Còn x là biểu diễn của dữ liệu góc ban đầu và có số chiều nhiều hơn so với lại z Với mỗi đầu vào xy, đây là mẫu dữ liệu và hàm fθ sẽ tính toán và tạo ra một biểu diễn thấp nhiều hơn tức là fθ của x,y thì nó sẽ ra là bằng z,y thì đây chính là cái biểu diễn ít chiều hơn của của x của dư tiêu góc ban đầu Và để mà giảm chiều, ở trong slide trước thì chúng ta nói về mô hình hóa bài toán một cách toàn quát Tức là từ một cái không gian x về không",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RGEA9a14dBs",
      "filename": "RGEA9a14dBs",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 1",
      "chunk_id": 15,
      "start_timestamp": "0:10:27",
      "end_timestamp": "0:11:10"
    }
  },
  {
    "page_content": "toàn quát Tức là từ một cái không gian x về không gian z, không gian z, ít chiều hơn Còn ở đây thì chúng ta sẽ giảm chiều một cách tuyến tính Tức là để chuyển đổi từ không gian nhiều chiều về không gian ít chiều thì chúng ta sẽ sử dụng một phép biến đổi tuyến tính Và đối với giảm chiều tiến tín thì nó vẫn thỏa mãn các tín chấp đó là x là 1 cái vector rd và z là 1 cái không gian là rp Thì p nó sẽ bấy hơn d, tức là không gian sau khi biến đổi nó sẽ nhỏ hơn sau với lại cái không gian ban đầu Và ở",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RGEA9a14dBs",
      "filename": "RGEA9a14dBs",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 1",
      "chunk_id": 16,
      "start_timestamp": "0:11:06",
      "end_timestamp": "0:11:59"
    }
  },
  {
    "page_content": "nhỏ hơn sau với lại cái không gian ban đầu Và ở đây thì chúng ta sẽ có cái phép biến đổi tiến tín đó là z, tức là cái điểm dữ liệu sau khi chúng ta đã giảm chiều dữ liệu thì là bằng S theta x là bằng cái hàm theta sẽ là bằng hàm w nhân với x trong đó thì cái tham số theta của mình nó chính là w hai thì thằng này thật ra là 1 trong trường hợp này là 1 nhưng mà về mặt ký hiệu tổng quát thì theta thường được sử dụng để ký hiệu cho tham số do đó chúng ta sẽ ký hiệu là theta nhưng mà trong trường",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RGEA9a14dBs",
      "filename": "RGEA9a14dBs",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 1",
      "chunk_id": 17,
      "start_timestamp": "0:11:51",
      "end_timestamp": "0:12:15"
    }
  },
  {
    "page_content": "ta sẽ ký hiệu là theta nhưng mà trong trường hợp này theta nó chính là bằng w là một cái ma trận có kích thước là d nhân với lại p khi ma trận w là d nhân với p, khi chúng ta chuyển vị nhân với x thì nó sẽ giúp chúng ta ánh sạ từ 1 vétter r, d chiều về vétter có r, rp chiều Vector chiều thấp hơn z sẽ được tạo ra từ vector x ban đầu và ma trận w Z được tạo ra bởi vector x ban đầu và ma trận w tức là chúng ta sẽ lấy khí phép nhân tuyên tính Hãy subscribe cho kênh Ghiền Mì Gõ Để không bỏ lỡ những",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RGEA9a14dBs",
      "filename": "RGEA9a14dBs",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 1",
      "chunk_id": 18,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video hấp dẫn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=RGEA9a14dBs",
      "filename": "RGEA9a14dBs",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 1",
      "chunk_id": 19,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Tiếp theo thì chúng ta sẽ cùng à thảo luận cái câu hỏi à đó là tại sao chúng ta không kế thừa cái mô hình hồi quy tuyến tính để giải quyết bài toán phân loại nhị phân. Thì trong cái mô hình hồi quy tiến tính chúng ta biết rằng là cái công thức của mình đó là y = wx + b. Thì một cái phương trình đường thẳng thì nó sẽ đưa về cái dạng đó là wx cộng b là bằng 0. Thì cái này chúng ta đã thảo luận trong à những cái phần trước rồi đó. Thì khi những cái điểm mà thế vào wx + b = 0 thì nó sẽ nằm trên một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rVJXIPI3laA",
      "filename": "rVJXIPI3laA",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 6",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:52"
    }
  },
  {
    "page_content": "điểm mà thế vào wx + b = 0 thì nó sẽ nằm trên một cái đường thẳng. Và những cái điểm nào mà nằm về cùng một phía thì khi chúng ta thế vào thì nó sẽ là cùng âm hoặc là cùng dương. Ví dụ với cái điểm màu xanh ở đây khi chúng ta thế vào thì wx + b sẽ là một con số lớn hơn hoặc bằng 0. Và tất cả những cái điểm nào mà nằm về cùng một phía thì cũng đều có tính chất này. Ở khía cạnh ngược lại thì tất cả những cái điểm nào mà nằm về phía đối diện thì đ khi thế vào wx + b thì nó sẽ ra con số là bé hơn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rVJXIPI3laA",
      "filename": "rVJXIPI3laA",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 6",
      "chunk_id": 1,
      "start_timestamp": "0:00:48",
      "end_timestamp": "0:01:34"
    }
  },
  {
    "page_content": "khi thế vào wx + b thì nó sẽ ra con số là bé hơn 0. Và chính nhờ cái tính chất mà phân chia ra làm hai phần như thế này thì nó sẽ dẫn đến chúng ta có thể viết một cái hàm mô hình để mà dự đoán xem cái nhãn của mình là nhãn âm à là nhãn 1 hay là nhãn 0 bằng cách đó là chúng ta sẽ viết bằng công thức f của x là bằng đây là hàm dự đoán ha đây là hàm dự đoán sẽ là bằng 1 nếu wx cộng b lớn hơn hoặc bằng 0 và bằng 0 nếu wx cộng bé hơn 0 thì như vậy là hàm này chúng ta sẽ thấy nó có một cái tính chất",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rVJXIPI3laA",
      "filename": "rVJXIPI3laA",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 6",
      "chunk_id": 2,
      "start_timestamp": "0:01:28",
      "end_timestamp": "0:02:25"
    }
  },
  {
    "page_content": "hàm này chúng ta sẽ thấy nó có một cái tính chất đó là nó không liên tục đồng thời đó là hàm này nó sẽ khó tính đạo hàm vì cái sự không liên tục này nên nó sẽ khó tính đạo hàm được. Đó do đó thì à chúng ta sẽ không có sử dụng cái hàm này à tại vì cái thuật toán chính mà dùng để huấn luyện cho các cái mô hình hồi quyến tính và à hồi quym hồi quy logistic thì đều dựa trên là radian design. Mà trong cái thuật toán gradient des thì chúng ta thấy nó có cái thao tác đó là W là bằng W trừ cho alpha",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rVJXIPI3laA",
      "filename": "rVJXIPI3laA",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 6",
      "chunk_id": 3,
      "start_timestamp": "0:02:20",
      "end_timestamp": "0:03:08"
    }
  },
  {
    "page_content": "có cái thao tác đó là W là bằng W trừ cho alpha nhân cho đạo hàm của J theo à W. Mà cái thao tác quan trọng nhất của chúng ta chính là thao tác tính đạo hàm. Mà chúng ta tính không được thì làm sao chúng ta có thể cập nhật được cái thuật toán design này. À cập nhật được cái tham số này. Do đó thì chúng ta sẽ phải có một cái hàm đó là hàm sigmo gắn thằng trước. Thế thì cái lợi điểm khi chúng ta gắn vào cái hàm sigo ở đây đó là gì? Khi gắn vào Sigmo thì chúng ta sẽ đưa cái miền giá trị của mình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rVJXIPI3laA",
      "filename": "rVJXIPI3laA",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 6",
      "chunk_id": 4,
      "start_timestamp": "0:03:04",
      "end_timestamp": "0:03:51"
    }
  },
  {
    "page_content": "thì chúng ta sẽ đưa cái miền giá trị của mình thay vì là từ cộng vô cùng à từ trừ vô cùng cho đến cộng vô cùng thì qua cái hàm set nó sẽ không có từ trừ vô cùng đến cộng vô cùng nữa mà nó sẽ kéo về cái đoạn là từ 0 cho đến 1. bạn từ 0 cho đến 1. Cái dạng thức của cái hàm sigmo của mình á thì nó sẽ có cái dạng là như thế này. Đó thì nó sẽ đi từ 0 cho đến cho đến 1. Thế thì ở đây chúng ta sẽ thấy nó có một cái ngưỡng là 0.5 5 ở đây thì nếu như wx + b mà bằng 0 á thì lúc đó tương ứng sig của wx +",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rVJXIPI3laA",
      "filename": "rVJXIPI3laA",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 6",
      "chunk_id": 5,
      "start_timestamp": "0:03:47",
      "end_timestamp": "0:04:44"
    }
  },
  {
    "page_content": "+ b mà bằng 0 á thì lúc đó tương ứng sig của wx + b lúc đó sẽ là bằng 0.5 thì đây sẽ là cái ngưỡng ở giữa để nó sẽ không biết là thuộc về cái điểm màu xanh hay là màu cam. Đó thì thay vì chúng ta dùng cái phân ngưỡng là 0 thì bây giờ cái phân ngưỡng của chúng ta lúc này nó chính là 0.5. Nhưng mà cái điểm lợi khi chúng ta sử dụng hàm sigmo ở đây đó là hàm sigmo nó sẽ ờ tính đạo hàm rất là đẹp. Cụ thể đó là đạo hàm của sig x thì nó sẽ là bằng sigmo nhân cho 1 - sig. Đó thì công thức này nó cũng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rVJXIPI3laA",
      "filename": "rVJXIPI3laA",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 6",
      "chunk_id": 6,
      "start_timestamp": "0:04:38",
      "end_timestamp": "0:05:33"
    }
  },
  {
    "page_content": "nhân cho 1 - sig. Đó thì công thức này nó cũng tương đối là đẹp. Và ngoài ra thì chúng ta sẽ phải kết hợp cái hàm sigmo này với một cái hàm so max, một cái hàm log loss để mà tính cái size số. Còn bản thân cái hàm sigmo của wx + b nó đã ép cái giá trị từ 0 cho đến 1 rồi. Nếu nó gần cái giá trị 0 hơn thì cái nhãn y của mình sẽ là bằng 0. Nếu nó gần cái số 1 hơn thì nhãn y của mình nó sẽ là bằng 1. Như vậy là qua cái hàm sig nó đã đưa cái miền giá trị từ trừ vô cùng cộng vô cùng về chính cái miền",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rVJXIPI3laA",
      "filename": "rVJXIPI3laA",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 6",
      "chunk_id": 7,
      "start_timestamp": "0:05:27",
      "end_timestamp": "0:06:11"
    }
  },
  {
    "page_content": "trị từ trừ vô cùng cộng vô cùng về chính cái miền giá trị mà có hai cái phân cực là hai cái nhãn chúng ta cần phân loại đó là 0 và 1. Đó. Rồi thì đó chính là hai cái tiện lợi khi chúng ta sử dụng hàm sigmo đó là đưa về chính cái không gian nhãn của mình luôn là 0 và 1. Và đạo hàm của nó thì là liên tục và tính một cách dễ dàng. Mặt khác khi chúng ta sử dụng cái hàm à sig này thì khi cái giải giá trị từ 0 cho đến 1 chúng ta cũng có thể tính được cái hàm loss. Tính được cái hàm loss một cách dễ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rVJXIPI3laA",
      "filename": "rVJXIPI3laA",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 6",
      "chunk_id": 8,
      "start_timestamp": "0:06:06",
      "end_timestamp": "0:06:58"
    }
  },
  {
    "page_content": "cái hàm loss. Tính được cái hàm loss một cách dễ dàng giữa i và p. Đó, trong trường hợp này thì p sigma của wx + b ha. Thì lúc này nó chỉ đơn giản nó là bằng trừ của p à trừ của y log p và trừ của 1 - y log của 1 - p. Thì cái công thức này cũng hoàn toàn có thể tính đạo hàm được một cách dễ dàng và nó là một cái hàm liên tục thì giúp cho cái việc huấn luyện cái thuật toán rent của mình nó sẽ dễ dàng hơn. Thì đó chính là lý do tại sao chúng ta sử dụng mô hình hồi quyến tính. không sử dụng mô đồ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rVJXIPI3laA",
      "filename": "rVJXIPI3laA",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 6",
      "chunk_id": 9,
      "start_timestamp": "0:06:53",
      "end_timestamp": "0:07:04"
    }
  },
  {
    "page_content": "dụng mô hình hồi quyến tính. không sử dụng mô đồ tiến tính mà chúng ta phải có thêm cái thành phần đó là sigmo ở đây.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rVJXIPI3laA",
      "filename": "rVJXIPI3laA",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 6",
      "chunk_id": 10,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề, học sâu, machine learning, imageNet. Chủ đề, học sâu, machine learning, imageNet. Rồi, thì rõ ràng là không có cách nào chúng ta có thể tìm được một cái đường thẳng có thể chia hai tập điểm màu xanh màu đỏ này ra làm hai Tuy nhiên, đây là một cái dĩ liệu điểm ngoại lệ và có thể đây sẽ là một cái điểm nhũ tức là cái nhãn của nó được gắn xai Nếu chúng ta cố gắng tìm một mô hình để phân loại điểm màu xai này, thì vô hình chúng ta đang làm xay lệch đi bản chất của mô hình của mình Do đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:18"
    }
  },
  {
    "page_content": "xay lệch đi bản chất của mô hình của mình Do đó chúng ta sẽ thích lập thuộc tán SVM để nó có thể chấp nhận cho một vài điểm dữ liệu ngoại lệ với assumption, với giải định rằng là cái điểm ngoại lệ của mình phải đủ ít và những điểm dữ liệu mà được gán nhãn chuẩn của mình sẽ là số đông ví dụ như ở đây chúng ta thấy là tập hợp các điểm này là những điểm gán nhãn đúng thì nó sẽ chiếm đa số còn điểm ngoại lại, ngoại lệ thì nó chỉ là thiểu số thôi thì mô hình SVM, thực tế SVM nó sẽ chấp nhận một vài",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 1,
      "start_timestamp": "0:01:09",
      "end_timestamp": "0:01:45"
    }
  },
  {
    "page_content": "mô hình SVM, thực tế SVM nó sẽ chấp nhận một vài điểm ngoại lệ này chúng ta sẽ đến tình huống thứ 2 tức là bản chất dữ liệu của mình là phi tuyến bản chất dữ liệu của mình là phi tuyến rồi và không thể nào mà chúng ta có thể tìm được 1 hyperplane để phân tách ra làm 2 ví dụ như ở đây chúng ta thấy không thể nào chúng ta có thể tìm được 1 đường đẳng để tắt ra làm 2 phần do đó thì chúng ta sẽ xử lý như thế nào thì thay vì chỉ giới hạn hyperplane tuyến tính Thay vì là hyperplane tuyến tính, thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 2,
      "start_timestamp": "0:01:42",
      "end_timestamp": "0:02:25"
    }
  },
  {
    "page_content": "tuyến tính Thay vì là hyperplane tuyến tính, thì chúng ta sẽ là trong cái không gian ban đầu thì chúng ta sẽ nâng cấp, tạm gọi là nâng cấp nâng cấp cái không gian của mình lên và SVM sẽ sử dụng cách tiếp cận đó là biến đổi đặc trưng biến đổi đặc trưng tạo ra đặc trưng mới và bổ sung thêm, đó là z là bằng x bình cộng y bình trong cái ngữ cảnh này, z là bằng x bình cộng y bình khi đó, chữ liệu từ không gian OSE chúng ta sẽ chuyển sang một không gian mới là OXZ X vẫn giữ nguyên chúng ta thấy là từ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 3,
      "start_timestamp": "0:02:23",
      "end_timestamp": "0:03:07"
    }
  },
  {
    "page_content": "mới là OXZ X vẫn giữ nguyên chúng ta thấy là từ trái sang vải X vẫn giữ nguyên nhưng mà trục Y của mình thay vì như trong không gian góc thì Y của mình trục tung của mình là Z sẽ là bằng X bình cộng Y bình tức là lấy giá trị theo trục y bình phương lên cộng cho x bình phương lên thì nói cách khác đó chính là cái khoảng cách nó chính là cái bình phương của cái khoảng cách đến các cái điểm này thì khi đó chúng ta thấy là cái trục z này sẽ là nhận những cái giá trị dương nó nằm ở phía trên của cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 4,
      "start_timestamp": "0:03:04",
      "end_timestamp": "0:03:41"
    }
  },
  {
    "page_content": "cái giá trị dương nó nằm ở phía trên của cái nó sẽ nằm ở phía trên của cái trục z của mình trục o x của mình và khi đó chúng ta thấy là hoàn toàn dễ dàng có thể chia tắt nó ra làm 2 bằng một cái siêu phẳng như thế này có thể chia tách nó làm 2 đây là một cái siêu phẳng mới trong một cái không gian mới thay vì chúng ta tìm nó trong không gian osz thì chúng ta tìm nó trong cái không gian osz Như vậy thì trong tình huống dữ liệu không có tuyến tính hay là phi tuyến tính thì dữ liệu của mình không",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 5,
      "start_timestamp": "0:03:39",
      "end_timestamp": "0:04:29"
    }
  },
  {
    "page_content": "hay là phi tuyến tính thì dữ liệu của mình không thể phân tác được hoàn toàn bằng một đường thẳng hay là siêu phẳng duy nhất trong không gian góc thì ở đây chúng ta sẽ có một số tình huống ở bên trái là Linear Separable tức là có thể tìm được một siêu phẳng chia tách làm 2 còn 2 tình huống bên đây là Non-linear Separable thì các cái điểm này nó sẽ được phân tác ra bởi những cái đường cong như thế này hoặc là trong tình huống này thì là một cái đường cong kép kín như thế này thì đây là hai cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 6,
      "start_timestamp": "0:04:26",
      "end_timestamp": "0:05:04"
    }
  },
  {
    "page_content": "đường cong kép kín như thế này thì đây là hai cái tình huống mà phi tuyến tính và để có cái giải pháp cho cái tình huống phi tuyến tính này thì chúng ta sẽ có hai cái giải pháp SVM nó có hai giải pháp giải pháp đầu tiên đó là soft margin và giải pháp thứ hai đó là kernel trick tức là chúng ta sử dụng cái phương pháp kernel đối với cái giải pháp soft margin là nó sẽ cho phép một số cái dữ liệu của mình nó sẽ vi phạm nó sẽ vi phạm cái vi phạm này có nghĩa là sao? tức là nó nằm xa phía nó nằm xa",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 7,
      "start_timestamp": "0:05:02",
      "end_timestamp": "0:05:37"
    }
  },
  {
    "page_content": "có nghĩa là sao? tức là nó nằm xa phía nó nằm xa phía lẽ ra nó nằm bên phía bên phải thì nó lại nằm phía bên trái lẽ ra nằm ở trên thì nó lại nằm ở dưới ví dụ vậy nó sẽ nằm ở xa phía trong cái vùng margin của mình và mục đích đó là gì? là tăng khả năng tổng hoa khóa của mô hình lên và xử lý được những dữ liệu nhỉu nhỉu ở đây thì chúng ta có thể hiểu đó là những điểm mà nó có thể gán nhãn sai thực tế thì việc mà gán nhãn dữ liệu mà bị sai là một việc rất phổ biến và các mô hình của chúng ta phải",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 8,
      "start_timestamp": "0:05:34",
      "end_timestamp": "0:06:28"
    }
  },
  {
    "page_content": "rất phổ biến và các mô hình của chúng ta phải tìm cách để mà có thể thích ứng được trong những tình huống như thế này thì cơ chế đó là chúng ta sẽ cân bằng giữa cái việc là tối đa hóa cái marine cái mục tiêu ban đầu của chúng ta, đây là cái mục tiêu ban đầu nè nhưng nếu chúng ta chỉ chăm chăm, bám theo cái mục tiêu ban đầu này á thì nó sẽ rất là khó khăn trong việc học khi có những điểm nhiễu như thế này do đó chúng ta phải cân bằng được cái yếu tố thứ 2 đó là phải giảm được cái lỗi phân loại",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 9,
      "start_timestamp": "0:06:25",
      "end_timestamp": "0:07:05"
    }
  },
  {
    "page_content": "tố thứ 2 đó là phải giảm được cái lỗi phân loại tức là vừa tối đa hóa cái margin nhưng đồng thời là cái lỗi phân loại của mình nó sẽ là thấp nhất thì khi đó chúng ta sẽ ứng dụng cái phương pháp shop margin này chúng ta ứng dụng cái phương pháp shop margin này khi dữ liệu của mình gần như tuyến tính nhưng mà nó có nhiễu, nó gần như tuyến tính thôi thì ở trong sơ đồ bên trái là một ví dụ mình họa dữ liệu là Linear Separable tức là chúng ta thấy có một cái đường phân tích nó làm 2 tức là nó hoàn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 10,
      "start_timestamp": "0:07:00",
      "end_timestamp": "0:07:45"
    }
  },
  {
    "page_content": "một cái đường phân tích nó làm 2 tức là nó hoàn toàn là Linear Separable nhưng có một cái điểm nhiễu thì ở đây chúng ta thấy có cái điểm này nó lại nằm rất gần so với các điểm màu xanh bên đây mặc dù cái nhãn của nó là màu đỏ Thế thì, mô hình của mình phải có cơ chế để cho phép chấp nhận điểm nhiều này Chứ nếu không chấp nhận thì đường phân lớp của mình không phải nằm ở đây Mà đúng ra nó phải nằm ở đây mới đúng Tại vì nó cần bằng À xin lỗi là nó phải nằm ở đây mới đúng Tức là đúng ra là đường",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 11,
      "start_timestamp": "0:07:41",
      "end_timestamp": "0:08:14"
    }
  },
  {
    "page_content": "phải nằm ở đây mới đúng Tức là đúng ra là đường phân lớp của mình sẽ phải nằm ở đây tại vì tại vị trí này thì nó sẽ chia hai tạp màu màu xanh màu đỏ ra làm hai và cái margin của mình lúc này là lớn nhất nhưng rõ ràng chúng ta thấy là cái điểm này về mặt trực quan thì đó là một cái điểm nhịu do đó nó cần phải xem xét lại bỏ cái điểm này đi để mà kéo cái đường biên của mình về đây thì với cái đường biên của mình về đây thì cái tính tổng bắt hóa của nó nó sẽ cao hơn cái tình huống bên tay phải đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 12,
      "start_timestamp": "0:08:12",
      "end_timestamp": "0:08:50"
    }
  },
  {
    "page_content": "nó nó sẽ cao hơn cái tình huống bên tay phải đó là dữ liệu thì không linear separable ví dụ chúng ta thấy là ở đây không có cách nào có cách nào để chúng ta có thể chia nó ra làm 2 phần không có cách nào để chia ra làm 2 phần mà bằng 1 đường thẳng hết á thì tuy nhiên nó lại gần linear separable tức là nó ngoại trừ những cái điểm như thế này những cái điểm như thế này hoặc là những cái điểm như thế này thì tất cả những cái, nếu bỏ đi 4 cái điểm này đi thì chúng ta thấy là nó là linear separable",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 13,
      "start_timestamp": "0:08:41",
      "end_timestamp": "0:09:24"
    }
  },
  {
    "page_content": "đi thì chúng ta thấy là nó là linear separable và cái margin của mình mà tách ra làm 2 thì nó hoàn toàn là và margin càng lớn thì tính tổng hóa quá sẽ càng cao thì đây là 2 cái thình huống là dữ liệu gần như linear separable tức là gần như có thể phi phân tách được ra bởi một đường thẳng tuy nhiên vì có một số điểm nhiễu nó sẽ ép cái mode của mình khiến cho cái margin của mình rất là bé tức là không có tính tổng hóa còn bên đây là Chữ liệu của mình nó hoàn toàn không có linear separable nhưng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 14,
      "start_timestamp": "0:09:20",
      "end_timestamp": "0:09:57"
    }
  },
  {
    "page_content": "mình nó hoàn toàn không có linear separable nhưng mà nó gần gần Thì cái khái niệm gần gần này có nghĩa là sao? Nó chỉ có một vài Nếu chúng ta bỏ một vài điểm rất là ít như là bà chị ở đây Thì nó sẽ chuyển sang cái dạng là linear separable Và cái giải pháp mà chúng ta sẽ sử dụng trong cái tình huống này đó là kernel trick Tức là chúng ta sẽ tìm một cái phép biến đổi sao cho cái dữ liệu ban đầu không phân biệt tuyến tính được sang một cái không gian mới tức là ban đầu dữ liệu của mình không thể",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 15,
      "start_timestamp": "0:09:52",
      "end_timestamp": "0:10:37"
    }
  },
  {
    "page_content": "mới tức là ban đầu dữ liệu của mình không thể nào phân biệt được, phân tách được ra bằng một cái siêu phẳng thì chúng ta sẽ biến nó sang một cái không gian mới và ở cái không gian mới này dữ liệu này hoàn toàn có thể phân biệt được một cách tuyến tính ví dụ như ở đây chúng ta thấy cái dữ liệu trong cái không gian góc thì rõ ràng chúng ta chỉ có thể phân tách nó ra bởi một cái vòng tròn thôi và rõ ràng vòng tròn là không có tiến tính thì chúng ta sẽ tìm cách map nó vào cái không gian nhiều chiều",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 16,
      "start_timestamp": "0:10:32",
      "end_timestamp": "0:11:13"
    }
  },
  {
    "page_content": "sẽ tìm cách map nó vào cái không gian nhiều chiều hơn thì ở đây chúng ta thấy đây là một không gian 2 chiều khi chuyển sang cái không gian 3 chiều thì nó đưa về một cái parabole và chúng ta có thể cắt nó ra bằng một cái mặt phẳng thì đây là một cái hyper parameter, ờ, một cái hyperplane 1 cái siêu phẳng có thể tách nó ra làm 2 thì đây chính là phương pháp kernel trick và một số kernel trick phổ biến ví dụ như linear kernel công thức rất đơn giản trong đó x,y và x,y chính là 2 cái đặc trưng đầu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 17,
      "start_timestamp": "0:11:09",
      "end_timestamp": "0:12:13"
    }
  },
  {
    "page_content": "trong đó x,y và x,y chính là 2 cái đặc trưng đầu vào là 2 cái vector đặc trưng đầu vào thì kernel này của mình đó là chỉ đơn giản là lấy x,y nhân với lại x,y ý nghĩa đó là chỉ là cái tích vô hướng giữa hai vector đặc trưng và nó được dùng khi dữ liệu của mình là gần như tuyến tính dữ liệu của mình là gần như tuyến tính, tức là có thể phân tách được ra làm 2 bởi 1 cái siêu phẳng, hoặc là nó chỉ có 1 vài điểm nhịu nhỏ nó chỉ có 1 vài điểm nhịu nhỏ còn nếu bỏ đi những điểm nhịu đó thì nó sẽ tắt ra",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 18,
      "start_timestamp": "0:12:07",
      "end_timestamp": "0:12:50"
    }
  },
  {
    "page_content": "còn nếu bỏ đi những điểm nhịu đó thì nó sẽ tắt ra được bằng 1 cái siêu phẳng hoặc là khi số chiều của mình rất cao Ví dụ như trong bài toán phân loại văn bản, số chiều của mình rất là cao Cơ nổ phổ biến tiếp theo là polynomial kernel Cơ nổ sẽ là xy xz cộng cho xe tất cả mũ đe Đây là mũ của đa thức Đây là cái bậc của đa thức của mình và ý nghĩa đó là cho phép mô hình học quan hệ ở bậc cao hơn nó cho phép mô hình học ở bậc cao hơn của rạch trưng và nó thường được sử dụng khi dữ liệu của mình có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 19,
      "start_timestamp": "0:12:45",
      "end_timestamp": "0:13:27"
    }
  },
  {
    "page_content": "và nó thường được sử dụng khi dữ liệu của mình có quan hệ phi tiến tính mối quan hệ phi tiến tính nhưng không quá phức tạp tại đây là đa thức, nó không quá phức tạp và nhược điểm của nó là tính toán rất chậm khi đa thức bậc cao đa thức bậc càng cao thì tốc độ tính toán của chúng ta sẽ càng chậm và một kernel rất phổ biến cho trường hợp phi tiến tính đó là RBF Radio Biasis Function, gào sinh tên gọi khác là gào sinh kernel công thức của nó là k của xe và xe sẽ là bằng hàm e mũ exponential trừ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 20,
      "start_timestamp": "0:13:24",
      "end_timestamp": "0:14:04"
    }
  },
  {
    "page_content": "của xe và xe sẽ là bằng hàm e mũ exponential trừ xy, trừ xz tất cả bình chia cho 2 sigma bình thì đây chính là công thức Gaussian và ý nghĩa của nó là ánh xạ một dữ liệu sang một không gian vô hạn chiều ánh xạ sang một không gian vô hạn chiều và nó thường được sử dụng khi hầu hết các tình huống phi tuyến tính thì thường chúng ta sẽ sử dụng cái RBF kernel này tức là khi chúng ta biết rằng là Cái dữ liệu của mình có mối quan hệ là phi tiến tính thì Mặc nhiên ban đầu chúng ta nên sử dụng cái RBF",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 21,
      "start_timestamp": "0:14:00",
      "end_timestamp": "0:14:40"
    }
  },
  {
    "page_content": "Mặc nhiên ban đầu chúng ta nên sử dụng cái RBF kernel này trước Do đó thì đây là một cái kernel rất là phủng Dùng cái từ gọi là đặc biệt phổ biến trong thực tế Và ưu điểm của nó đó là rất là mạnh mẽ và linh hoàng Và nhược điểm của nó đó là Nó sẽ cần phải tối ưu Nó sẽ cần phải tìm ra cái tham số sigma phù hợp Chứ nếu không thì nó sẽ dễ bị hiện tượng là Overfitting, tức là quá khớp với dữ liệu Rồi một kernel tiếp theo nữa đó là SIGMOY kernel thì công thức của nó sẽ là bằng tanh của alpha nhân với",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 22,
      "start_timestamp": "0:14:37",
      "end_timestamp": "0:15:20"
    }
  },
  {
    "page_content": "thức của nó sẽ là bằng tanh của alpha nhân với XA nhân với XZ cộng C tuy nhiên kernel này thì nó không có phổ biến lắm ý nghĩ của nó thì nó cũng tương tự như một cái neural network nó cũng tương tự như một cái neural network có một lớp ẩn với cái hàm kích hoạt là hàm tanh đây là cái hàm Activation Function và nó được dùng khi ờ một cái phép thử trong cái đôi khi dùng trong như một cái phép thử trong cái dữ liệu phi tiến tính còn thực tế thì cái này là rất là ít sử dụng và nó ít dùng hơn so với",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 23,
      "start_timestamp": "0:15:17",
      "end_timestamp": "0:16:09"
    }
  },
  {
    "page_content": "này là rất là ít sử dụng và nó ít dùng hơn so với RBS hoặc là Linear Kernel rất là nhiều khi dữ liệu phi tiến tính thì chúng ta dùng Linear Kernel khi dữ liệu phi tiến tính thì thường chúng ta dùng RBS nguyên nhân đó là vì nó dễ gây ra bất bổn khi và các thông tin không phù hợp Hình bên đây là một trực quan hóa cho đường bao đường phân tách các tập dữ liệu của mình trong một số tình huống là kernel thì ở đây chúng ta sẽ thấy là đặc trưng của mình sẽ là kiếp SelecWidth và SelecLand và chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 24,
      "start_timestamp": "0:16:03",
      "end_timestamp": "0:16:43"
    }
  },
  {
    "page_content": "sẽ là kiếp SelecWidth và SelecLand và chúng ta sẽ chia nó ra làm các cái thơ phần này và ở đây chúng ta đang lấy một ví dụ đó là phân lớp là nhiều lớp thì trong phần sau chúng ta sẽ nói rõ hơn đó là SVM hoàn toàn không phải chỉ có thể phân lớp nhị phân mà nó có thể phân lớp nhiều lớp ví dụ trong trường hợp này là 3 lớp thì ở trong cái ví dụ này, cái ví dụ trực hoan hóa này chúng ta chỉ thấy được là cái đường biên của linear kernel là chúng ta thấy nó rất là thẳng, nó thẳng thớm Còn đường biên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 25,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "nó rất là thẳng, nó thẳng thớm Còn đường biên của ABF rất là smooth, cong và trơn Còn polinomial và sigmoid, đặc biệt là sigmoid kernel rất là tệ Do đó thì thường chúng ta sẽ không sử dụng sigmoid kernel Còn polinomial kernel thì nó sẽ tạo ra các đường cong nhưng mà cái đường cong này nó cũng còn tương đối là đơn giản cái RBS kernel thì nó sẽ tạo ra các đường cong nó sẽ uốn lượng và nó sẽ phức tạp hơn và vì nó phức tạp hơn nên nó có thể phủ được nhiều tình huống sử dụng trong thực tế hơn Hãy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 26,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "nhiều tình huống sử dụng trong thực tế hơn Hãy subscribe cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video hấp dẫn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=rzKMT_sUg7Q",
      "filename": "rzKMT_sUg7Q",
      "title": "[CS114 - Chương 8] SVM (Part 2)",
      "chunk_id": 27,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chúng ta sẽ cùng bàn về những cái ưu điểm của thực toán PCA. PCA à nó là một cái công cụ để hỗ trợ cho việc trực quan hóa dữ liệu. Hãy tưởng tượng rằng là cái dữ liệu của chúng ta có D chiều và D này thì nó lớn hơn 3. Thì trong cái khả năng tưởng tượng của chúng ta thì chúng ta chỉ có thể hình dung được cái không gian hai chiều hoặc là ba chiều là cùng. Khi cái số chiều lớn hơn, chúng ta sẽ không thể quan sát và chúng ta có thể thấy được cái mối quan hệ giữa các cái dữ liệu với nhau. Do đó thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SINj0Z-2acc",
      "filename": "SINj0Z-2acc",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 4",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:50"
    }
  },
  {
    "page_content": "quan hệ giữa các cái dữ liệu với nhau. Do đó thì cái việc giảm dữ liệu từ X ờ D chiều xuống còn à một cái vecơ là R ờ P chiều trong đó P nhỏ hơn D thì và cụ thể luôn P ở đây có thể là bằng 2 hoặc là P là bằng 3 thì nó sẽ đưa về cái không gian hai chiều hoặc là ba chiều thì chúng ta có thể vẽ biểu đồ và quan sát cái mối mối quan hệ giữa các dữ liệu một cách dễ dàng. Và một cái ứng dụng, một cái ưu điểm nữa của PC đó là loại bỏ đa cộng tuyến. À tức là cái vấn đề mà cái đặc trưng nó bị phụ thuộc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SINj0Z-2acc",
      "filename": "SINj0Z-2acc",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 4",
      "chunk_id": 1,
      "start_timestamp": "0:00:41",
      "end_timestamp": "0:01:29"
    }
  },
  {
    "page_content": "là cái vấn đề mà cái đặc trưng nó bị phụ thuộc lẫn nhau. Giả sử như trong một cái mô hình máy học, chúng ta có các cái trường thông tin là x1, x2 và x3. Trong đó x1 là một cái hàm tuyến tính của x2 và x3, tức là một cái mối quan hệ phụ thuộc tiến tính. Thế thì việc xử lý tính toán trên cả x1, x2, x3 nó sẽ vừa tốn tài nguyên tính toán cũng như là có thể gây cái sai lệch trong cái xây dựng mô hình. Do đó chúng ta có thể loại bỏ đi một cái thành phần đa cộng tuyến x1 để chỉ giữ lại cái x2 và x3",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SINj0Z-2acc",
      "filename": "SINj0Z-2acc",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 4",
      "chunk_id": 2,
      "start_timestamp": "0:01:23",
      "end_timestamp": "0:02:05"
    }
  },
  {
    "page_content": "phần đa cộng tuyến x1 để chỉ giữ lại cái x2 và x3 thôi. Và giảm nhiễu thì nhờ cái việc phân tích à phương sai rồi trong cái quá trình mà tực thi tính toán cái cái thành phần chính thì nó đã giúp cho chúng ta loại bỏ đi những cái thành phần à nhiễu và không có liên quan đến cái bài toán của mình. Tại vì cái sai số nó quá bé, tức là cái hàm lượng thông tin của mình nó không có nhiều. Đó. Rồi cái ưu điểm nữa đó là giúp giảm tham số của mô hình. Thay vì chúng ta xây dựng một cái mô hình có D tham",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SINj0Z-2acc",
      "filename": "SINj0Z-2acc",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 4",
      "chunk_id": 3,
      "start_timestamp": "0:02:02",
      "end_timestamp": "0:02:35"
    }
  },
  {
    "page_content": "vì chúng ta xây dựng một cái mô hình có D tham số, bây giờ chúng ta đưa về một cái mô hình chỉ có P tham số thì cái chi phí tính toán nó thấp hơn. Nhưng đồng thời à nó sẽ làm cho cái tham số mô hình giảm xuống, nó sẽ vô hình chung làm giảm cái hiện tượng gọi là overfitting. Giảm cái hiện tượng overfitting. Đây là một trong những cái vấn đề rất là kinh điển của máy học. Và khi giảm cái khối lượng tính toán thì đồng thời cái tốc độ huấn luyện của chúng ta cũng sẽ nhanh hơn.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=SINj0Z-2acc",
      "filename": "SINj0Z-2acc",
      "title": "[CS114 - Chương 5] Dimension Reduction - Phần 4",
      "chunk_id": 4,
      "start_timestamp": "0:02:29",
      "end_timestamp": "0:03:07"
    }
  },
  {
    "page_content": "Như vậy thì chúng ta có thể thấy đó là ở đây nó đang bị lỗi và nó chưa có thể phân ra làm hai được. Thì bây giờ chúng ta sẽ cùng debx xem cái lỗi này có nguyên nhân là do đâu thì chúng ta sẽ dừng cái thực toán. Và chúng ta sẽ xem đầu tiên đó là chúng ta sẽ kiểm tra cái dữ liệu đầu vào trước là x của mình có đúng như chúng ta kỳ vọng hay không. Rồi sau đó là y. Rồi các cái phần còn lại thì chúng ta sẽ tạm thời là vòng lập là chúng ta sẽ khóa lại và chúng ta sẽ chạy. Thì chúng ta thấy là cái dữ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Sp6gOu7BFpo",
      "filename": "Sp6gOu7BFpo",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 2)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:01:03"
    }
  },
  {
    "page_content": "và chúng ta sẽ chạy. Thì chúng ta thấy là cái dữ liệu x của mình à y đây là y nè. Thì nếu đúng nó phải ra là một mãng gồm à các số là 00 sau đó sẽ là nối 111. Nhưng mà hình như nó đang hiểu sai là cho với mỗi cái thành phần x1 nó sẽ là có một cái nhãn riêng. Thành phần x2 là gán một cái nhãn riêng. Rồi à như vậy thì cái này là sai cái y. Tương tự như vậy x của mình thì lẽ ra là chúng ta sẽ phải nối lại à thì chúng ta lại đi trồng lên nó lên. Như vậy thì chúng ta sẽ sửa lại cái code này là ở chỗ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Sp6gOu7BFpo",
      "filename": "Sp6gOu7BFpo",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 2)",
      "chunk_id": 1,
      "start_timestamp": "0:00:56",
      "end_timestamp": "0:01:42"
    }
  },
  {
    "page_content": "vậy thì chúng ta sẽ sửa lại cái code này là ở chỗ cái hàm concast. Đầu tiên đối với y thì ở đây chúng ta sẽ không lấy là chấm shap tại vì nó sẽ lấy theo x1 và x2 mà chúng ta sẽ lấy là lens của hoặc ở đây chúng ta có cái biến nsf rồi chúng ta sẽ truyền nsf luôn đó. Rồi thì bây giờ chúng ta sẽ chạy lại ha. Và y lúc này nó đã đúng như chúng ta muốn. Đó là những 10 cái phần tử đầu tiên 10 cái điểm đầu tiên nó sẽ là số 0. 10 cái điểm tiếp theo thì nó sẽ là cái nhãn là 1. Đó. Bây giờ chúng ta sẽ sửa",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Sp6gOu7BFpo",
      "filename": "Sp6gOu7BFpo",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 2)",
      "chunk_id": 2,
      "start_timestamp": "0:01:36",
      "end_timestamp": "0:02:20"
    }
  },
  {
    "page_content": "sẽ là cái nhãn là 1. Đó. Bây giờ chúng ta sẽ sửa đến X. Thế thì X thay vì chúng ta còn c như thế này thì chúng ta sẽ phải thêm cái asis là bằng 1. Tại vì nếu không mặc định nó sẽ bằng 0. Tức là nó sẽ trồng lên. Còn chúng ta đang muốn trồng ngang ra. Đó. Rồi chúng ta sẽ in x. Rồi thì ở đây chúng ta sẽ để là 1 ha. Đó. Đó thì ở đây nè chúng ta thấy là nó đã có nối lại chúng ta sẽ có 20 cái x1 và chúng ta sẽ có 20 cái x2. Rồi bây giờ chúng ta sẽ chạy lại cái chương trình của mình. Chúng ta sẽ chạy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Sp6gOu7BFpo",
      "filename": "Sp6gOu7BFpo",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 2)",
      "chunk_id": 3,
      "start_timestamp": "0:02:15",
      "end_timestamp": "0:03:21"
    }
  },
  {
    "page_content": "lại cái chương trình của mình. Chúng ta sẽ chạy lại cái vòng lập này của mình lưu lại. Rồi chúng ta thấy là nó có cái sự à ok. Ở đây chúng ta thấy là nó đã bị thay đổi, biến đổi và tự nhiên ép về một cái đường tròn. Thì cái này nguyên nhân đó là vì à chúng ta thấy là một cái ô bên đây nó sẽ có ba 30 à xin lỗi là 200 có cái khoảng là giá trị là 200 trong khi một ô ở đây thì nó chỉ có khoảng là 0.5 đó thì dẫn đến đó là nó bị bẻ méo cái hình đó. Do đó thì ở đây chúng ta sẽ giới hạn lại. Khi chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Sp6gOu7BFpo",
      "filename": "Sp6gOu7BFpo",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 2)",
      "chunk_id": 4,
      "start_timestamp": "0:03:18",
      "end_timestamp": "0:04:09"
    }
  },
  {
    "page_content": "đó thì ở đây chúng ta sẽ giới hạn lại. Khi chúng ta vẽ thì chúng ta sẽ giới hạn lại là PLT chấ xlim thì nó sẽ kéo là từ 1 cho đến từ 0 cho đến 4 thôi. Và tương tự như vậy cho Yim thì chúng ta sẽ kéo là từ 0 cho đến 4. Rồi ở trên đây cũng vậy. Đó thì chúng ta sẽ thấy là khi chúng ta cố định cái X lim và Y lim á limit á thì nó sẽ không có bị bép bóp méo cái cái khung hình của mình. Và ban đầu thì nó sẽ nằm ở tuốc phía bên dưới. Cái mô hình của mình nó sẽ nằm cái đường boundary của mình nó nằm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Sp6gOu7BFpo",
      "filename": "Sp6gOu7BFpo",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 2)",
      "chunk_id": 5,
      "start_timestamp": "0:04:05",
      "end_timestamp": "0:04:53"
    }
  },
  {
    "page_content": "mình nó sẽ nằm cái đường boundary của mình nó nằm tuốc ở bên dưới. Sau đó chúng ta thấy là nó đã tịnh tiến dần đúng không? Nó tịnh tiến dần về khu vực này. Sau rồi nó sẽ xoay xoay lại để tách nó ra làm hai. Thì ở đây chúng ta có thể cho nó chậm hơn một chút để chúng ta hình dung cái cách mà nó chạy ha. Chúng ta nâng lên đó là khoảng 0.3 đi. Chậm lại khoảng ba lần. Thì ban đầu như đã nói là cái mô hình của mình nó sẽ nằm ở phía dưới. Sau đó thì à nó tiến dần về đây rồi sau đó nó xoay lại. Thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Sp6gOu7BFpo",
      "filename": "Sp6gOu7BFpo",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 2)",
      "chunk_id": 6,
      "start_timestamp": "0:04:48",
      "end_timestamp": "0:05:38"
    }
  },
  {
    "page_content": "à nó tiến dần về đây rồi sau đó nó xoay lại. Thì đây chính là à cái cách mà thực toán à logistic nó đã cập nhật cái trọng số như thế nào. Và trong cái quá trình cập nhật trọng số thì nó kéo cái đường phân lớp của mình đi về phía trung tâm của các cái điểm dữ liệu. Sau đó nó sẽ xoay lại. À nó sẽ xoay lại để cho các cái điểm dữ liệu của mình nó tách ra làm hai phần. Thì đây chính là cái phần minh họa cho thực toán logistic.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Sp6gOu7BFpo",
      "filename": "Sp6gOu7BFpo",
      "title": "[CS114 - Chương 4] Mô phỏng (Phần 2)",
      "chunk_id": 7,
      "start_timestamp": "0:05:29",
      "end_timestamp": "0:05:41"
    }
  },
  {
    "page_content": "Chủ đề, học sâu, machine learning, logistic regression, imageNet. một cái hành động tại cái trạng thái đó nó sẽ giúp cho Agent không chỉ nhìn vào những cái phần thưởng ngắn hạn tại vì trong nhiều cái tình huống các cái bài toán nếu như chúng ta chỉ dựa trên cái hành động ngắn hạn thì có thể là kết quả dài hạn của mình nó rất là tệ đặc biệt là trong cái game chơi cờ đúng không ví dụ như đối thủ của mình họ có thể là nhữ cho chúng ta ăn một cái con nào đó nhưng mà sau khi chúng ta ăn xong thì có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:54"
    }
  },
  {
    "page_content": "nào đó nhưng mà sau khi chúng ta ăn xong thì có thể chúng ta sẽ bị chiêu bí do đó thì cái phần thưởng dài hạn sẽ là một cái Kỳ vọng rất là quan trọng để chúng ta cần phải ước lượng và có thể đưa ra những action phù hợp tối ưu Đặc điểm của nó chính là giá trị vài value của trạng thái Giá trị của trạng thái là bằng phần thưởng tại thời điểm hiện tại Cộng với lại phần thưởng kỳ vọng trong tương lai Tại vì trong tương lai là cái thứ mà chúng ta nó sẽ có cái yếu tố khó đoán định Stochastic nó khó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 1,
      "start_timestamp": "0:00:52",
      "end_timestamp": "0:01:33"
    }
  },
  {
    "page_content": "sẽ có cái yếu tố khó đoán định Stochastic nó khó đoán định do đó nó sẽ phải dùng cái từ đó là từ kỳ vọng Kỳ vọng trong tương lai thôi Và đây là cái công cụ để mà đánh giá chất lượng của một chính sách Chất lượng của một policy Là tốt hay không tốt dựa trên cái giá trị trạng thái Giá trị kỳ vọng trong vài hạng này Phần thưởng trong vài hạng này Bí dụ thay vì chúng ta ăn một cái đồng su nhưng mà ăn đồng su đó nó sẽ bị rớt vào cái hố thì Mario nó sẽ bỏ qua cái đồng su đó để nhảy qua cái hố tức là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 2,
      "start_timestamp": "0:01:28",
      "end_timestamp": "0:02:08"
    }
  },
  {
    "page_content": "bỏ qua cái đồng su đó để nhảy qua cái hố tức là ưu tiên không ăn cái đồng su vì nó sẽ rớt vào cái hố để mà chọn cái action tiếp theo đó là nhảy qua cái hố để duy trì cái mạng sống của cái nhân vật game này đạt được cái điểm cao hơn về sau tức là bỏ qua cái tối ngắn hạng để mà có thể đạt được cái reward trong giai hạn thì cái robot mà chọn đường vòng Chọn đường vòng xa hơn nhưng ít chứng ại vật hơn Trong khi đó nếu chúng ta đi đường thẳng mà gặp rất nhiều chứng ại vật giữa đường Rõ ràng là đi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 3,
      "start_timestamp": "0:02:06",
      "end_timestamp": "0:02:48"
    }
  },
  {
    "page_content": "rất nhiều chứng ại vật giữa đường Rõ ràng là đi đường vòng mà ít chứng ại vật sẽ tốt hơn Cho phần thưởng cao hơn, đạt đi đến được đích đến nhanh hơn Và giá trị kỳ vọng của mình thì hàm giá trị trạng thái Hành giá trị trạng thái S là đầu vào của mình S đầu vào V là giá trị của kỳ vọng của mình Nó sẽ là bằng công thức kỳ vọng trên tập policy của mình Với policy hiện tại của mình, kỳ vọng của policy này sẽ là kỳ vọng của tổng của biểu thức này Trong đó là giá trị kỳ vọng của tổng các phần thưởng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 4,
      "start_timestamp": "0:02:43",
      "end_timestamp": "0:03:52"
    }
  },
  {
    "page_content": "đó là giá trị kỳ vọng của tổng các phần thưởng Trong tương lai, nó được ký hiệu bởi R T cộng 1 R chính là reward và T cộng 1 tức là tương lai và t sẽ chạy từ 0 cho đến vô cùng chạy từ 0 cho đến vô cùng tức là từ thời điểm t trở về sau và nếu như chúng ta bắt đầu tại trạng thái S tức là đây là điều kiện cho trước cho trước trạng thái bắt đầu của mình đây là giá trị kỳ vọng tổng phần thưởng trong tương lai nếu chúng ta bắt đầu ở trạng thái S gamma này là ý tố về xác xúc của mình P-A cho trước S",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 5,
      "start_timestamp": "0:03:47",
      "end_timestamp": "0:04:57"
    }
  },
  {
    "page_content": "này là ý tố về xác xúc của mình P-A cho trước S là xác xúc Công thức này sẽ được diễn đạt ra bằng công thức như thế này trong đó là P-A cho trước S là xác xúc để chọn hành động này khi chúng ta ở trạng thái S P-S-S-A Tức là cái xác suất chuyển từ cái trạng thái S sang cái trạng thái S phải khi chúng ta thực hiện cái hành động A đó là gì? xác suất để mà từ trạng thái này sang trạng thái này khi chúng ta thực hiện cái hành động A rồi R S A S phải tức là cái phần thưởng nhận được khi chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 6,
      "start_timestamp": "0:04:51",
      "end_timestamp": "0:05:47"
    }
  },
  {
    "page_content": "tức là cái phần thưởng nhận được khi chúng ta chuyển từ S sang S phải và khi thực hiện cái hành động A Vần thưởng khi thực hiện nguyên bộ x sang s phải sử dụng hành động ra Công thức này sẽ thể hiện được giá trị kỳ vọng của trạng thái Hàm giá trị trạng thái Và khái niệm tiếp theo là giá trị hành động thì ở đây sẽ là cái hiệu bằng chữ Quy đó là cái giá trị hành động là giá trị kỳ vọng khi ở trạng thái S khi ở trạng thái S ta chọn cái hành động A rồi tiếp tục thực hiện theo cái policy P thì đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 7,
      "start_timestamp": "0:05:44",
      "end_timestamp": "0:06:38"
    }
  },
  {
    "page_content": "rồi tiếp tục thực hiện theo cái policy P thì đây sẽ là kỳ vọng trên cái policy P và nó sẽ là bằng tổng Các cái tổng các cái reward trong tương lai Khi chúng ta ở trạng thái S và sử dụng các hành động A Thì cái công thức diễn đạt ra của nó sẽ là như thế này Trong đó, tương tự như vậy thì P của AS tức là cái sát xuất để chúng ta chọn Sát xuất, ví dụ ở đây là sát xuất chọn các hành động A phải Tiếp theo, khi cho trước cái trạng thái S phải PS phải SA chính là sát xuất chuyển từ trạng thái S sang",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 8,
      "start_timestamp": "0:06:34",
      "end_timestamp": "0:07:31"
    }
  },
  {
    "page_content": "SA chính là sát xuất chuyển từ trạng thái S sang trạng thái S phải với hành động A RSA phải tức là phần thưởng nhận được khi chuyển từ trạng thái S sang trạng thái S phải với hành động A Với công thức này, chúng ta thấy nó có vẻ khá phức tạp Nhưng ý tưởng chung của nó là giá trị kỳ vọng Tại trạng thái S mà khi chúng ta thực hiện chọn hành động A Với mô hình reinforcement learning, nó sẽ có khái niệm đó chính là Markov Decision Process Tức là quá trình mà đưa ra quyết định Markov MDP là một mô",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 9,
      "start_timestamp": "0:07:29",
      "end_timestamp": "0:08:15"
    }
  },
  {
    "page_content": "trình mà đưa ra quyết định Markov MDP là một mô hình toán học để mô tả một bài toán ra quyết định Mô tả một bài toán ra quyết định trong môi trường không chắc chắn Markov là nổi tiếng với ý tố không chắc chắn Và nó chỉ là khung lý thuyết với các thành phần S là tập trạng thái môi trường A là tập hợp các hành động mà agent có thể thực hiện được P là sát xuất để chuyển trạng thái từ S sang S phải khi thực hiện các hành động A này R là phần thưởng nhận được khi thực hiện các hành động A tại trạng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 10,
      "start_timestamp": "0:08:09",
      "end_timestamp": "0:09:04"
    }
  },
  {
    "page_content": "nhận được khi thực hiện các hành động A tại trạng thái S Gamma là mức độ coi trọng phần thưởng trong tương lai Dĩa biến của Markov Decision Process là T bằng 0, môi trường xin ra trạng thái băng đầu là S0 Và với mỗi bước T, Aison sẽ chọn Action là AT Môi trường sẽ trả về là RT Môi trường sẽ trả về phần thưởng là RT Môi trường sẽ xin ra trạng thái mới là ST cộng 1 và agent sẽ nhận được là cái RT và cái ST cộng 1 mục tiêu đó là chúng ta sẽ đi tìm cái chính sách tối U,P sau để cho tối đa hóa tổng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 11,
      "start_timestamp": "0:09:00",
      "end_timestamp": "0:09:51"
    }
  },
  {
    "page_content": "cái chính sách tối U,P sau để cho tối đa hóa tổng cái phần tưởng tích lưỡi tổng cái phần tưởng tích lưỡi là như thế này tổng của T với T lớn không, tức là T chạy từ 0 cho đến vô cùng của gamma T RT Mục tiêu là tìm chính sách p-sal tối ưu, sao cho cái này là lớn nhất Và chính sách tối ưu p-sal trong Markov này là làm sao cho tổng phần thưởng nhận được lớn nhất Môi trường sẽ có yếu tố ngộ nhiên Trạng thái ban đầu và sát xuất chuyện trạng thái là những yếu tố ngộ nhiên bắt đầu chúng ta sẽ không",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 12,
      "start_timestamp": "0:09:48",
      "end_timestamp": "0:10:37"
    }
  },
  {
    "page_content": "những yếu tố ngộ nhiên bắt đầu chúng ta sẽ không biết chúng ta sẽ rớt vô cái trạng thái nào rồi cái xác xúc để chuyển từ trạng thái S sang cái trạng thái S S phải nó cũng là một cái yếu tố ngộ nhiên vì vậy ta sẽ không thể tối ưu cái phần thưởng này một cách tuyệt đối mà chúng ta chỉ có thể là tối ưu cái kỳ vọng thôi tối ưu cái kỳ vọng của tổng cái phần thưởng thôi do đó thì cái P sau là chúng ta sẽ đi cần tìm cái thằng P sau cho kỳ vọng của phần thưởng trong tương lai đó là cao nhất trong đó S0",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 13,
      "start_timestamp": "0:10:33",
      "end_timestamp": "0:11:24"
    }
  },
  {
    "page_content": "thưởng trong tương lai đó là cao nhất trong đó S0 chính là trạng thái ban đầu được lấy theo phân phối sát xuất ban đầu là P của S0 rồi AT là bằng P của ST tức là cái hành động được dựa chọn dựa trên chính sách P Chúng ta sẽ chọn hành động tiếp theo là gì khi cho trước trạng thái ST ST cộng 1 cho trước sát xuất của STAT Tức là trạng thái tiếp theo sẽ được lựa chọn dựa trên sát xuất từ trạng thái hiện tại và action hiện tại Gamma là hệ số chiếc khấu phần thưởng Vấn đề đối với Markov process là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 14,
      "start_timestamp": "0:11:12",
      "end_timestamp": "0:11:55"
    }
  },
  {
    "page_content": "khấu phần thưởng Vấn đề đối với Markov process là chúng ta sẽ không biết trước được sát xuất chuyển trạng thái từ S sang ST và khi cho trước hành động A Chúng ta sẽ không biết trước chính xác hàm phần thưởng RSA là gì, Reward SA và không thể duyệt hết tất cả trạng thái vì nó quá lớn Đó chính là những vấn đề khi chúng ta làm với Markov Decision Process Do đó thì lý thuyết về Markov Decision Process thường là chuẩn nhưng nó không có khả thi trong thực tế Nó sẽ không khả thi trong thực tế Và học",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 15,
      "start_timestamp": "0:11:53",
      "end_timestamp": "0:12:31"
    }
  },
  {
    "page_content": "thực tế Nó sẽ không khả thi trong thực tế Và học tăng cường thì Reinforcement Link nó xuất hiện Nó làm tự học chính sách tối ưu thông qua trải nghiệm thử và sai Nó sẽ học chính sách tối ưu thông qua thử và sai Tức là cho agent nó sẽ chơi nhiều lần nó sẽ nhận phần thưởng hoặc bị phạt rồi từ đó nó sẽ điều chỉnh hành vi của mình và học tăng cường nó là một giải pháp để giải quyết vấn đề của Markov Decision Process học tăng cường nó xuất hiện để học các chính sách tối ưu thông qua trải nghiệm thay",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 16,
      "start_timestamp": "0:12:26",
      "end_timestamp": "0:13:09"
    }
  },
  {
    "page_content": "các chính sách tối ưu thông qua trải nghiệm thay vì chỉ cần biết trước quy luật của môi trường thì một số thuộc toán hoặc phương pháp học tăng cường ví dụ như Monte Carlo ý tưởng đó là Học bằng cách chơi lại trò chơi nhiều lần Chúng ta sẽ chơi đi chơi lại trò chơi Giống như là khi chúng ta mới học tập chơi cờ thì chúng ta sẽ tìm cách chơi nhiều lần Và mỗi lần lần thua hoặc lần thắng thì chúng ta sẽ đúc kết ra được những kinh nghiệm cho mình Mỗi lần chơi xong khi nhận được kết quả chúng ta sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 17,
      "start_timestamp": "0:13:05",
      "end_timestamp": "0:13:46"
    }
  },
  {
    "page_content": "lần chơi xong khi nhận được kết quả chúng ta sẽ tính trung bình để ước lượng giá trị từ đó phù hợp Để mà chơi tới cuối ván chơi để có thể chơi cho đến cuối ván Phương pháp tiếp theo là Temporal Difference TD Ý tưởng là học từng chút một trong khi chơi và không cần chờ đến tới cuối và cập nhật giá trị ngay khi có dữ tiệu mới và thường phương pháp Temporal Difference này sẽ nhanh hơn Monte Carlo và áp dụng được trong môi trường không có điểm kết thúc Queue Learning đây cũng là một trong những",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 18,
      "start_timestamp": "0:13:40",
      "end_timestamp": "0:14:22"
    }
  },
  {
    "page_content": "thúc Queue Learning đây cũng là một trong những phương pháp hiện đại được đề cập trong rất nhiều các nghiên cứu gần đây ý tưởng đó là xây dựng một bảng giá trị Qtable cho mỗi một hành động hoặc một trạng thái và học cách chọn hành động nào mang lại kết quả tốt nhất thì đây là phương pháp nền tảng nhưng khó áp dụng khi không gian trạng thái của mình quá lớn thì phương pháp này rất khó áp dụng policy radian là ý tưởng đó là học trực tiếp chính sách thay vì là Qtable dùng để tối ưu hóa cái Radiant",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 19,
      "start_timestamp": "0:14:15",
      "end_timestamp": "0:15:06"
    }
  },
  {
    "page_content": "thay vì là Qtable dùng để tối ưu hóa cái Radiant Ascent để mà cải thiện chính sách của mình thích hợp cho các bài toán ra quyết định hành động có tính chấp liên tục ví dụ như điều khiển robot thì là policy radiant là phù hợp tại vì cái radiant này nó chỉ có thể là thực thi được trên những cái hàm mà có dạng liên tục thôi Deep reinforcement learning, ý tưởng là kết hợp học tăng cường với lại học sâu để dùng neural network để sắp xỉ chính sách và xử lý được môi trường, phức tạp, nhiều trạng thái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 20,
      "start_timestamp": "0:15:02",
      "end_timestamp": "0:15:44"
    }
  },
  {
    "page_content": "xử lý được môi trường, phức tạp, nhiều trạng thái DQN, Deep Q Network, Actor Critic đã thành công trong việc chơi game như Atari, AlphaGo, robot, xe tự lái Deep reinforcement learning là một trong những hướng tiếp cận cho nhiều thành tự và học tăng cường với multi-agent, tức là nguyên lý chúng ta sẽ cho nhiều agent cùng học và cùng tương tác trong môi trường ứng dụng cụ thể là chiến thuật game, ví dụ như chúng ta có AlphaStar của DeepMind để chơi game Starcraft 2 và đạt được trình độ chuyên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 21,
      "start_timestamp": "0:15:37",
      "end_timestamp": "0:16:21"
    }
  },
  {
    "page_content": "chơi game Starcraft 2 và đạt được trình độ chuyên nghiệp. Rồi robot phối hợp, nhiều robot cùng làm việc trong kho hàng của Amazon Robotics điều phối giao thông thông minh để điều chỉnh tín hiệu để giảm tắt ngãn. Rồi học tăng cường từ dữ liệu có sẵn, Offline Reinforcement Learning. Nguyên lý đó là học các chính sách từ dữ liệu có sẵn ví dụ như là log hoặc là lịch sử mà không cần phải tương tác trực tiếp Thì đây là những dữ liệu mà chúng ta thu thập sẵn từ trước, nó gọi là Offline Reinforcement",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 22,
      "start_timestamp": "0:16:18",
      "end_timestamp": "0:16:56"
    }
  },
  {
    "page_content": "sẵn từ trước, nó gọi là Offline Reinforcement Và ứng dụng trong thực tế đó là phân tích dữ liệu lịch sử bình án để gợi ý các phát đồ điều trị Trong lĩnh vực tài chính, đó là học từ các dữ liệu giao dịch trong quá khứ để tối ưu hóa danh mục đầu tư recommender system tức là Netflix dùng học tăng cường để gợi ý các phim tự lóc của người dùng sau đây là một vài thành tự mà đáng kể chúng ta có thể thấy trong thời gian gần đây ví dụ như năm 2016 là các cao thủ kà vây của Hàn Quốc thì AlphaGo đã thắng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 23,
      "start_timestamp": "0:16:52",
      "end_timestamp": "0:17:30"
    }
  },
  {
    "page_content": "cao thủ kà vây của Hàn Quốc thì AlphaGo đã thắng với tỷ số 41 và Lee Sedol là người duy nhất thắng được 1 gà ván trước AlphaGo thôi Còn lại là AlphaGo đã thắng đến 4 ván Rồi năm 2017 kỳ thủ số 1 thế giới lúc cái giờ là Trung Quốc thì AlphaGo đã thắng được là 3-0 tức là không có cho cơ hội nào cho kỳ thủ này có thể thắng được AlphaGo một lần Rồi ứng dụng học tăng cường trong tối ưu năng lượng của các tòa nhà điều khiển các hệ thống là Mac tòa nhà bằng học tăng cường kết quả đó là nó có thể tiết",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 24,
      "start_timestamp": "0:17:28",
      "end_timestamp": "0:18:03"
    }
  },
  {
    "page_content": "bằng học tăng cường kết quả đó là nó có thể tiết kiệm được năng lượng từ 9% cho đến 13% Ưng dụng học tăng cường trong gợi ý Netflix Ưng dụng học tăng cường trong Chatbot Đây có thể nói là một trong những ứng dụng học tăng cường được sử dụng phổ biến nhất hiện nay Đó là trên công cụ ChatGPT ChatGPT hiện nay là nó đã học tăng cường từ Human Feedback để tinh chỉnh ChatGPT sao cho nó có thể trả lời tự nhiên và an toàn hơn Trong các xe tự hành của Waymo hoặc Tesla cũng đều có áp dụng công nghệ của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 25,
      "start_timestamp": "0:17:58",
      "end_timestamp": "0:18:50"
    }
  },
  {
    "page_content": "hoặc Tesla cũng đều có áp dụng công nghệ của học tăng cường này Chúng ta sẽ cùng đến với ý cuối cùng đó là ưu điểm và nhược điểm của học tăng cường Chúng ta sẽ không cần gán nhãn như Supervibe Learning học tăng cường là chúng ta không cần phải gán nhãn học từ trải nghiệm không cần biết trước mô hình của môi trường có khả năng tổng bác hoa cao chúng ta có thể áp dụng trong rất nhiều những lĩnh vực khác nhau ví dụ như trong game, tài chính, robot xe tự hành nó có thể thích nghi cao để học được",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 26,
      "start_timestamp": "0:18:40",
      "end_timestamp": "0:18:50"
    }
  },
  {
    "page_content": "xe tự hành nó có thể thích nghi cao để học được khi môi trường thay đổi tức là nếu như trong môi trường mà cố định thì chúng ta không nói kể cả môi trường thay đổi theo thời gian thì tính thích nghi của học tăng cường cũng rất cao và tối ưu trong dài hạn cân bằng giữa lợi ích ngán hạn và lợi ích dài hạn nược điểm đó là nó tốn rất nhiều dữ liệu và thời gian tại vì nó cần phải có những trải nghiệm thử và sai chi phí cao do khó áp dụng thử sai tốn kém tức là nếu như mỗi lần thử mà sai nhiều thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 27,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "kém tức là nếu như mỗi lần thử mà sai nhiều thì chúng ta sẽ tốn kém rất là nhiều chi phí thì nếu mà cái thử sai nhiều quá thì nó cũng sẽ gây ra chi phí cao tính không ổn định sẽ dễ bị mắc kẹt ở những chiến lược mà chưa có được tối ưu thì đó chính là những ưu nhược điểm của các phương pháp học tăng cường hiện nay Hãy subscribe cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video hấp dẫn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=tGizMGafWY0",
      "filename": "tGizMGafWY0",
      "title": "[CS114 - Chương 7] Học tăng cường - Phần 4",
      "chunk_id": 28,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Trong phần tiếp theo thì chúng ta sẽ cùng à đến với phần thực hành cho chủ đề học không có giám sát. Và đối với lĩnh vực học không có giám sát thì có hai ứng dụng rất là nổi tiếng đó là giảm chiều dữ liệu và phân cụm. Thế thì nếu như trước đây à khi mà chưa có các cái thành tựu của GenI hoặc là cụ thể hơn đó là các cái công cụ như là chat GPT thì chúng ta sẽ tập trung vào cài đặt các cái thuật toán giảm chiều dữ liệu và phân cụm. Tuy nhiên thì gần đây với cái sự phát triển của geni thì cái việc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:52"
    }
  },
  {
    "page_content": "đây với cái sự phát triển của geni thì cái việc cài đặt các cái thuật toán này thì đã trở nên dễ dàng hơn và chúng ta sẽ sử dụng các cái công cụ này để giải quyết một số cái bài toán của mình. Cụ thể hơn đó là mục đích của cái bài thực hành này đó là chúng ta sẽ ứng dụng học có không giám sát trong cái việc đó là chúng ta sẽ tiến hành giảm chiều dữ liệu và sau đó chúng ta sẽ trực quan hóa. Rồi sau đó là chúng ta sẽ tiến hành phân cụm các cái dữ liệu không có gác nhãn. Lưu ý là khi chúng ta nói",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 1,
      "start_timestamp": "0:00:47",
      "end_timestamp": "0:01:26"
    }
  },
  {
    "page_content": "liệu không có gác nhãn. Lưu ý là khi chúng ta nói đến học không có giám sát thì chúng ta sẽ có một cái dữ liệu đầu vào x nhưng mà chúng ta sẽ không có y. Rồi nhãn Y là chúng ta không có. Và tập dữ liệu ở đây chúng ta sẽ tiến hành thực nghiệm đó chính là tập dữ liệu. Thì tại sao lại dùng cái tập dữ liệu? Tập dữ liệu này là một cái tập dữ liệu đủ đơn giản và khối lượng của nó là đủ nhỏ. Đồng thời là cái tính chất dữ liệu của nó đủ đơn giản và không quá phức tạp để chúng ta có thể trực quan hóa dữ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 2,
      "start_timestamp": "0:01:21",
      "end_timestamp": "0:02:04"
    }
  },
  {
    "page_content": "quá phức tạp để chúng ta có thể trực quan hóa dữ liệu nó tốt hơn. Và đầu vào của chúng ta sẽ là à các cái tập dữ liệu thô giống như ở bên tay trái và bên phải chính là cái kết quả mà chúng ta kỳ vọng sẽ trả về. Lưu ý là đây chỉ là kết quả mang tính chất tham khảo và không nhất thiết là chúng ta sẽ ra đúng như thế này nhưng mà chúng ta sẽ cố gắng đó là từ cái dữ liệu à với mỗi một cái ảnh một cái mẫu dữ liệu ha thì nó có kích thước là 28 x 28. Thế thì chúng ta sẽ tìm cách biến một cái dữ liệu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 3,
      "start_timestamp": "0:01:58",
      "end_timestamp": "0:02:44"
    }
  },
  {
    "page_content": "Thế thì chúng ta sẽ tìm cách biến một cái dữ liệu nhiều chiều này về cái dữ liệu ở trong không gian có hai chiều thôi. À và hai chiều à ở đây là component 1 và component 2. Và sau đó thì chúng ta giả định là chúng ta sẽ không sử dụng cái nhãn của cái dữ liệu y. À tức là chúng ta giả sử như chúng ta không biết à số này là số 0, số này là số 1, số này số 2 đó thì chúng ta sẽ vẽ và trong cái không gian hai chiều này và chúng ta xem thử xem các cái con số mà có cùng một cái cùng một cái ờ biểu diễn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 4,
      "start_timestamp": "0:02:39",
      "end_timestamp": "0:03:20"
    }
  },
  {
    "page_content": "số mà có cùng một cái cùng một cái ờ biểu diễn cho một cái cái chữ số á nó có nằm gần nhau hay không. Đó thì ví dụ như nếu mà chúng ta làm đúng á thì có thể là các cái số bảy nó sẽ co cụm lại trong một cái khu rồi số 2 nó sẽ nằm co cụng trong một khu số sáu nó sẽ nằm co cụng trong một cái khu. Đó thì chúng ta thử thử xem xem là cái những cái dữ liệu mà nằm trong cùng một cái khu vực á nằm trong nằm trong nằm trong một cái cụm á thì cái à có phải là những cái số mà có cùng một cái chữ số hay",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 5,
      "start_timestamp": "0:03:15",
      "end_timestamp": "0:03:58"
    }
  },
  {
    "page_content": "là những cái số mà có cùng một cái chữ số hay không. Thì sau đây thì chúng ta sẽ tiến hành à thử nghiệm trên cái nền tảng đó là Google Collab. Thì với cái nền tảng Google Collab á thì nó sẽ cho phép là chúng ta có thể lập trình và có kết hợp với lại cái GNI. Ở đây chúng ta thấy cái giao diện JNI khá là giống với lại chat GVT. Thì ở đây chúng ta sẽ có hai cái công việc. Bước số một là chúng ta sẽ load cái dữ liệu. Rồi bước số hai là chúng ta sẽ tiến hành là trực quan hóa. À chúng ta sẽ tiến hành",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 6,
      "start_timestamp": "0:03:53",
      "end_timestamp": "0:05:24"
    }
  },
  {
    "page_content": "hành là trực quan hóa. À chúng ta sẽ tiến hành giảm chiều. trong cái không gian hai chiều. Rồi sang bước số ba là chúng ta sẽ tiến hành gom cụng và trực quang hóa các cụm. Rồi, bước số 4 là chúng ta sẽ tiến hành lấy một số cụm, một số cái mẫu dữ liệu [Vỗ tay] để xem có cùng à chữ số hay không. Rồi thì đây chính là à những cái bước mà chúng ta sẽ tiến hành thử nghiệm. Thì đầu tiên đó là chúng ta sẽ tiến hành à load dữ liệu lên. Thế thì để load cái dữ liệu lên á thì chúng ta sẽ sử dụng cái thư",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 7,
      "start_timestamp": "0:05:18",
      "end_timestamp": "0:06:39"
    }
  },
  {
    "page_content": "cái dữ liệu lên á thì chúng ta sẽ sử dụng cái thư viện si kitler. Thế thì ở đây chúng ta sẽ dùng cái công cụ JMI để tạo sin code ha. À chúng ta sẽ load dữ liệu bằng rồi. Và lưu ý là chúng ta sẽ loại bỏ cái dữ liệu nhãn. Tại vì chúng ta đang làm cái học không có giám sát, chỉ sử dụng. Thế thì ở đây chúng ta sẽ trước mắt là chúng ta accept, chúng ta khoan rồi. Đ chúng ta tránh cái thói quen đó là chúng ta quá tin tưởng vào geni ha. Rồi thì bước số một là chúng ta sẽ load cái dữ liệu ha. Ch dữ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 8,
      "start_timestamp": "0:06:33",
      "end_timestamp": "0:07:35"
    }
  },
  {
    "page_content": "số một là chúng ta sẽ load cái dữ liệu ha. Ch dữ liệu. Rồi thì ở đây chúng ta thấy là nó sẽ sử dụng cái size kitl.dataset. À sau đó thì là dùng cái à phương thức là fake open email và dùng cái tập dữ liệu là em 784. Trong đó 784 tức là cái vecơ có 784 chiều. Thế thì tại sao cái ảnh của mình hai chiều nhưng mà lại ở đây là 784? Thì là vì cái ảnh gốc của mình á là có kích thước là 28 nhân 28 thì 28 x 28 chính là bằng 784 nhân 784. Tức là ảnh 28 nhân 28 nó đã được convert, nó đã được flat về 7",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 9,
      "start_timestamp": "0:07:29",
      "end_timestamp": "0:08:25"
    }
  },
  {
    "page_content": "nhân 28 nó đã được convert, nó đã được flat về 7 tháng. Rồi sau đó thì nó sẽ load cái dữ liệu X và Y lên. Đó thì ở đây chúng ta sẽ không sử dụng cái x à xin lỗi chúng ta không sử dụng y mà chúng ta chỉ sử dụng x thôi. Rồi thế thì loại bỏ đi. Chúng ta chỉ lấy cái x image thôi. Đó. Thế thì sau khi chúng ta load lên thì chúng ta thấy là cái shap của image data là nó có 70.000 nhân cho 784. Thế thì hàm ý của nó là gì? 70.000 này nó chính là có tất cả 70.000 mẫu và 784 này là một cái vecơ biểu diễn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 10,
      "start_timestamp": "0:08:19",
      "end_timestamp": "0:09:28"
    }
  },
  {
    "page_content": "70.000 mẫu và 784 này là một cái vecơ biểu diễn cho một cái tấm ảnh của mình. Đó. Thế thì bây giờ chúng ta muốn xem thử một cái tấm ảnh của mình nó nhìn như thế nào thì chúng ta sẽ lấy cái X image. Rồi chúng ta lấy một cái dòng dữ liệu đầu tiên ví dụ vậy. Và lấy tất cả là 784 thì chúng ta sẽ xem ha. Sh. Đó là 784. Tức là đây là một cái mẫu ảnh đầu tiên. Và bây giờ chúng ta sẽ à reset nó về 28 à nhân với 28 đó thì nó sẽ ra cái ảnh có kích thước là 28 x 28 và sau đó thì chúng ta sẽ vẽ nó lên. Thế",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 11,
      "start_timestamp": "0:09:19",
      "end_timestamp": "0:10:22"
    }
  },
  {
    "page_content": "28 x 28 và sau đó thì chúng ta sẽ vẽ nó lên. Thế thì để vẽ lên thì chúng ta sẽ à import map. SPLT rồi sau đó là plt ch tìm show. Đó thì đây chính là con số 5. Chúng ta nhìn thì chúng ta sẽ thấy là đây là số 5. Chúng ta có thể thử những cái mẫu dữ liệu khác. Ví dụ sample index là bằng 123 đi thì đây sẽ là sample index. Đây chính là số 7 đó. Ví dụ như mẫu thứ 58 thì đây chính là số 4. Thế thì ở đây chúng ta nhìn vô chúng ta là biết số 4 nhưng mà chúng ta đã loại bỏ đi cái y rồi. Chúng ta sẽ không",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 12,
      "start_timestamp": "0:10:16",
      "end_timestamp": "0:11:20"
    }
  },
  {
    "page_content": "ta đã loại bỏ đi cái y rồi. Chúng ta sẽ không sử dụng y nữa. Đó. Như vậy thì chúng ta đã xong cái bước đầu tiên đó chính là à load cái dữ liệu lên và trực quang hóa. Tuy nhiên thì ở đây là chúng ta trực quan thì chúng ta sẽ đưa cái vecơ 784 chiều về vectơ 28 x 28. Nhưng mà các cái bước ở phía sau á, bước giảm chiều dữ liệu á thì chúng ta sẽ ờ không có sử dụng cái ma cái ma trận kích thước 28 x 28 mà chúng ta sẽ sử dụng luôn cái vecơ 784 784 chiều. Vậy chúng ta sẽ sang cái bước số hai đó là giảm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 13,
      "start_timestamp": "0:11:14",
      "end_timestamp": "0:12:37"
    }
  },
  {
    "page_content": "Vậy chúng ta sẽ sang cái bước số hai đó là giảm chiều dữ liệu. Rồi thế thì ở đây chúng ta sẽ tiến hành là à giảm chiều dữ liệu của X image từ 784 chiều về hai chiều. và à vẽ trong cái mặt phẳng rồi thì ở đây nó sẽ có nhiều cái phương pháp ví dụ như là PCI hoặc là TSNI để giảm chiều dữ liệu từ 728 784 xuống còn hai chiều đó. Và trực quan hóa dữ liệu là vẽ biểu đồ là trong không gian hai chiều. Rồi ok như vậy thì chúng ta sẽ run step by step ha. Rồi bước đầu tiên là nó đã dùng cái thuậc toán là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 14,
      "start_timestamp": "0:12:32",
      "end_timestamp": "0:13:15"
    }
  },
  {
    "page_content": "Rồi bước đầu tiên là nó đã dùng cái thuậc toán là PCI. Sau này thì chúng ta có thể thử các cái thuậc toán khác ví dụ như là TSNI. Rồi thì chúng ta sẽ access và run. Thì cái việc khai báo này cũng khá là đơn giản đó là chúng ta sẽ dùng cái gói thư viện là si kichl.decompos. decomposition và import cái PCI. Rồi trong đó thì cái số chiều số n component ở đây ch không gian đích mà chúng ta muốn giảm chiều về. Thì ví dụ như chúng ta đang muốn giảm về hai chiều thì chúng ta sẽ để là n component là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 15,
      "start_timestamp": "0:13:10",
      "end_timestamp": "0:13:54"
    }
  },
  {
    "page_content": "về hai chiều thì chúng ta sẽ để là n component là bằng 2. Sau đó chúng ta sẽ gọi pca.fit transform cái dữ liệu x của mình vào. Đó thì nó sẽ ra là cái nó sẽ trả về cái x PCI. Thế thì cái hàm bình thường á là PC.fit sau rồi chúng ta sẽ transform tức là chúng ta tách ra hai bước nhưng mà si kitl họ sẽ cung cấp luôn một cái phương thức đó là fit và transform luôn. Tức là nó sẽ làm cùng một lúc luôn. Và sau khi giảm chiều về thì ở phía trên chúng ta thấy là một cái mẫu dữ liệu của mình thì nó sẽ có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 16,
      "start_timestamp": "0:13:48",
      "end_timestamp": "0:14:40"
    }
  },
  {
    "page_content": "thấy là một cái mẫu dữ liệu của mình thì nó sẽ có kích thước đó là 784 tức là 70.000 70.000 cái vectơ 784 chiều. Thì sau khi chúng ta xử lý xong nó sẽ ra là 70.000 cái vecơ chỉ có hai chiều thôi. Đó. Và sau đó chúng ta sẽ tiến hành trực quan hóa dữ liệu à vẽ biểu đồ. Rồi đó thì chúng ta thấy là à sau khi chúng ta vẽ trong cái không gian, chúng ta vẽ trong cái không gian thì chúng ta thấy là nó trần chịt như thế này đúng không? Chúng ta sẽ chàn như thế này. Và chút nữa thì chúng ta sẽ xem xem là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 17,
      "start_timestamp": "0:14:34",
      "end_timestamp": "0:15:59"
    }
  },
  {
    "page_content": "thế này. Và chút nữa thì chúng ta sẽ xem xem là các cái điểm dữ liệu này thì những cái điểm mà ở gần nhau nó có cùng một cái ờ phải là cùng một cái chữ số hay không. Rồi đó thì đây chính là cái kết quả của cái bước số 2. Rồi thì để cho gọn lại thì chúng ta sẽ vẫn cứ chúng ta có thể đọc thêm cái mô tả này. Tuy nhiên để cho gọn cái chương trình tutorial này thì chúng ta sẽ lại bỏ bớt cái phần tóm tắt của nó. Rồi tiếp theo sang bước số ba đó là chúng ta sẽ tiến hành gom cụm và trực quan hóa các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 18,
      "start_timestamp": "0:15:53",
      "end_timestamp": "0:16:46"
    }
  },
  {
    "page_content": "ta sẽ tiến hành gom cụm và trực quan hóa các cái cụm. Rồi thì chúng ta sẽ copy cái này luôn ha. Rồi gom cộng bằng camin và trực quang hóa trong không gian hai chiều. Rồi à nó sẽ giảm chiều bằng camin à áp dụng camin xin lỗi áp dụng camin để gom cụm ví dụ như là 10 cụm tại vì chúng ta đã biết trước cái tập dữ liệu này là tập dữ liệu chữ biết tay nên chúng ta biết là nó sẽ có 10 cụm rồi trực quan hóa rồi chúng ta sẽ run step by step ha. Rồi đầu tiên đó là nó sẽ khởi tạo cái thực toán camin trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 19,
      "start_timestamp": "0:16:42",
      "end_timestamp": "0:17:28"
    }
  },
  {
    "page_content": "đó là nó sẽ khởi tạo cái thực toán camin trong cit learn và nó nằm trong cái nhóm cái module đó là cluster và số cụm ở đây chúng ta sẽ dùng đó là 10 cụm. À còn các cái tham số còn lại đó là các cái tham số mặc định random state đó là 42. Thì trong cái thuục toán gom cụm chúng ta biết rằng là cái bước khởi tạo á là cái bước lấy ngẫu nhiên 10 cái cụm đầu tiên à 10 cái điểm làm cái tâm cụm đầu tiên. Thế thì cái hàm ngẫu nhiên đó nó sẽ cần có một cái gọi là random state để đảm bảo là những cái lần",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 20,
      "start_timestamp": "0:17:23",
      "end_timestamp": "0:18:03"
    }
  },
  {
    "page_content": "gọi là random state để đảm bảo là những cái lần thử nghiệm khác nhau nó sẽ ra cùng một cái kết quả. Và à cái việc là init là chúng ta sẽ là tự động. Rồi sau đó chúng ta sẽ gọi cái hàm fit camin.fit và chúng ta sẽ truyền vào là thay vì cái x image ban đầu là 784 chiều thì chúng ta sẽ fit cái xca tức là cái mà đã giảm chiều dữ liệu. Rồi sau đó là chúng ta sẽ ờ xem xem là các cái nhãn của mình của từng cái điểm sau khi chúng ta đã gom cụm xong thì cái nhãn của từng điểm dữ liệu là gì. Đó. Rồi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 21,
      "start_timestamp": "0:17:56",
      "end_timestamp": "0:18:59"
    }
  },
  {
    "page_content": "thì cái nhãn của từng điểm dữ liệu là gì. Đó. Rồi chúng ta accept và run. Rồi nó chạy rất là nhanh đúng không? Tại vì chúng ta đã giảm xuống còn hai chiều dữ liệu nên cái tốc độ chạy PC rất là nhanh. Nhưng nếu chúng ta để là cái X image góc á tức là 784 chiều thì nó sẽ phải chạy gấp ờ nhiều lần nó phải gấp 300 lần. Rồi tiếp theo đó là chúng ta sẽ tìm cách trực quan hóa. Chúng ta sẽ lấy cái XPCA và thành phần X thành phần đầu tiên ha. Thành phần số 1 và thành phần số hai là hai cái trục trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 22,
      "start_timestamp": "0:18:53",
      "end_timestamp": "0:20:07"
    }
  },
  {
    "page_content": "số 1 và thành phần số hai là hai cái trục trong cái không gian hai chiều của mình để vẽ lên. Và cái cluster thì chúng ta sẽ lấy cái cluster ở trên đây tức chính là cái nhãn của mình luôn. Rồi C chính là cái cái giá trị của cái cụm của mình. Rồi bây giờ chúng ta sẽ chấp nhận và run. Đó thì sau khi chúng ta chạy cái thuục toán gom cụm xong thì chúng ta thấy là nó ra rất là đều như thế này. Nó sẽ ra 10 cụm rất là đều như thế này. Rồi sau đó thì chúng ta sẽ chờ nó trực quan hóa các cái cụm này và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 23,
      "start_timestamp": "0:20:00",
      "end_timestamp": "0:20:41"
    }
  },
  {
    "page_content": "ta sẽ chờ nó trực quan hóa các cái cụm này và kiểm tra coi các cái mẫu trong cụm. Và kiểm tra các cái mẫu trong cụm. Thì à thuậc toán này đó là chúng ta sẽ ờ với mỗi cái cụm đúng không? Với mỗi cái cụm thì chúng ta sẽ tiến hành là lấy ra random choice. Tức là chúng ta sẽ bóc ra một vài điểm. Bốc ra một vài điểm. Sau đó là chúng ta sẽ hiển thị nó lên. Đó. Rồi. Rồi thì ở đây chúng ta sẽ thấy là trong cái cụm đầu tiên đó nó sẽ có các cái cụm là cụm số 0 à nó sẽ có các cái à số đó là 7 4 9 7 4. Rồi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 24,
      "start_timestamp": "0:20:35",
      "end_timestamp": "0:21:03"
    }
  },
  {
    "page_content": "số 0 à nó sẽ có các cái à số đó là 7 4 9 7 4. Rồi cụm số 1 thì nó sẽ có 530 ờ 2 5. Rồi cụm số 2. Lưu ý là cái cụm này nó không có nghĩa đó là à cái cụm này không có nghĩa đó là nó chính là cái nhãn của mình là cái chữ số hai ha. Thì chúng ta thấy là ví dụ trong cái cụm số 2 thì chúng ta sẽ thấy là có các cái số na ná nhau. Ví dụ số 0, số 0, số 0 và số 6. Thì đâu đó chúng ta thấy cái đường nét nó cũng có một cái vòng tròn vòng cung như thế này. Rồi riêng cái cụm số ba chúng ta thấy đó là cái cụm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 25,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "riêng cái cụm số ba chúng ta thấy đó là cái cụm số ba tức không có nghĩa nhắc lại lần nữa đó là cụm số 3 không có nghĩa đó là cái nhãn của nó là số 3 ha. nó chỉ là một cái số thứ tự của cụm thôi. Thì chúng ta thấy là các cái cụm này đều có chung một cái nhãn đó là số 1. Đó, tức là những cái con số 1 này nó nằm chung trong một cái cụm. Rồi tương tự như vậy cụm số 5 thì là các cái số 7 đó là nằm trong một cái cụm. Như vậy chúng ta th có thể thấy đó là cái thuục toán này khá là hiệu quả khi mà",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 26,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "đó là cái thuục toán này khá là hiệu quả khi mà chúng ta gom cụm mà không có à không ch có biết trước cái nhãn của mình là gì.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TioSSfpy1yA",
      "filename": "TioSSfpy1yA",
      "title": "[CS114 - Tutorial] Unsupervised Learning (Phần 1)",
      "chunk_id": 27,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Trong phần thảo luận tiếp theo thì chúng ta sẽ cùng xem có những cái biến thể nào để mà ờ tính toán cái sự sai số giữa giá trị dự đoán và giá trị thực tế. Thì đây là cái công thức mà chúng ta đã được tìm hiểu ở trong những phần trước. Thế thì giá trị dự đoán của chúng ta chính là wxy cộng cho b. Còn đây là dự đoán. Còn giá trị thực tế thì chính là y. Thì đây là cái công thức mà mặc định chúng ta đã được học. Tuy nhiên có những cái biến thể nào và cái ưu khuyết điểm của nó ra sao thì chúng ta sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TXPLlYL6Dms",
      "filename": "TXPLlYL6Dms",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 3)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:59"
    }
  },
  {
    "page_content": "cái ưu khuyết điểm của nó ra sao thì chúng ta sẽ cùng tìm hiểu. Cái biến thể đầu tiên đó là chúng ta đã thảo luận ở trong câu hỏi trước đó là liệu chúng ta có thể dùng hàm bậc một hay được hay không? Thì câu trả lời là không. Tại vì nếu chúng ta sử dụng hàm bậc một thì một đó là nó sẽ bị triệt tiêu các cái size số đi. Có những size số âm, size số dương chúng ta cộng lại nó sẽ triệt tiêu đi. Đó. Và hai đó là nó một cái hàm bậc một thì nó sẽ không có giá trị nhỏ nhất. Cái biến thể thứ hai đó là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TXPLlYL6Dms",
      "filename": "TXPLlYL6Dms",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 3)",
      "chunk_id": 1,
      "start_timestamp": "0:00:53",
      "end_timestamp": "0:01:39"
    }
  },
  {
    "page_content": "có giá trị nhỏ nhất. Cái biến thể thứ hai đó là chúng ta có thể sử dụng cái thế thì cái mee tức là j của w b thì sẽ là bằng trung bình cộng của tổng của trị tuyệt đối của y trừ cho w xy cộng b. Thì cái công thức này về mặt à con số thì nó đúng là thể hiện cái sai số giữa giá trị dự đoán và giá trị thực tế. Tuy nhiên vì cái dấu trị tuyệt đối này nè khiến cho cái việc tính đạo hàm à nó sẽ khó khăn hơn. À công thức của mình nó sẽ không có đẹp. Và biến thể tiếp theo đó là chúng ta sẽ có cái hàm bậc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TXPLlYL6Dms",
      "filename": "TXPLlYL6Dms",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 3)",
      "chunk_id": 2,
      "start_timestamp": "0:01:29",
      "end_timestamp": "0:02:21"
    }
  },
  {
    "page_content": "thể tiếp theo đó là chúng ta sẽ có cái hàm bậc hai như thế này. Và tại câu hỏi là tại sao chúng ta không sử dụng những cái hàm bậc chẳng. Ví dụ như là J của W B bằng 1/N của tổng à của một cái hàm bậc chẳng. Ví dụ như là yy trừ ờ w xy cộng b thì công thức này chúng ta thấy là bậc à bậc 4 đi chẳng hạn. Thì rõ ràng đây là một cái hàm chẳng nên cái size số này khi chúng ta cộng dồn nó sẽ không bị triệt tiêu cho nhau. Đó. Tại sao chúng ta lại sử dụng hàm bậc hai? Thì rõ ràng giữa một cái hàm bậc 4",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TXPLlYL6Dms",
      "filename": "TXPLlYL6Dms",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 3)",
      "chunk_id": 3,
      "start_timestamp": "0:02:11",
      "end_timestamp": "0:03:03"
    }
  },
  {
    "page_content": "hàm bậc hai? Thì rõ ràng giữa một cái hàm bậc 4 và một cái hàm bậc hai thì cái hàm bậc 4 nó sẽ phức tạp hơn. Và khi chúng ta tính đạo hàm thì cái J phẩ á tính đạo hàm thì nó sẽ là một cái hàm bậc 3. Mà một cái hàm bậc 3 thì khi chúng ta tính toán sai số rồi tính đạo hàm nó cũng sẽ khó hơn và phức tạp hơn. Do đó thì chúng ta có một cái nguyên lý nó gọi là nguyên lý cam. Tức là giữa cái việc chọn bậc hai, bậc ba, bậc 4, bậc 6 thì chúng ta chọn cái bậc nào đơn giản nhất? nhưng mà nó hiệu quả thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TXPLlYL6Dms",
      "filename": "TXPLlYL6Dms",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 3)",
      "chunk_id": 4,
      "start_timestamp": "0:02:58",
      "end_timestamp": "0:03:39"
    }
  },
  {
    "page_content": "bậc nào đơn giản nhất? nhưng mà nó hiệu quả thì chúng ta sẽ chọn. Thì đó là lý do tại sao chúng ta chọn cái hàm bậc hai. Nó vừa giải quyết được cái vấn đề của cái hàm bậc một không giải quyết được. Đó là bị triệt tiêu các cái sai số. Và đồng thời là cái đạo hàm của cái hàm bậc hai là khi chúng ta tính đạo hàm xong thì đạo hàm của nó sẽ là một cái hàm bậc một. Và một cái hàm bậc một thì tính toán rất là dễ dàng. Đó. Và chúng ta cũng có thêm một biến thể nữa. Thì cái biến thể này thực ra không có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TXPLlYL6Dms",
      "filename": "TXPLlYL6Dms",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 3)",
      "chunk_id": 5,
      "start_timestamp": "0:03:33",
      "end_timestamp": "0:04:16"
    }
  },
  {
    "page_content": "thể nữa. Thì cái biến thể này thực ra không có quá quan trọng nhưng mà có nhiều cái tài liệu thì họ lại dùng cái biến thể này. Đó là chúng ta thêm cái số 2 ở đằng mẫu số như thế này. Rồi sau đó tính tổng của y trừ cho wxy cộng b. Đó rồi tất cả bình phương. Thì tại sao nó lại có cái con số 2 này? Đó thì mục tiêu đó là để cho khi chúng ta tính đạo hàm xong á thì nó sẽ đẹp. Con số 2 này nó sẽ biến mất. Tại vì một cái hàm bậc hai thì khi tính đạo hàm ra nó sẽ là bằng 2 nhân cho cái đạo hàm nhân cho",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TXPLlYL6Dms",
      "filename": "TXPLlYL6Dms",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 3)",
      "chunk_id": 6,
      "start_timestamp": "0:04:11",
      "end_timestamp": "0:04:45"
    }
  },
  {
    "page_content": "ra nó sẽ là bằng 2 nhân cho cái đạo hàm nhân cho cái bậc 1 đúng không? Thì 2 nhân cho 1/2 ở đây thì nó sẽ triệt tiêu đi. Thì cái công thức này nó sẽ là đẹp công thức thôi. Còn về mặt ý nghĩa thì nó không có nhiều ý nghĩa. Thì đây là một vài cái biến thể của cái hàm mất mát trong cái mô hình hồi quy logistic.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=TXPLlYL6Dms",
      "filename": "TXPLlYL6Dms",
      "title": "[CS114 - Chương 3] Câu hỏi thảo luận (Phần 3)",
      "chunk_id": 7,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Từ trước đến nay, lập trình truyền thống đã đóng vai trò cốt lõi trong phát triển phần mẹn. Nhưng khi dữ liệu ngày càng trở nên phong phú, học máy đang nổi lên như một giải pháp tối ưu hơn cho nhiều bài toán phức tạp. Chúng ta hãy cùng khám phá sự chuyển đổi này. Chúng ta hãy cùng khám phá sự chuyển đổi này. Nhưng khi dữ liệu ngày càng trở nên phong phú, học máy đang nổi lên như một giải pháp tối ưu hơn cho nhiều bài toán phức tạp. Chúng ta hãy cùng khám phá sự chuyển đổi này. Lập trình truyền",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:39"
    }
  },
  {
    "page_content": "cùng khám phá sự chuyển đổi này. Lập trình truyền thống, Traditional Programming hay còn gọi là lập trình chi tiết từng bước là một phương pháp phát triển phần mềm. Trong đó, lập trình viên phải xác định rõ ràng tất cả các quy tắc, logic và thao tác cần thiết mà máy tính cần thực hiện để giải quyết một vấn đề cụ thể. Mỗi bước trong quá trình giải quyết vấn đề được lập trình một cách chi tiết và không có khả năng tự học hoặc tự điều chỉnh. Đặc điểm chính của lập trình truyền thống là xác định",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 1,
      "start_timestamp": "0:00:30",
      "end_timestamp": "0:01:06"
    }
  },
  {
    "page_content": "điểm chính của lập trình truyền thống là xác định quy tắc thủ công và không tự học. Lập trình viên phải viết tường minh các quy tắc và logic cho từng tác vụ. Hệ thống không thể tự học từ dữ liệu mà chỉ thực hiện đúng các bước đã được lập trình. Khả năng tổng quát hóa hạn chế, chương trình chỉ hoạt động tốt trong các trường hợp đã được định nghịa trước, khó thích nghi với dữ liệu hoặc tình huống mới nếu không có sửa đổi mạng nguồn. Phù hợp với bài toán có cấu trúc cố định, hiệu quả với các bài",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 2,
      "start_timestamp": "0:01:02",
      "end_timestamp": "0:01:36"
    }
  },
  {
    "page_content": "toán có cấu trúc cố định, hiệu quả với các bài toán rõ ràng, ít biến thể nhưng trở nên phức tạp khi xử lý các quấn đề không có cấu trúc hoặc dữ liệu lớn. Trong khi đó, với học máy thì đặc điểm chính là tự động học từ dữ liệu. Mô hình học máy được huấn luyện bằng cách sử dụng dữ liệu đầu vào Training Data để học các mẫu và mối quan hệ trong dữ liệu. Điều này giúp hệ thống không cần lập trình tường minh từng quý tắc 1. Khả năng tổng quát hóa cao, học máy có khả năng dự đoán hoặc xử lý các trường",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 3,
      "start_timestamp": "0:01:25",
      "end_timestamp": "0:02:21"
    }
  },
  {
    "page_content": "học máy có khả năng dự đoán hoặc xử lý các trường hợp mới không nằm trong dữ liệu huấn luyện, miễn là những trường hợp đó có sự tương đồng với các mẫu trong dữ liệu đa học. Thích nghi và cải thiện hiệu xuất. Mô hình học máy có thể cải thiện hiệu xuất khi được cung cấp thêm dữ liệu hoặc khi được điều chỉnh lại giúp nó thích nghi với những thay đổi trong dữ liệu nguôi trường. Hiệu quả với bài toán phức tạp, các lĩnh vực như nhận diện hình ảnh, xử lý ngôn ngữ tự nhiên và dự đoán hành vi là những",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 4,
      "start_timestamp": "0:02:10",
      "end_timestamp": "0:02:43"
    }
  },
  {
    "page_content": "lý ngôn ngữ tự nhiên và dự đoán hành vi là những bài toán không có cấu trúc rõ ràng mà học máy có thể xử lý hiệu quả trong khi lập trình truyền thống thường gặp nhiều khó khăn. Hình này minh họa sự khác biệt giữa mô hình hóa truyền thống và học máy. Đối với hình hóa truyền thống, chúng ta nhìn bên trái sẽ thấy quy trình truyền thống bao gồm hai thành phần đầu vào là dữ liệu và mô hình được thiết kế thủ công hang PracticModel. Máy tính xử lý dữ liệu dựa trên mô hình này và đưa ra kết quả. Điểm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 5,
      "start_timestamp": "0:02:35",
      "end_timestamp": "0:03:16"
    }
  },
  {
    "page_content": "liệu dựa trên mô hình này và đưa ra kết quả. Điểm mấu chốt ở đây là mô hình được con người tạo ra dựa trên kiến thức chuyên môn và các quy tắc được xác định trước. Đối với học máy, ở bên phải quy trình học máy phức tạp hơn và gồm có hai giai đoạn học. Dữ liệu mẫu, Sample data và kết quả mong đo expected result được đưa vào máy tính, máy tính sẽ xử lý thông tin này để tạo ra một mô hình. Giai đoạn này chính là quá trình học của máy tính, nơi nó tự động tìm ra các mẫu hình từ dữ liệu và xây dựng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 6,
      "start_timestamp": "0:03:12",
      "end_timestamp": "0:03:52"
    }
  },
  {
    "page_content": "động tìm ra các mẫu hình từ dữ liệu và xây dựng mô hình dự đoán. Ở giai đoạn dự đoán, sau khi có mô hình từ giai đoạn học, dữ liệu mới được đưa vào máy tính cùng với mô hình đã học được, máy tính sẽ sử dụng mô hình này để xử lý và đưa ra kết quả mũi tên màu xanh lá cây nối mô hình từ giai đoạn học đến giai đoạn dự đoán thể hiện việc mô hình được học được sử dụng để dự đoán trên dữ liệu mới. Tóm lại hình ảnh này so sánh cách tiếp cận truyền thống nơi con người tạo ra mô hình và học máy, nơi máy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 7,
      "start_timestamp": "0:03:42",
      "end_timestamp": "0:04:29"
    }
  },
  {
    "page_content": "nơi con người tạo ra mô hình và học máy, nơi máy tính tự học mô hình từ dự liệu và sử dụng nó để dự đoán. Bảng sau, Minh hoạng sự khác biệt giữa lập trình truyền thống và học máy giúp hiểu rõ hơn cách hai phương pháp này xử lý dữ liệu để đưa ra kết quả. Với lập trình truyền thống, có người cung cấp dữ liệu đầu vào cùng với mô hình hoặc là các quy tắc được lập trình thủ công. Máy tính sử dụng các quy tắc này để xử lý dữ liệu và tạo ra kết quả đầu ra. Cách tiếp cận này yêu cầu lập trình viên phải",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 8,
      "start_timestamp": "0:04:21",
      "end_timestamp": "0:04:58"
    }
  },
  {
    "page_content": "ra. Cách tiếp cận này yêu cầu lập trình viên phải xác định rõ ràng các quy tắc và logic từ trước. Với học máy, máy tính không được cung cấp sẵn các quy tắc mà thay vào đó học từ dữ liệu. Ban đầu nó nhận dữ liệu mẫu và kết quả mong đợi để tạo ra một mô hình dự đoán. Khi có dữ liệu mới, mô hình đã học được. Được học sẽ giúp máy tính đưa ra kết quả mà không cần lập trình viên phải viết các quy tắc cụ thể. Như vậy thì lập trình truyền thống dựa trên quy tắc được con người xác định trước. trong khi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 9,
      "start_timestamp": "0:04:54",
      "end_timestamp": "0:05:31"
    }
  },
  {
    "page_content": "quy tắc được con người xác định trước. trong khi học máy cho phép máy tính tự động tìm ra quy tắc từ dữ liệu, giúp nó thích nghi với các tình huống mới mà không cần phải tình chỉnh mạng ngục. Trong bài giới thiệu về khóa học Machine Learning Crash Course của Google, Peter Novik đã đề cập đến việc học máy không chỉ mang lại những lợi ích thiết thực, mà còn thay đổi căng bản cách chúng ta tiếp cận và giải quyết vấn đề. Cụ thể, các lợi ích học máy được ông chỉ ra là giảm thời gian lập trình. Việc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 10,
      "start_timestamp": "0:05:25",
      "end_timestamp": "0:06:04"
    }
  },
  {
    "page_content": "được ông chỉ ra là giảm thời gian lập trình. Việc sử dụng học máy có thể tự động hóa các tác vụ phức tạp, thay vì phải viết mã thủ công và tốn thời gian. Ví dụ như thay vì phải mất hàng tuần để viết một chương trình sửa lội chính tả, thì ta có thể dùng học máy để đạt kết quả tốt hơn chỉ trong một thời gian ngắn. Thứ hai là tùy chỉnh sản phẩm. Học máy cho phép dễ dàng tùy chỉnh sản phẩm cho các nhóm người dùng cụ thể. Ví dụ nếu muốn tạo ra một chương trình sự lỗi chính tả cho 100 ngôn ngữ, thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 11,
      "start_timestamp": "0:05:57",
      "end_timestamp": "0:06:38"
    }
  },
  {
    "page_content": "trình sự lỗi chính tả cho 100 ngôn ngữ, thì việc viết mả thủ công sẽ mất rất nhiều năm. Với học máy thì chúng ta chỉ cần thu thập dữ liệu cho ngôn ngữ mới và đưa vào cái mô hình đã có. Thứ ba là giải quyết các bài toán khó. Học máy có thể giải quyết các bài toán mà con người hoặc không thể lập trình thủ công, chẳng hạn như dận nhiệm khuôn mặt hay là dận nhiệm giọng nói. Thay vì phải viết mã chi tiết thì chúng ta chỉ cần cung cấp cho mô hình học máy nhiều dữ liệu mẫu. Đối với tư duy giải quyết",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 12,
      "start_timestamp": "0:06:35",
      "end_timestamp": "0:07:51"
    }
  },
  {
    "page_content": "máy nhiều dữ liệu mẫu. Đối với tư duy giải quyết vấn đề thì học máy đã thay đổi cách chúng ta tiếp cận vấn đề. Lập trình truyền thống chỉ dựa vào logic và toán học để chứng minh cái chương trình đúng. Tuy nhiên, học máy lại dựa trên quan sát, thí nghiệm và thống kê để đưa ra dự đoán từ dữ liệu không chắc chắn. Cái việc chuyển từ tư duy toán học logic sang khoa học tự nhiên sẽ giúp mở rộng khả năng giải quyết vấn đề và khám phá các lĩnh vực mới. Sau đây là các quiz. Để hiểu rõ sức mạnh và tìm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 13,
      "start_timestamp": "0:07:46",
      "end_timestamp": "0:08:22"
    }
  },
  {
    "page_content": "Sau đây là các quiz. Để hiểu rõ sức mạnh và tìm năng của học máy, chúng ta cần phải nhìn lại hành trình phát triển của nó. Bài giảng hôm nay sẽ đưa chúng ta qua những cuộc mức quan trọng trong lịch sử học máy. Lịch sử Chí tòa Nhân tạo thì có mối liên hệ chặt chẽ với lịch sử phát triển của học máy, bởi những thuộc toán và tiến bộ trong tính toán của học máy đã góp phần quan trọng vào quá trình hình thành Chí tòa Nhân tạo. Mặc dù hai lĩnh vực này bắt đầu và định hình rõ ràng từ những năm 50,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 14,
      "start_timestamp": "0:08:17",
      "end_timestamp": "0:08:54"
    }
  },
  {
    "page_content": "này bắt đầu và định hình rõ ràng từ những năm 50, nhưng nếu nhiều phát minh quan trọng về thuật toán, thống kê, toán học và công nghệ đã xuất hiện từ trước và tiếp tục ảnh hưởng đến giai đoạn này, thực tế đã, con người thì đã quan tâm đến những câu hỏi về trí tuệ nhân tạc trong hàng thế kỷ. Chính vì vậy để dễ theo dõi thì chúng ta có thể chia cái lịch sử, học máy thành một số giai đoạn chính và mỗi giai đoạn được đánh dấu bởi những đột phá quan trọng và những thách thức riêng. Cụ thể, Giai đoạn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 15,
      "start_timestamp": "0:08:47",
      "end_timestamp": "0:09:29"
    }
  },
  {
    "page_content": "và những thách thức riêng. Cụ thể, Giai đoạn thứ nhất là khởi nguồn của tri tệ nhân tạo và học máy vào những thập niên 1950 thì có các cục mốc đó là Alan Turing và bài kiểm tra Turing năm 1950 tức là Alan Turing đề xuất rằng máy có thể suy nghĩ nếu nó có thể bắt chước con người đủ tốt để đánh lừa người khác thế kì Turing đề xuất cái bài kiểm tra Turing, gọi là Turing test một thí nghiệm để xác định xem máy tính có thể thể hiện trí thông minh như con người hay không Mục tiêu không chỉ là đánh",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 16,
      "start_timestamp": "0:09:18",
      "end_timestamp": "0:10:05"
    }
  },
  {
    "page_content": "con người hay không Mục tiêu không chỉ là đánh lừa người khác mà xem máy tính có thể giao tiếp một cách không thể phân biệt được với con người hay không. Đây là một hút mốc quan trọng. Và đặt ra câu hỏi về bản chất của trí tuệ và khả năng máy mốc là có thể suy nghĩ. Arthur Samuel đã phát triển chương trình chơi cờ đầu tiên trên máy tính và cũng là chương trình máy tính đầu tiên có khả năng tự học vào 1952. Trong giai đoạn này thì có hội nghị DatMuth và sự đa đời của thực ngữ AI năm 1956 thì một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 17,
      "start_timestamp": "0:09:58",
      "end_timestamp": "0:10:41"
    }
  },
  {
    "page_content": "và sự đa đời của thực ngữ AI năm 1956 thì một nhóm các nhà khoa học bao gồm John Martin Marvin Minsky, Nathaniel Rochester và Claude Sano tổ chức hội nghị DatMuth và đánh dấu sự khởi đầu chính thức của nghiên cứu AI. thuật ngữ Artificial Intelligence trí tuệ nhân tạo lần đầu tiên được sử dụng tại đây, đặt ra mục tiêu đầy tham vọng là mô phóng mọi khía cạnh của việc học tập và trí thông minh của con người bằng máy móc. Giai đoạn tiếp theo là Perceptron và mạng Neuron Sarkai vào thập niên 1960.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 18,
      "start_timestamp": "0:10:37",
      "end_timestamp": "0:11:16"
    }
  },
  {
    "page_content": "và mạng Neuron Sarkai vào thập niên 1960. Frank Rosenblatt đã phát triển Perceptron năm 1958, đây là mạng Neuron đơn giản đầu tiên có thể học từ dự lệm. Tuy nhiên nó chỉ có thể giải quyết các bài toán tiến tính. Mạng neuron này được kỳ vọng sẽ giúp máy tính có thể học hỏi giống con người. Tuy nhiên, nghiên cứu này đã gặp phải chỉ trích của Minsky và Pepper năm 1969. Marvin Minsky và Seymour Pepper chỉ ra rằng perceptron bị giới hạn và không thể được giải quyết các bài toán phi tiến tính, ví dụ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 19,
      "start_timestamp": "0:11:13",
      "end_timestamp": "0:11:54"
    }
  },
  {
    "page_content": "được giải quyết các bài toán phi tiến tính, ví dụ như pháp tiến so. Điều này khiến nghiên cứu về mạng neuron bị đình trễ trong nhiều năm. Thời kỳ tiếp theo là thời kỳ hoàng kim của AI năm 1956 và đến 1974. Thời kỳ này có sự lạc quan mạnh mẽ về AI. Năm 1967, Marvin Minsky của MIT đã tự tin tuyên bố rằng, trong vòng một thế hệ, vấn đề tạo ra trí tuệ nhân tạo về cơ bản sẽ được giải quyết. nghiên cứu về sự lý ngôn ngữ tự nhiên phát triển mạnh mẽ, các thuật toán tìm kiếm được cải tiến để trở nên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 20,
      "start_timestamp": "0:11:46",
      "end_timestamp": "0:12:21"
    }
  },
  {
    "page_content": "các thuật toán tìm kiếm được cải tiến để trở nên hiệu quả hơn, và khái niệm thế giới Vimo Microworks ra đời nơi các nhiệm vụ đơn giản có thể được thực hiện thông qua các khiến dẫn bằng ngôn ngữ tự nhiên, và các nghiên cứu về AI được chính phủ tài trợ mạnh mẽ. Các thành cựu quan trọng trong giai đoạn này có Sarky Teroport Năm 1966-1972 thì robot này có thể di chuyển và thực hiện nhiệm vụ một cách thông minh. Hay là chương trình ELISA năm 1966 là chatbot đầu tiên giả lập một nhà trị liệu dịch từ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 21,
      "start_timestamp": "0:12:17",
      "end_timestamp": "0:13:02"
    }
  },
  {
    "page_content": "chatbot đầu tiên giả lập một nhà trị liệu dịch từ chữ therapist. Và Blox Group là một ví dụ về microgroup nơi các khối có thể được xếp trồng và sắp xét, đồng thời là môi trường để thử nghiệm việc dạy máy tính đa quyết định. Ví dụ như ở đây chúng ta thấy Sarkazer robot năm 1972 là robot di động đầu tiên, có khả năng nhận thức và lý luận về môi trường xung quanh. Sake có thể thực hiện các nhiệm vụ đòi hỏi phải lập kế hoạch, tìm đường và sắp xếp lại các thập thể đơn giản. Sake được phát triển tại",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 22,
      "start_timestamp": "0:12:55",
      "end_timestamp": "0:13:37"
    }
  },
  {
    "page_content": "các thập thể đơn giản. Sake được phát triển tại Trung tâm Chí tế Nhân tạo của Viện nghiên cứu Stanford này, còn gọi là SRI International. Elyia năm 1966 là một chương trình máy tính tiên phong trong lĩnh vực xử lý ngôn ngực tự nhiên. Được phát triển từ năm 1964 đến 1967 tại MIT và bởi Joseph Weizenbaum. Elisa được tạo ra nhằm nghiên cứu sự tương tác giữa con người và máy móc, sử dụng phương pháp khớp mẫu và thay thế để mô phỏng hồi thoại. Nó tạo ra ảo giác rằng máy tính có thể hiểu người dùng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 23,
      "start_timestamp": "0:13:28",
      "end_timestamp": "0:14:08"
    }
  },
  {
    "page_content": "ra ảo giác rằng máy tính có thể hiểu người dùng mặc dù thực tế nó không thực sự hiểu được ý nghĩa của cuộc trò chuyện. Giai đoạn tiếp theo là mùa đông AI lần thứ nhất, 1974-1980. Lý do của AI xoay thoái đó là các hạt chế về phần cứng, máy tính không đủ mạnh để chạy các mô hình AI phức tạp. Thứ hai là thiếu dữ liệu. AI thời kỳ này chủ yếu dựa trên các quy tắc, các rule base nên không thể học được từ dữ liệu lớn. Và báo cáo Light Hill năm 1973 đã chỉ trích AI không đạt được tiến bộ thực tế cùng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 24,
      "start_timestamp": "0:14:02",
      "end_timestamp": "0:14:42"
    }
  },
  {
    "page_content": "chỉ trích AI không đạt được tiến bộ thực tế cùng với sự thất vại của các dự án dịch máy dẫn đến việc chính phủ cắt giảm tài trợ. Hậu quả là tài trợ AI bị cắt giảm mạnh và niềm tin vào AI suy giảm, dẫn đến một giai đoạn trưởng lại trong nghiên cứu về AI. Sau đó là giai đoạn tiếp theo là sự hồi sinh và mùa đông AI lần thứ 2. Thì sự trở lại của AI nhờ hệ chuyên gia thì các hệ thống, chuyên gia như MySync, Trận đoán e-Khoa và X-Con cấu hình hệ thống máy tính đã đạt được một số thành công nhất định.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 25,
      "start_timestamp": "0:14:37",
      "end_timestamp": "0:15:17"
    }
  },
  {
    "page_content": "máy tính đã đạt được một số thành công nhất định. và các công ty bắt đầu áp dụng AI vào thực tế trong lĩnh vực tài chính và y tế. Tuy nhiên, nó lại chứng kiến sự suy giảm lần nữa bởi vì chi phí duy trì cao, các hệ chuyên gia đòi hỏi cập nhật liên tục, tốn kém và không linh hoạt, không thể xử lý dữ liệu lớn và không có khả năng học hỏi từ dữ liệu. Một mùa đông AI khác diễn ra vào cuối thập niên 1980, từ 1987-1993 khi sự kỳ vọng quá cao từ thập niên trước dẫn đến thất vọng AI. AI khi không đạt",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 26,
      "start_timestamp": "0:15:12",
      "end_timestamp": "0:15:44"
    }
  },
  {
    "page_content": "niên trước dẫn đến thất vọng AI. AI khi không đạt những bước tiến như mong đợi. Giai đoạn tiếp theo từ thập niên 1990 đến 2000, thì giai đoạn này Machine Learning đã tắt khỏi AI truyền thống, chuyển từ lập trình, quy tắc, tức là lập trình giữa trên chi tiết từng bước sang học từ dữ liệu. Thay vì sử dụng hệ chuyên gia với các quy tắc cố định, thì Machine Learning đã dựa vào các thuật toán có thể học từ dữ liệu, giúp mô hình linh hoạt và chính xác hơn. Các thuật toán ra đời như Super Vector",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 27,
      "start_timestamp": "0:15:38",
      "end_timestamp": "0:16:22"
    }
  },
  {
    "page_content": "xác hơn. Các thuật toán ra đời như Super Vector Machine, Random Forest, Knife Bayer trở thành nền tảng của Machine Learning. Bên cạnh đó là sự gia tăng vượt bậc về tính toán và dữ liệu. Internet và điện thoại thông minh đã tạo ra cái dự liệu khổng lồ và sự xuất hiện của Big Data giúp có thể học từ dữ liệu kimô lớn hơn bao giờ hết. Máy tính thì ngày càng mạnh hơn và cho phép hướng luyện các mô hình phức tạp hơn. Ngoài ra thì các thuộc toán Machine Learning ngày càng được sử dụng rộng rãi trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 28,
      "start_timestamp": "0:16:12",
      "end_timestamp": "0:16:54"
    }
  },
  {
    "page_content": "Learning ngày càng được sử dụng rộng rãi trong nhiều ngành công nghiệp Ví dụ như trong tài chính, y tế, thương mại, điện tử Các hệ thống gợi ý bắt đầu được sử dụng rộng rãi, ví dụ như Amazon hay Netflix Gia đoạn tiếp theo là Bitdata và sự trở dạy của Deep Learning từ năm 2000 cho tới 2010 Gia đoạn này tiếp tục chứng kiến sự bùng nổ của Bitdata và sự trở lại mạnh mẽ của mạng neuron nhân tạo Những yếu tố đã thúc đẩy, bao gồm sự gia tăng dữ liệu, sự phát triển của Internet và mạng xã hội đã giúp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 29,
      "start_timestamp": "0:16:51",
      "end_timestamp": "0:17:19"
    }
  },
  {
    "page_content": "sự phát triển của Internet và mạng xã hội đã giúp tạo ra một lượng dữ liệu khổng lồ và phân cứng mạnh mẽ hơn. Sự ra đời của GPU, Đơn vị Sử lý Đồ Họa, giúp tăng tốc hướng luyện các mô hình neuron dân tạo và Deep Learning hồi sinh, với Hilton, Benjo và LeCun đi tiên phong trong việc phát triển các mạng neuron học sau. Giai đoạn của Deep Learning và AI hiện đại từ năm 2010 tới 2021, đó là sự phát triển mạnh mẽ của Deep Learning. Năm 2012, AlexNet, một mạng neuron học sau, là lần đầu tiên đã chiến",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 30,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "một mạng neuron học sau, là lần đầu tiên đã chiến thắng trong cuộc thi ImageNet, chứng minh sức mạnh của Deep Learning và đánh dấu sự thống trị của mạng neuron trong nhận diện hình ảnh. Mô hình Transformer năm 2017 đã đặt nền tảng cho GPT và Perp cách mạng hóa, xử lý ngôn ngữ tự nhiên. tự nhiên làm đền tảng cho các hệ thống AI hiện đại. AI trong đời sống thực tế thì AI ngày càng được hỗ trợ trong các lĩnh vực tài chính, y tế, xe tự lái, thương mại, điện tử. Tuy nhiên cũng gặp các thách thích về",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 31,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "điện tử. Tuy nhiên cũng gặp các thách thích về đạo đức. Quý dụ như AI có thể tạo ra thiên vị và gây ra các vấn đề xã hội. Su hướng AI có trách nhiệm, Responsible AI và AI minh bạch ngày càng được quan tâm. Hãy subscribe cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video hấp dẫn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=u3GD0b9vCDo",
      "filename": "u3GD0b9vCDo",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 1)",
      "chunk_id": 32,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề, học sâu, machine learning, logistic regression, imageNet. những cách sử dụng Machine Learning để giải quyết bài toán Image Classification đơn giản và chạy thử bằng Google Collab. Thì ở đây các thầy đã chuẩn bị cho các bạn một cái Collab Notebook các bạn có thể truy cập ở đường link bên dưới này. Khi các bạn vào cái Collab Notebook, công việc đầu tiên mà các bạn nên làm Đó là vào menu File và các bạn tùy chọn là Save a copy in Drive Có nghĩa là tạo ra một cái bản copy của cái Collab",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:10"
    }
  },
  {
    "page_content": "nghĩa là tạo ra một cái bản copy của cái Collab Notebook này và trong tài khoản Google Drive của các bạn Trước khi các bạn bắt đầu chạy hay là bắt đầu làm gì thì các bạn nên tạo một cái bản copy trước Tại vì nếu các bạn muốn hiểu rõ một ví dụ thì cách duy nhất là các bạn phải chạy thử Quang sát kết quả và sau đó phải điều chỉnh cái code này và chạy lại để xem cái mình hiểu nó có chính xác hay không Mình có thể chỉnh sửa lại cái chương trình của người khác theo ý mình hay không Thì để làm được",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 1,
      "start_timestamp": "0:01:02",
      "end_timestamp": "0:01:46"
    }
  },
  {
    "page_content": "người khác theo ý mình hay không Thì để làm được video đó các bạn phải tạo một cái bản copy Còn version mà các thầy share cho các bạn là version chỉ đọc Trong bài dạng này, cái đường link kia là cái notebook chỉ có thể đọc thôi à không có quyền chỉnh sửa, sửa thì cũng không safe lại được đó nên các bạn nên tạo một cái bản copy trong Google Drive của mình để các bạn có thể chỉnh sửa thì giới thiệu đơn sản nhanh về bài toán này đây là bài toán Image Classification, phân loại chó mèo, đó là tên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 2,
      "start_timestamp": "0:01:40",
      "end_timestamp": "0:02:30"
    }
  },
  {
    "page_content": "Classification, phân loại chó mèo, đó là tên bài toán bài toán này input đầu vào sẽ là một hình ảnh và chúng ta mong muốn đầu ra là kết quả phức loại cho biết hình đưa vào là thuộc nhóm mèo hay nhóm chó, cast hay docs. Và cái loại bài toán như vậy người ta gọi là bài toán binary classification. Trong số các lớp bài toán machine learning đây là lớp bài toán classification. Là lớp bài toán phổ biến nhất hiện nay trong machine learning mà người ta đã giải quyết được. Và trong classifications, loại",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 3,
      "start_timestamp": "0:02:22",
      "end_timestamp": "0:03:06"
    }
  },
  {
    "page_content": "giải quyết được. Và trong classifications, loại đơn giản nhất là binary classification có nghĩa là output chỉ có thể là 1 trong 2 giá trị thôi, mail hoặc là chó thôi Nếu như lọt ảnh đầu vào chứa con gì khác 1 trong 2 con này thì model cũng chỉ trả lời là mail hoặc chó thôi Lúc đó không cần quan tâm đúng sai nữa, lúc nào kết quả ra cũng là 1 trong 2 giá trị mà thôi thì bài toán đó gọi là binary classification thì tiêu trí để đánh giá một cái model, thực hiện cái công việc dự đoán như thế đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 4,
      "start_timestamp": "0:02:59",
      "end_timestamp": "0:03:44"
    }
  },
  {
    "page_content": "model, thực hiện cái công việc dự đoán như thế đó thường trong bài toán binary classification tiêu trí đánh giá phổ biến hay được dùng là accuracy, độ chính xác Đổi chính xác được tính là số lần mà model dự đoán đúng Trên tổng số lần các bạn thực hiện kiểm tra Trên tập kiểm tra có bao nhiêu ảnh các bạn đưa cho model Tiến hành phân lợp, phân loại ra xem đây là ảnh chó hay là ảnh mèo Số lượng ảnh đúng chia cho tổng số ảnh thì cái thông số đó gọi là accuracy Chúng ta còn nhiều tiêu chí đánh giá",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 5,
      "start_timestamp": "0:03:40",
      "end_timestamp": "0:04:19"
    }
  },
  {
    "page_content": "là accuracy Chúng ta còn nhiều tiêu chí đánh giá khác cho bài toán Classification nhưng đối với bài toán hiện tại để dùng để minh họa thì chúng ta dùng tiêu chí Accuracy cho nó đơn giản đó là bước xác định một cái bài toán Machine Learning các bạn phải chỉ rõ được đầu vào đầu ra của bài toán và từ đầu vào đầu ra các bạn sẽ xác định được cái loại bài toán này là loại bài toán gì trong Machine Learning và chúng ta sử dụng tiêu chí gì để tránh giá là cái model có tốt hay không. Phải xác định đầy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 6,
      "start_timestamp": "0:04:16",
      "end_timestamp": "0:04:55"
    }
  },
  {
    "page_content": "là cái model có tốt hay không. Phải xác định đầy đủ những cái thông tin này thì chúng ta mới tiến hành bước tiếp theo sử dụng Machine Learning để giải quyết bài toán. Thì bước tiếp theo là bước thu thập dữ liệu. Thu thập dữ liệu thì đúng ra là chúng ta phải đi thu thập các hình ảnh. Dính đầu vào dữ liệu chúng ta là hình ảnh mà chúng ta phải đi thu thập các hình ảnh này từ trong thực tế nhưng mà rất may là bài toán này nó cũng phổ biến và đơn giản nên cái thư viện TensorFlow người ta đã thu thập",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 7,
      "start_timestamp": "0:04:51",
      "end_timestamp": "0:05:41"
    }
  },
  {
    "page_content": "nên cái thư viện TensorFlow người ta đã thu thập mẫu cho chúng ta một cái bộ dữ liệu gồm rất là nhiều ảnh, chó với mèo và cái label tương ứng cho từng tấm ảnh rồi nên chúng ta quyết thu thập dữ liệu này đơn giản là chúng ta tải data set từ tư viện TensorFlow về và sử dụng. Nhà bước 3 là bước chuẩn bị dữ liệu. Mã nguồn cụ thể của 3 bước này được cho ở cell bên dưới. Bên trên này là mô tả và ngay bên dưới này là mã nguồn. Vía cầm bài thanh, phía dưới sẽ là kết quả thực thi. Thì phần đầu là phần",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 8,
      "start_timestamp": "0:05:38",
      "end_timestamp": "0:06:18"
    }
  },
  {
    "page_content": "dưới sẽ là kết quả thực thi. Thì phần đầu là phần import các tư viện cần thiết. Ở đây chúng ta sẽ xây dựng model sử dụng một thư viện Deep Learning là thư viện TensorFlow Kỹ thuật Deep Learning là kỹ thuật phổ biến nhất hiện nay để giải quyết bài toán image classification và hiện nay nó cũng đang làm khá tốt và với sự hỗ trợ của thư viện thì chúng ta viết code nó cũng nhẹ nhàng hơn Rồi, ở đây chúng ta kiểm tra cái phiên bản TensorFlow được cài đặt bên trong. Ở đây chúng ta muốn kiểm tra chủ yếu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 9,
      "start_timestamp": "0:06:12",
      "end_timestamp": "0:07:02"
    }
  },
  {
    "page_content": "bên trong. Ở đây chúng ta muốn kiểm tra chủ yếu là cài đặt bên trong cái máy ảo Google Collab nè, tại vì Google Collab họ cung cấp cái máy ảo cài gần như là đầy đủ các thư viện, nhưng không phải là version mới nhất. nhưng không phải là version mới nhất các thư viện machine learning cài đã khó rồi giữ cho nó được update cập nhật thường xuyên còn khó hơn và thực hiện công việc update này trên 1 kệ thống server rất nhiều máy tính thì nó còn khó dữ nữa nên Google cũng không cập nhật cài version thư",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 10,
      "start_timestamp": "0:06:56",
      "end_timestamp": "0:07:47"
    }
  },
  {
    "page_content": "nên Google cũng không cập nhật cài version thư viện quá thường xuyên Vì thế các thầy có thói quen là khi chạy ứng dụng thì in luôn thông tin về version phiên bản của cái thư viện ra để biết kết quả này là mình chạy hồi nào, hồi version bao nhiêu. Rồi, bước xác định bài toán chúng ta đã làm rồi. Tiếp theo là chúng ta tải về và sử dụng lại bộ dữ liệu cat-visit-doc từ TensorFlow Dataset. xét, chúng ta import thư viện và chúng ta dùng hamlot, hamlot này các bạn có thể hoàn toàn chạy thử ở đây chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 11,
      "start_timestamp": "0:07:36",
      "end_timestamp": "0:08:20"
    }
  },
  {
    "page_content": "này các bạn có thể hoàn toàn chạy thử ở đây chúng ta xin cấp máy ảo ở đây bằng đầu chúng ta cấp máy ảo chạy cpu thôi ha để các bạn hình dung cái code này nó hoạt động như thế nào trước và máy ảo CPU thì có ưu điểm là các bạn được giữ máy trong thời gian rất là dài các bạn có thể dùng để kiểm tra chạy thử nghiền ẩm nghiên cứu cái mã nguồn mà không lo máy bị thu hồi đến thường là chúng ta sẽ yêu cầu request 1 cái máy CPU trước để chúng ta chạy thử khi nào thấy code đúng rồi và chúng ta cần nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 12,
      "start_timestamp": "0:08:12",
      "end_timestamp": "0:09:05"
    }
  },
  {
    "page_content": "thử khi nào thấy code đúng rồi và chúng ta cần nó chạy nhanh hơn thì chúng ta mới request tới những tài nguyên cấp cao hơn của Google Collab thì cái lệnh data.load này các bạn thấy đầu tiên là chúng ta in ra version của TensorFlow thì hôm nay Google Collab cung cấp version 2.19 có thể lúc các bạn học hôm nay thì nó sẽ là version khác cũng không sao Nhi vọng những dòng code này tới lúc các bạn học nó vẫn chưa bị đào thái, vẫn còn chạy được Rồi sau khi bring version ra xong chúng ta load kế",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 13,
      "start_timestamp": "0:09:00",
      "end_timestamp": "0:09:41"
    }
  },
  {
    "page_content": "sau khi bring version ra xong chúng ta load kế dataset thì hàm load này nó sẽ tự động thực hiện luôn công việc download kế dataset từ trên trang chủ của TensorFlow về không phải data set này nằm sẵn trong thư viện nha các bạn data set này nằm trên internet và các bạn phải có internet thì các bạn mới download về được dĩ nhiên phải có internet thì chúng ta mới dùng được Google Collab rồi và cái công việc download này sẽ do máy ảo của Google Collab thực hiện các bạn thấy tốc độ tải ở đây là 71",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 14,
      "start_timestamp": "0:09:38",
      "end_timestamp": "0:10:34"
    }
  },
  {
    "page_content": "thực hiện các bạn thấy tốc độ tải ở đây là 71 megabyte một giây megabyte không phải megabit nha các bạn có nghĩa là nó vào khoảng 500, gần 600 megabit per second một tốc độ cũng khá là cao đặc biệt đây là server trigger to quốc tế ngoài tài nguyên tính toán CPU và CPU Google Collab cung cấp cho các bạn một máy chủ ảo còn một loại tài nguyên đáng quý nữa đó là kết nối mạng rất là nhanh. Rồi, và sau khi Dow về xong thì tiếp theo chúng ta sẽ thực hiện cái bước tiền xử lý dữ liệu. Bước tiền xử lý",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 15,
      "start_timestamp": "0:10:24",
      "end_timestamp": "0:11:04"
    }
  },
  {
    "page_content": "hiện cái bước tiền xử lý dữ liệu. Bước tiền xử lý này bao gồm chia tập dữ liệu thành hai phần và thực hiện cái hàm format này lên từng tấm hình Bên trong data set này, chúng ta chỉ thực hiện một bước tiền xử lý đơn giản thôi đó là bước resize. Thực ra có hai bước mà chúng ta viết gọn trên một hàm. Đầu tiên là resize ảnh trong data set về kích thước 150 x 150. 150 x 150 là ảnh nó hình vuông như thế này. Đây là ví dụ về data set của chúng ta. ảnh nó hình vuông như thế này, còn trong thực tế ảnh",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 16,
      "start_timestamp": "0:10:56",
      "end_timestamp": "0:11:47"
    }
  },
  {
    "page_content": "nó hình vuông như thế này, còn trong thực tế ảnh chụp từ các thiết bị ngày nay đa số sẽ là hình chữ nhật Nhưng mà khi các bạn đã xây dựng một cái model thì model không thể nhận ảnh có nhiều kích thước khác nhau đến từ nhiều cái thiết bị khác nhau mà ảnh lúc to lúc nhỏ lúc ngang lúc dọc vâng vâng model sẽ rất là khó xử lý Nên ở đây để loại trừ những trường hợp đó, chúng ta muốn model chỉ tập trung, những gì chủ thể trong ảnh là chó hay mèo thôi, chúng ta không muốn để model giải quyết những vấn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 17,
      "start_timestamp": "0:11:45",
      "end_timestamp": "0:12:22"
    }
  },
  {
    "page_content": "chúng ta không muốn để model giải quyết những vấn đề liên quan đến kích thước, cái chiều xoay của ảnh thì ở đây tôi sử dụng cách đơn giản đó là resize tấm ảnh về kích thước là 150 x 150. Resize là một cái thao tác rất là cơ bản trong xử lý ảnh vững dụng Và mả nguồn của thuật toán Resize này thì nó cũng đã nằm sẵn trong thư viện TensorFlow rồi chúng ta chỉ cần gọi một hàm là xong khấu này chưa liên quan gì đến model và đến machine learning cả nên đó là khấu tiền xử lý chúng ta chưa đụng gì đến",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 18,
      "start_timestamp": "0:12:17",
      "end_timestamp": "0:13:13"
    }
  },
  {
    "page_content": "đó là khấu tiền xử lý chúng ta chưa đụng gì đến machine learning ở bước này hết chúng ta chỉ đang Thu nhỏ cái ảnh lại thôi ha 150 x 150 là kích thước không phải to lắm Nên hầu hết ảnh ở đây sẽ bị thu nhỏ lại Nhưng mà do chủ thể của chúng ta là chó hoặc mèo là hai động vật cũng khá là to Nên chắc là thu nhỏ ảnh lại thì vẫn có thể nhìn được Rồi, cuối cùng chúng ta chia data set này thành 2 phần là training set Và validation set Training dataset sẽ chiếm 0,8, tức là 80% Kích thước của dataset và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 19,
      "start_timestamp": "0:13:09",
      "end_timestamp": "0:14:01"
    }
  },
  {
    "page_content": "chiếm 0,8, tức là 80% Kích thước của dataset và validation set sẽ chiếm 20% còn lại Đó là bước tiền xử lý Bước thứ 4 là chúng ta phát triển lựa chọn và xây dựng model Ở đây có một model đơn giản được xây dựng ra bằng thư viện Keras trong TensorFlow Keras là một thư viện đường đi kèm với TensorFlow Nó chưa danh sách các model mà các bậc tiền nhân đi trước đã Đề xuất suy nghĩ rất là đắng, đó thật ra nhiều nghiên cứu khoa học họ đề xuất thì Keras lưu những model này để chúng ta có thể gép lại với",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 20,
      "start_timestamp": "0:13:49",
      "end_timestamp": "0:14:41"
    }
  },
  {
    "page_content": "những model này để chúng ta có thể gép lại với nhau để hình thành một model mới cho bài toán của mình Ở đây chúng ta có một model dựa trên ký tưởng là CNN, tức là Convolutional Neural Network là một cái mạng học sâu sử dụng cái phép toán Convolution đó phép toán này, cái mạng này đã có rất là nhiều tài liệu chứng minh là nó hiệu quả trong bài toán Image Classification và chúng ta sử dụng lại thôi, đoạn code này kế thừa từ rất nhiều cái mạng nguồn khác có trên mạng Đầu vào chúng ta sẽ là một cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 21,
      "start_timestamp": "0:14:36",
      "end_timestamp": "0:15:23"
    }
  },
  {
    "page_content": "khác có trên mạng Đầu vào chúng ta sẽ là một cái ảnh có kích thước là 150 x 150 và đây là ảnh màu có 3 kênh màu red green blue đó là 3 con số ở đây. Đó, đầu tiên là chiều dài của ảnh, chiều cao của ảnh và số lượng màu của mỗi điểm ảnh. Chúng ta sẽ thực hiện qua rất nhiều cái thao tác trên neural network để cuối cùng đầu ra của chúng ta là một con số duy nhất. Đầu ra của chúng ta là một cái layer trong nội neural network chỉ chứa một số duy nhất và cái số này sẽ được đưa vào một cái hàm sigmoid",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 22,
      "start_timestamp": "0:15:15",
      "end_timestamp": "0:16:12"
    }
  },
  {
    "page_content": "và cái số này sẽ được đưa vào một cái hàm sigmoid để cho ra kết quả sau cục. Đó là một kỹ thuật Image Classification đơn giản Các bạn từ từ sẽ được học về kỹ thuật này trong những chương tiếp theo Ở đây chúng ta ghi ra để chạy thử vinh hỏa cho khả năng của Google Collab Tiếp theo là yêu cầu của thư viện thì chúng ta sẽ chạy lệnh model.com.by Và chúng ta đưa các tham số vào để có thể thực hiện quá trình huấn luyện cho cái model này. Chúng ta sẽ cố gắng dựa trên kiến trúc convolutional neural",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 23,
      "start_timestamp": "0:16:03",
      "end_timestamp": "0:16:53"
    }
  },
  {
    "page_content": "cố gắng dựa trên kiến trúc convolutional neural network này. Chúng ta tìm ra cái model cho kết quả dự đoán tốt nhất bằng thuật toán ADAM optimization. và chúng ta đánh giá mức độ tốt bằng metric accuracy sau đó chúng ta sẽ đi huấn luyện model huấn luyện là tìm ra các cái tham số để model có thể đưa ra kết quả dự đoán tốt nhất thì quá trình huấn luyện này sẽ được thực hiện thông qua 5 lần lập 5 lần lập. Đầu tiên là chúng ta đưa tất cả các ảnh trong training set và trong model chúng ta xem. Thử",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 24,
      "start_timestamp": "0:16:46",
      "end_timestamp": "0:17:39"
    }
  },
  {
    "page_content": "training set và trong model chúng ta xem. Thử dụng thuộc toán Adam Optimization này chúng ta điều chỉnh lại các tham số để cho model có thể nhận diện được cái ảnh này. Sau khi chạy qua hết một loạt các ảnh, chúng ta sẽ inh ra kết quả hiện tại của model, accuracy của model đang là bao nhiêu. Và trên tập Accuracy Validation Set thì Accuracy là bao nhiêu? Sau khi dây chúng ta lập 5 lần lập như thế, chúng ta đưa toàn bộ cáp ảnh vào model, chúng ta điều chỉnh tham số cho kết quả nó tốt hơn. Rồi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 25,
      "start_timestamp": "0:17:36",
      "end_timestamp": "0:18:23"
    }
  },
  {
    "page_content": "ta điều chỉnh tham số cho kết quả nó tốt hơn. Rồi chúng ta qua ảnh tiếp theo, thì các ảnh này sẽ đưa vào lần lượt theo từng patch, là một lần model sẽ học trong 32 tấm ảnh một lần các bạn hoàn toàn có thể tăng con số 32 này lên không nhất các bạn thậm chí các bạn có thể đưa toàn bộ ảnh trong model vào một lần cũng được nếu như máy các bạn đủ mạnh ở đây chúng ta có 580 bách mỗi bách 30 ảnh là chúng ta có khoảng 15.000 Mình có khoảng 15.000 tấm ảnh trong training set Nếu mấy bạn có đủ bộ nhớ để",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 26,
      "start_timestamp": "0:18:21",
      "end_timestamp": "0:19:09"
    }
  },
  {
    "page_content": "trong training set Nếu mấy bạn có đủ bộ nhớ để lưu toàn bộ 15.000 tấm ảnh Mỗi ảnh bao gồm 150 x 150 ohm 150 bình phương pixel Nếu các bạn nhắm mấy mình đủ RAM để chứa Các bạn có thể tăng patch size lên là 15.000 chạy toàn bộ Cái data set này một lần duy nhất cũng được Nhưng mà Điều đó thì thương là cũng không khôn quan cho lắm Tại vì nếu các bạn tin ý các bạn sẽ nhìn thấy ở đây là thời gian chạy Của một lần lập có nghĩa là để trend hết một lần Gần 15.000 thám ảnh đó Sẽ phải tốn 1.123 giây Nếu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 27,
      "start_timestamp": "0:18:57",
      "end_timestamp": "0:19:54"
    }
  },
  {
    "page_content": "Gần 15.000 thám ảnh đó Sẽ phải tốn 1.123 giây Nếu bạn nào tính nhảm nhanh thì có thể thấy con số này là Gần 20 phút 1 phút là 1200 giây 1 giờ là 3600 giây thì 20 phút chứ 20 phút sẽ là 1200 ở đây chúng ta 1123 gần 20 phút cho 1 lần lập 15000 tấm ảnh mà chúng ta lập 5 lần như vậy như vậy thời gian training tổng cộng là khoảng hơn đâu đó cỡ 2 tiếng đồ hồ có nghĩa là cái notebook này các thầy đã phải chạy từ trước và là treo máy cho tấm ảnh và là treo máy cho nó chạy mới lấy được kết quả này về",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 28,
      "start_timestamp": "0:19:47",
      "end_timestamp": "0:20:38"
    }
  },
  {
    "page_content": "treo máy cho nó chạy mới lấy được kết quả này về cho các bạn. Rõ do ở đây Google Collab cho chúng ta sử dụng CPU tới khoảng 70 giờ họ cũng đã có dùng ý rồi, một cái ví dụ đơn giản nhỏ thế này thôi nhưng mà các bạn dùng một kỹ thuật cân cái khối lượng tính toán cao như Deep Learning thì nếu chỉ dùng 2 CPU Core đơn giản của Google Collab các bạn sẽ phải tốn gần 2 tiếng đầu hồ cho kết quả 5 lần lập như thế này và kết quả này theo biểu đồ xong thì trên tập training set model càng học thì càng thông",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 29,
      "start_timestamp": "0:20:23",
      "end_timestamp": "0:21:18"
    }
  },
  {
    "page_content": "tập training set model càng học thì càng thông minh hơn, accuracy càng ngày càng cao nhưng mà trên tập validation set thì ban đầu model học thì accuracy có vẻ tăng nhưng từ từ nó đang hình như là nó tăng không được cao nữa, đầu tiên là nó từ 0.70 mấy nó nhảy lên gần 0.8 tức là từ đây là khoảng 7.3, 7.4% nhảy lên gần 80%, sau đó nhảy lên hơn 80% và hai lần training tiếp theo thì chưa đầy 5% có nghĩa là đối với dữ liệu mà model đã được thấy tức là model, dữ liệu đã được sử dụng để train cho model",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 30,
      "start_timestamp": "0:21:09",
      "end_timestamp": "0:21:59"
    }
  },
  {
    "page_content": "model, dữ liệu đã được sử dụng để train cho model thì model nó xử lý rất là tốt các bạn thấy training accuracy, tức là kể độ chính xác trên tập dữ liệu mà model được dùng để train nó đã gần 90% rồi, và có vẻ nó còn có thể tăng nữa Nhưng trên dữ liệu mà chúng ta chừa lại, chúng ta không dùng để tên Chúng ta trên validation set thì tốc độ tăng của accuracy không được nhanh như thế Đây cũng là một đặc trưng mà các bạn sẽ quan sát học cho các bài tiếp theo khi chúng ta làm quen với một số các mô",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 31,
      "start_timestamp": "0:21:51",
      "end_timestamp": "0:22:37"
    }
  },
  {
    "page_content": "tiếp theo khi chúng ta làm quen với một số các mô hình mại học khác Khi chúng ta có được một model, thì tiếp theo tới giai đoạn này chúng ta đã thấy được kết quả trend của model và chúng ta thấy được trên tập dữ liệu dùng để kiểm tra hay tập validation. Kết quả tăng không được mỹ mạng cho lắm thì chúng ta có thể đề xuất triển khai hoặc là chúng ta cải thiện mô hình Nếu chúng ta thấy mô hình accuracy này đối với tôi như vậy là được tạm được Tôi không yêu cầu quá cao 80% nghĩa là đưa 10 tấm ảnh,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 32,
      "start_timestamp": "0:22:30",
      "end_timestamp": "0:23:12"
    }
  },
  {
    "page_content": "yêu cầu quá cao 80% nghĩa là đưa 10 tấm ảnh, máy dự đoán đúng được 8 tấm, sai 2 tấm và tôi thấy như vậy là đủ rồi đủ rồi thì thôi có thể triển khai sử dụng mô hình này trong bài toán thực tế của tôi. Nếu không thì tôi sẽ phải tiến hành cải tiến model. Mà muốn cải tiến model thì thường các bạn sẽ phải có một bước đó là các bạn quan sát cái model nó chạy. Các bạn xem những trường hợp nào nó hay sai để các bạn cố gắng tìm ra ý tưởng xem model sai ở đâu và từ đó chúng ta đề xuất phương án cải tiến",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 33,
      "start_timestamp": "0:23:09",
      "end_timestamp": "0:23:12"
    }
  },
  {
    "page_content": "đâu và từ đó chúng ta đề xuất phương án cải tiến nếu được chúng ta chỉnh sửa điều chỉnh lại model sau đó chúng ta huấn luyện lại model và chúng ta lại quan sát kết quả đánh giá xem nó có ổn hay chưa, chúng ta lập đi lập lại quá trình này thì đó chính là sự cải tiến Rồi, mà sự cải tiến này sẽ phải trả giá nó tương đối đắc, tại vì mỗi lần huấn luyện lại model là 2 tiếng đồng hồ Trong các phần tiếp theo, chúng ta sẽ quan sát thêm cũng với ví chủ này. Làm sao để khâu training model này được nhanh",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 34,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Làm sao để khâu training model này được nhanh hơn. Làm sao chúng ta cắt giảm cái 2 tiếng đồng hồ này xuống. Chúng ta muốn cải tiến được model thì chúng ta phải thử nghiệm. chúng ta phải quan sát thực tiện nó hoạt động, chúng ta đưa hình vào sẵn, chúng ta quan sát khách nó chạy rồi từ đó chúng ta mới có ý tưởng để chúng ta thực hiện cải tiến và sau đó chúng ta phải kiểm tra lại xem cải tiến đó có thực sự là đưa ra kết quả tốt hơn hay không nhưng mà cứ mỗi lần thực hiện một thay đổi tốt 2 tiếng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 35,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "mà cứ mỗi lần thực hiện một thay đổi tốt 2 tiếng đồng hồ để train thì cái tốc độ cải tiến các bạn sẽ không thể nhanh được chúng ta phải bằng cách nào đó kéo giảm cái tốc độ này xuống chúng ta sẽ đến với điều đó trong những video tiếp theo",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=UPHRxByLZnU",
      "filename": "UPHRxByLZnU",
      "title": "[CS114 - Tutorial] Google Colab (Phần 3)",
      "chunk_id": 36,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Trong những phần trước thì chúng ta à đã tìm hiểu về cái mô hình hồi quy đơn biến. Còn ở đây chúng ta sẽ tiến hành tổng quát hóa nó lên để có cái mô hình đó là hồi quy đa biến. Thế thì ở đây cái khái niệm đơn biến và đa biến ở đây có ý nghĩa là gì? Đó là đối với cái giá trị mà dự đoán y của chúng ta thì trước đây là y này chỉ bị phụ thuộc bởi duy nhất một biến x đó là một biến số. Nhưng thực tế chúng ta thấy để đưa ra một cái dự đoán nào đó thì thường nó sẽ phải dựa trên rất nhiều những cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=uyUMxrFwKCE",
      "filename": "uyUMxrFwKCE",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 1)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:57"
    }
  },
  {
    "page_content": "thường nó sẽ phải dựa trên rất nhiều những cái thông tin đầu vào chứ không thể nào mà chỉ dựa trên thông tin của một biến số là chúng ta có thể đưa ra được cái dự đoán chính xác được. Do đó thì chúng ta sẽ có cái hàm y dự đoán của mình lúc này nó sẽ là một cái hàm gồm nhiều hơn một biến. Cụ thể ở đây đó là x1 ph x2 chấm chấm chấm. Và cái số biến này thì nó sẽ có thể kéo rất là dài. Như vậy thì áp dụng cái mô hình hồi quy đa biến như thế nào thì đó chính là cái nội dung chính mà chúng ta sẽ cùng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=uyUMxrFwKCE",
      "filename": "uyUMxrFwKCE",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 1)",
      "chunk_id": 1,
      "start_timestamp": "0:00:53",
      "end_timestamp": "0:01:38"
    }
  },
  {
    "page_content": "chính là cái nội dung chính mà chúng ta sẽ cùng tìm hiểu trong phần tiếp theo. Thì đầu tiên đó là giới thiệu về cái mô hình hồi quyến tính à nhiều biến tên tiếng Anh đó là multiple linear regression. Đây là cái phương pháp mở rộng của hồi quy tuyến tính một biến. và mô hình hóa được cái mối quan hệ tiến tính giữa nhiều biến đầu vào hay còn gọi là nhiều cái đặc trưng đầu vào à và một cái biến đầu ra liên tục. Thì như đã nói tức là cái biến đầu ra liên tục của mình nó chính là Y. Và mình mong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=uyUMxrFwKCE",
      "filename": "uyUMxrFwKCE",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 1)",
      "chunk_id": 2,
      "start_timestamp": "0:01:33",
      "end_timestamp": "0:02:16"
    }
  },
  {
    "page_content": "ra liên tục của mình nó chính là Y. Và mình mong muốn mình kỳ vọng là y này nó sẽ bị phụ thuộc một cách tuyến tính với một tập hợp các cái biến số đầu vào cho đến XM. Thì khi đó cái hàm i của mình nó sẽ là một cái hàm liên tục mà có nhiều cái biến tầ vào. Có nhiều biến đầ vào. Và mô hình của cái mối quan hệ này thì lúc này nó sẽ không còn gọi là một đường thẳng nữa mà lúc này nó sẽ chuyển sang một cái khái niệm gọi là siêu phẳn hay còn gọi là hyperplan. Trong cái không gian nhiều chiều. Trước",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=uyUMxrFwKCE",
      "filename": "uyUMxrFwKCE",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 1)",
      "chunk_id": 3,
      "start_timestamp": "0:02:09",
      "end_timestamp": "0:02:55"
    }
  },
  {
    "page_content": "Trong cái không gian nhiều chiều. Trước đây thì chúng ta chỉ có một cái không gian là một trục hành. là thể hiện cái đặc trưng và trục tung là thể hiện cái giá trị liên tục mà cần dự đoán. Thế thì nếu mà cái mô hình tuyến tính mà chỉ có một cái ển biến x vào như thế này thì nó sẽ tạo ra những cái đường thẳng tạo ra những cái đường thẳng như thế này thì lúc này là mô hình của mình sẽ chỉ đơn giản là biểu diễn cho một biến nhưng thực tế thì chúng ta sẽ có rất nhiều biến. Ví dụ x1, x2 đây. Và cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=uyUMxrFwKCE",
      "filename": "uyUMxrFwKCE",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 1)",
      "chunk_id": 4,
      "start_timestamp": "0:02:50",
      "end_timestamp": "0:03:42"
    }
  },
  {
    "page_content": "ta sẽ có rất nhiều biến. Ví dụ x1, x2 đây. Và cái chiều cao của mình chính là cái y. Thì lúc này cái mặt để mà phân loại à để mà dự đoán của chúng ta thì nó sẽ là một cái siêu phẳng. Nếu trong cái bài toán phân loại thì đó nó sẽ tách ra làm hai cái không gian. Đó. Rồi thì cái mối quan hệ siêu phẳng này á à trong không gian nhiều chiều thì thay vì nó là một đường thẳng như một biến thì đối với trường hợp mà nhiều biến thì cái à công thức của wx cộng b thì nếu như nó bằng một cái giá trị nào đó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=uyUMxrFwKCE",
      "filename": "uyUMxrFwKCE",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 1)",
      "chunk_id": 5,
      "start_timestamp": "0:03:37",
      "end_timestamp": "0:04:18"
    }
  },
  {
    "page_content": "cộng b thì nếu như nó bằng một cái giá trị nào đó bằng một cái giá trị c nào đó thì đây là một phương trình và cái phương trình này thì nó sẽ đại diện bởi một cái mặt phẳng như thế này. nó không phải là một đường thẳng nữa mà nó sẽ là một mặt phẳng. Thì ví dụ như chúng ta làm cái bài toán là dự đoán giá nhà thì giá của một căn nhà nó sẽ phụ thuộc bởi rất nhiều cái thông tin chứ nó không phải chỉ phụ thuộc vào thông tin về mặt diện tích. Nó có thể là diện tích nè, vị trí nè, số phòng ngủ nè, rồi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=uyUMxrFwKCE",
      "filename": "uyUMxrFwKCE",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 1)",
      "chunk_id": 6,
      "start_timestamp": "0:04:13",
      "end_timestamp": "0:04:54"
    }
  },
  {
    "page_content": "là diện tích nè, vị trí nè, số phòng ngủ nè, rồi số năm xây dựng. Đó thì đối với những cái đặc trưng đầu vào này, chúng ta sẽ xác định xem cái tính chất phụ thuộc giữa giá nhà với lại các cái biến số đầu vào này có tỉ lệ thuận hay tỉ lệ nghịch thì chúng ta sẽ làm các cái phép phân tích đó. Đó thì chúng ta à sử dụng cái độ đo tương đồng giữa giá nhà là output và cái một cái biến x. y nào đó thì chúng ta thấy ví dụ giá nhà theo diện tích thì nó sẽ là giá nhà tỉ lệ thuận với diện tích. Nhưng ngược",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=uyUMxrFwKCE",
      "filename": "uyUMxrFwKCE",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 1)",
      "chunk_id": 7,
      "start_timestamp": "0:04:47",
      "end_timestamp": "0:05:33"
    }
  },
  {
    "page_content": "là giá nhà tỉ lệ thuận với diện tích. Nhưng ngược lại đối với vị trí thì nó sẽ tỉ lệ nghịch với lại cái à vị trí của mình. Tức là cái khoảng cách đến trung tâm chẳng hạn. Khoảng cách đến trung tâm càng cao thì cái giá trị giá căn nhà của mình sẽ càng thấp. Đó tương tự như vậy thì giá nhà nó sẽ à tỉ lệ thuận với lại cái số phòng hoặc là tỷ lệ nghịch với lại cái số năm xây dựng. năm xây dựng càng cao thì cái giá nhà của mình nó có thể sẽ giảm dần. Và tại sao chúng ta cần phải có nhiều biến? Thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=uyUMxrFwKCE",
      "filename": "uyUMxrFwKCE",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 1)",
      "chunk_id": 8,
      "start_timestamp": "0:05:29",
      "end_timestamp": "0:06:14"
    }
  },
  {
    "page_content": "Và tại sao chúng ta cần phải có nhiều biến? Thì trong thực tế là một số cái à kết quả hay còn gọi là output á thì thường là bị chi phối bởi nhiều cái yếu tố khác nhau. Đó. Ví dụ giá nhà thì không chỉ phụ thuộc vào diện tích mà nó còn phụ thuộc vào vị trí, vào số phòng, vào năm xây dựng vân vân. rồi lộ giới trước nhà. Nếu chúng ta chỉ dùng một biến để ô hình bỏ qua nhiều cái thông tin quan trọng dẫn tới là cái việc kém trong cái việc là dự đoán chính xác đó thì đó hay còn gọi là overfitting hoặc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=uyUMxrFwKCE",
      "filename": "uyUMxrFwKCE",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 1)",
      "chunk_id": 9,
      "start_timestamp": "0:06:08",
      "end_timestamp": "0:06:55"
    }
  },
  {
    "page_content": "xác đó thì đó hay còn gọi là overfitting hoặc là underfitting. Thì nếu như cái dữ liệu của mình nó không fit được thì đó là underfit. Và trong thực tế thì tường ta sẽ à có nhiều cái đặc trưng để mô tả đối tượng và việc sử dụng nhiều cái biến thì sẽ giúp cho chúng ta khai thác tối đa cái giá trị của dữ liệu. Đó thì người ta gọi là dữ liệu trong cái thời buổi ngày nay đó là dữ liệu là vàng. Thì nếu như chúng ta biết cách và khai thác được các cái đặc trưng dữ liệu đầu vào để mà có thể tìm ra cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=uyUMxrFwKCE",
      "filename": "uyUMxrFwKCE",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 1)",
      "chunk_id": 10,
      "start_timestamp": "0:06:45",
      "end_timestamp": "0:07:35"
    }
  },
  {
    "page_content": "đặc trưng dữ liệu đầu vào để mà có thể tìm ra cái mối liên kết liên đới đến cái à giá trị giá của căn nhà đồ ra thì đó chính là những cái công việc mà chúng ta cần phải làm đó là tìm cách để khai thác những cái biến giúp cho mô hình dự đoán chính xác hơn. Thì sau đây là một cái dữ liệu tổng quát cho cái tình huống đó là dự đoán giá căn nhà. Thì đây chính là cái output I của mình nè là cái mình cần dự đoán. Và toàn bộ cái diện tích số phòng số 5 à xây dựng tình trạng tổng thể thì nó chính là X",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=uyUMxrFwKCE",
      "filename": "uyUMxrFwKCE",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 1)",
      "chunk_id": 11,
      "start_timestamp": "0:07:26",
      "end_timestamp": "0:08:17"
    }
  },
  {
    "page_content": "à xây dựng tình trạng tổng thể thì nó chính là X Y. Còn ở đây là Y E Y. Thì xy của mình á nó có thể là 3 4 5 xin lỗi nó không có tính cái cột cuối cùng chúng ta bỏ đi. Như vậy thì nó sẽ thuộc R là R4. Thì bốn cái đặc trưng này sẽ rất là đại diện cho cái cho cái dữ liệu của mình và cho cái mô hình của mình. Rồi vậy thì cái mô hình toán học của hồi quy nhiều biến đó là gì? nó cũng hoàn toàn tương tự như là hồi quyến tính mà một biến. Thì đối với trường hợp mà chúng ta chỉ bị phụ thuộc bởi duy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=uyUMxrFwKCE",
      "filename": "uyUMxrFwKCE",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 1)",
      "chunk_id": 12,
      "start_timestamp": "0:08:12",
      "end_timestamp": "0:09:01"
    }
  },
  {
    "page_content": "trường hợp mà chúng ta chỉ bị phụ thuộc bởi duy nhất một biến đó thì chúng ta có thể à khai thác để mà phát triển thêm cái công thức của mình. Ví dụ như đối với trường hợp mà một biến thì là wx cộng b đó. Nhưng nếu chúng ta muốn nâng lên là dự biến thì chúng ta có thể rã cái này ra thành là W1 x1 cộng cho W2 X2 cộng chấm chấm chấm. Và cuối cùng thì chúng ta mới cộng cho cái bias. Thì đây chính là cái tiền đề để dẫn nhập cho việc áp dụng mô hình hồi quyến tính mà chỉ có một biến sang cái mô hình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=uyUMxrFwKCE",
      "filename": "uyUMxrFwKCE",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 1)",
      "chunk_id": 13,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "quyến tính mà chỉ có một biến sang cái mô hình đa biến.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=uyUMxrFwKCE",
      "filename": "uyUMxrFwKCE",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 1)",
      "chunk_id": 14,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Thì để các bạn có thể dễ dàng làm một quen và sử dụng Google Collab thì tiếp theo chúng ta sẽ nói về một số các cái tiện ích hoặc là cái tips and trick các mẹo vật cơ bản các bạn nên biết khi các bạn làm việc với cái công cụ này. Đầu tiên là các bạn phải biết cách quản lý các cái tệp tin và thương mục mà chúng ta đã tạo trong cái máy ảo mà Google Collab cung cấp cho chúng ta cũng như là phải biết làm việc một cách cơ bản với cái máy ảo này thì các bạn có thể hoàn toàn làm việc với cái máy ảo này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=v3ArDvxkL-U",
      "filename": "v3ArDvxkL-U",
      "title": "[CS114 - Tutorial] Google Colab (Phần 2)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:55"
    }
  },
  {
    "page_content": "bạn có thể hoàn toàn làm việc với cái máy ảo này bằng các cái lệnh hoặc là lệnh của Python ha. À các bạn có thể sử dụng các cái lệnh Python như lệnh file, upload, download hoặc là các bạn import thư viện OS trong Python để các bạn xử lý liên quan đến thư mục hoặc các bạn có thể sử dụng các lệnh của Linux hoặc không. À đơn giản hơn các bạn có thể đơn bấm vào cái biểu tượng file bên trong cái thành công cụ của Google Collab. các bạn sẽ đến một cái cây thư mục à đây là cây thư mục ừ của một máy ư",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=v3ArDvxkL-U",
      "filename": "v3ArDvxkL-U",
      "title": "[CS114 - Tutorial] Google Colab (Phần 2)",
      "chunk_id": 1,
      "start_timestamp": "0:00:50",
      "end_timestamp": "0:01:33"
    }
  },
  {
    "page_content": "cây thư mục à đây là cây thư mục ừ của một máy ư Linux, một máy a chủ ảo chạy hệ điều hành Linux thì các bạn có thể trực tiếp làm những thao tác cơ bản như là upload file lên đây hoặc là các bạn có thể phải chuột vào file và chọn download về máy của mình. Hoặc các bạn có thể bật một cái terminal là các bạn trực tiếp gõ các cái lệnh liên quan đến hệ điều hành Linux. Trong cái terminal này nó cũng mô phỏng gần giống với một terminal của hệ điều hành Linux thật ha. Khi các bạn đã học môn mach",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=v3ArDvxkL-U",
      "filename": "v3ArDvxkL-U",
      "title": "[CS114 - Tutorial] Google Colab (Phần 2)",
      "chunk_id": 2,
      "start_timestamp": "0:01:29",
      "end_timestamp": "0:02:08"
    }
  },
  {
    "page_content": "hành Linux thật ha. Khi các bạn đã học môn mach Machine learning thì có vẻ như sớm muộn à các bạn cũng sẽ dần tập làm quen với hệ điều hành Linux và đặc biệt là Ubuntu. Đây là cái hệ điều hành gần như là phổ biến nhất trong hệ sinh thái của các cái máy chủ hoặc là những cái máy có đủ cấu hình để chạy những cái chương trình à machine learning nghiêm túc thì hầu hết đều phải chạy trên các cái máy chủ thật hay ảo gì thì cũng không ít ai mà chạy trực tiếp những cái code machine learning trên máy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=v3ArDvxkL-U",
      "filename": "v3ArDvxkL-U",
      "title": "[CS114 - Tutorial] Google Colab (Phần 2)",
      "chunk_id": 3,
      "start_timestamp": "0:02:01",
      "end_timestamp": "0:02:44"
    }
  },
  {
    "page_content": "tiếp những cái code machine learning trên máy tính cá nhân của mình. Hầu hết người ta chỉ viết code phát triển và để kiểm thử code thôi. Còn chạy thật thì sẽ phải đưa lên máy chủ. Đó các bạn phải làm quen với những cái thao tác trên máy chủ như thế này. Và Google Collab cung cấp cho các bạn đầy đủ các loại giao diện. Giao diện Jupiter Notebook. Các bạn có thể gõ code bằng Python hoặc bằng các cái lệnh của U2. Ví dụ như lệnh xem tập tin thư mục kiểm tra đường dẫn của thư mục Thiện Hành à hoặc là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=v3ArDvxkL-U",
      "filename": "v3ArDvxkL-U",
      "title": "[CS114 - Tutorial] Google Colab (Phần 2)",
      "chunk_id": 4,
      "start_timestamp": "0:02:39",
      "end_timestamp": "0:03:21"
    }
  },
  {
    "page_content": "tra đường dẫn của thư mục Thiện Hành à hoặc là các bạn làm việc theo giao diện Terminal hoặc theo giao diện web có một cái trình quản lý file dạng cây thư mục mà Google cung cấp sẵn cho các bạn. Các bạn có thể dùng tool nào phù hợp với yêu cầu của mình. Tiếp theo các bạn phải biết cách tận dụng GPU trong Google Collab. TPU là cũng là một loại tài nguyên mà Google cung cấp cho các bạn sử dụng miễn phí. Tuy nhiên nó sẽ khó sử dụng hơn và trong đa số trường hợp thì à hầu hết sinh viên là mới làm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=v3ArDvxkL-U",
      "filename": "v3ArDvxkL-U",
      "title": "[CS114 - Tutorial] Google Colab (Phần 2)",
      "chunk_id": 5,
      "start_timestamp": "0:03:14",
      "end_timestamp": "0:04:05"
    }
  },
  {
    "page_content": "số trường hợp thì à hầu hết sinh viên là mới làm quen với môn này sẽ không biết sử dụng TPU. Đây là viết tắc của Tenser Processing Unit. Thì nó là một cái bộ vi xử lý do Google tự phát triển à tự thiết kế ha. Đây là một cái chip xử lý Google tự phát triển hoàn toàn để làm việc phục vụ cho việc xử lý tính toán hiệu năng cao. Nhưng mà do Google tự phát triển nên tài liệu về cái vi xử lý này khá là ít. Cách sử dụng thì cũng khó nên hầu hết trường hợp các bạn sẽ sử dụng GPU là nhiều. Thì các bạn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=v3ArDvxkL-U",
      "filename": "v3ArDvxkL-U",
      "title": "[CS114 - Tutorial] Google Colab (Phần 2)",
      "chunk_id": 6,
      "start_timestamp": "0:04:00",
      "end_timestamp": "0:04:42"
    }
  },
  {
    "page_content": "hợp các bạn sẽ sử dụng GPU là nhiều. Thì các bạn đầu tiên là phải biết cái run time của mình, cái máy ảo của mình đang chạy có hỗ trợ các cái tài nguyên này hay không. Ở đây là ví dụ một cái run time không có hỗ trợ các cái tài nguyên tính toán nâng cao này. Nó chỉ có hai core CPU đơn giản thôi. Thì các bạn sẽ thấy giao diện ở đây. Đó, chúng ta không có thông tin về các cái tài nguyên tính toán. Hoặc là các bạn có thể vào menu run time các bạn chọn manage session. Đó thì các bạn cũng có thể",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=v3ArDvxkL-U",
      "filename": "v3ArDvxkL-U",
      "title": "[CS114 - Tutorial] Google Colab (Phần 2)",
      "chunk_id": 7,
      "start_timestamp": "0:04:37",
      "end_timestamp": "0:05:21"
    }
  },
  {
    "page_content": "chọn manage session. Đó thì các bạn cũng có thể thấy loại section mình đang sử dụng à các danh sách các section mình đang sử dụng nó thuộc loại gì. Và nếu cần các bạn có thể trend run time tể mình đổi loại tài nguyên. Đó thì các cái tài nguyên này Google Collab sẽ update tùy theo từng thời kỳ ha. Đó à mặc định là sử dụng CPU. Đây là loại tài nguyên đơn giản nhất và các bạn có thể sử dụng cái máy này khoảng 65 giờ. Nếu các bạn yêu cầu cấp thêm à à không yêu cầu cấp lại một cái run time, một cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=v3ArDvxkL-U",
      "filename": "v3ArDvxkL-U",
      "title": "[CS114 - Tutorial] Google Colab (Phần 2)",
      "chunk_id": 8,
      "start_timestamp": "0:05:15",
      "end_timestamp": "0:05:58"
    }
  },
  {
    "page_content": "à không yêu cầu cấp lại một cái run time, một cái máy chủ ảo có sử dụng các tài nguyên tính toán nâng cao thì đầu tiên là bạn sẽ disconnect cái run time cũ. Tức là bạn phải tắt kết nối với run time cũ. à hầu hết là sẽ bị ngắt kết nối. Sau đó các bạn sẽ ngồi chờ connecting tức là ngồi chờ Google Collab cấp lại cả run time cho các bạn. Và khi bạn được cấp lại run time mới bạn có thể thấy ở đây à tôi đã được c một cái Google comp như backend là sử dụng TB Tensor processing unit. Rồi thì tenser",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=v3ArDvxkL-U",
      "filename": "v3ArDvxkL-U",
      "title": "[CS114 - Tutorial] Google Colab (Phần 2)",
      "chunk_id": 9,
      "start_timestamp": "0:05:50",
      "end_timestamp": "0:06:42"
    }
  },
  {
    "page_content": "sử dụng TB Tensor processing unit. Rồi thì tenser processing unit là cái tài nguyên tính toán do Google tự phát triển nên họ cấp cho các bạn dùng thử. Nó cũng khá là dư giả. Chúng ta có khoảng 300 GB RAM nha. 300 GB RAM nha các bạn. Đây không phải là đĩa cứng, đây là RAM. Đó. Do tenser processing unit nó có rất là nhiều core. À mỗi core có khoảng vài GB RAM có sẵn cho nó ha. Tổng lại thì các bạn có hàng trăm GB RAM như thế này nhưng mà nó sẽ khó sử dụng. Còn thường thì các bạn sẽ dùng à tài",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=v3ArDvxkL-U",
      "filename": "v3ArDvxkL-U",
      "title": "[CS114 - Tutorial] Google Colab (Phần 2)",
      "chunk_id": 10,
      "start_timestamp": "0:06:34",
      "end_timestamp": "0:07:22"
    }
  },
  {
    "page_content": "khó sử dụng. Còn thường thì các bạn sẽ dùng à tài nguyên GPU là nhiều. Đó. Đây là các cái tài nguyên xử lý a do Nvidia thiết kế. Và cuối cùng các bạn sẽ làm quen với việc viết các cái đoạn ghi chú ha. Thì các cái ghi chú trong Google Collab sẽ được viết bằng cái ngôn ngữ gọi là ngôn ngữ mark down. Đây là một cái ngôn ngữ à cho phép người ta sử dụng văn bản thuần túy nhưng có thể đính kèm những cái ký hiệu đặc biệt. Ví dụ như dấu hai dấu sau này là đánh dấu cái đoạn tác này cần phải được in đậm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=v3ArDvxkL-U",
      "filename": "v3ArDvxkL-U",
      "title": "[CS114 - Tutorial] Google Colab (Phần 2)",
      "chunk_id": 11,
      "start_timestamp": "0:07:16",
      "end_timestamp": "0:08:07"
    }
  },
  {
    "page_content": "là đánh dấu cái đoạn tác này cần phải được in đậm khi hiển thị lên trên giao diện. Đó, hai dấu sau là những đoạn text được in đậm. Hoặc nếu các bạn muốn in nghiêng thì chúng ta sẽ sử dụng một dấu sau đó là in nghiêng. Đó, các bạn có thể đính kem đường link hyperlink. Các bạn có thể đánh số thứ tự cho những cái đoạn văn này. Các bạn có thể đánh hoa thị cho chúng và thậm chí các bạn có thể gõ công thức toán. À công thức toán thì sẽ được gõ bằng ngôn ngữ latex. Ví dụ ngôn ngữ latex nếu th ở đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=v3ArDvxkL-U",
      "filename": "v3ArDvxkL-U",
      "title": "[CS114 - Tutorial] Google Colab (Phần 2)",
      "chunk_id": 12,
      "start_timestamp": "0:08:02",
      "end_timestamp": "0:08:47"
    }
  },
  {
    "page_content": "ngôn ngữ latex. Ví dụ ngôn ngữ latex nếu th ở đây chúng ta có thể gõ một à fraction, một phân số. phân số thì ta sẽ phải à gõ là theo ngôn ngữ là t chúng ta sẽ đưa vào trong cái cặp dấu đô la như thế này ha. Đó, đây là ký hiệu của công thức toán học theo ngôn ngữ latex. Thì ngôn ngữ latex có thể cho các bạn gõ những cái công thức à có thể rất là phức tạp. Ví dụ như phân số của chúng ta có thể có tử và mẫu là hai biểu thức chẳng hạn và biểu thức có thể chứa những thành phần phức tạp hơn. Ví dụ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=v3ArDvxkL-U",
      "filename": "v3ArDvxkL-U",
      "title": "[CS114 - Tutorial] Google Colab (Phần 2)",
      "chunk_id": 13,
      "start_timestamp": "0:08:43",
      "end_timestamp": "0:09:25"
    }
  },
  {
    "page_content": "có thể chứa những thành phần phức tạp hơn. Ví dụ như biểu thức này có thể chứa một lũy thừa à vân vân. À và các bạn có thể gõ gần như là tất cả những công thức toán trong các cái môn toán cơ bản mà các bạn đã học bằng cái ngôn ngữ LEX này trực tiếp vào trong phần mô tả. của cái notebook bên trong Google collab ha. Các bạn cũng có thể insert em chèn cười vân vân. Và đây đây là cái phần phần note của một cái notebook à nó cho phép các bạn định dạng lại cái note này rất là dễ nhìn, rất là bắt mắt",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=v3ArDvxkL-U",
      "filename": "v3ArDvxkL-U",
      "title": "[CS114 - Tutorial] Google Colab (Phần 2)",
      "chunk_id": 14,
      "start_timestamp": "0:09:20",
      "end_timestamp": "0:10:04"
    }
  },
  {
    "page_content": "lại cái note này rất là dễ nhìn, rất là bắt mắt không phải chỉ như mã nguồn ha. Nếu như mã nguồn mà các bạn muốn ghi chú thì các bạn chỉ có thể comment. Trong Python thì chúng ta dùng dấu thăng để comment và comment hoàn toàn là plan text là chữ trơn thôi. Đó, còn trong notebook các bạn có thể ghi chú comment bằng những đoạn văn bản REX như thế này. Ngoài ra Google Collab còn cung cấp cho các bạn những cái đoạn code mẫu ha. Các bạn nhấn vào biểu tượng code snipest. Ở đây các bạn sẽ được danh",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=v3ArDvxkL-U",
      "filename": "v3ArDvxkL-U",
      "title": "[CS114 - Tutorial] Google Colab (Phần 2)",
      "chunk_id": 15,
      "start_timestamp": "0:09:59",
      "end_timestamp": "0:10:49"
    }
  },
  {
    "page_content": "tượng code snipest. Ở đây các bạn sẽ được danh sách những cái đoạn code mẫu mà người ta thường hay sử dụng trong Google Collab à để các bạn có thể từ đó chỉnh sửa ra thành cái đoạn chương trình riêng của mình. Có rất là nhiều đoạn code mẫu ở đây người ta đặt tên. Ha, khi các bạn bấm vô một cái đoạn code mẫu như thế này thì code đó các bạn bấm dấu cộng nó sẽ được chen vào trong cái notebook của các bạn. Rồi à ví dụ tại một vị trí nào đó các bạn có thể bấm à bạn bấm lệnh insert ở đây đó để chen",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=v3ArDvxkL-U",
      "filename": "v3ArDvxkL-U",
      "title": "[CS114 - Tutorial] Google Colab (Phần 2)",
      "chunk_id": 16,
      "start_timestamp": "0:10:43",
      "end_timestamp": "0:11:29"
    }
  },
  {
    "page_content": "có thể bấm à bạn bấm lệnh insert ở đây đó để chen cái đoạn code mẫu này vào bên trong notebook của mình giờ các bạn có thể chạy thử ha không thì các bạn có thể bỏ cái sales đó đi ha mỗi đoạn như thế này dù là đoạn code hay đoạn tag như thế này thì Google collab notebook người ta gọi nó là một sales đó các bạn có thể thử những cái code mẫu ở đây có rất là nhiều thứ thú vị. Ví dụ như các bạn có thể đây dùng thử một đoạn code mẫu nó này. Trên máy tính của mình đây là đoạn code dùng để điều khiển",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=v3ArDvxkL-U",
      "filename": "v3ArDvxkL-U",
      "title": "[CS114 - Tutorial] Google Colab (Phần 2)",
      "chunk_id": 17,
      "start_timestamp": "0:11:24",
      "end_timestamp": "0:11:58"
    }
  },
  {
    "page_content": "tính của mình đây là đoạn code dùng để điều khiển camera à hay là webcam của chính cái thiết bị mà các bạn đang chạy. Đó các bạn có thể insert nó vào và chạy thử. À trong bài giảng số thì chúng ta không khó có thể demo tính năng này ha. Tại vì nó bị sẽ bị xung đột giữa camera quay bài giảng và camera à mà đoạn code này điều khiển. Các bạn có thể ho không cần phải cài đặt gì phức tạp. Các bạn mở một cái collab notebook bất kỳ các bạn search một cái đoạn code mẫu ha snippet và các bạn search",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=v3ArDvxkL-U",
      "filename": "v3ArDvxkL-U",
      "title": "[CS114 - Tutorial] Google Colab (Phần 2)",
      "chunk_id": 18,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "cái đoạn code mẫu ha snippet và các bạn search những cái cụm từ camera là các bạn có thể ra được đoạn code này và chạy thử. Thì trong quá trình học môn máy học các bạn sẽ được làm quen nhiều với Google Collab. Rồi nếu các bạn không có khả năng setup máy tính của mình thì cũng không sao, chúng ta có thể học và thực hành thông qua sự hỗ trợ của cái công cụ hiện giờ vẫn còn đang là miễn phí này.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=v3ArDvxkL-U",
      "filename": "v3ArDvxkL-U",
      "title": "[CS114 - Tutorial] Google Colab (Phần 2)",
      "chunk_id": 19,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Trong phần tiếp theo thì chúng ta sẽ cùng à tiến hành thực hành với mạng Neuron nhân tạo. thì có thể nói các cái kiến trúc mạng hiện đại à hiện nay à ví dụ như là deep learning thì đều dựa trên lý thuyết của mạng neuron nhân tạo. Tuy nhiên làm sao mạng neuron nhân tạo có thể giải quyết được các cái bài toán hiện đại và nó có thể tổng quát hóa để có thể phát triển thành những cái mạng nâng cao ví dụ như là mạng CNN hoặc là mạng RNN. Và gần đây hơn thì có cái kiến trúc mạng là Transformer thì mạng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:01:01"
    }
  },
  {
    "page_content": "thì có cái kiến trúc mạng là Transformer thì mạng Neuro Network nó đã tạo một cái tiền đề để có thể phát triển những cái kiến thức mạng deep learning, những cái mạng học sau về sau. Vậy thì bằng cách nào mà mạng Neuronetwork có thể giúp cho chúng ta giải quyết được các cái bài toán phức tạp hay cụ thể hơn đó là các cái bài toán phi tiếng tính? Thì trong nội dung của bài thực hành này, chúng ta sẽ đến với à những cái bước đi đầu tiên trong việc là cài đặt một cái mạng neuron nhân tạo. Và chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 1,
      "start_timestamp": "0:00:53",
      "end_timestamp": "0:01:33"
    }
  },
  {
    "page_content": "là cài đặt một cái mạng neuron nhân tạo. Và chúng ta sẽ lý giải tại sao mạng Nuron nhân tạo có thể giải quyết được các cái bài toán phức tạp. Thì nội dung của bài thực hành ngày hôm nay sẽ có những cái phần chính như sau. Đầu tiên đó là chúng ta sẽ cài đặt một cái kiến trúc mạng neuron nhân tạo đơn giản. Thì ở đây chỉ bao gồm là một lớp ẩn. Rồi sau đó thì chúng ta sẽ cùng trực quan hóa và tìm hiểu cái vai trò của lớp ẩn trong cái việc là giải quyết các cái bài toán phi tuyến tính. Tức là tại",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 2,
      "start_timestamp": "0:01:27",
      "end_timestamp": "0:02:06"
    }
  },
  {
    "page_content": "quyết các cái bài toán phi tuyến tính. Tức là tại sao chúng ta có thể dùng lớp ẩn thêm một cái lớp ẩn thì nó sẽ giải quyết được một cái bài toán phi tuyến tính. Trong khi đó nếu như chỉ có hai lớp input và lớp output thôi thì lại không có thể giải quyết được các cái bài toán phi tiếng tính. Và chúng ta sẽ tiến hành trực quan hóa cái mạng neuron nhân tạo này trong cái việc đó là à các cái mạng neuron ở các cái lớp ẩn, các cái neuron ở lớp ẩn nó đã giúp cho chúng ta phi tuyến hóa và giải quyết",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 3,
      "start_timestamp": "0:02:02",
      "end_timestamp": "0:02:41"
    }
  },
  {
    "page_content": "đã giúp cho chúng ta phi tuyến hóa và giải quyết các cái bài toán phức tạp như thế nào. Thế thì để ạ neuro nhân tạo thì chúng ta sẽ phải có một cái tập dữ liệu để thực nghiệm. Thì cụ thể ở đây đó là tập bao gồm hai cái vòng tròn. lồng nhau. Thì với cái tập dữ liệu là hai màu đỏ và màu xanh chúng ta thấy ở đây thì đây là một cái tập dữ liệu mà nó có tính chất phi tuyến tính. Thế thì cái tính chất phi tuyến tính á nó thể hiện ở chỗ nào? Cái tính chất phi tuyến tính nó thể hiện ở chỗ là không thể",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 4,
      "start_timestamp": "0:02:36",
      "end_timestamp": "0:03:21"
    }
  },
  {
    "page_content": "phi tuyến tính nó thể hiện ở chỗ là không thể chia hai cái tập ờ màu đỏ và màu xanh này bằng duy nhất một cái đường thẳng. Chúng ta thấy là với cái đường thẳng này thì nó chỉ có thể tách ra là màu đỏ và một vùng còn lại là vừa có đỏ và xanh. Đó. Hoặc đường này thì chúng ta thấy là nó chia ra làm hai phần. Tuy nhiên cả hai phần thì đều có các điểm màu đỏ và màu xanh. Như vậy thì rõ ràng không thể nào có thể chia tách ra được bằng một đường thẳng. Mà nếu muốn chia tách được thì chúng ta sẽ phải",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 5,
      "start_timestamp": "0:03:16",
      "end_timestamp": "0:03:52"
    }
  },
  {
    "page_content": "Mà nếu muốn chia tách được thì chúng ta sẽ phải có một cái đường cong như thế này thì mới có thể chia ra làm hai phần thôi. Như vậy thì trong cái tập dữ liệu mà có tính chất phi tuyến tính này và lưu ý là cái dữ liệu này nó cũng chỉ mới là một cái dữ liệu bước đầu thôi chứ nó chưa thực sự quá là phức tạp. Trong các cái bài toán phức tạp hơn thì cái tính chất phi tuyến tính và cái tính zízắc của nó nó còn nhiều hơn như thế này nữa. Đây chỉ là một cái tập dữ liệu đơn giản thôi. Nhưng với cái dữ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 6,
      "start_timestamp": "0:03:47",
      "end_timestamp": "0:04:25"
    }
  },
  {
    "page_content": "cái tập dữ liệu đơn giản thôi. Nhưng với cái dữ liệu đơn giản này thì nó chúng ta giúp cho chúng ta có thể lý giải được tại sao khi chúng ta thêm vô một cái lớp ẩn tức là một cái hidden layer thì nó lại có thể giúp cho chúng ta giải quyết được các cái bài toán phi tuyến tính này. Thì vai trò của mỗi cái notốe neuron ở đây đó là gì? Đó thì chúng ta sẽ cùng tìm hiểu ở trong cái bài thực hành này. Và trong bài thực hành này thì chúng ta sẽ thực nghiệm là có rất nhiều cái mạng à có rất nhiều cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 7,
      "start_timestamp": "0:04:18",
      "end_timestamp": "0:05:05"
    }
  },
  {
    "page_content": "là có rất nhiều cái mạng à có rất nhiều cái neuron. Thì cụ thể ở đây là cái lớp Hidden layer sẽ có 8 neuron. Vì cái à yếu tố là đơn giản hóa của hình ảnh thì ở đây chúng ta chỉ vẽ tượng trưng là bốn thôi chứ còn nếu để vẽ 8 neuron thì nó sẽ rất là dày đặc ha. Rồi thì chúng ta sẽ cùng qua cái bài lab. Chúng ta sẽ cùng qua sử dụng cog của lap để có thể cài đặt bài này. Rồi thì trên hình chúng ta thấy là có cái đồ vào của mình là bao gồm à cái lớp input của mình sẽ bao gồm giá trị. Thì hai cái giá",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 8,
      "start_timestamp": "0:04:59",
      "end_timestamp": "0:05:40"
    }
  },
  {
    "page_content": "của mình sẽ bao gồm giá trị. Thì hai cái giá trị này á nó tương ứng sẽ là hai cái điểm à hai cái tọa độ theo trục hoành và trục tung. Tức là cái tọa độ của các cái điểm trong không gian. Còn cái output này của mình á thì nó chỉ có một neuron thôi. Thì lý do đó là vì chúng ta chỉ phân loại ra làm hai tập dữ liệu đó là nằm cái vòng tròn nằm trong và vòng tròn nằm bên ngoài. Thì với cái bài toán mà phân loại nhị phân như thế này á thì chúng ta chỉ cần sử dụng một cái lớp neuron và cái hàm loss ở",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 9,
      "start_timestamp": "0:05:34",
      "end_timestamp": "0:06:35"
    }
  },
  {
    "page_content": "cần sử dụng một cái lớp neuron và cái hàm loss ở đây chúng ta sẽ sử dụng là binary cross centropy. hàm loss để phân tách để giúp phân tách ra làm hai. Rồi sau đây thì chúng ta sẽ tiến hành à thử nghiệm. Thì trong cái bài láp này thì chúng ta sẽ sử dụng cái thư viện Kas và có các cái lớp là input và dance. Trong đó dance là một cái lớp kết nối đầy đủ. các cái lớp đằng trước với lại cái lớp đằng sau chúng ta sẽ có đầy đủ các cái trọng số thì nó gọi là dance. Và đóng gói lại hết cả input và output",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 10,
      "start_timestamp": "0:06:25",
      "end_timestamp": "0:07:10"
    }
  },
  {
    "page_content": "là dance. Và đóng gói lại hết cả input và output thì chúng ta sẽ có đó là model. Rồi thì đây là cái lớp mà đã được tạo sẵn và chúng ta sẽ cùng tiến hành cài đặt ở những cái dòng code mà có cái hiện chữ todo. Thì cái class neuronetwork sẽ có một cái phương thức là build à để xây dựng cái mô hình của mình. Trong đó input dimension là cho biết cái số chiều của cái dữ liệu đầu vào. Thì cụ thể ở đây chúng ta sẽ có input dimension là bằng 2. Tại vì các cái điểm mà chúng ta sẽ sử dụng ở đây sẽ là các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 11,
      "start_timestamp": "0:07:06",
      "end_timestamp": "0:07:52"
    }
  },
  {
    "page_content": "cái điểm mà chúng ta sẽ sử dụng ở đây sẽ là các cái điểm trong không gian hai chiều. Output dim ở đây chính là cái số chiều của cái lớp output. Thì cụ thể ở đây chúng ta sẽ là 1 tại vì như đã đề cập đó là chúng ta là phân lớp nhị phân nên ở đây chỉ cần là 1 neuron thôi. Rồi thì à cái lớp input này ờ ở đây thì cái geni nó đã giúp cho chúng ta ờ khởi tạo đúng không? Khởi tạo cái đoạn code. Thì ở đây chúng ta sẽ xem ha. Để cho đơn g giảng thì chúng ta chỉ cần để là input thôi. Đặt tên biến là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 12,
      "start_timestamp": "0:07:45",
      "end_timestamp": "0:08:32"
    }
  },
  {
    "page_content": "ta chỉ cần để là input thôi. Đặt tên biến là input và nó sẽ gọi cái đối tượng đó là input và truyền vào cái shap là bằng là input dim. Sau đó thì chúng ta sẽ gọi cái lớp dense để tạo ra cái lớp hidden đó. Thì hidden sẽ là bằng dance. Rồi thì chúng ta sẽ có cái unit tức là cái số neuron đầu ra của cái lớp hiden này. Thì như đã nói tức là chúng ta có 8 neuron đầu ra. Chúng ta có 8 neuron đầu ra nên ở đây output sẽ là 8. Tiếp theo đó là activation. Thì activation ở đây chúng ta có thể dùng hàm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 13,
      "start_timestamp": "0:08:28",
      "end_timestamp": "0:09:21"
    }
  },
  {
    "page_content": "Thì activation ở đây chúng ta có thể dùng hàm relue hoặc là sigmid. Nhưng mà để đơn giản thì chúng ta sẽ sử dụng cái phiên bản đời đầu của Nuro Network đó chính là à Sigmile. Rồi tiếp theo đó là chúng ta có sử dụng bias hay không. Thì ở đây chúng ta sẽ có một cái thuộc tính đó là use by. thì mặc định use bằng true nhưng mà tuy nhiên để cho từ minh thì chúng ta sẽ cài đặt luôn. Use bias là bằng true. Rồi thì ở đây chúng ta mới chỉ khởi tạo cái đối tượng là một cái lớp ẩn. Chúng ta sẽ phải cho",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 14,
      "start_timestamp": "0:09:14",
      "end_timestamp": "0:10:08"
    }
  },
  {
    "page_content": "đối tượng là một cái lớp ẩn. Chúng ta sẽ phải cho biết là nó nhận đầu vào. Nó nhận đầu vào là cái cái biến nào thì cái đầu vào của mình nó sẽ là input. Do đó ở đây chúng ta về mặt khú pháp chúng ta sẽ để thêm input đầu vào. Nó sẽ nhận đầu vào là lớp input. Sau đó nó sẽ tra trả ra là cái hidden. Rồi tiếp theo là từ cái hidden này chúng ta tiếp tục có một cái lớp kết nối đầy đủ để tạo ra cái lớp output thì sẽ là bằng dense và 1. Tương tự như vậy, activation thì ở đây chúng ta cũng dùng hàm là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 15,
      "start_timestamp": "0:10:02",
      "end_timestamp": "0:10:50"
    }
  },
  {
    "page_content": "activation thì ở đây chúng ta cũng dùng hàm là sigmo và use by bằng true. Và cái đầu vào cho cái lớp output để mà tạo ra được cái lớp output này thì nó phải có cái đầu vào và đầu vào của mình chính là cái lớp input đằng trước đó chính là lớp hidden. Rồi đó chúng ta sẽ để đây là hidden. Rồi sau đó thì chúng ta sẽ đóng gói à chúng ta sẽ đóng gói cái model của mình lại bằng cái input và cái output này. Rồi sau đó chúng ta sẽ trả ra cho cái s.model. Rồi như vậy là chúng ta đã tạo xong, đã cài đặt",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 16,
      "start_timestamp": "0:10:44",
      "end_timestamp": "0:11:28"
    }
  },
  {
    "page_content": "Rồi như vậy là chúng ta đã tạo xong, đã cài đặt xong cái kiến trúc của mô hình của mình. Đó thì cái model của mình nó sẽ được đóng gói vào cái thuộc tính đó là sale.model. Và khi chúng ta gọi cái hàm trend thì nó sẽ sử dụng cái optimizer đó là stocastic gradient design với cái tham số learning ray là 0.1 và momentum là 0.9. Thì đây là cái cấu hình mặc định. Rồi hàm loss ở đây chúng ta sẽ sử dụng là binary cross central tại vì ở đây chúng ta đang phân loại nhị phân. Rồi và dữ liệu trend chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 17,
      "start_timestamp": "0:11:22",
      "end_timestamp": "0:12:08"
    }
  },
  {
    "page_content": "phân loại nhị phân. Rồi và dữ liệu trend chúng ta sẽ truyền dữ liệu trend với bass size là bằng 64 và số ipox và chúng ta huấn luyện là 500. Thì đây là các cái tham số mặc định. Sau đó thì chúng ta sẽ tiến hành thực thi cái đoạn code này. Rồi à tiếp theo thì chúng ta sẽ à load cái dữ liệu thực nghiệm. Thì cái dữ liệu thực nghiệm ở đây chúng ta sẽ dùng một cái à tập nó gọi là circle. Thì để mà có thể tạo ra được cái tập circle này thì chúng ta sẽ dùng cái phương thức là make circle của module",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 18,
      "start_timestamp": "0:12:04",
      "end_timestamp": "0:12:44"
    }
  },
  {
    "page_content": "sẽ dùng cái phương thức là make circle của module dataset của thư viện size kitl. Rồi sau đó chúng ta sẽ có được cái dữ liệu xtrend và rồi sau đó thì chúng ta sẽ gán nhãn cho nó. Đối với những cái điểm màu đỏ thì trend của mình là bằng nếu mà trend của mình là bằng 0 và x trend của mình à màu xanh sẽ là trend của mình là bằng 1. Đó thì nếu như cái trend nào mà có nhãn bằng 0 thì đó chính là những cái điểm màu đỏ bên ngoài. Còn nếu trend của mình mà bằng 1 thì đó là những cái điểm màu xanh bên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 19,
      "start_timestamp": "0:12:38",
      "end_timestamp": "0:13:16"
    }
  },
  {
    "page_content": "mà bằng 1 thì đó là những cái điểm màu xanh bên trong. Và chúng ta gọi cái hàm map plot lip để truyền lần lượt là cái trục x và trục Y. trục hoành và trục tung. Đối với những cái điểm màu đỏ thì chúng ta sẽ dùng cái marker đó là R0. R là red và O chính là cái O chính là cái biểu tượng hình tròn. Đối với những điểm màu xanh thì chúng ta sẽ dùng là G mũ. G là rin là màu xanh lá và mũ là cái ký hiệu cho hình tam giác. Thế thì chúng ta sẽ chạy cái đoạn code này và sẽ thấy đó là cái tập dữ liệu của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 20,
      "start_timestamp": "0:13:11",
      "end_timestamp": "0:14:01"
    }
  },
  {
    "page_content": "code này và sẽ thấy đó là cái tập dữ liệu của mình à tập dữ liệu của mình thì nó sẽ bao gồm hai cái điểm nằm trong và nằm ngoài vòng tròn. Thì với đây chính là cái tập dữ liệu mà đã được đề cập ở trong cái slide của mình. Rồi tiếp theo thì chúng ta sẽ khởi tạo và xây dựng mô hình. Thì đối tượng mà chúng ta sẽ khởi tạo đó là một cái class là một cái đối tượng thuộc cái class là neuro network đã cài đặt ở trên. là neuronetwork model là khởi tạo. Sau đó chúng ta sẽ gọi hàm build với cái input đầu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 21,
      "start_timestamp": "0:13:58",
      "end_timestamp": "0:15:07"
    }
  },
  {
    "page_content": "đó chúng ta sẽ gọi hàm build với cái input đầu vào là 2. Thì tại sao ở đây lại là 2? là vì trục hoành và trục tung tương ứng là cái tọa độ của các cái điểm trong cái không gian của mình ở đây và output của mình là bằng 1 thì vì ở đây chúng ta chỉ là phân loại nhị phân nên output sẽ là bằng 1. Rồi sau đó thì chúng ta sẽ tiến hành đó là huấn luyện và thực thi. Thì ở đây là cái đoạn cái này chúng ta chưa có dịch ha. Sau đó là sẽ huấn luyện. và vẽ cái biểu đồ. Thì cái bài tutorial này là mình đã",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 22,
      "start_timestamp": "0:15:02",
      "end_timestamp": "0:15:40"
    }
  },
  {
    "page_content": "cái biểu đồ. Thì cái bài tutorial này là mình đã soạn trước đây nhưng mà bằng phiên bản tiếng Anh. Rồi thì chúng ta thấy là trong cái quá trình huấn luyện á thì cái L của mình á càng về sau là càng giảm. Chúng ta thấy là từ 0.65 6 5 4 rồi 3. Rồi đó ở những cái vòng đầu tiên đó là nó bằng là 0.4 mấy đó. Ở những cái đầu tiên đó là khoảng 0.69. Sau một hồi thì nó giảm xuống. đó và chỉ còn là khoảng à 0.003. Rồi thì để đơn giản để cho nó không có bị chiếm nhiều cái không gian thì chúng ta sẽ xóa",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 23,
      "start_timestamp": "0:15:36",
      "end_timestamp": "0:16:25"
    }
  },
  {
    "page_content": "bị chiếm nhiều cái không gian thì chúng ta sẽ xóa này đi và chúng ta sẽ vẽ lại ha. Chúng ta sẽ vẽ lại cái history này. Đó thì ban đầu chúng ta sẽ thấy là cái loss của mình nó sẽ rớt nó sẽ gần như là đi ngang. Tuy nhiên đến một cái ngưỡng nào đó thì mô hình của mình nó bắt đầu nó tìm ra được cái trọng số đúng thì nó giảm xuống rất là nhanh. Nó giảm xuống rất là nhanh và đến một cái mức độ nào đó chúng ta thấy là nó gần như là đi ngang bảo hòa là không có huấn luyện thêm được nữa. Thì đây chính",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 24,
      "start_timestamp": "0:16:17",
      "end_timestamp": "0:17:15"
    }
  },
  {
    "page_content": "không có huấn luyện thêm được nữa. Thì đây chính là cái biểu đồ của à cái giá trị loss theo ipox. thì đâu đó đến khoảng số 200 trở đi thì nó mới bắt đầu là giảm xuống một cách gọi là rất là mạnh. Thì sau khi huấn luyện xong thì chúng ta sẽ tiến hành à sử dụng cái mô hình dự đoán để mà dự đoán trên lưới các cái điểm dữ liệu của mình. Sau đó chúng ta sẽ vẽ các cái bộ phân loại yếu. Thế thì à để mà chúng ta có thể trực quan hóa được thì chúng ta sẽ phải xem là cái trọng số của à cái mô hình của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 25,
      "start_timestamp": "0:17:11",
      "end_timestamp": "0:18:22"
    }
  },
  {
    "page_content": "sẽ phải xem là cái trọng số của à cái mô hình của mình như thế nào. Đó. Thế thì cái mô hình của mình là nó sẽ nằm ở trong neuro network model và nó sẽ nằm trong một cái thuộc tính đó là model. Thế thì model này á nó sẽ có rất nhiều layer thì chúng ta sẽ bỏ cái layer đầu tiên. Ở đây chúng thấy ta thấy là có layer số 1 nè là input nè, rồi layer hidden nè và layer output. Thì cái layer của mình từ trái sang phải cũng sẽ được đánh số. Layer input sẽ là số 0 và layer ion của mình sẽ là số 1. Mà",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 26,
      "start_timestamp": "0:18:14",
      "end_timestamp": "0:19:52"
    }
  },
  {
    "page_content": "sẽ là số 0 và layer ion của mình sẽ là số 1. Mà chúng ta muốn lấy cái trọng số của cái ờ player số 1. Do đó chúng ta cái trọng số lớ của lớp ẩn nên chúng ta sẽ để là layer 1. Rồi sau đó là getway. Thế thì chúng ta quan sát cái ờ các cái giá trị trọng số. Chúng ta sẽ quan sát các cái giá trị trọng số ở đây ha. Rồi thì à ở đây nó sẽ có hai phần. Chúng ta thấy là cái phần đầu tiên là một cái cấu trúc array. Và thứ hai cũng là một cấu trúc array. Trong đó vì chúng ta có sử dụng bias nên cái thành",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 27,
      "start_timestamp": "0:19:47",
      "end_timestamp": "0:20:32"
    }
  },
  {
    "page_content": "đó vì chúng ta có sử dụng bias nên cái thành phần thứ hai này sẽ là cái bias của mình. Đó. Rồi bây giờ chúng ta sẽ có ờ thành phần đầu tiên á thì bao gồm là một cái ma trận. R chúng ta sẽ tách nó ra ha. Chúng ta sẽ tách nó ra là quay. Rồi thành phần đầu tiên nó sẽ là cái trọng số của à tương ứng với lại từng cái neuron đầu vào số 1 và số 2 thì chúng ta sẽ thấy là đây là trọng số của hai cái input của mình và cái trọng số. Ok. Thì đây chúng ta sẽ in ra là wave 0. Đó. Rồi sau đó sẽ là cái trọng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 28,
      "start_timestamp": "0:20:27",
      "end_timestamp": "0:21:09"
    }
  },
  {
    "page_content": "in ra là wave 0. Đó. Rồi sau đó sẽ là cái trọng số wave 1 tức là tương ứng là cái bias. Thì chúng ta sẽ in ra trọng số của các cái bias rồi. [Vỗ tay] Rồi và nếu cần thì chúng ta sẽ in ra cái set của nó. Rồi thì ở cái trọng số của cái hidden á thì nó sẽ có kích thước là 2 x 8. Lý do đó là vì ở đây chúng ta kết nối đầy đủ, đầu vào của chúng ta là có 2 và đầu ra của chúng ta là có 8 narol nên nó là một cái ma trận kích thước là 2 x 8. Còn bias của chúng ta thì vì chúng ta có 8 naron nên nó sẽ có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 29,
      "start_timestamp": "0:21:05",
      "end_timestamp": "0:22:09"
    }
  },
  {
    "page_content": "chúng ta thì vì chúng ta có 8 naron nên nó sẽ có tám cái bias. Nó sẽ có tám cái bias. Rồi như vậy thì chúng ta sẽ quan sát xem là ý nghĩa của các cái neuron này đó là gì. Thì mỗi một cái neuron như trong cái bài linear à bài logistic chúng ta đã biết mỗi một cái nro này nó tương ứng sẽ là một cái mạng là một cái logistication nó là một cái đường phân lớp nhị phân tức là nó là một cái đường thẳng à nó là một cái đường thẳng như vậy thì chúng ta sẽ tìm cách là trực quan hóa từng cái bộ phân lớp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 30,
      "start_timestamp": "0:22:02",
      "end_timestamp": "0:22:52"
    }
  },
  {
    "page_content": "sẽ tìm cách là trực quan hóa từng cái bộ phân lớp của từng cái neuro này. Mỗi neuro này tương ứng là một cái đường thẳng để mà phân loại nhị phân. Vậy thì ở đây có bao nhiêu cái neuron thì chúng ta sẽ vẽ bấy nhiêu đường thẳng. Tuy nhiên chúng ta sẽ xem xét xem là cái trọng số của từng neuron để mà nó tạo ra cái output này giá trị của nó sẽ là bao nhiêu. Thì chúng ta sẽ cùng quan sát xem là cái trọng số đầu ra, trọng số đầu ra của từng của tám cái neuro này nó ảnh hưởng đến cái output này như",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 31,
      "start_timestamp": "0:22:46",
      "end_timestamp": "0:23:42"
    }
  },
  {
    "page_content": "cái neuro này nó ảnh hưởng đến cái output này như thế nào. Thì với những cái neuron nào mà có cái trọng số cao thì chúng ta sẽ ưu tiên vẽ trước. Còn những cái trọng số nào mà có thấp thì chúng ta sẽ vẽ sau. Thế thì ở đây chúng ta sẽ thêm một cái đoạn code nữa để lấy cái trọng số của cái lớp thứ hai à layer số 2. Rồi đây sẽ là cái hidden away tức là cái trọng số của các cái neuron hidden. Rồi. Rồi thì khi chúng ta quan sát vào cái trọng số của cái lớp hidden thì chúng ta sẽ thấy như thế này. Ở",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 32,
      "start_timestamp": "0:23:36",
      "end_timestamp": "0:24:45"
    }
  },
  {
    "page_content": "lớp hidden thì chúng ta sẽ thấy như thế này. Ở đây sẽ là một cái ma trận 8 x 1. Nó sẽ là một cái ma trận 8 x 1 tại vì đầu vào của mình là 8 và đầu ra của mình là 1. Thế thì chúng ta sẽ quan sát xem cái giá trị của các cái cạnh này nè, giá trị của các cái trọng số này nè lần lượt sẽ là bao nhiêu? Thì chúng ta thấy là sẽ có những cái trọng số rất là cao. Ví dụ như là 9 4 - 3 6 đó. Nhưng cũng sẽ có những cái trọng số rất là thấp. Ví dụ như là à -1 hoặc là -2 thì đó là những cái trọng số thấp. Còn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 33,
      "start_timestamp": "0:24:39",
      "end_timestamp": "0:25:33"
    }
  },
  {
    "page_content": "hoặc là -2 thì đó là những cái trọng số thấp. Còn trọng số cao như là 9 - 8 - 8 4 - 6 đó thì đó là trọng số cao. Thế thì bây giờ chúng ta sẽ cùng trực quan hóa các cái ờ neuron mà có trọng số cao trước và các cái neuron có trọng số thấp chúng ta sẽ trực quan hóa sau. Thì ở đây chúng ta sẽ có những neuron có trọng số cao. Ví dụ như là neuron số 0 nè, 1 nè, 3 nè, 0 5 à 7. Thì ở đây chúng ta sẽ là duyệt qua 0 1 3 5 7. Còn những cái trọng số còn lại là trọng số thấp thì chúng ta sẽ vẽ sau ha. Thế",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 34,
      "start_timestamp": "0:25:26",
      "end_timestamp": "0:26:23"
    }
  },
  {
    "page_content": "là trọng số thấp thì chúng ta sẽ vẽ sau ha. Thế thì bây giờ chúng ta sẽ vẽ cái hyperpl tức là cái đường phân loại như thế nào? Thì ở đây chúng ta đã được khởi tạo trước là các cái trọng số của cái lớp hidden đó. Theta chính là cái trọng số của hidden. Trong bias thì là theta0 theta 1 và trọng số theta 0 sẽ là nằm trong cái biến param. thì chúng ta sẽ sử dụng hai cái biến này để mà chúng ta à trực quan hóa. Thế thì chúng ta sẽ quay lại cái slide ở đây. Rồi thì trong cái slide này, mỗi một cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 35,
      "start_timestamp": "0:26:17",
      "end_timestamp": "0:27:13"
    }
  },
  {
    "page_content": "ở đây. Rồi thì trong cái slide này, mỗi một cái neuron này thì tương ứng sẽ là một cái một cái phân lớp, một cái đường thẳng. Thì ví dụ như cái neuro này nó sẽ chứa cái thông tin của cái đường thẳng này. Thì ở đây chúng ta thấy là Nuro này nó sẽ có ba thành phần. Thành phần đầu tiên đó chính là bias. Và hai cái trọng số này tương ứng thì nó sẽ nằm ở trong cái biến nó gọi là cái biến param. Hai cái cạnh này nè sẽ là nằm trong cái biến param. Còn cái giá trị của cạnh này thì nó sẽ nằm trong cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 36,
      "start_timestamp": "0:27:09",
      "end_timestamp": "0:28:30"
    }
  },
  {
    "page_content": "cái giá trị của cạnh này thì nó sẽ nằm trong cái biến là bias. Vậy thì bây giờ chúng ta làm sao có thể trực quan hóa được cái đường thẳng này dựa trên các cái thông tin của ba cái cạnh này. Thế thì chúng ta biết rằng là hồi xưa khi mà chúng ta nói về phương trình đường thẳng thì chúng ta sẽ có cái công thức đó là à ax cộng cho by cộng cho c bằng 0 đúng không? Thì đây là một cái phương trình đường thẳng tổng quát. Thì trong cái ngữ cảnh này, trong cái ngữ cảnh này thì cái bias của chúng ta chính",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 37,
      "start_timestamp": "0:28:25",
      "end_timestamp": "0:29:15"
    }
  },
  {
    "page_content": "cái ngữ cảnh này thì cái bias của chúng ta chính là C. Bias của chúng ta chính là C. Và cái trọng số A và B chính là hai cái trọng số của cái param này. Thì ở giả sử chúng ta ký hiệu đây là param 1 và param 2 ha. Thì cái công thức của mình nó sẽ là param param 1 nhân với x. cộng cho param 2 nhân với y rồi cộng cho bias bằng 0. Vậy thì chúng ta muốn xác định được cái đường thẳng này thì chúng ta sẽ phải lấy cái tọa độ của hai điểm ít nhất tọa độ của hai điểm. Rồi thì đối với cái điểm bên trái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 38,
      "start_timestamp": "0:29:09",
      "end_timestamp": "0:29:39"
    }
  },
  {
    "page_content": "của hai điểm. Rồi thì đối với cái điểm bên trái thì giả sử như x ở đây chúng ta lấy là -1. À ví dụ chúng ta lấy cái điểm này thì x của mình là -1. Mình thế vào x là bằng -1 thì hỏi cái y của mình sẽ là bao nhiêu? thì chúng ta sẽ có được cái tọa độ ở đây là -1 và chấm hỏi. Tương tự như vậy à với cái điểm ở trên đây ở trên cùng đây ha. Ví dụ như là 1 đi. Rồi thì cái y của mình nó sẽ là bao nhiêu? để chúng ta có được một cái cặp điểm, từ đó chúng ta có thể vẽ được cái đường thẳng này. Thì bây giờ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 39,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "có thể vẽ được cái đường thẳng này. Thì bây giờ từ cái phương trình này chúng ta sẽ suy ra là y là bằng cái gì? y sẽ là bằng à trừ param 1 à chia cho param 2 nhân với x rồi sau đó là trừ cho chia cho par đó. Rồi thì ở đây chúng ta sẽ viết lại ha cho nó rõ. Thì từ cái công thức ở trên, từ công thức ở trên thì chúng ta sẽ có là y là bằng trừ param 1 nhân x trừ cho bias tất cả chia cho param 2 thì param 1 tức là cái trọng số này và param 2 chính là này thì đây tương ứng giống như là à cái công",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 40,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "là này thì đây tương ứng giống như là à cái công công thức mà hồi xưa chúng ta đã học. Từ cái công thức hồi xưa chúng ta đã học, chúng ta đưa về cái giá trị mà chúng ta lấy từ mô hình ra. Trong đó, bias chính là cái giá trị này và param là hai cái giá trị mà tương ứng hai cái đầu bào ở đây. Param 2. Rồi thì với cái công thức này chúng ta sẽ thế vào à thế vào x là bằng -1. Thế vào x là bằng trừ à x = 1 thì tương ứng y của mình sẽ là bằng bao nhiêu? Đó, dùng công thức này thế vào ở đây y thì sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 41,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Đó, dùng công thức này thế vào ở đây y thì sẽ bằng bao nhiêu? Chúng ta sẽ dựa trên cái công thức này. Như vậy thì đây là cái cách thức để mà chúng ta à có thể vẽ được cái phương trình đường thẳng của mình.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vqDSl2cle2o",
      "filename": "vqDSl2cle2o",
      "title": "[CS114 - Tutorial] Neural Network (Phần 1)",
      "chunk_id": 42,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chào các bạn! Hôm nay chúng ta sẽ cùng nhau tìm hiểu thực toán, gây quyết định hay còn được gọi là decision tree. Đây là một thực toán thuộc nhóm học có giám sát Đòi hỏi chúng ta phải chuẩn bị dữ liệu đồ vào có nhãn Để thực toán có thể hoạt động được Khác với các thực toán trước đó, chẳng hạn như Low Viscid Creation Đòi hỏi chúng ta phải tự thiết kế đặt trân một cách cẩn thận Để giúp thực toán có thể học được các tình huống dữ liệu phê tuyến Và chẳng hạn như thực toán Support Beta Machine chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:00:59"
    }
  },
  {
    "page_content": "hạn như thực toán Support Beta Machine chúng ta sẽ phải tự tìm ra các kernel để biến đổi không gian đặc trưng ban đầu về không gian đặc trưng có thể tách ra tuyến tính được để giúp thực toán phân lộ tốt hơn. Ngoài ra, chúng ta có thực toán Neural Network. Trong thực toán đó, chúng ta sẽ phải tự chọn số layer và số node trong 1 layer để giúp thực toán có thể tìm ra các hàm biến đấu đặc trưng một cách tự động. với mục tiêu cuối cùng là cải thiện hiệu xuất phăng loại hoặc là hồi quy. Tuy nhiên,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 1,
      "start_timestamp": "0:00:50",
      "end_timestamp": "0:01:32"
    }
  },
  {
    "page_content": "hiệu xuất phăng loại hoặc là hồi quy. Tuy nhiên, đối với thực toán cái quyết định, ý tưởng chính của nó lại là đi tìm ra các vùng thuộc không gian đặt trên ban đầu. Sau cho, toàn bộ điểm thuộc vùng đó sẽ thuộc về một lớp hoặc là thuộc về một giá trị hồi quy cụ thể. Đây là một điểm khá thú vị so với các thực toán học máy trước đó. Đây cũng là nền tảng cho các thực toán cây nâng cao về sau, trả hàng như XCBoot, LiveGBM hoặc là Catbook. Các thực toán này là những thực toán nổi tiếng được các đội",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 2,
      "start_timestamp": "0:01:22",
      "end_timestamp": "0:02:13"
    }
  },
  {
    "page_content": "này là những thực toán nổi tiếng được các đội thi trên nền tảng Kygo sử dụng trong các cuộc thi học máy thuộc dữ liệu dạng bản trong những năm gần đây. Trong phần này, chúng ta sẽ cùng nhau tìm hiểu các phương pháp dự trang cây, hay nó rõ hơn là các phương pháp học dự trang cây. Các khái niệm quan trọng được địa cập trong phần này bao gồm phân tầng hoặc phân đoạn, không nhanh đặt chân và bùng. Các khái niệm này sẽ giúp chúng ta hiểu rõ hơn về thực toán cây quyết định và các thực toán học dự",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 3,
      "start_timestamp": "0:02:09",
      "end_timestamp": "0:02:58"
    }
  },
  {
    "page_content": "thực toán cây quyết định và các thực toán học dự trang cây về sau trả hạn như là XGBoot, LiveGBM và CardBoost. Về không gian đặc trưng, chúng ta sẽ có một cái ví dụ thứ nhất cho bài toán phân loại. Giả sử chúng ta sẽ có một cái không gian đặt trân gồm hai là đặt trân là x1 và x2 Và một số điện di liệu mẫu chẳng hạn như là OO, x đi hả? O được ký hiệu là một cái điện di liệu thuộc về lớp O và x được ký hiệu cho một cái địa nhiệm liệu thuộc về lâm x Chúng ta sẽ có một cái ví dụ về không gian đặc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 4,
      "start_timestamp": "0:02:51",
      "end_timestamp": "0:04:05"
    }
  },
  {
    "page_content": "x Chúng ta sẽ có một cái ví dụ về không gian đặc trưng ấn vào bài toán hồi huy Chúng ta sẽ chỉ có một loại đặc trưng là x thôi và giá trị cần hồi huy là y và phân bố dữ liệu sẽ được thể hiện như sau Rồi, bây giờ chúng ta sẽ tìm hiểu khái niệm phân tầng hoặc phân đoạn Về các phương pháp học dựa tranh cây thì chúng ta sẽ quy ước Nếu chúng ta có thể phân tần hoặc phân đoạn thì chúng ta sẽ phân dựa trên từng đặc trưng duy nhất thôi Có nghĩa là, mọi lúc chúng ta phân tần thì chúng ta sẽ chỉ phân một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 5,
      "start_timestamp": "0:03:59",
      "end_timestamp": "0:04:56"
    }
  },
  {
    "page_content": "chúng ta phân tần thì chúng ta sẽ chỉ phân một đặc trưng thôi Ví dụ, chúng ta có thể phân tần hoặc là phân đoạn cái không gian đặc trưng bắt đầu thành hai cái vùng Yết ngột lớn hơn 0.5 và yết ngột nhỏ hơn 0.5 Tương tự chúng ta cũng có quyền kể tiếp một cái đường thẳng hân đoạn dựa tranh đặt trong x2 giả sự chỗ này giá trị lớn hơn 0.1 và nhỏ hơn 0.1 Ở bài toán hồi huy thì giả sử chúng ta sẽ kẽ một cái đường phân đoạn như sau chỗ này sẽ có hai vùng là x lớn hơn 0.5 và x nhỏ hơn 0.5 trả hàng như",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 6,
      "start_timestamp": "0:04:45",
      "end_timestamp": "0:05:57"
    }
  },
  {
    "page_content": "là x lớn hơn 0.5 và x nhỏ hơn 0.5 trả hàng như vậy sau khi chúng ta đã phân tần hoặc phân đoạn không gian đặt trên băng đầu thành các vùng Bây giờ chúng ta sẽ đưa ra dự đoán cho một mẫu thuộc về một cái vùng Bằng cách nào hả? Giả sử ấn với bài toán phân loại Bây giờ giả sử chúng ta sẽ có một cái mẫu mới là dấu chấm ở đây Thì cái dấu chấm này sẽ được phân loại bằng cách là chúng ta sẽ xác định Lớp mà xuất hiện nhiều nhất trong đuổi trình này Trong cái đuổi trình này, trong cái vùng này chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 7,
      "start_timestamp": "0:05:50",
      "end_timestamp": "0:06:39"
    }
  },
  {
    "page_content": "cái đuổi trình này, trong cái vùng này chúng ta sẽ có lớp x xuất hiện nhiều nhất Và tương tự như vậy, trong cái đuổi trình này thì chúng ta sẽ có lớp o là xuất hiện nhiều nhất Đó chính là cách phân loại của bài toán phân loại khi sử dụng cái quyết định. Tuy nhiên, đối với bài toán hồi quy thì chúng ta không thể làm như vậy được. Thông thường, người ta sẽ sử dụng giá trị trung bình. Có nghĩa là, giả sử có một điện dữ liệu là dấu chấm thuộc cái đôi trình x lớn hơn 0.5 thì giá trị hồi quy cho điểm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 8,
      "start_timestamp": "0:06:35",
      "end_timestamp": "0:07:35"
    }
  },
  {
    "page_content": "trình x lớn hơn 0.5 thì giá trị hồi quy cho điểm x mới này sẽ là giá trị trung bình của tất cả các điểm trong vùng này và tương tự như vậy, cái vùng nhỏ hơn 0.5 thì chúng ta sẽ có một trái trị hồi quý khác nó bằng giá trị trung bình của tất cả các điểm trong đây như vậy chúng ta vừa tìm hiểu các khái niệm quan trọng của các phương tác học mấy dựa trên cây Tuy nhiên, khi chúng ta biểu diễn các vùng này trong máy tính thì chúng ta sẽ sử dụng một cách biểu diễn khác Đó là chúng ta sẽ sử dụng cây Ở",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 9,
      "start_timestamp": "0:07:24",
      "end_timestamp": "0:08:37"
    }
  },
  {
    "page_content": "biểu diễn khác Đó là chúng ta sẽ sử dụng cây Ở đây, giả sử chúng ta sẽ minh hoạt lại các vùng trong bài toán thăng loại bằng cây như sau Lúc nãy, chúng ta đã phân tần dự trang giá trị x1 trước bây giờ chúng ta sẽ biểu diễn lại như sau nhánh phải là chúng ta quy hước là cái nhánh đúng đi ha và nhánh trái là nhánh sai rồi nếu như x1 lớn hơn 0.5 đúng đi ha thì chúng ta sẽ có cái đường chạy miền phải chỗ này và chúng ta tiếp tục chúng ta sẽ có x2 lớn 0.1 Nếu như nó đúng thì chúng ta sẽ kết luận các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 10,
      "start_timestamp": "0:08:22",
      "end_timestamp": "0:09:38"
    }
  },
  {
    "page_content": "0.1 Nếu như nó đúng thì chúng ta sẽ kết luận các điểm thuộc bề vòng đó sẽ là x và nhỏ hơn là o Tương tự như vậy chúng ta sẽ có điểm x2 lớn hơn 0.1 tương ứng của miền trái Nếu như mà đúng, lớn hơn, tức là 5k, chúng ta sẽ phân loại nó thành O và phần nhỏ hơn thì chúng ta sẽ quyết định là X Vì vậy, chúng ta vừa thử biểu diễn các vùng của bài toán phân loại thành cái quyết định Tương tự cho bài toán hồi huy, giả sử ở đây chúng ta sẽ có cái điểm phân loại Vì vậy, chúng ta có 2 vùng bên phải và bên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 11,
      "start_timestamp": "0:09:26",
      "end_timestamp": "0:10:26"
    }
  },
  {
    "page_content": "loại Vì vậy, chúng ta có 2 vùng bên phải và bên trái, vùng bên phải này là vùng lớn hơn Giả sử chỗ này chúng ta sẽ có giá trị Y2 là giá trị trung bình của tất cả các điểm thuộc vị trình 2 và chúng ta sẽ có Y1 là giá trị trung bình của tất cả các cái điểm nằm trong vị trình 1 giá trị trung bình của giá trị hậu huy chúng ta ký hình như vậy để đẩy nhằm như vậy là chúng ta vừa tiền hiểu các khái niệm quan trọng để chúng ta hiểu về thực toán kế quyết định một cách tốt hơn và chúng ta có thể dễ dàng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 12,
      "start_timestamp": "0:10:14",
      "end_timestamp": "0:11:08"
    }
  },
  {
    "page_content": "định một cách tốt hơn và chúng ta có thể dễ dàng thấy là chúng ta hoàn toàn có thể sử dụng cây quyết định cho bài toán hầu uy và bài toán phân loại Trong phần này, chúng ta sẽ tìm hiểu các thuật ngữ để mô tả các thành phần và đấu tượng nằm trong một cây quyết định Để dạy vài theo dõi thì chúng ta sẽ có một ví dụ về cây hầu huy Trong tình huống này, cây hầu huy sẽ đi dự đoán lương của một cầu thử bóng chay đang thi đấu tại giải bóng chay có hàng cao nhất ở Mỹ là Major League Baseball Thì thông",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 13,
      "start_timestamp": "0:11:02",
      "end_timestamp": "0:11:57"
    }
  },
  {
    "page_content": "cao nhất ở Mỹ là Major League Baseball Thì thông thường lương của một cầu thử bóng chay sẽ phổ thuộc vào số năm thi đấu của họ tại giải Ngoài ra thì còn phụ thuộc vào số lần đánh trấn bấm của họ trong một trận. Thực sự thì khái niệm nút trong hay còn được gọi là internal node và nút đầu cuối hay còn được gọi nút lá thì đây là hai cái khái niệm mà chúng ta đã học rất là kỹ trong môn học, cấu trúc dữ liệu và giải thuật. Tuy nhiên thì các khái niệm này sẽ có vai trò khác nhau đôi chút so với hai",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 14,
      "start_timestamp": "0:11:46",
      "end_timestamp": "0:12:50"
    }
  },
  {
    "page_content": "này sẽ có vai trò khác nhau đôi chút so với hai môn học. Vai trò của nút trung gian trong cây quyết định sẽ có nhiệm vụ chia không gian đặc trưng bắt đầu thành các vùng. Các vùng này có vai trò quyết định tất cả các điểm thuộc về vùng đó thuộc về một lớp hoặc là được hồi quy thành một giá trị cụ thể. Ở đây chúng ta sẽ có một ví dụ đó là các nút lá của chúng ta sẽ hiển thị các giá trị dự đoán cho các vùng. Dải trì dự đoán ở đây chính là mức lương dự đoán Nó được tính bằng mức lương trung bình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 15,
      "start_timestamp": "0:12:36",
      "end_timestamp": "0:13:38"
    }
  },
  {
    "page_content": "dự đoán Nó được tính bằng mức lương trung bình của tất cả cầu thủ trong tập dữ liệu huấn luyện tại vùng đó Vùng đó ở đây sẽ có nghĩa như nào hó, nó phản ánh tất cả những cầu thủ khác có cùng đặc trưng đặc điện với cầu thủ cần dự đoán Chẳng hạn, một cầu thủ thi đấu với kinh nghiệm số 5 ít hơn 4.5 năm thì cầu thủ này thông thường sẽ nhận được mức lương trung bình là 5.11 Một cầu thủ thí đấu với kinh nghiệm trang 4.55 và hiệu số của cầu thủ đó lớn hơn 117.5 thì cầu thủ đó sẽ được nhận mức lương là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 16,
      "start_timestamp": "0:13:29",
      "end_timestamp": "0:14:32"
    }
  },
  {
    "page_content": "117.5 thì cầu thủ đó sẽ được nhận mức lương là 6.74 cao hơn cầu thủ thí đấu với lại số lần đánh giống bóng ít hơn trong một trận. Trong slide trước, chúng ta đã thử chuyển từ việc biểu diễn cây quyết định bằng cắt vùng thành cấu trúc cây Trong slide này, chúng ta sẽ thử chuyển từ cấu trúc cây sang cấu trúc vùng như sau Đầu tiên thì chúng ta sẽ có cái thuộc tính là số 5 thi đấu nhỏ hơn 4.55 thì chúng ta sẽ có một cái đường phăng đoạn ở đây hả Ấn với lại cái vùng mà nhỏ hơn 4.55 thì chúng ta chỉ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 17,
      "start_timestamp": "0:14:16",
      "end_timestamp": "0:15:32"
    }
  },
  {
    "page_content": "với lại cái vùng mà nhỏ hơn 4.55 thì chúng ta chỉ có một cách quyết định thôi Con số 5.11 có nghĩa là giá trị trung bình lương của tất cả cầu thủ nằm trong vùng này Tương tự, trong trường hợp lớn hơn 1.5 năm thì chúng ta sẽ có 2 trường hợp con ở đây Nếu như giá trị số lần đánh chống bấm của một cầu thủ lớn hơn 117.5 thì chúng ta sẽ có vùng là ở đây và trường hợp ít hơn là vùng r2 Ngoài ra thì chúng ta sẽ có một cách biểu diễn khác Ở đây thì chúng ta vẫn lấy ví dụ về một cái bài hồ huy Nhưng mà",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 18,
      "start_timestamp": "0:15:21",
      "end_timestamp": "0:15:46"
    }
  },
  {
    "page_content": "ta vẫn lấy ví dụ về một cái bài hồ huy Nhưng mà khác so với các biểu diễn trước thì chúng ta nhìn vào một cái ruyền thì chúng ta không biết được giá trị dự đoán là bao nhiêu Thì trong cái biểu diễn trong slide này thì chúng ta có thể hành dung Đây là giá trị dự đoán theo trục Z-Hut Chúng ta sẽ thấy là các vùng sẽ có giá trị khác nhau thôi Vì vậy chúng ta đã tìm hiểu kỹ các thuật ngữ để mô tả các thành phần trong một cây quyết định Hãy subscribe cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 19,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video hấp dẫn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=vrLmplmZbLk",
      "filename": "vrLmplmZbLk",
      "title": "[CS114 - Chương 8] Decision Tree (Part 1)",
      "chunk_id": 20,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Hồi quy tuyến tính đa biến. Hồi quy tiến tính nhiều biến Multiple linear regression là phương pháp mở rộng của hồi quy tuyến tính một biến. Mô hình hóa mối quan hệ tuyến tính giữa nhiều biến đầu vào và một biến đầu ra liên tục. Mô hình của mối quan hệ này là một siêu phẳng hyperplan trong không gian nhiều chiều thay vì chỉ là một đường thẳng như trường hợp một biến đầu vào. Ví dụ như dự đoán giá nhà dựa trên các yếu tố như diện tích, vị trí, số phòng, năm xây dựng vân vân. Tại sao chúng ta cần",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 0,
      "start_timestamp": "0:00:19",
      "end_timestamp": "0:01:03"
    }
  },
  {
    "page_content": "phòng, năm xây dựng vân vân. Tại sao chúng ta cần nhiều biến? Bởi vì trong thực tế một kết quả output thường bị chi phối bởi nhiều yếu tố khác nhau. Ví dụ giá nhà không chỉ phụ thuộc vào diện tích mà còn phụ thuộc vào vị trí số phòng năm xây dựng. Nếu chỉ dùng một biến mô hình bỏ qua nhiều thông tin quan trọng dẫn tới dự đoán kém chính xác underfitting không phản ánh được đầy đủ mối quan hệ trong dữ liệu thực tế. Trong thực tế thì ta thường có nhiều đặc trưng để mô tả đối tượng. Việc sử dụng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 1,
      "start_timestamp": "0:00:58",
      "end_timestamp": "0:01:35"
    }
  },
  {
    "page_content": "nhiều đặc trưng để mô tả đối tượng. Việc sử dụng nhiều biến giúp chúng ta khai thác tối đa giá trị của dữ liệu và sử dụng nhiều biến giúp mô hình dự đoán chính xác hơn, có thể phân tích được ảnh hưởng của từng yếu tố đến kết quả đầu ra. Để minh họa cụ thể cho bài toán hồi quy tuyến tính nhiều biến, chúng ta sẽ sử dụng một bộ dữ liệu giả lập về giá nhà. Trong bảng này thì mỗi dòng đại diện cho một căn nhà với các thông tin đặc chân như diện tích mét vuông, số phòng ngủ, năm xây dựng, tình trạng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 2,
      "start_timestamp": "0:01:30",
      "end_timestamp": "0:02:06"
    }
  },
  {
    "page_content": "mét vuông, số phòng ngủ, năm xây dựng, tình trạng tổng thể, đánh giá chất lượng theo thang điểm từ 1 đến 10. Và cột cuối cùng là giá nhà thực tế tính bằng đơn vị triệu Việt Nam đồng. Đây chính là biến đầu ra mà chúng ta muốn dự đoán. Với dữ liệu như thế này thì mô hình sẽ không chỉ dựa vào một yếu tố mà sẽ học cách kết hợp đồng thời nhiều yếu tố từ đó đưa ra giá trị dự đoán hợp lý và xát thực tế hơn. Như vậy bài toán dự đoán giá nhà không còn đơn giản là càng rộng càng đắc mà còn phải xét đến",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 3,
      "start_timestamp": "0:02:02",
      "end_timestamp": "0:02:42"
    }
  },
  {
    "page_content": "giản là càng rộng càng đắc mà còn phải xét đến số phòng, độ mới, chất lượng tổng thể và nhiều yếu tố khác. Tương tự thì chúng ta sẽ thiết kế cái mô hình toán học cho bài toán hồi ký tuyến tính nhiều biến thì nó sẽ được mô tả bằng một tập hợp các cái đặc trưng. Ở đây là XJ là feature thứ J với J chạy từ 1 đến n. Trong đó N là số lượng đặc trưng. XY là vectơ đặc trưng của mẫu thứ y. Ví dụ nếu y = 2 thì chúng ta sẽ thấy cái giống như tương tự biểu diễn dưới dạng à biểu đồ vecơ cột. Thì chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 4,
      "start_timestamp": "0:02:36",
      "end_timestamp": "0:03:21"
    }
  },
  {
    "page_content": "diễn dưới dạng à biểu đồ vecơ cột. Thì chúng ta vừa học cách mô tả của một đối tượng ví dụ như căn nhà bằng vecơ đặc trưng x để dự đoán giá trị. Ví dụ giá nhà thì chúng ta xây dựng một mô hình hồi quyến tính. Trong đó mỗi đặc chân được gán với một trọng số à WJ. Thế thì như vậy chúng ta sẽ có công thức tổng quát là FWBX thì bằng W1X1 + W2X2 cộng WN Xn + B. Thì ở đây B là hệ số chặn, X là vectơ đặc trưng của một đối tượng và WJ là trọng số hệ số tương ứng với đặc trưng XJ. Để công thức gọn hơn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 5,
      "start_timestamp": "0:03:16",
      "end_timestamp": "0:04:08"
    }
  },
  {
    "page_content": "tương ứng với đặc trưng XJ. Để công thức gọn hơn thì chúng ta dùng ký hiệu vecơ gom tất cả trọng số vào thành vectơ W. Tất cả các đặc trưng thì thành vectơ x. Và khi đó thì mô hình chúng ta sẽ còn chỉ còn viết lại là w x + pi và trong đó wx là tích vô hướng hai vectơ. Ở phần trước thì chúng ta biết hàm mất mát MSC dùng để đo lường mức độ sai lệch giữa giá trị thực tế và giá trị dự đoán của mô hình. Thì khi mổ rộng sang hồi quy nhiều biến thì chúng ta vẫn sử dụng MSC như là hàm bất mắt lot",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 6,
      "start_timestamp": "0:04:02",
      "end_timestamp": "0:04:43"
    }
  },
  {
    "page_content": "chúng ta vẫn sử dụng MSC như là hàm bất mắt lot function và định nghĩa hàm chi phí cost function JWB. À như chúng ta thấy trên slide ở đây. Ở đây thì W là trọng vectơ trọng số và hệ số chặn. XY là vectơ đặc trưng của mẫu thứ y và M là số lượng mẫu trong tập huấn luyện. Mục tiêu của thuật toán là tìm W và B sao cho hàm chi phí JW WB đạt giá trị nhỏ nhất, tức là mô hình dự đoán càng gần với giá trị thực tế càng tốt. Để giải cái bài toán tối ưu này thì chúng ta dùng thuật toán gradient design",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 7,
      "start_timestamp": "0:04:37",
      "end_timestamp": "0:05:20"
    }
  },
  {
    "page_content": "này thì chúng ta dùng thuật toán gradient design tương tự giống như hồi quy tiến tính một biến. Thì trước khi đi vào cụ thể thì chúng ta hãy nhìn vào một cách tổng quát của thuậc toán gradient sau hiệu kỳ tiến tính nhiều biến. Thì ở mỗi bước lập thì chúng ta sẽ cập nhật lại trọng số của WJ theo hướng ngược lại với đạo hàm riêng của hàm chi phí JW đối với WJ với tốc độ điều chỉnh do tham số lên ray quyết định. Ở đây thì chúng ta có công thức của đạo hình riêng của hàm chi phí J theo từng WJ và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 8,
      "start_timestamp": "0:05:12",
      "end_timestamp": "0:06:00"
    }
  },
  {
    "page_content": "đạo hình riêng của hàm chi phí J theo từng WJ và từ đó thì chúng ta sẽ có công thức cập nhật cho từng trọng số WJ và pi là như trên hình mà chúng ta thấy. Cái này các bạn có thể kiểm tra lại chi tiết hơn. Và cuối cùng thì đây là cái thuật toán gradient des thì nó cũng sẽ gồm có ba bước chính. Thứ nhất là khởi tạo và thông thường thì các giá trị khởi tạo các giá trị của trọng số và hệ số chặn thường là không hoặc là lấy một cái giá trị ngẫu nhiên và chúng ta lập cho tới khi hội tụ thì chúng ta",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 9,
      "start_timestamp": "0:05:55",
      "end_timestamp": "0:06:49"
    }
  },
  {
    "page_content": "và chúng ta lập cho tới khi hội tụ thì chúng ta sẽ tính đạo hàm chi phí theo từng wj và pi và sau đó chúng ta sẽ cập nhật lại các trọng số đó theo cái tốc độ học learning ray alpha và dừng lại khi thuật toán nó hội tụ tức là khi ví dụ như khi à các thay đổi của đập L và B rất nhỏ hoặc là số vòng lọc đạt tối đa. Một số vấn đề khi huấn luyện. Chúng ta sẽ tìm hiểu một kỹ thuật giúp radian design hoạt động hiệu quả hơn trong trường hợp nhiều đặc trưng đó là feature scaling. Tỷ lệ hóa đặc trưng.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 10,
      "start_timestamp": "0:06:44",
      "end_timestamp": "0:07:16"
    }
  },
  {
    "page_content": "trưng đó là feature scaling. Tỷ lệ hóa đặc trưng. Trong thực tế thì các đặc trưng trong dữ liệu thường có giải giá trị rất khác nhau. Ví dụ như khi dự đoán giá nhà, diện tích giá nhà có thể từ vài trăm đến vài nghìn. Trong khi đó số phòng ngủ thì chỉ từ 0 đến 5. Điều này dẫn đến việc các trọng số tương ứng cũng phải điều chỉnh theo. Trọng số gắn với đặc trưng lớn thường nhỏ, còn đặc trưng nhỏ thì ngược lại. Nếu ta vẽ các tham số và hàm chi phí thì sẽ thấy các đường đồng mức bị kéo dài và dẹt",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 11,
      "start_timestamp": "0:07:12",
      "end_timestamp": "0:07:49"
    }
  },
  {
    "page_content": "thì sẽ thấy các đường đồng mức bị kéo dài và dẹt khiến cho gradient des phải đi loanh quanh mất nhiều thời gian để hội tụ cực tiểu. Giải pháp là chúng ta sẽ dùng vure scaling tức là biến đổi các đặc trưng về cùng một giải giá trị. Ví dụ như đều trong khoảng từ 0 đến 1. Khi đó các đường đồng bức sẽ trở nên tròn hơn gradient desen có thể đi thẳng tới cực tiểu nhanh hơn. Tóm lại là khi có các giải giá trị đặt khác biệt thì việc chuẩn hóa giúp tăng tốc và ổn định quá trình hóa tối ưu hóa của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 12,
      "start_timestamp": "0:07:44",
      "end_timestamp": "0:08:22"
    }
  },
  {
    "page_content": "tăng tốc và ổn định quá trình hóa tối ưu hóa của gradient desen. Chúng ta sẽ tìm hiểu về một phương pháp feature scan rất phổ biến đó là min normalization. Thì min normalization là một kỹ thuật để đưa dữ liệu về quanh giá trị zero dựa vào giá trị trung bình và nổ rộng của khoảng giá trị. Thì cách làm này giúp các feature có than đo khác biệt được đưa về cùng một mặt bằng và từ đó giúp mô hình machine learning học hiệu quả hơn. Công thức của min nobelization được trình bày như trên hình vẽ. Ở",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 13,
      "start_timestamp": "0:08:15",
      "end_timestamp": "0:08:54"
    }
  },
  {
    "page_content": "nobelization được trình bày như trên hình vẽ. Ở trong đó thì chúng ta thấy thấy sau khi giá trị sau khi hồi tụ thì bằng giá trị góc trừ tức là giá trị trung bình min của feature và và chia cho x max trừ x min là lần lượt là giá trị lớn nhất và nhỏ nhất của feature. Thì sau khi áp dụng công thức này các giá trị của feature sẽ được phân bố quanh 0 nhưng giá trị có thể nằm ngoài -1 hoặc 1 tùy vào phân bối của dữ liệu. Chúng ta sẽ cùng phân tích ưu và nhược điểm và khi nào nên sử dụng phương pháp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 14,
      "start_timestamp": "0:08:49",
      "end_timestamp": "0:09:26"
    }
  },
  {
    "page_content": "và nhược điểm và khi nào nên sử dụng phương pháp min normalization. Đối với ưu điểm thì min normalization giúp đưa các feature về quan zero và từ đó giảm được hiện tượng byas do các feature có scale khác biệt. Ngoài ra phương pháp này rất đơn giản và dễ tính toán phù hợp, dễ để áp dụng nhanh trong nhiều trường hợp. Tuy nhiên phương pháp này cũng có một số nhược điểm. Min normalization khá nhạy cảm với outlier. Tức là nếu dữ liệu có các giá trị ngoại lệ lớn thì giá trị max hoặc min sẽ bị kéo lên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 15,
      "start_timestamp": "0:09:20",
      "end_timestamp": "0:09:53"
    }
  },
  {
    "page_content": "lệ lớn thì giá trị max hoặc min sẽ bị kéo lên làm cho kết quả chuẩn hóa không còn ý nghĩa. Phương pháp này cũng không phù hợp với các dữ liệu có phân phối lệch mạnh vì trung bình và khoảng giá trị sẽ không phản ánh đúng bản chất của dữ liệu. Vậy khi nào chúng ta nên dùng min normalization? Thứ nhất là dữ liệu bạn không có outlier quá lớn. Và thứ hai khi bạn muốn các feature được chuẩn hóa về quanh không nhưng vẫn giữ được tỉ lệ giờ các giá trị chứ không bị nén vào một khoảng cố định như min max",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 16,
      "start_timestamp": "0:09:49",
      "end_timestamp": "0:10:37"
    }
  },
  {
    "page_content": "không bị nén vào một khoảng cố định như min max scaling từ -1 tới 1. Như vậy normalization min normalization là một lựa chọn tốt trong các trường hợp dữ liệu khá sạch không có ngoại lệ lớn và bạn muốn giữ lại sự tỷ lệ giữa các giá trị góc chúng ta sẽ tìm hiểu về một phương pháp chuẩn khoác khóa khác đó là Z score normalization hay còn gọi là standardization thì khác với min normalization yes score normalization chuẩn hóa dữ liệu dựa trên trung bình và độ lệch chuẩn của từng features sau khi áp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 17,
      "start_timestamp": "0:10:31",
      "end_timestamp": "0:11:06"
    }
  },
  {
    "page_content": "và độ lệch chuẩn của từng features sau khi áp dụng phương pháp này các giá trị sẽ được chuyển về có dạng trung bình là 0 và độ lịch chuẩn là 1. Công thức của score như sau. Tức x' thì bằng x là giá trị góc ban đầu trừ mi và chia cho sigma. Trong đó thì mi là giá trị trung bình của feature và sigma là độ lịch chuẩn. Ưu điểm của Z score normalization là giúp dữ liệu có phân phối gần chuẩn được chuẩn hóa hiệu quả loại bỏ ảnh hưởng của scale đồng thời không bị giới hạn trong một khoảng cố định như",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 18,
      "start_timestamp": "0:11:02",
      "end_timestamp": "0:11:33"
    }
  },
  {
    "page_content": "không bị giới hạn trong một khoảng cố định như min max scaling. Tuy nhiên nếu dữ liệu có outl và giá trị chuẩn hóa có thể vượt xa khoảng -11 nên chúng ta cũng cần lưu ý khi áp dụng phương pháp này. Feature engineering. Chúng ta sẽ cùng tìm hiểu về Future Engineering, một biết rất quan trọng trong quá trình xây dựng mô hình học máy. Feuture Engineering hay còn gọi là kỹ thuật xây dựng đặc trưng là quá trình tạo ra hoặc biến đổi các đặc trưng đầu vào cho mô hình học máy. Có nhiều cách thực hiện",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 19,
      "start_timestamp": "0:11:30",
      "end_timestamp": "0:12:03"
    }
  },
  {
    "page_content": "vào cho mô hình học máy. Có nhiều cách thực hiện điều này. Điều đơn giản nhất là biến đổi feature gốc. Ví dụ như lấy lock bình phương hoặc lấy căn bậc hai của một features. Ngoài ra chúng ta cũng có thể kết hợp các feature góc với nhau để tạo ra feature mới. Ví dụ điện hình với dữ liệu nhà đất thay vì chỉ dùng chiều rộng và chiều sau thì chúng ta có thể tạo ra thêm future diện tích bằng cách nhân hai giá trị này lại. Việc xây dựng được những đặc trưng phù hợp sẽ giúp mô hình học tốt hơn và dự",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 20,
      "start_timestamp": "0:11:58",
      "end_timestamp": "0:12:32"
    }
  },
  {
    "page_content": "trưng phù hợp sẽ giúp mô hình học tốt hơn và dự đoán chính xác hơn. Thậm chí trong nhiều trường hợp thực tế, Feature Engineering còn quan trọng hơn cả việc lựa chọn thực toán học máy. Nhờ Feature Engineering, mô hình có khả năng khai thác và tận dụng các mối quan hệ phức tạp ẩn trong dữ liệu mà các mô hình tuyến tính thông thường có thể bỏ qua. Đây chính là lý do mà kỹ thuật này luôn được xem là một trong những yếu tố then chốt quyết định đến hiệu quả của hệ thống học máy. Polynomial",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 21,
      "start_timestamp": "0:12:28",
      "end_timestamp": "0:12:58"
    }
  },
  {
    "page_content": "đến hiệu quả của hệ thống học máy. Polynomial regression. Tiếp theo thì chúng ta sẽ tìm hiểu về polynomial regression hay còn gọi là hồi quy đa thức. Polynomial regression là một mở rộng của linear regression cho phép mô hình hóa các mối quan hệ phi tuyến tính giữa đặc chân đầu vào và giá trị cần dự đoán. Cụ thể, thay vì sử dụng feature góc thì chúng ta sẽ tạo thêm các feature mới bằng cách lấy luy thừa của feature ví dụ như x bình x m 3. Vậy tại sao chúng ta cần polynomial regression? Thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 22,
      "start_timestamp": "0:12:54",
      "end_timestamp": "0:13:30"
    }
  },
  {
    "page_content": "tại sao chúng ta cần polynomial regression? Thì linear regression chỉ phù hợp khi dữ liệu có xu hướng tiến tính. Tức là các điểm dữ liệu nằm gần một đường thẳng. Tuy nhiên trong thực tế nhiều bài toán có dữ liệu dạng công hoặc phức tạp khi đó line region có thể không mô hình hóa tốt mối quan hệ giữa feature và target. Thế thì polynomial regression sẽ giúp chúng ta khắc phục nhược điểm này bằng cách cho phép mô hình học các mối quan hệ phi tuyến. Ví dụ với các bài toán dự báo giá nhà theo diện",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 23,
      "start_timestamp": "0:13:26",
      "end_timestamp": "0:13:57"
    }
  },
  {
    "page_content": "Ví dụ với các bài toán dự báo giá nhà theo diện tích, nếu dữ liệu thực tế có dạng cong thì lini regression sẽ không cho kết quả chính xác. Nhưng với polynomial regression, mô hình có thể fitting được một đường cong phù hợp hơn với xu hướng dữ liệu của thực tế. Tiếp theo thì chúng ta sẽ xem cách xây dựng polynomial regression và cách kỹ thuật này liên quan đến feature engineering. Thực chất thì polynomial regression là một ví dụ điển hình của việc xây dựng đặc chân feature engineering. Ở đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 24,
      "start_timestamp": "0:13:52",
      "end_timestamp": "0:14:26"
    }
  },
  {
    "page_content": "việc xây dựng đặc chân feature engineering. Ở đây thay vì chỉ sử dụng feature gốc thì chúng ta chủ động tạo ra các feature mới bằng cách lấy lũy thừa bậc hai, bậc ba hoặc áp dụng các phép biến đổi phi tuyến khác lên feature gốc. Ví dụ với dữ liệu diện tích giá nhà thì có thể tạo thêm diện tích bình phương hoặc diện tích mũ 3. Và sau khi mở rộng tập feature như vậy thì chúng ta vẫn áp dụng linear regression để huấn luyện mô hình trên các feature mới này. Nhờ quá trình feature engineering, mô",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 25,
      "start_timestamp": "0:14:20",
      "end_timestamp": "0:14:43"
    }
  },
  {
    "page_content": "mới này. Nhờ quá trình feature engineering, mô hình không còn bị giới hạn trong việc học các mối quan hệ tuyến tính mà có thể học được cả mối quan hệ phi tuyến giữa feature và giá trị báo. Tuy nhiên thì khi sử dụng polynomial regression có một số điểm cần lưu ý đó là các feature bậc cao thường có giá trị rất lớn gây mất cân bằng và làm cho quá trình tối ưu trở nên khó khăn. Vì vậy chúng ta nên thực hiện feature scaling để chuẩn hóa tất cả các feature về cùng một thang đo. Ngoài ra việc lựa chọn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 26,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "về cùng một thang đo. Ngoài ra việc lựa chọn bậc đa thức cũng rất quan trọng. Nếu chọn bậc đ thấp quá thấp thì mô hình sẽ không đủ linh hoạt under fitting. Còn nếu mô hình chọn bậc quá cao thì sẽ dẫn đến hiện tượng overfitting tức là mô hình học thuộc lòng dữ liệu mà không tổng quạt khóa được cho dữ liệu mới. Đây là các cái ví dụ liên quan tới underfitting, overfitting và good fit mà chúng ta đã đề cập ở bài trước.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wBAm3lczqXs",
      "filename": "wBAm3lczqXs",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 3)",
      "chunk_id": 27,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề, học sâu, machine learning, logistic regression, ImageNet. Qua lấp layer cuối cùng, chúng ta đã tính ra được y ngã là phân bố sát xuất thuộc về lấp nào đó Tuy nhiên đó là giá trị chúng ta dự đoán Và chúng ta mong muốn giá trị này phải sắp xỉ với giá trị y, tức là giá trị thực tế để giả trị dự đoán và giá trị thực tế giống nhau, chúng ta phải có hàm mất mát, hàm loss đó là lý do có nội dung này mục đích đó là đo lường sai số giữa giá trị dự đoán và giá trị thực tế quy tắc đó là loss càng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:18"
    }
  },
  {
    "page_content": "đoán và giá trị thực tế quy tắc đó là loss càng cao thì dự đoán sẽ càng tệ tại vì cái size số giữa y ngã và y này càng lớn tức là y ngã nó không khớp với cái y tức là chúng ta đang đoán rất là tệ loss càng thấp thì cái dự đoán sẽ là càng tốt loss càng thấp có nghĩa là y ngã nó sắp xỉ với cái y tức là chúng ta đang đoán đúng, đoán đúng tức là cái kết quả dự đoán rất là tốt và mục tiêu của cái việc huấn luyện này đó là tìm ra được một cái bộ trọng số của một cái mô hình neural network này trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 1,
      "start_timestamp": "0:01:13",
      "end_timestamp": "0:01:46"
    }
  },
  {
    "page_content": "số của một cái mô hình neural network này trong cái mô hình neural network này nó sẽ có cái bộ trọng số tức là cái giá trị trọng số của các cạnh nối của mạng Neural trọng số chính là cái giá trị của các cạnh nối này mục tiêu đó là tìm ra bộ trọng số và độ lệch để bias để cho chúng ta tối hiệu hóa hàm loss tối hiệu hóa giá trị mức mát này Ví dụ về hàm loss mức mát, đối với bài toán hồi quy thì thường chúng ta sẽ sử dụng mean square error tức là các độ đo thi hướng về khoảng cách Còn đối với mô",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 2,
      "start_timestamp": "0:01:43",
      "end_timestamp": "0:02:40"
    }
  },
  {
    "page_content": "các độ đo thi hướng về khoảng cách Còn đối với mô hình phân loại thì chúng ta thường sẽ sử dụng cross entropy tức là các độ đo có ếu tố hàm loss trong đó Tại vì nó sẽ giúp chúng ta khuất đại được giá trị sai số Mà mô hình của mình khuất đại được sai số thì nó sẽ giúp cho việc huấn luyện nó sẽ nhanh hơn Cái này nói cho vui, đó là thương cho rơi cho vọt Tức là khi mà hà mất mát này càng lớn thì mô hình của mình sẽ thập mau chóng để giảm bớt tối đa sai số này và để mà từ cái ham loss chúng ta tìm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 3,
      "start_timestamp": "0:02:37",
      "end_timestamp": "0:03:25"
    }
  },
  {
    "page_content": "sai số này và để mà từ cái ham loss chúng ta tìm cái tham số để cho cái loss của mình là nhỏ nhất thì nó có một cái thuộc toán là gọi là thuộc toán Lan truyền ngược hay còn gọi là Back propagation vấn đề đó là khi chúng ta đã có loss rồi, tức là cái size số rồi thì làm sao để cập nhật cái trọng số nữa tiếp theo để cho cái việc cập nhật trọng số này nó sẽ có xu hướng khiến cho cái loss của mình là đi xuống Tại vì hồi nãy chúng ta có một nguyên tắc đó là loss mà càng nhỏ là càng tốt Thực toán của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 4,
      "start_timestamp": "0:03:20",
      "end_timestamp": "0:03:56"
    }
  },
  {
    "page_content": "đó là loss mà càng nhỏ là càng tốt Thực toán của chúng ta là lang truyền ngược Nhưng mà trước khi để thực hiện được thực toán lang truyền ngược thì chúng ta sẽ phải thực hiện thực toán feedforward process là chúng ta sẽ lang truyền theo chiều thuận chúng ta lang truyền theo chiều thuận sau đó đến được giá trị dự đoán, đây là y ngã y ngã này chúng ta sẽ đi so sánh với y thực tế để tính ra giá trị là Loss khi đã có Loss này rồi thì chúng ta sẽ tiến hành Backward Delta này chính là size số, chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 5,
      "start_timestamp": "0:03:54",
      "end_timestamp": "0:04:41"
    }
  },
  {
    "page_content": "hành Backward Delta này chính là size số, chúng ta sẽ đi ngược lại Cứ mỗi lần chúng ta đi ngược lại thì chúng ta sẽ cập nhập cái trọng số này Đi ngược lại chúng ta sẽ cập nhập cái trọng số Với hy vọng khi chúng ta thay đổi cái trọng số này thì cái loss này sẽ có cái xu hướng là giảm số Mục tiêu đó là tính mức độ đóng góp của mỗi trọng số trong cái hàm loss Thì công cụ để tính toán mức độ đóng góp của mỗi trọng số cho cái loss này là công cụ tính đạo hàm đạo hàm của loss theo từng trọng số khi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 6,
      "start_timestamp": "0:04:37",
      "end_timestamp": "0:05:28"
    }
  },
  {
    "page_content": "đạo hàm đạo hàm của loss theo từng trọng số khi nói đến radian, tức là đạo hàm cho một bector tức là cho một tổ hợp tất cả các trọng số công cụ đạo hàm này sẽ cho biết 2 điều điều đầu tiên là cho biết hướng là trọng số này xu hướng, trọng số này chúng ta sẽ tăng lên hay giảm xuống Thì hướng của đạo hàm sẽ cho chúng ta biết chúng ta nên cập nhật theo chiều nào Tăng lên hay giảm xuống Trọng số này là tăng lên hay giảm xuống Và thứ 2 đó là cái độ lớn Khi chúng ta đã biết là tăng lên hay giảm xuống",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 7,
      "start_timestamp": "0:05:23",
      "end_timestamp": "0:06:05"
    }
  },
  {
    "page_content": "Khi chúng ta đã biết là tăng lên hay giảm xuống rồi Thì chúng ta sẽ thay đổi là nhiều hay là ít Tăng nhiều hay là tăng ít Giảm nhiều hay là giảm ít Và cái việc cập nhật trọng số thì chúng ta sẽ có một cái thuật toán Nó gọi là thuật toán Radian Descent Thục toán Radian Descent là một thuật toán để cập nhật trọng số nhằm giảm giá trị loss này Và idea cũng rất là đơn giản Ý tưởng đó là với một cái hàm mất mát Nếu chúng ta xem cái hàm mất mát của mình Nó là một cái thung lũng giống như trong hình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 8,
      "start_timestamp": "0:06:01",
      "end_timestamp": "0:06:40"
    }
  },
  {
    "page_content": "Nó là một cái thung lũng giống như trong hình này hoặc trong hình này Thì hai cái hình này là hai cái hình mà ở hình bên trái là cái hàm mất mát mà ở dạng liên tục chúng ta thấy là vẽ bằng các đường nét liên tục màu từ đỏ xuống xanh đỏ là những cái màu nóng là ở trên cao còn màu xanh là ở dưới đáy là ở dưới vùng thấp thì chúng ta tưởng tượng cái hàm Loss này là cái thung lũng thì mục tiêu đó là chúng ta sẽ bắt đầu bằng một cái trọng số cái không gian trọng số ở đây không gian trọng số Với một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 9,
      "start_timestamp": "0:06:36",
      "end_timestamp": "0:07:22"
    }
  },
  {
    "page_content": "gian trọng số ở đây không gian trọng số Với một trọng số, chúng ta có cái Loss tại vị trí này Giả sử như vị trí này là chúng ta đang đặt quả bóng trong cái tham số của mô hình thì chúng ta nhìm vụ là phải cho cái quả bóng này lăng xuống để đến được cái vị trí màu xanh dương đậm nhất chính là cái khu vực mà cái hàm Loss của mình có cái giá trị nhỏ nhất tại vì chúng ta nhắc lại cái nguyên tắc của mình là Loss càng nhỏ thì càng tốt do đó thì chúng ta phải cho cái quả bóng này đến cái nơi chuẩn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 10,
      "start_timestamp": "0:07:17",
      "end_timestamp": "0:07:56"
    }
  },
  {
    "page_content": "ta phải cho cái quả bóng này đến cái nơi chuẩn nhất của cái thùng lũn và cái tại cái vị trí này thì đây chính là cái bộ tham số đây chính là cái bộ tham số tối ưu nhất của mình nếu bạn nhìn bên đây thì chúng ta nhìn nó ở một góc độ đó là đường bình độ mỗi đường nét như thế này, đó là có độ cao giống nhau và càng xuống dưới thì đường bình độ này sẽ có xu hướng là độ cao càng thấp và ban đầu quả bóng của mình ở đây, sau đó nó sẽ nhảy qua đây, nhảy qua đây, nhảy qua đây cho đến một hồi nó sẽ chạm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 11,
      "start_timestamp": "0:07:53",
      "end_timestamp": "0:08:34"
    }
  },
  {
    "page_content": "qua đây, nhảy qua đây cho đến một hồi nó sẽ chạm đến cái điểm cuối cùng Đây là một cách biểu diễn khác, nó không dựa trên độ sau Nó lại bỏ đi với tổ độ sau Giống như chúng ta đang nhìn từ phía trên nhìn xuống Cái hình này chính là cái hình mà nhìn từ phía trên nhìn xuống Không thấy được cái độ sau Thì cái đường đi của tham số của mình sẽ như thế này, đi zigzag như thế này Với cái mục tiêu này, chúng ta sẽ có công thức cập nhật rất là đơn giản Thục toán Radiant Ascent là một thuật toán đơn giản",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 12,
      "start_timestamp": "0:08:31",
      "end_timestamp": "0:09:19"
    }
  },
  {
    "page_content": "toán Radiant Ascent là một thuật toán đơn giản và dễ cài đạt Đó là cái bộ trọng số mới thì sẽ bằng cái bộ trọng số hiện tại hay là trọng số cũ trừ đi Learning Rate, tức là cái hệ số học nhân với lại cái đạo hàm Đạo hàm của cái trọng số Đạo hàm của hàm Loss Đạo hàm của hàm Loss theo cái trọng số theo cái trọng số W của mình Và sơ độ ở đây là mình họa một cách trực quan, là quá trình huấn luyện, một cái training loop Cái việc học của một cái mạng neural network nó sẽ được thực hiện lần lượt theo",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 13,
      "start_timestamp": "0:09:10",
      "end_timestamp": "0:10:06"
    }
  },
  {
    "page_content": "neural network nó sẽ được thực hiện lần lượt theo các cái bước như sau Đầu tiên chúng ta sẽ có một cái bộ dữ liệu, nó là một cái data set, input data set Chúng ta có hình ảnh như là Airplane, Automobile, Bird, Cat, Deer, Dog và với từng ảnh này, chúng ta truyền vào mạng Neural Network qua nhiều các lớp ẩn, ra đến lớp Output, nó gọi là Feed Forward toàn bộ quá trình này là Feed Forward thì nó sẽ ra được giá trị dự đoán đây là giá trị dự đoán Với ảnh này, chúng ta đoán là trắc, tức là những chỗ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 14,
      "start_timestamp": "0:10:02",
      "end_timestamp": "0:10:43"
    }
  },
  {
    "page_content": "ảnh này, chúng ta đoán là trắc, tức là những chỗ màu đỏ là chúng ta đang đáng sai Còn những cái màu xanh chính là cái chúng ta đáng đúng Đây là cái target, tức là cái thực tế, hay còn gọi là cái route thì khi chúng ta so lẽ ra thằng này phải là airplane nhưng cuối cùng nó lại để là truck lẽ ra là automobo thì nó lại để là deer lẽ ra là deer thì nó lại để là automobo tức là nó đang bị sai thì từ cái giá trị dự đoán và giá trị thực tế này chúng ta sẽ đi tính cái loss có cái loss này rồi thì chúng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 15,
      "start_timestamp": "0:10:38",
      "end_timestamp": "0:11:13"
    }
  },
  {
    "page_content": "sẽ đi tính cái loss có cái loss này rồi thì chúng ta sẽ đi điều chỉnh lại điều chỉnh lại Adjust Way and Bias điều chỉnh lại cái bộ trọng số điều chỉnh lại các cái bộ trọng số này Điều chỉnh cái bội trong số này vâng vâng Để làm sao đó, hi vọng là cái Loss này có xu hướng là giảm xuống Và cứ cái việc này nó sẽ được thực hiện đi, thực hiện lại nhiều lần Repeat until là cái error này nó thấp dưới một cái ngưỡng nào đó Hoặc là khi chúng ta lập nhiều hơn cái số lần mà chúng ta kỳ vọng Thì cái việc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 16,
      "start_timestamp": "0:11:10",
      "end_timestamp": "0:11:52"
    }
  },
  {
    "page_content": "hơn cái số lần mà chúng ta kỳ vọng Thì cái việc mà chúng ta từ cái Loss Function này nè Chúng ta đi ngược trở về để cập nhật lại cái bộ trọng số này thì nó gọi là Back Propagation, tức là thuật toán lan truyền ngược và cái Back Propagation này thì nó vẫn phải... khi chúng ta tính toán xong các giá trị đạo hàm rồi thì chúng ta sẽ đi update nó bằng thuật toán Radiant Descent Back Propagation để đi tính đạo hàm trên từng cái trọng số này tính đạo hàm trên từng cái trọng số này sau đó chúng ta sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 17,
      "start_timestamp": "0:11:50",
      "end_timestamp": "0:12:37"
    }
  },
  {
    "page_content": "hàm trên từng cái trọng số này sau đó chúng ta sẽ cập nhật theo công thức của Radiant Descent đã được hậu ở đây thì Radiant này sẽ được tính bằng Backpropagation sau khi tính xong đạo hàm này xong thì chúng ta sẽ đi cập nhật mới theo thuốc toán Radiant Descent ở đây và trong quá trình huấn luyện thì nó sẽ có một khái niệm gọi là Hyperprimator tức là siêu tham số ở trong mô hình mạng Neural Network thì chúng ta có các trọng số là tham số chính là những trọng số của các cạnh nối này là trọng số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 18,
      "start_timestamp": "0:12:36",
      "end_timestamp": "0:13:16"
    }
  },
  {
    "page_content": "những trọng số của các cạnh nối này là trọng số của các cạnh nối này và chúng ta sẽ có khái niệm là hyperparameter ở đây là parameter, còn bên đây là hyperparameter thì các siêu tham số trong 1 quá trình huấn luyện sẽ bao gồm Đầu tiên đó là cái Epoch tức là 1 lượt toàn bộ dữ liệu đi qua cái mạng giả sử như cái bộ dữ liệu này của mình có 1000 mẫu thì mỗi 1 cái lượt mà huấn luyện hết 1000 mẫu này đó là 1 Epoch Thì thuật toán của chúng ta sẽ huấn luyện qua bao nhiêu Epoch tức là chúng ta sẽ đọc đi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 19,
      "start_timestamp": "0:13:12",
      "end_timestamp": "0:13:55"
    }
  },
  {
    "page_content": "qua bao nhiêu Epoch tức là chúng ta sẽ đọc đi đọc lại 1000 mẫu này bao nhiêu lần ví dụ nếu Epoch của chúng ta là bằng 3 3 epoch, tức là chúng ta sẽ duyệt qua 1 ngàn mẫu này 3 lần huấn luyện đi, huấn luyện lại nhiều lần thì ví dụ như đến epoch số 80, epoch số 3 với epoch số 3, giả sử ở đây chúng ta thấy là cái loss của mình mới giảm, chưa có đủ nhiều chúng ta phải cho huấn luyện đi, huấn luyện lại 1 ngàn mẫu này 80 lần thì chúng ta thấy là nó sẽ có xu hướng là hết cập nhật được nhiều rồi thì đến",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 20,
      "start_timestamp": "0:13:46",
      "end_timestamp": "0:14:45"
    }
  },
  {
    "page_content": "xu hướng là hết cập nhật được nhiều rồi thì đến đây chúng ta có thể dừng thì siêu thăm số đầu tiên đó là ipop là số lượt mẫu chúng ta hướng luyện trên toàn bộ dữ liệu kêu tổ thứ 2 đó là pass size thì trong trường hợp đó là cái dữ liệu của mình nó quá lớn, nó nặng quá chúng ta không thể nào mà lót hết một lượt 1000 mẫu thì chúng ta chỉ có thể là lót một subset, một tập con một tập con của 1000 mẫu này thôi, ví dụ như là 128 mẫu một lần 1 subset này gọi là batch size là số lượng mẫu dữ liệu trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 21,
      "start_timestamp": "0:14:37",
      "end_timestamp": "0:15:38"
    }
  },
  {
    "page_content": "gọi là batch size là số lượng mẫu dữ liệu trong 1 lần cập nhật chúng ta sẽ lấy ra 128 mẫu này để cập nhật trong số thôi chứ không lấy hết thì nó gọi là batch size trong đường hợp dữ liệu lớn phức tạp và mô hình phức tạp siêu tham số tiếp theo là learning rate learning rate này rất quan trọng Nó quyết định đến việc thành bại trong việc huấn luyện Ở trong hình ví dụ ở đây Nếu learning rate là cái gì? Learning rate chính là cái hệ số học ở đây Nó sẽ cho biết độ lớn, quyết định độ lớn của việc huấn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 22,
      "start_timestamp": "0:15:36",
      "end_timestamp": "0:16:18"
    }
  },
  {
    "page_content": "cho biết độ lớn, quyết định độ lớn của việc huấn luyện Cảm ơn các bạn đã xem video hãy đăng ký kênh để ủng hộ kênh để xem video mới nhất. và nó phân kỳ thế thì learning rate quá lớn thì nó sẽ hội tụ không ổn định thậm chí là không hội tụ và nếu mà learning rate quá nhỏ thì nó sẽ hội tụ rất là chậm và một số cái loại neuro một số cái loại mạng neuro network phổ biến hiện nay thì trong cái kiến trúc mà chúng ta vừa mới tìm hiểu nó gọi là MLP tuy nhiên cái kiến trúc này thì nó cũng đã khá là cũ và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 23,
      "start_timestamp": "0:16:14",
      "end_timestamp": "0:16:54"
    }
  },
  {
    "page_content": "cái kiến trúc này thì nó cũng đã khá là cũ và nó không còn hiệu lực trong thời gian gần đây nó không còn phổ biến trong thời gian còn đây một số cái mạng Deep Neural Network mới nó cũng dựa trên ANN, cái mạng Neural Network cũ đó chính là mạng CNN, là Convolutional Neural Network thì chuyên để xử lý cho cái loại dữ liệu là 2 chiều, là dữ liệu 2D image đầu vào thì nó sẽ giống như một cái ma trận địa mảnh như thế này nó sẽ là một cái ma trận địa mảnh thì ở đây chúng ta thấy là nó sẽ có 2 chiều 1",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 24,
      "start_timestamp": "0:16:50",
      "end_timestamp": "0:17:40"
    }
  },
  {
    "page_content": "thì ở đây chúng ta thấy là nó sẽ có 2 chiều 1 cái lưới 2 chiều thì nó sẽ ứng dụng trong lĩnh vực thị giác máy tính để nhận diện hình ảnh, phân loại video hoặc là cho cái xe tự lái thì ở đây chúng ta thấy là cái input đầu vào của mình sẽ là 1 cái tấm ảnh 2 chiều và output của mình sẽ là cái vector thế cái sự khác biệt giữa cái mạng CNN và cái mạng NLP mạng Neural Network đó chính là nó sẽ không phải nằm ở đây Đây chính là kiến trúc cũ Chỗ mới của nó chính là ở chỗ này Đó là các lớp biến đổi là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 25,
      "start_timestamp": "0:17:33",
      "end_timestamp": "0:18:26"
    }
  },
  {
    "page_content": "nó chính là ở chỗ này Đó là các lớp biến đổi là convolution Và lớp convolution này Giúp chúng ta rút trích đặt trưng hợp hiệu quả hơn Sau khi đặt trưng đã đủ tiến tính rồi Chúng ta sẽ đưa vào mạng fully connected Tức là mạng neural network MLP của mình Phần sau chính là phần cũ Còn phần đầu sẽ là phần mới Mạng Recurrent Neural Network sẽ chuyên xử lý cho dữ liệu dạng chuỗi hoặc là có ếu tố tuần tự Ví dụ như trong lĩnh vực xử lý văn bản, chúng ta có 3 chữ là chữ Do, U và Understand Nhưng mà 3",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 26,
      "start_timestamp": "0:18:19",
      "end_timestamp": "0:19:05"
    }
  },
  {
    "page_content": "ta có 3 chữ là chữ Do, U và Understand Nhưng mà 3 chữ này nếu mà để theo thư tự này thì đây sẽ là câu hỏi Nhưng cũng là 3 chữ này nhưng chúng ta sẽ xếp lại là You Do Understand Lúc này nó không còn là câu hỏi nữa mà nó đã là một câu khẳng định Do đó nó đão thay đổi nghĩa của mình hoàn toàn Chỉ cần chúng ta thay đổi thứ tự Do đó thì cái mạng ANN này sẽ giúp chúng ta xử lý được tình huống dữ liệu có yếu tố thứ tự này Nó sẽ ngầm encode thứ tự của mình Thông qua các dấu mũi tên chuyển từ trạng thái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 27,
      "start_timestamp": "0:18:59",
      "end_timestamp": "0:19:42"
    }
  },
  {
    "page_content": "Thông qua các dấu mũi tên chuyển từ trạng thái thứ T sang thứ T cộng 1 Rồi chuyển trạng thái từ thứ T cộng 1 sang T cộng 2 Các dấu mũi tên này chính là việc encode tính tuần tự của mình Như vậy tổng kết lại đó là Mạng MLP, mạng Multi-layer Perceptron hoặc là mạng Neural Network Nó lấy cảm hứng từ cái nảo bộ của con người bao gồm là input, trọng số, độ lật, bias và output kiến trúc của mình sẽ từ lấp input qua các lớp ẩn để đến lấp output hoạt động của mô hình này sẽ bao gồm thực toán là lang",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 28,
      "start_timestamp": "0:19:38",
      "end_timestamp": "0:19:53"
    }
  },
  {
    "page_content": "động của mô hình này sẽ bao gồm thực toán là lang truyền xuôi, lang truyền thuận là forward propagation để đưa ra giá trị dự đoán từ x, chúng ta sẽ qua hàm forward để ra giá trị dự đoán y ngã và trong lang truyền thuận này thì nó sẽ có các hàm kích hoạt, là các hàm activation function để nhằm phi tiến hóa bài toán của mình và khi chúng ta tính ra được cái y ngã này rồi thì chúng ta sẽ đi so sánh cái y ngã này với giá trị thực tế để có cái hàm loss và có cái loss này rồi thì chúng ta sẽ lang",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 29,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "loss và có cái loss này rồi thì chúng ta sẽ lang truyền ngược lại để update để cập nhật trọng số của mạng Neural Network ở đây Đó là thuật toán Backpropagation để tỉ tính radian Khi chúng ta đã có được radian theo từng trọng số rồi thì chúng ta sẽ dùng thuật toán radian để xem chúng ta sẽ cập nhật lại trọng số w mới sẽ là bằng w cũ trừ cho learning rate tức là alpha nhân ra đạo hàm của hàm loss theo các biến trọng số của mô hình Mục tiêu của mình là để cho cái loss này càng dãm Thì đây chính là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 30,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "là để cho cái loss này càng dãm Thì đây chính là toàn bộ nội dung của bài mạng neuro nhân tạo",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wdIjgotVuQM",
      "filename": "wdIjgotVuQM",
      "title": "[CS114 - Chương 8] Neural Network (Part 3)",
      "chunk_id": 31,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chủ đề, học sâu, machine learning, logistic regression, imageNet. đang nổi lên như một hiện tượng, thì chúng ta quan sát hình ảnh AI landscape ở bên tay phải với các vòng tròn đồng tâm thể hiện mối quan hệ giữa AI, Machine Learning, Deep Learning và GenAI. Chúng ta hãy cùng tìm hiểu sự khác biệt cốt lõi giữa AI, Machine Learning, Deep Learning và GenAI để có cái nhìn tổng quan về bức tranh toàn cảnh của trí tài nhân tạo. AI là lĩnh vực rộng nhất bao gồm tất cả các hệ thống có khả năng mô phỏng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 0,
      "start_timestamp": "0:00:00",
      "end_timestamp": "0:01:01"
    }
  },
  {
    "page_content": "bao gồm tất cả các hệ thống có khả năng mô phỏng trí tuệ của con người. Mục tiêu của AI là tạo ra các hệ thống có thể thực hiện các tác vụ như nhận thức, suy luật, học tập và ra quyết định. Machine Learning là một nhánh con của AI và tập trung vào việc xây dựng các thuộc toán có khả năng học từ dữ liệu và tự động cải thiện hiệu xuất mà không cần phải lập trình chi tiết. chi tiết từng bước. Machine Learning chủ yếu sử dụng các mô hình toán học và thống kê để dự đoán hoặc phân loại dữ liệu dựa",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 1,
      "start_timestamp": "0:00:51",
      "end_timestamp": "0:01:41"
    }
  },
  {
    "page_content": "và thống kê để dự đoán hoặc phân loại dữ liệu dựa trên các mẫu đã học từ quá khứ. Deep Learning là một nhánh con của Machine Learning. Sử dụng các mạng neuron nhân tạo với nhiều lớp nên gọi là show để phân tích dữ liệu. Các mạng neuron này có thể học các đặc trưng phức tạp từ dữ liệu thô mà không cần phải được thiết kế thủ công. Deep learning đặc biệt hiệu quả trong việc xử lý dữ liệu phi cấu trúc như hình ảnh, âm thanh và văn bản. Gần đây chúng ta có Generative AI AI tạo sinh, là một nhánh con",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 2,
      "start_timestamp": "0:01:31",
      "end_timestamp": "0:02:08"
    }
  },
  {
    "page_content": "ta có Generative AI AI tạo sinh, là một nhánh con của AI, tập trung vào việc sử dụng các mô hình học sau để tạo ra mô hình mới, tạo ra nội dung mới bao gồm văn bản, hình ảnh, âm thanh và mã nguồn. GenAI hoạt động dựa trên các mô hình như mạng neuron tạo sinh đối kháng GAN, mô hình biến đổi Transformer, và trong đó phổ biến nhất là GBT, Generative Pre-trained Transformer. Saddle dưới đây thể hiện mối quan hệ bao hàm giữa AI, Machine Learning và Deep Learning. Lấp ngoài cùng Chí tội Nhân tạo là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 3,
      "start_timestamp": "0:02:04",
      "end_timestamp": "0:02:54"
    }
  },
  {
    "page_content": "Deep Learning. Lấp ngoài cùng Chí tội Nhân tạo là lĩnh vực lớn nhất bao gồm nhiều phương pháp giúp máy móc thực hiện các tác vụ thông minh. Chúng ta thấy một số lĩnh vực trong AI xuất hiện trong hình bao gồm các hệ thống chơi game, biểu diễn chi thức và si luận, tính toán nguyện đề, các thuật toán tìm kiếm, lập kế hoạch và mô hình nhận thức. Ở lớp giữa thì chúng ta có học máy, là một nhánh của AI, và tập trung vào việc phát triển các thuật toán có khả năng học từ dữ liệu. Thế một số thuật toán",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 4,
      "start_timestamp": "0:02:36",
      "end_timestamp": "0:03:22"
    }
  },
  {
    "page_content": "có khả năng học từ dữ liệu. Thế một số thuật toán machine learning được liệt kê trong hình, ví dụ như hội quy Tiến tính Linear Regression, hội quy Logistic Regression, Super Vector Machine, K-Mean Clustering, Random Forest, v.v. Lấp trong quần thì chúng ta có học sau, tìa là một nhánh con của Machine Learning, sử dụng mạng neuron nhân tạo để học từ dữ liệu. Một số mô hình học sau ví dụ như MLP, Multi-layer Perceptron, CNN, Convolutional Neural Network, ANN, Recurrent Neural Network, LSTM, Long",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 5,
      "start_timestamp": "0:03:20",
      "end_timestamp": "0:03:58"
    }
  },
  {
    "page_content": "ANN, Recurrent Neural Network, LSTM, Long Short Term Memory, GAN, Generative Adversarial Networks, Autoencoder, v.v. Thì ý nghĩa của hình ảnh này đó là AI là một lĩnh vực rất rộng, bao gồm machine learning, deep learning và nhiều phương pháp khác. Machine learning nhắc lại là một phần của AI và sử dụng các thuộc toán học từ dữ liệu để đưa ra dự đoán. Deep learning là một phần của machine learning sử dụng mạng neuron sâu để xử lý các dữ liệu phức tạp. Các thuộc toán machine learning truyền thống",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 6,
      "start_timestamp": "0:03:55",
      "end_timestamp": "0:04:31"
    }
  },
  {
    "page_content": "tạp. Các thuộc toán machine learning truyền thống và deep learning đều được liệt kê để giúp phân biệt các phương pháp trong các tử lĩnh vực. Chúng ta thấy là dữ liệu lớn đang thay đổi cách chúng ta tiếp cận về học máy. Hình ảnh sau sẽ minh họa sự khác biệt giữa học máy truyền thống và học sau trong xử lý dữ liệu lớn. Các mô hình học máy truyền thống sẽ thường bị giới hạn về hiệu sức khi lượt dữ liệu tăng lên do chúng không thể tận dụng được hiệu quả của dữ liệu lớn. Nếu chúng ta nhìn cái hình,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 7,
      "start_timestamp": "0:04:28",
      "end_timestamp": "0:05:02"
    }
  },
  {
    "page_content": "quả của dữ liệu lớn. Nếu chúng ta nhìn cái hình, chúng ta sẽ thấy tới mức nào đó là hiệu sức sẽ đi ngang. Ngược lại thì Deep Learning có khả năng tiếp tục cải thiện hiệu sức khi dữ liệu tăng và giúp nó trở thành lựa chọn ưu việc trong các bài toán yêu cầu xử lý ngôn ngữ ờ yêu cầu xử lý dừng lực liệu lớn như thị giác máy tính, xử lý ngôn ngữ tự nhiên và nhiều lĩnh vực khác thì chúng ta sẽ thấy cái hiệu sức của performance của các tập tán Deep Learning thì nó càng ngày càng gia tăng khi mà cái dữ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 8,
      "start_timestamp": "0:04:55",
      "end_timestamp": "0:05:38"
    }
  },
  {
    "page_content": "thì nó càng ngày càng gia tăng khi mà cái dữ liệu càng ngày càng lớn, càng gia tăng Học máy không còn là khoa học viễn tưởng nữa và nó đang hiện diện trong cuộc sống hàng ngày của chúng ta, từ những gợi ý phim ảnh đến chẳng đoán y tế. Chúng ta hãy cùng tìm hiểu tại sao học máy lại quan trọng như vậy. Thứ nhất đó là khả năng sử liệu lớn, dữ liệu lớn để giúp đưa ra quyết định. thì chúng ta đang sống trong một thời đại bùng nổ thông tin với lượng dữ liệu khổng lồ được tạo ra mỗi ngày. Thế học máy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 9,
      "start_timestamp": "0:05:33",
      "end_timestamp": "0:06:05"
    }
  },
  {
    "page_content": "liệu khổng lồ được tạo ra mỗi ngày. Thế học máy đã cung cấp các công cụ và kỹ thuật để phân tích, xử lý và trích xuất thông tin hữu ích từ khối lượng dữ liệu này. Điều mà con người khó có thể làm được một cách hiệu quả. Thứ hai là tự động hóa và tối ưu hóa quy trình. Học máy giúp tự động hóa các quy trình phức tạp, được lập đi lập lại, giảm thiểu nhu cầu về sự can thiệp của con người. Thứ 3 là thúc đẩy nghiên cứu khoa học. Học máy đã trở thành một công cụ quan trọng đang được ứng dụng rộng rãi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 10,
      "start_timestamp": "0:06:01",
      "end_timestamp": "0:06:35"
    }
  },
  {
    "page_content": "công cụ quan trọng đang được ứng dụng rộng rãi trong nghiên cứu khoa học từ phân tích dữ liệu gen đến khám phá vũ trụ. Học máy giúp các nhà khoa học xử lý dữ liệu phức tạp và tìm ra các khám phá mới. Chúng ta thấy là các giải thưởng Nobel năm 2024 cũng vinh danh các nhà khoa học các công trình liên quan tới các vụ tài liệu, các vụ tài liệu, các vụ tài liệu, các vụ tài liệu. hoặc các công trình liên quan tới học máy. Phát triển trí tuệ nhân tạo, học máy là nền tảng quan trọng để xây dựng các hệ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 11,
      "start_timestamp": "0:06:31",
      "end_timestamp": "0:07:10"
    }
  },
  {
    "page_content": "học máy là nền tảng quan trọng để xây dựng các hệ thống trí tuệ nhân tạo đóng vai trò lớn trong các lĩnh vực như robot tự hành, chatbot và trợ lý ảo. Điều này mở ra một kỷ nguyên mới cho tương tác giữa con người và máy móc. Cá nhân hóa, học máy được sử dụng để cá nhân hóa trải nghiệm người dùng trong nhiều lĩnh vực từ quảng cáo trực tuyến, đề xuất sản phẩm đến giáo dục và chăm sóc sức khỏe. Việc này giúp đáp ứng tốt hơn nhu cầu và sở thích của thần cá nhân. Tạo lợi thế cạnh tranh trong kinh",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 12,
      "start_timestamp": "0:07:07",
      "end_timestamp": "0:07:37"
    }
  },
  {
    "page_content": "thần cá nhân. Tạo lợi thế cạnh tranh trong kinh doanh, các công ty sử dụng học máy để tối ưu hóa chuỗi cung ứng, dự báo nhu cầu thị trường và cải thiện chiến lược chiếp thị. Điều này giúp họ xây dựng lợi thế cạnh tranh so với các đối thủ không sử dụng công nghệ này. Các ứng dụng của học máy thì rất là nhiều và ở đây chỉ liệt kê một số ứng dụng điển hình. Thứ nhất là nhận dạng hình ảnh thì học máy được sử dụng để nhận dạng và phân loại hình ảnh. Ví dụ như nhận dạng khuôn mặt, nhận dạng biển số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 13,
      "start_timestamp": "0:07:30",
      "end_timestamp": "0:08:10"
    }
  },
  {
    "page_content": "Ví dụ như nhận dạng khuôn mặt, nhận dạng biển số xe, xử lý ngôn ngữ tự nhiên. Học máy được sử dụng để xử lý và phân tích về ngôn ngữ tự nhiên, chẳng hạn như dịch thực tự động, phân tích cảm xúc. Dự đoán và phân tích dữ liệu thì học máy được sử dụng để dự đoán và phân tích dữ liệu. Chẳng hạn như dự đoán giá cổ phiếu, phân tích hành vi của khách hàng. Một ứng dụng khác là trợ lý ảo, học máy được sử dụng để tạo ra các trợ lý ảo, chẳng hạn như là Siri hay Google Assistant. Lái xe tự động, học máy",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 14,
      "start_timestamp": "0:08:06",
      "end_timestamp": "0:08:45"
    }
  },
  {
    "page_content": "hay Google Assistant. Lái xe tự động, học máy được sử dụng để tạo ra các hệ thống lái xe tự động, phát hiện gian lật, học máy được sử dụng để phát hiện gian lật, chẳng hạn như phát hiện gian lật thái tiến dục. Trong khuyến nghị sản phẩm thì học máy được sử dụng để khuyến nghị sản phẩm, Vì vậy, tổng tắc văn bản có thể tự động viết bài báo, tóm tắc văn bản, tạo nội dung blog và hỗ trợ lập trình bằng cách gợi ý mả hoặc kiểm tra lỗi trong code của chúng ta. Trong hình ảnh, tổng tắc văn bản có thể",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 15,
      "start_timestamp": "0:08:39",
      "end_timestamp": "0:09:18"
    }
  },
  {
    "page_content": "chúng ta. Trong hình ảnh, tổng tắc văn bản có thể tự động viết bài báo, tóm tắc văn bản, tạo nội dung blog và hỗ trợ lập trình bằng cách gợi ý mả hoặc kiểm tra lỗi trong code của chúng ta. để gửi ý mả hoặc kiểm tra lỗi trong code của chúng ta. Trong hình ảnh, lĩnh vực hình ảnh và video, AI có thể tạo hình ảnh từ mô tả văn bản, hỗ trợ, hỗ trợ chỉnh sửa ảnh chuyên sâu, hoặc thậm chí dựng video tự động dựa trên kịch bản cho trước. Về âm thanh và âm nhạc, AI có thể sáng tác nhạc theo phong cách",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 16,
      "start_timestamp": "0:09:13",
      "end_timestamp": "0:09:46"
    }
  },
  {
    "page_content": "âm nhạc, AI có thể sáng tác nhạc theo phong cách mong muốn, tạo giọng nói nhân tạo với độ chân thực cao và hỗ trợ lòng tiếng tự động, giúp tiết kiệp thời gian và chi phí sản xuất nội dung đa phương tiện. Trong giao tiếp thì AI tạo ra các chatbot thông minh có khả năng đối thoại tự nhiên hơn, hỗ trợ khách hàng, làm trợ lý ảo hoặc đón vai nhân vật trong các game tương tác. Trong doanh nghiệp thì AI giúp tối ưu hóa quy trình làm việc bằng cách tự động hóa việc tạo các báo cáo, phân tích dữ lệ kinh",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 17,
      "start_timestamp": "0:09:40",
      "end_timestamp": "0:10:18"
    }
  },
  {
    "page_content": "hóa việc tạo các báo cáo, phân tích dữ lệ kinh doanh và hỗ trợ viết email hay tài liệu một cách rất là nhanh chóng. Học máy đang định hình tương lai, nhưng tương lai của học máy sẽ là sao? Chúng ta sẽ thấy rằng nó sẽ có các ý như sau. Thứ nhất là phát triển trí tệ nhân tạo tổng quốc AGI. Các nhà nghiên cứu đang hướng đến việc phát triển AI có khả năng học và hiểu nhiều lĩnh vực khác nhau, giống như con ngựa. Nếu đạt được AGI thì AI sẽ có thể thực hiện nhiều nhiệm vụ phức tạp mà hiện tại chỉ có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 18,
      "start_timestamp": "0:10:13",
      "end_timestamp": "0:10:49"
    }
  },
  {
    "page_content": "hiện nhiều nhiệm vụ phức tạp mà hiện tại chỉ có con người làm được. Học máy cũng sẽ được ứng dụng rộng rãi trong nhiều lĩnh vực, như trong y tế, trận đoán bệ, cá nhân hóa điều trị, tài chính, phát hiện gian lật, dự đoán thị trường và nhiều ngành công nghiệp khác. Các găng nghiệp đang tận dụng AI để tối ưu hóa quy trình, giảm chi phí và nâng cao hiệu suất. Một vấn đề khác đó là AI có đạo đức và AI có thể giải thích. Xua hướng phát triển AI minh bạch có thể giải thích và tránh thiên vị đang được",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 19,
      "start_timestamp": "0:10:44",
      "end_timestamp": "0:11:22"
    }
  },
  {
    "page_content": "có thể giải thích và tránh thiên vị đang được chủ trọng. Các tiêu chuẩn và quy định về đạo đức AI ngay càng chặt chẽ nhằm đảm bảo AI được sử dụng một cách có chất nhiệt. Học sâu và AI lượng tử, học sâu tiếp tục phát triển và giúp tăng cường khả năng xử lý dị liệu hình ảnh, âm thanh và ngôn ngữ tự nhiên. Và AI lượng tử Quantum AI có thể đẩy nhanh việc giải quyết các bài tán phức tạp mà máy tính truyền thống không thể xử lý hiệu quả. Đối với cơ hội nghệ nghiệp thì nhu cầu nhân lực tăng cao. Nhu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 20,
      "start_timestamp": "0:11:13",
      "end_timestamp": "0:11:53"
    }
  },
  {
    "page_content": "nghệ nghiệp thì nhu cầu nhân lực tăng cao. Nhu cầu về chuyên gia AI và học máy đang tăng mạnh trong nhiều ngành nghệ tạo ra nhiều cơ hội việc làm hấp dẫn. Đa dạng vị trí công việc sinh viên có thể lựa chọn nhiều vị trí khác nhau từ kỹ sư học máy, nhà khoa học dữ liệu đến chuyên gia AI trong các lịch vực cụ thể, ví dụ như y tế, tài chính. Đối với khởi nghiệp và đội mới thì học máy đã tạo điều kiện cho việc khởi nghiệp và phát triển các ứng dụng AI mới và mang lại giá trị cho xã hội. Học máy cũng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 21,
      "start_timestamp": "0:11:49",
      "end_timestamp": "0:12:06"
    }
  },
  {
    "page_content": "mới và mang lại giá trị cho xã hội. Học máy cũng là một lĩnh vực nghiên cứu hấp dẫn, với nhiều cơ hội tham gia các dự án AI tiên tiến tại các trường đại học và viện nghiên cứu. Kỹ năng quan trọng cho sinh viên thì cần phải trang bị kiến thức về lập trình, tán học, thống kê, cũng như kỹ năng mềm như giao tiếp và làm việc nhóm. Bên cạnh đó là học tập suốt đời, lĩnh vực AI liên tục phát triển và đòi hỏi người làm việc phải liên tục cập nhật các kiến thức và kỹ năng để theo kiếp xu hướng. Hứu...",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 22,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "thức và kỹ năng để theo kiếp xu hướng. Hứu... Thôi xong, nghỉ đã, mai làm tiếp. 70 sờ lai 40 phút. Hãy subscribe cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video hấp dẫn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=Wi3K4XkzqTA",
      "filename": "Wi3K4XkzqTA",
      "title": "[CS114 - Chương 2] Lập trình truyền thống & Học máy (Part 2)",
      "chunk_id": 23,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chào mừng các bạn đến với môn học CS114 học máy. Và bài hôm nay thì chúng ta sẽ đến với một nội dung rất là thú vị đó chính là hệ thống khuyến nghị hay là recommendation system. Thì nếu mà nói một trong những cái ứng dụng học máy được sử dụng lâu đời và có cái tầm ảnh hưởng à rộng nhất thì đó chính là hệ thống khí nghị. Tại vì ngay từ khi thương mại điện tử ra đời từ cách đây hơn 20 năm thì đều tất cả các cái hệ thống ờ thương mại điện tử này đều có sử dụng một cái hệ thống khí nghị. thì tùy vào",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:56"
    }
  },
  {
    "page_content": "có sử dụng một cái hệ thống khí nghị. thì tùy vào cái mức độ quy mô và tính chất của các cái hệ thống này mà chúng ta có thể sử dụng các cái thuật toán khuyến nghị khác nhau. Nhưng mà tóm lại đó là à hệ thống khuyến nghị đó là một trong những cái ứng dụng của máy học được sử dụng từ rất là lâu và tầm ảnh hưởng rất là sâu rộng trong xã hội. Và nói về hệ thống khuyến nghị thì đây là một cái hệ thống để mà giúp cho chúng ta có thể dự đoán được cái sở thích hoặc là cái dự đoán được cái hành vi của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 1,
      "start_timestamp": "0:00:49",
      "end_timestamp": "0:01:31"
    }
  },
  {
    "page_content": "sở thích hoặc là cái dự đoán được cái hành vi của người dùng dựa trên những cái dữ liệu à mà chúng ta đã có. Dữ liệu này thì nó có thể đến từ cái dữ liệu quá khứ của à cái hành vi của người dùng hoặc là của những cái cộng đồng có cái mối quan hệ gần gũi với cái người dùng này. Từ đó để đưa ra những cái gợi ý sản phẩm nội dung phù hợp thì đây chính là cái tính chất quan trọng đó là phải phù hợp với từng cái người dùng của mình. Thì nếu mà nói về ứng dụng trong thực tế thì nó có rất nhiều những",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 2,
      "start_timestamp": "0:01:25",
      "end_timestamp": "0:02:03"
    }
  },
  {
    "page_content": "ứng dụng trong thực tế thì nó có rất nhiều những cái ví dụ chúng ta có thể kể đến. Ví dụ như nói về thương mại điện tử thì chúng ta sẽ có trang web là Amazon hoặc là ở Việt Nam thì có Tiki đó. Thì à cái trang thương mại điện tử này thì nó sẽ gửi cho à người sử dụng à cái sản phẩm nào mà họ nghĩ rằng là họ sẽ quan tâm và họ có khả năng cao là sẽ chốt đơn để mà mua cái sản phẩm đó. Rồi trong cái à ví dụ khác đó chính là Netflix thì đây là một cái à dịch vụ cho phép là người ta có thể xem phim à",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 3,
      "start_timestamp": "0:01:56",
      "end_timestamp": "0:02:45"
    }
  },
  {
    "page_content": "à dịch vụ cho phép là người ta có thể xem phim à tại nhà và nó sẽ gợi ý những cái phim mà người ta cái hệ thống này nó nghĩ rằng là người dùng họ sẽ thích cái sản phẩm, thích cái bộ phim của mình. Rồi Spotify đó là gợi ý nhạc. TikTok là gợi đi gợi ý video thì à cái cơ chế của nó đó là với người dùng thì khi mà họ tiến hành họ mua một cái sản phẩm họ mua một cái sản phẩm nào đó thì cái hệ thống nó sẽ tìm một cái sản phẩm tương tự hoặc một cái sản phẩm mà họ nghĩ rằng là cái người dùng này họ có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 4,
      "start_timestamp": "0:02:37",
      "end_timestamp": "0:03:24"
    }
  },
  {
    "page_content": "phẩm mà họ nghĩ rằng là cái người dùng này họ có thể quan tâm để từ đó đi gợi ý cho cho người dùng. Đó thì cái tử chữ similar này á thì nó có rất nhiều cái cách hiểu khác nhau. Đó thì cái similar này có thể là một cái sản phẩm đi kèm với cái sản phẩm này hoặc có thể là một cái sản phẩm mà kế tiếp của sản phẩm này. Ví dụ như khi chúng ta mua một cái à một chiếc áo thì cái similar này nó có thể là cái quần mà đi kèm nó cái quần đi kèm à để mà có thể phối được với cái bộ đồ đó cho nó thời trang",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 5,
      "start_timestamp": "0:03:19",
      "end_timestamp": "0:04:05"
    }
  },
  {
    "page_content": "thể phối được với cái bộ đồ đó cho nó thời trang hơn. đó hoặc là các cái phụ kiện đi kèm với cái chiếc áo đó thì chúng ta sẽ tiến hành à phân loại các cái hệ thống à khuyến nghị thì à chúng ta sẽ có những cái kiểu à phương thức để mà à đề xuất cho cái người dùng. Hình thức đầu tiên đó là gợi ý thủ công. hay nói cách khác đó là dùng cái run bay. Tức là nếu người dùng mà mua sản phẩm A thì chúng ta sẽ gợi ý cái sản phẩm B. Kiểu như vậy thì đây là cái kiểu gợi ý thủ công. Còn hai á là chúng ta sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 6,
      "start_timestamp": "0:03:59",
      "end_timestamp": "0:04:48"
    }
  },
  {
    "page_content": "cái kiểu gợi ý thủ công. Còn hai á là chúng ta sẽ tổng hợp một cách đơn giản. Tổng cộng một cách đơn giản. Ví dụ như top 10 những cái sản phẩm hoặc là danh sách những cái sản phẩm mà phổ biến nhất tức là những cái hot trend trên mạng xã hội hoặc là trên cái kênh thương mại của mình. À nếu chúng ta thấy là 10 cái sản phẩm mà người dùng người ta mua nhiều nhất trong khoảng thời gian là một tuần gần đây thì mình sẽ gợi ý đi những cái sản phẩm đó. Hy vọng rằng là nó sẽ có một cái xu hướng mà được",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 7,
      "start_timestamp": "0:04:44",
      "end_timestamp": "0:05:20"
    }
  },
  {
    "page_content": "Hy vọng rằng là nó sẽ có một cái xu hướng mà được người dùng người ta quan tâm, cộng đồng người dùng người ta quan tâm một cách rộng rãi. Thì nhiều người thích thì hy vọng rằng là cái người tiếp theo mà mua hàng cũng sẽ thích cái sản phẩm này. Và một cái dạng thứ ba đó chính là cá nhân hóa cho từng người. à cá nhân hóa cho từng người. Tức là à chúng ta sẽ không có gợi ý chung chung cho một cộng đồng, một nhóm cộng đồng hoặc là gợi ý theo kiểu là nếu thì như thế này thì nó rất là thủ công mà",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 8,
      "start_timestamp": "0:05:16",
      "end_timestamp": "0:05:56"
    }
  },
  {
    "page_content": "là nếu thì như thế này thì nó rất là thủ công mà chúng ta sẽ đi theo từng cái người sử dụng cụ thể. Mình sẽ biết được những cái đặc trưng cho cái người à cho cái người đó là gì để từ đó mình sẽ trích xuất và mình lấy ra những cái sản phẩm phù hợp cho cái người đó. Thì đây có lẽ là một trong những cái vấn đề lớn nhất mà chúng ta cần phải giải trong các cái hệ thống khuyến nghị đó là cá nhân hóa khuyến nghị cho từng người dùng. Thì chúng ta sẽ tiến hành là mô hình hóa cái bài toán này. Đặt à tập",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 9,
      "start_timestamp": "0:05:50",
      "end_timestamp": "0:06:45"
    }
  },
  {
    "page_content": "hành là mô hình hóa cái bài toán này. Đặt à tập khách hàng của chúng ta là X và cái sản phẩm của mình là S. Và cái hàm đánh giá U à hàm đánh giá của chúng ta là U thì U ở đây sẽ là từ cái tập X à đến cái tập S. Từ tập X nhân với lại X nhân với S tức là người dùng và sản phẩm. người dùng và sản phẩm thì nó sẽ tạo ra à một cái rating. R là rating. R là rating tức là cái mức độ mà ưa thích của người dùng đối với cái sản phẩm này. Thì R này nó có thể có rất nhiều cái cách thức khác nhau. Ví dụ như",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 10,
      "start_timestamp": "0:06:40",
      "end_timestamp": "0:07:18"
    }
  },
  {
    "page_content": "có rất nhiều cái cách thức khác nhau. Ví dụ như R là từ 0 cho đến 5 sao. Thì đây là một cái than đo phổ biến trên mạng xã hội khi mà đánh giá cái sở thích của một người đối với một cái sản phẩm. Nó cũng có thể là một cái con số thực cho đến 1. Không có nghĩa là không hề quan tâm à không thích thú. Và một đó là rất là thích thú đến cái sản phẩm đó. Thì cái vấn đề chúng ta cần phải giải quyết đó chính là làm sao chúng ta có thể đi thu thập được cái rating của từng user và item. Vì hai cái không",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 11,
      "start_timestamp": "0:07:13",
      "end_timestamp": "0:07:57"
    }
  },
  {
    "page_content": "rating của từng user và item. Vì hai cái không gian tập khách hàng và cái sản phẩm này nó rất là lớn. X nhân với S này là rất là lớn. Rồi và chúng ta sẽ phải dự đoán những cái rating mà chưa biết. Tức là chúng ta nhìn vào cái bảng đây, chúng ta thấy là đại đa số những cái dữ liệu của chúng ta là chúng ta thu thập được thì nó rất là ít. Ví dụ như cái cô này cậu thích cái bộ phim này là chấm bốn điểm, cái anh này anh không thích cái bộ phim này thì chấm là một điểm. Nhưng mà đại đa số những cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 12,
      "start_timestamp": "0:07:53",
      "end_timestamp": "0:08:33"
    }
  },
  {
    "page_content": "chấm là một điểm. Nhưng mà đại đa số những cái chỗ khác sẽ là dấu chấm hỏi. Tức là chúng ta chưa biết. Thì hay nói cách khác, đây là một cái ma trận thưa. Đây là một cái ma trận thưa. Và nhiệm vụ của chúng ta đó là làm sao có thể dự đoán được những cái dấu chấm hỏi này. Tức là dự đoán từng cái user này có thích cái sản phẩm này không và nếu thích thì chấm là bao nhiêu điểm. Rồi sau đó chúng ta sẽ đi đánh giá coi cái việc mà chúng ta gán nhãn vào à đánh cái cái cái rating vào những cái dấu chấm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 13,
      "start_timestamp": "0:08:25",
      "end_timestamp": "0:09:04"
    }
  },
  {
    "page_content": "à đánh cái cái cái rating vào những cái dấu chấm hỏi này thì cái độ chính xác của nó là bao nhiêu? Thì đó chính là những cái vấn đề mà chúng ta cần phải giải quyết. Và đầu tiên đó là thu thập cái rating. Thì chúng ta sẽ có hai cái à cách thu thập một cách gọi là à tường minh là explicit và không tường minh tức là implicit. Thì đối với cái cách từ minh á là chúng ta có thể thông qua cái việc là hỏi đáp trực tiếp người dùng xem họ có thích cái sản phẩm này hay không hoặc là họ chấm điểm là bao",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 14,
      "start_timestamp": "0:09:00",
      "end_timestamp": "0:09:44"
    }
  },
  {
    "page_content": "phẩm này hay không hoặc là họ chấm điểm là bao nhiêu. Hoặc là thông qua cái hình thức gọi là crows trên cái số lượng lớn à người dùng trên mạng internet chúng ta sẽ nhờ một cái lượng lớn người dùng để đánh giá. Thì đối với cái lit feedback thì chúng ta sẽ có những cái công cụ ví dụ như th up th down tức là cái nút like trong mạng xã hội á hoặc là nút dislike trong mạng xã hội. Thì ở đây chúng ta có một cái ví dụ là có 71 cái lượt thích sản phẩm này. Hoặc là chúng ta yêu cầu người dùng đánh ờ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 15,
      "start_timestamp": "0:09:39",
      "end_timestamp": "0:10:15"
    }
  },
  {
    "page_content": "này. Hoặc là chúng ta yêu cầu người dùng đánh ờ đánh sao từ 1 cho đến 5 đó hoặc là viết cái review. Thì từ cái câu review này chúng ta sẽ thấy là nó sẽ là positive tức là tích cực hay là negative là tiêu cực. Đó. Rồi thì cái ờ cách mà đánh giá một cách tường minh này thì nó sẽ tương đối rõ ràng và cụ thể và nó sẽ có được cái à cho chúng ta một cách đầy đủ cái tập dữ liệu positive và negative. tức là người đó thích và người đó không thích sản phẩm này. Còn đối với cái phản hồi mà không tường",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 16,
      "start_timestamp": "0:10:11",
      "end_timestamp": "0:10:50"
    }
  },
  {
    "page_content": "phẩm này. Còn đối với cái phản hồi mà không tường minh implicit á thì chúng ta sẽ phải dựa trên cái hành vi của người dùng. Cái hành vi này nó có thể thông qua ví dụ như là chúng ta click vào và xem một cái bản nhạc thì tức là chúng ta đang quan tâm đến cái bản nhạc đó hoặc là một cái video đó hoặc là cái hành vi chúng ta mua một cái sản phẩm purchase một cái item tức là chúng ta rất là quan tâm và có nhu cầu bất thiết để mua cái sản phẩm đó hoặc là browsing history tức là chúng ta lịch sử",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 17,
      "start_timestamp": "0:10:43",
      "end_timestamp": "0:11:29"
    }
  },
  {
    "page_content": "hoặc là browsing history tức là chúng ta lịch sử chúng ta à click vào chúng ta xem các cái sản phẩm này xem sản phẩm này hoặc là bỏ vào giõ hàng Tức là chúng ta chưa chốt đơn mua cái vỏ hàng đó, mua cái sản phẩm đó nhưng chúng ta đưa nó vào bên trong cái cái vỏ hàng. Tức là chúng ta đã rất là quan tâm rồi nhưng mà chỉ là chờ thời điểm để mà mua thôi. Thì vấn đề tiếp theo đó là chúng ta làm sao có thể dự đoán được cái rating mà chưa biết. Thì khi mà dự đoán được các cái rating chưa biết từ những",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 18,
      "start_timestamp": "0:11:24",
      "end_timestamp": "0:11:58"
    }
  },
  {
    "page_content": "mà dự đoán được các cái rating chưa biết từ những cái rating đã có thì vấn đề gặp phải đó là cái ma trận tiện ích này à nó rất là thưa. Cái ma trận mà rating này rất là rất là thưa thì còn gọi là spark. Và hầu hết người dùng là chưa đánh giá các cái mục của mình hầu hết. Và đồng thời là nó sẽ có cái tình trạng nó gọi là con stack. Constag thì ở đây có thể là con stack user hoặc là con stack item. Constat user tức là với cái người dùng mới tức là người này lần đầu tiên tham gia vào à cái hệ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 19,
      "start_timestamp": "0:11:52",
      "end_timestamp": "0:12:43"
    }
  },
  {
    "page_content": "là người này lần đầu tiên tham gia vào à cái hệ thống của mình và mình chưa có bất cứ cái dữ liệu lịch sử nào của họ thì làm sao chúng ta có thể dự đoán được. Làm sao chúng ta có thể dự đoán được? Còn đối với cái người nào đối với những cái loại const thứ hai đó là cái item tức là cái mục hoặc là cái sản phẩm này là mới và chúng ta chưa có ai đánh giá đúng không? Những cái sản phẩm mới ra trên thị trường chưa có ai đánh giá thì làm sao chúng ta có thể dự đoán được cái nhu cầu của khách hàng họ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 20,
      "start_timestamp": "0:12:37",
      "end_timestamp": "0:13:21"
    }
  },
  {
    "page_content": "có thể dự đoán được cái nhu cầu của khách hàng họ có quan tâm đến cái sản phẩm mới này hay không. Đó thì đó là con. Thì cái khó của cái con đó chính là chúng ta không có thông tin, không có nhiều thông tin. không có nhiều thông tin và ba cách ba cái loại mô hình để giúp cho chúng ta có thể à dự đoán được cái rating chưa biết đó là dựa trên nội dung hay còn gọi là content based filtering dựa trên lọc cộng tác là collaborative filtering và mô hình nhân tố tìm ẩn hay còn gọi là latent factor model",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 21,
      "start_timestamp": "0:13:16",
      "end_timestamp": "0:13:21"
    }
  },
  {
    "page_content": "nhân tố tìm ẩn hay còn gọi là latent factor model thì đây là một cái mô hình để giúp cho chúng ta có thể tìm được một cái à vecơ tiềm ẩn. Tức là mỗi một cái user sẽ được biểu diễn bởi một cái vecơ rồi mỗi một cái item, một cái sản phẩm hoặc là một cái đối tượng cần gợi ý đó chúng ta sẽ biểu diễn bằng một cái vecơ. Và muốn biết cái user này với cái item này nó có cái mối tương đồng cao hay không thì chúng ta thực hiện cái phép tích vô hướng giữa hai cái vecơ biểu diễn này. Thì hai cái vecơ biểu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 22,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "hai cái vecơ biểu diễn này. Thì hai cái vecơ biểu diễn này thì nó sẽ được gọi là cái vecơ nằm trong cái không gian laton không gian tiềm ẩn.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=wqjn-IA5iu4",
      "filename": "wqjn-IA5iu4",
      "title": "[CS114 - Chương 6] Hệ khuyến nghị - Phần 1",
      "chunk_id": 23,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Tiếp theo thì chúng ta sẽ cùng mô hình hóa toán học cho cái trường hợp mà hồi quy đa biến. Thì đối với mô hình mà hồi quy nhiều biến thì chúng ta cũng hoàn toàn tương tự mở rộng ra từ mô hình à đơn biến. Công thức của mô hình đơn biến đó là wx + b. Thì đây chính là cái giá trị dự đoán và chúng ta sẽ mở rộng nó ra. Thay vì chúng ta chỉ có một biến x thì bây giờ chúng ta sẽ có x1 x2. Và tương ứng thì chúng ta cũng sẽ có hệ số W1 cho X1 cộng cho W2 cộng chấm chấm chấm cộng cho WN Xn với n là số đặc",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XdW4AEPxahk",
      "filename": "XdW4AEPxahk",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 2)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:01:06"
    }
  },
  {
    "page_content": "chấm chấm chấm cộng cho WN Xn với n là số đặc trưng của mình. Sau đó chúng ta cũng không quên là cộng cho cái thành phần bias B. Thì nhắc lại là tại sao phải có cái thành phần BASP này. Đó là đại diện à cho những cái đặc trưng mà chúng ta không thống kê được, chúng ta không thu thập thông tin được. Rõ ràng nếu mà nói để dự đoán một cái giá trị output thì có chắc là chúng ta cần đủ n đặc trưng này là có thể à xác định được hay không? Thì câu trả lời là không. Do đó thì chúng ta sẽ luôn có một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XdW4AEPxahk",
      "filename": "XdW4AEPxahk",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 2)",
      "chunk_id": 1,
      "start_timestamp": "0:01:01",
      "end_timestamp": "0:01:43"
    }
  },
  {
    "page_content": "lời là không. Do đó thì chúng ta sẽ luôn có một cái thành phần bias để cho mô hình của mình nó có cái tính chất tổng quát và có thể khắc phục được cái tình trạng đó là thu thập thiếu thông tin của đặc trưng. Thì khi này cái hệ thống của chúng ta cái mô hình của chúng ta sẽ ký hiệu X. Thì cái chúng ta lưu ý là cái chỉ số bên dưới là chỉ số J sẽ là thể hiện cái feature thứ mấy? Feature thứ mấy? trong cái tập n feature này và chi của mình nó sẽ chạy từ 1 cho đến 2n và n là số đặc trưng của mình.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XdW4AEPxahk",
      "filename": "XdW4AEPxahk",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 2)",
      "chunk_id": 2,
      "start_timestamp": "0:01:37",
      "end_timestamp": "0:02:28"
    }
  },
  {
    "page_content": "từ 1 cho đến 2n và n là số đặc trưng của mình. Hay nói cách khác đó là cái độ rộng của cái bảng dữ liệu của mình. Đó chính là cái độ rộng của cái bản dữ liệu của mình. Rồi à theo chiều à theo hàng thì chúng ta sẽ có cái chỉ số y ở phía trên. Còn J thì chúng ta sẽ đi từ cột là từ cột thứ nhất, thứ hai cho đến cột thứ N. Còn J à còn Y cái chỉ số ở phía trên được viết bằng à cái dấu mũ này ha. Thì cái giới này á dùng cái dấu mở ngoặc đóng ngoặc này á để hàm ý đó là đây không phải là phép mũ mà đây",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XdW4AEPxahk",
      "filename": "XdW4AEPxahk",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 2)",
      "chunk_id": 3,
      "start_timestamp": "0:02:22",
      "end_timestamp": "0:03:09"
    }
  },
  {
    "page_content": "á để hàm ý đó là đây không phải là phép mũ mà đây chỉ là một cái ký hiệu mẫu dữ liệu thứ mấy thôi. Thì y của mình sẽ là một cái dòng dữ liệu. Và đây là vecơ đặc trưng đại diện cho mẫu dữ liệu thứ y hay là còn gọi là training example thứ y. Thì nếu y = 2 thì chúng ta sẽ có cái công thức như thế này. Cái dạng biểu diện như thế này. Mẫu dữ liệu thứ hai nó là một cái vectơ n chiều bao gồm x21, x22 cho đến x2m. Thì trên cái bảng dữ liệu, mỗi một cái hàng chính sẽ là một cái vecơ. à sẽ là một cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XdW4AEPxahk",
      "filename": "XdW4AEPxahk",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 2)",
      "chunk_id": 4,
      "start_timestamp": "0:03:01",
      "end_timestamp": "0:03:59"
    }
  },
  {
    "page_content": "hàng chính sẽ là một cái vecơ. à sẽ là một cái vecơ như vậy đó. Thì đây sẽ là x y một cái hàng dữ liệu này. Rồi và bây giờ thì chúng ta sẽ qua cái công thức của à cái mô hình dự đoán đó là F của WB. Lưu ý ở đây à tham số của mình lúc này sẽ là WB. Trong đó W ở trong cái bài mà hồi quy đơn biến thì W chỉ là một giá trị scalar, là một giá trị đơn. Còn ở đây W nó sẽ là một cái vecơ nó sẽ là một vectơ và W nhân với X thì nó sẽ ra cái vế bên đây. Nhưng chúng ta cũng không quên là sẽ có một cái thành",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XdW4AEPxahk",
      "filename": "XdW4AEPxahk",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 2)",
      "chunk_id": 5,
      "start_timestamp": "0:03:52",
      "end_timestamp": "0:04:40"
    }
  },
  {
    "page_content": "chúng ta cũng không quên là sẽ có một cái thành phần bias. Chúng ta sẽ cộng bias ở phía cuối đó. Thì xy à xj ở đây chính là cái đặc trưng thứ j và wj chính là cái trọng số hay là hệ số tương ứng với lại đặc trưng thứ j cái chỉ số j đây. Rồi b là cái hệ số chặn hoặc là bias và vecơ à đặc trưng của một cái đối tượng thì nó sẽ được ký hiệu là bằng x có cái vecơ phía trên. Và tiếp theo thì chúng ta sẽ biểu diễn nó dưới dạng là một cái vecơ. Thì bình thường W của mình sẽ là bao gồm W1 rồi W2 vân",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XdW4AEPxahk",
      "filename": "XdW4AEPxahk",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 2)",
      "chunk_id": 6,
      "start_timestamp": "0:04:34",
      "end_timestamp": "0:05:08"
    }
  },
  {
    "page_content": "thường W của mình sẽ là bao gồm W1 rồi W2 vân vân. Thì chúng ta sẽ gom nó lại thành một cái vectơ W như thế này. Tương tự như vậy cho cái đặc trưng X độ vào thì nó sẽ gồm có n phần tử. Thì X của mình là một cái vectơ n phần tử.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XdW4AEPxahk",
      "filename": "XdW4AEPxahk",
      "title": "[CS114 - Chương 3] Hồi quy đa biến (Phần 2)",
      "chunk_id": 7,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Một số vấn đề khi huấn luyện với hồi quyến tính. Trước khi sang các phần tiếp theo, chúng ta hãy cùng nhắc lại khái niệm rất quan trọng trong học máy đó là Loss function hay còn gọi là hàm mất mát. Lost function là hàm dùng để đo lường mức độ sai lầm, sai lệch hay là chi phí của mô hình khi dự đoán sai so với giá trị thực tế. Nói cách khác, nó cho chúng ta biết mô hình đang dự toán đốt tốt tới đâu. Càng gần với giá trị thực tế thì loss càng nhỏ. và càng xa thì loss càng lớn. Trong quá trình huấn",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 0,
      "start_timestamp": "0:00:18",
      "end_timestamp": "0:01:02"
    }
  },
  {
    "page_content": "càng xa thì loss càng lớn. Trong quá trình huấn luyện, giá trị hàm loss không chỉ là một con số đánh giá mà nó còn đóng vai trò như mục tiêu để tối ưu hóa. Tất cả các thuật toán học có giám sát đều cố gắng tìm ra bộ tham số sao cho lost function nhỏ nhất có thể. Ngoài ra, Lost function còn rất quan trọng đối với các thuật toán tối ưu như gradient, descent. Giá trị loss và đặc biệt là đạo hàm của loss sẽ chỉ dẫn cho mô hình biết nên điều chỉnh các tham số theo hướng nào và điều chỉnh bao nhiêu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 1,
      "start_timestamp": "0:00:56",
      "end_timestamp": "0:01:39"
    }
  },
  {
    "page_content": "tham số theo hướng nào và điều chỉnh bao nhiêu để giảm sai số. Tóm lại, lot function không chỉ là thước đo cho chất lượng mô hình mà còn là kim chỉ nam giúp mô hình học hỏi từ dữ liệu. Tiêu chí chọn L function. Khi xây dựng một mô hình học máy, việc lựa chọn lot function phù hợp là điều rất quan trọng bởi vì lot function sẽ ảnh hưởng trực tiếp đến cách mô hình học từ dữ liệu. Tiêu chí đầu tiên là phải phù hợp với bài toán và đầu ra. Nếu bài toán là hồi Q thì chúng ta ưu tiên các log function",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 2,
      "start_timestamp": "0:01:34",
      "end_timestamp": "0:02:21"
    }
  },
  {
    "page_content": "là hồi Q thì chúng ta ưu tiên các log function cho dự đoán số liên tục như là MSA, MI hoặc là Uber. Nếu là phân loại classification thì chúng ta sẽ dùng các hàm như cross entropy phù hợp cho phân loại, nhị phân hoặc là đa lớp. Kế tiếp thì chúng ta cũng cần cân nhắc đặc điểm của dữ liệu. Nếu dữ liệu có nhiều điểm ngoại lai outliers, các lot function như ma hoặc là hubber sẽ giúp mô hình bớt nhạy cảm hơn với outliers, tránh bị ảnh hưởng quá nhiều bởi các cái giá trị bất thường này. Và cuối cùng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 3,
      "start_timestamp": "0:02:16",
      "end_timestamp": "0:02:58"
    }
  },
  {
    "page_content": "bởi các cái giá trị bất thường này. Và cuối cùng là tính chất toán học của Los Function cũng rất là quan trọng. Một hàm khả vi và có đạo hàm mược mà như là MSI sẽ dễ dàng cho quá trình tối ưu hóa bằng các thuật toán dựa trên gradient. Ngoài ra nếu l function là hàm lồi thì chúng ta có thể đảm bảo thuật toán tìm được nghiệm tối ưu toàn cục và không bị kẹt ở điểm cực tiểu cục bộ. Bên cạnh các ý nghĩa, yếu tố thực tiễn và toán học, Lord Fon còn có thể được lựa chọn dựa trên ý nghĩa xác suất. Trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 4,
      "start_timestamp": "0:02:51",
      "end_timestamp": "0:03:33"
    }
  },
  {
    "page_content": "được lựa chọn dựa trên ý nghĩa xác suất. Trong nhiều trường hợp, việc chọn hàm mất mát phù hợp cũng đồng nghĩa với việc chúng ta đang giả định một mô hình xác suất ngầm cho bài toán. Ví dụ như khi sử dụng MSE làm los fon chung hồi khuy tiếng tính. Điều này tương đương với việc giả định rằng phần sai số của dữ liệu tuân theo phân phối chuẩn Gumm. Khi đó tối thiểu hóa MSI cũng chính là thực hiện ước lượng hợp lý cực đại maximum light estimation cho phân phối này. Tương tự thì với bài toán phân",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 5,
      "start_timestamp": "0:03:28",
      "end_timestamp": "0:04:05"
    }
  },
  {
    "page_content": "cho phân phối này. Tương tự thì với bài toán phân loại sử dụng cross entropy loss tương ứng với giả định xác suất đầu ra phân phối theo back nully hoặc phân phối đa thức phù hợp với bản chất của các bài toán phân loại phân hoặc là đa lớp. Như vậy việc hiểu rõ ý nghĩa xác suất của từng lotion sẽ giúp chúng ta chọn lựa và xây dựng mô hình tốt hơn phù hợp với đặc thù và giả thiết của dữ liệu thực tế. Các tiêu chí đánh giá Evaluation Matrix. Sau khi đã huấn luyện mô hình thì chúng ta cần có những",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 6,
      "start_timestamp": "0:04:01",
      "end_timestamp": "0:04:43"
    }
  },
  {
    "page_content": "đã huấn luyện mô hình thì chúng ta cần có những tiêu chí cụ thể để đánh giá hiệu quả dự đoán của mô hình trên dữ liệu thực tế. Với các bài toán hồi quy, một số thước đo phổ biến nhất bao gồm đầu tiên là sai số tuyệt đối trung bình hay còn gọi là MAI, viết tắc của min absolute error. Đây là giá trị trung bình của khoảng cách tuyệt đối giữa giá trị dự đoán và giá trị thực tế. Mai giúp chúng ta hình dung được trung bình mỗi dự đoán của mô hình là bao nhiêu so với thực tế. Tiếp theo là sai số bình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 7,
      "start_timestamp": "0:04:38",
      "end_timestamp": "0:05:26"
    }
  },
  {
    "page_content": "nhiêu so với thực tế. Tiếp theo là sai số bình phương trung bình hay còn gọi là msi mean square r. MSI tính trung bình bình phương của sai số giữ dự đoán và thực tế. Sử dụng do sử dụng bình phương MSI sẽ phạt nặng hơn các trường hợp dự đoán sai lệch lớn. Và cuối cùng thì chúng ta có sai số căn bình phương trung bình AMSI root min square error là căn bậc hai của MSI. R MSI giúp diễn giải sai số dự đoán về cùng đơn vị với giá trị đầu ra nên rất là trực quan khi đánh giá. Ngoài các thước đo sai số",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 8,
      "start_timestamp": "0:05:20",
      "end_timestamp": "0:06:05"
    }
  },
  {
    "page_content": "trực quan khi đánh giá. Ngoài các thước đo sai số thì chúng ta có thể đánh giá mô hình bằng hệ số xác định ký hiệu là A square theo công thức giống như trong hình vẽ. A square là một chỉ số đánh giá mức độ mô hình giải thích được phương sai của biến mục tiêu dựa trên các biến đầu vào. Giá trị của a square dao động từ 0 đến 1. Nếu a squ càng gần 1 thì điều này có nghĩa mô hình giải thích được phần lớn sự biến động của dữ liệu thực tế, tức là mô hình càng tốt. Ngược lại, nếu a square gần 0 thì mô",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 9,
      "start_timestamp": "0:06:00",
      "end_timestamp": "0:06:37"
    }
  },
  {
    "page_content": "càng tốt. Ngược lại, nếu a square gần 0 thì mô hình gần như không giải thích được gì về dữ liệu. Và chúng ta cần cân nhắc lại mô hình hoặc dữ liệu sử dụng. Việc kết hợp nhiều tiêu chí đánh giá sẽ giúp chúng ta có cái nhìn toàn diện hơn về hiệu quả dự đoán của mô hình và từ đó đưa ra quyết định điều chỉnh hoặc cải tiến phù hợp. Khởi tạo giá trị ban đầu W và B. Trước khi bắt đầu quá trình tối ưu hóa để tìm ra đường hồi quy tốt nhất, chúng ta cần khởi tạo giá trị ban đầu cho hai tham số của mô",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 10,
      "start_timestamp": "0:06:32",
      "end_timestamp": "0:07:10"
    }
  },
  {
    "page_content": "khởi tạo giá trị ban đầu cho hai tham số của mô hình là W và Bi. Đối với hồi quy tuyến tính thì hàm mất mát của chúng ta là một hàm lồi, nghĩa là chỉ có một điểm cực tiểu toàn cục duy nhất. Vì vậy, việc chọn giá trị khởi tạo ban đầu cho W và B sẽ không ảnh hưởng đến kết quả cuối cùng nếu thuật toán tối ưu như gradient design được cấu hình với learning ray phù hợp. Tuy nhiên, khởi tạo giá trị ban đầu lý sẽ giúp tăng tốc độ hội tụ của mô hình. Nếu chúng ta khởi tạo W và B gần với giá trị tối ưu,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 11,
      "start_timestamp": "0:07:05",
      "end_timestamp": "0:07:47"
    }
  },
  {
    "page_content": "chúng ta khởi tạo W và B gần với giá trị tối ưu, thuật toán sẽ tìm ra nhiệm nhanh hơn. Trong thực tế, việc khởi tạo tất cả trọng số bằng 0 là một lựa chọn phổ biến và đối với và hiệu quả đối với hồi quy tuyến tính. Tuy nhiên, đối với mô hình phức tạp hơn như mạng neuron, hàm mất mát không còn là hàm lồi và việc khởi tạo ban đầu sẽ ảnh hưởng lến lớn đến kết quả lẫn hiệu suất của mô hình. Nhưng với hồi quyến tính, chúng ta có thể yên tâm lựa chọn bằng khách khởi tạo đơn giản này. Hyper parameter",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 12,
      "start_timestamp": "0:07:41",
      "end_timestamp": "0:08:16"
    }
  },
  {
    "page_content": "bằng khách khởi tạo đơn giản này. Hyper parameter si tham số. Tiếp theo chúng ta sẽ tìm hiểu về siêu tham số hay còn gọi là hyperparameter trong học máy. Thì khác với các tham số như W và B mà mô hình sẽ học được trong quá trình huấn luyện. Siêu tham số là các giá trị được thiết lập trước khi quá trình huấn luyện bắt đầu. Nói cách khác, chúng ta phải chọn giá trị cho các siêu tham số này trước chứ mô hình không tự động học ra được. Vai trò của các siêu tham số là điều chỉnh cách thức hoạt động",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 13,
      "start_timestamp": "0:08:13",
      "end_timestamp": "0:08:52"
    }
  },
  {
    "page_content": "siêu tham số là điều chỉnh cách thức hoạt động của quá trình huấn luyện. Chúng ảnh hưởng trực tiếp đến tốc độ hội tụ, chất lượng tối ưu và khả năng tổng quát hóa của mô hình. Vậy tại sao chúng ta cần các siêu tham số? Thực tế là với mỗi bài toán và bộ dữ liệu sẽ có những đặc điểm riêng biệt. Nếu không có một bộ siêu tham số nào nên nên không có một bộ siêu tham số nào là tối ưu cho mọi trường hợp. Vì vậy việc tin chỉnh siêu tham số là rất quan trọng để tìm ra cấu hình huấn luyện hiệu quả nhất",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 14,
      "start_timestamp": "0:08:48",
      "end_timestamp": "0:09:35"
    }
  },
  {
    "page_content": "trọng để tìm ra cấu hình huấn luyện hiệu quả nhất cho bài toán cụ thể của mình. Siêu tham số xuất hiện trong hầu hết các thuậc toán học máy, đặc biệt là với các thuật toán tối ưu hóa như gradient, design. Đối với hồi quy tiến tính thì hai siêu tham số quan trọng nhất chính là learning rate tốc độ học và số lần lập qua tập dữ liệu. Việc lựa chọn hợp lý các siêu tham số này sẽ giúp mô hình học hiệu quả, tránh hội tụ chậm hoặc là không hội tụ. Learning ray. Learning ray ký hiệu alpha là một trong",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 15,
      "start_timestamp": "0:09:30",
      "end_timestamp": "0:10:10"
    }
  },
  {
    "page_content": "ray. Learning ray ký hiệu alpha là một trong những siêu tham số quan trọng nhất. Trong quá trình huấn luyện bằng gradient des learning ray còn còn gọi là tốc độ học và ký hiệu là alpha. Learning ray quyết định kích thước bước nhảy khi mô hình cập nhật các tham số trong mỗi vòng lập. Nói cách khác, nó kiểm soát mức độ thay đổi của W và B sau mỗi lần tính toán gradient. Nếu chọn lên rray quá lớn, mô hình có thể nhảy qua điểm tối ưu khiến hàm chi phí dao động mạnh, thậm chí là không hội tục. Ngược",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 16,
      "start_timestamp": "0:10:07",
      "end_timestamp": "0:10:37"
    }
  },
  {
    "page_content": "dao động mạnh, thậm chí là không hội tục. Ngược lại, nếu lên ray quá nhỏ, mô hình sẽ cập nhật rất chậm, quá trình huấn luyện sẽ tốn nhiều thời gian và có thể kẹt ở các điểm chưa tối ưu. Vì vậy, việc lựa chọn learning ray phù hợp là cực kỳ quan trọng để đảm bảo mô hình học hiệu quả và ổn định. Ở slide này thì chúng ta có thể quan sát trực quan ảnh hưởng của learning ray đến quá trình huấn luyện. Hình phía bên trên thể hiện trường hợp learning ray quá nhỏ, các bước nhảy ngắn, mô hình tiến chậm về",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 17,
      "start_timestamp": "0:10:33",
      "end_timestamp": "0:11:14"
    }
  },
  {
    "page_content": "quá nhỏ, các bước nhảy ngắn, mô hình tiến chậm về điểm tối ưu, mất nhiều thời gian để hội tụ. Ngược lại ở hình bên dưới minh họa cho trường hợp learning ray quá lớn, các bước nhảy rất dài khiến hàm chi phí dao động mạnh và mô hình không thể hội tụ về điểm tối ưu. Qua hai trường hợp này, chúng ta càng thấy rõ tầm quan trọng của việc lựa chọn learning ray hợp lý để quá trình tối ưu hóa diễn ra nhanh chóng mà vẫn đảm bảo độ chính xác của mô hình. Vậy làm thế nào để lựa chọn một giá trị learning",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 18,
      "start_timestamp": "0:11:09",
      "end_timestamp": "0:11:47"
    }
  },
  {
    "page_content": "Vậy làm thế nào để lựa chọn một giá trị learning ray phù hợp? Thông thường thì chúng ta sẽ thử nghiệm nhiều giá trị khác nhau để quan sát tốc độ hội tụ của mô hình. Một phương pháp cổ phổ biến là thử các giá trị theo thang logris. Ví dụ như 0.1, 0.01, 0.001 hay là 0.001. Sau đó thì chúng ta sẽ theo dõi sự thay đổi của hàm chi phí qua các vòng lập. Nếu lên ray quá lớn, hàm chi phí sẽ dao động hoặc thậm chí tăng lên. Và nếu lên ray quá nhỏ, hàm chi phí sẽ giảm rất chậm. Ngoài ra trong thực tế có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 19,
      "start_timestamp": "0:11:43",
      "end_timestamp": "0:12:18"
    }
  },
  {
    "page_content": "phí sẽ giảm rất chậm. Ngoài ra trong thực tế có thể áp dụng các chiến lược nâng cao như giảm dần lên ray theo thời gian hoặc sử dụng các thuật toán tự động điều chỉnh lên ray như adam hay ams prop. Những kỹ thuật này sẽ giúp mô hình hội tụ nhanh và ổn định hơn đặc biệt là khi làm việc với các mô hình phức tạp. Tuy nhiên đối với các với hồi quy tiến tính cơ bản thì việc thử nghiệm các lựa chọn lên ray vẫn là cách tiếp cận hiệu quả nhất. Ebox. Bên cạnh lên rray, một siêu tham số quan trọng khác",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 20,
      "start_timestamp": "0:12:13",
      "end_timestamp": "0:12:49"
    }
  },
  {
    "page_content": "cạnh lên rray, một siêu tham số quan trọng khác của quá trình huấn luyện, mô hình là Ebox. Ebox được hiểu là số lần toàn bộ dữ liệu huấn luyện được đưa qua mô hình trong quá trình huấn luyện. Nói cách khác, một ebox tương ứng với một lần mà mô hình được học từ đầu đến cuối trên toàn bộ dữ liệu. Nếu số ebook quá nhỏ, mô hình có thể chưa học đủ các mẫu hình trong dữ liệu dẫn đến tình trạng underfitting. Nghĩa là mô hình quá đơn giản và hoạt động kém trên cả dữ liệu huấn luyện lần dữ liệu mới.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 21,
      "start_timestamp": "0:12:43",
      "end_timestamp": "0:13:17"
    }
  },
  {
    "page_content": "kém trên cả dữ liệu huấn luyện lần dữ liệu mới. Ngược lại, nếu số ebox quá lớn, quá trình huấn luyện sẽ tốn thời gian, đặc biệt là vứt các mô hình phức tạp hơn. Điều này còn có thể dẫn đến overfitting, tức là mô hình học thuộc lòng dữ liệu huấn luyện và hoạt động không tốt trên dữ liệu mới. Chúng ta cần lưu ý rằng Learning Ray và Epox là hai siêu tham số bổ sung cho nhau trong quá trình hấn luyện. Larin ray kiểm soát độ lớn của từng bước cập nhật tham số. Còn Ebox thì quyết định tổng số lần lặp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 22,
      "start_timestamp": "0:13:14",
      "end_timestamp": "0:13:49"
    }
  },
  {
    "page_content": "tham số. Còn Ebox thì quyết định tổng số lần lặp lại quá trình học trên toàn bộ dữ liệu. Nếu chọn learning ray nhỏ thì thường cần ebox nhiều hơn để mô hình hội thụ. Ngược lại, nếu lên ray lớn, quá trình hội thụ có thể nhanh hơn nhưng chúng ta cũng phải cẩn thận để tránh mô hình bỏ qua điểm tối ưu. Vì vậy, việc dừng huấn luyện ở một số ebox hợp lý sẽ giúp tiết kiệm thời gian và tránh nguy cơ overfitting, đồng thời đảm bảo mô hình học đủ các đặc trưng quan trọng từ dữ liệu. Bit, một số siêu tham",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 23,
      "start_timestamp": "0:13:45",
      "end_timestamp": "0:14:18"
    }
  },
  {
    "page_content": "quan trọng từ dữ liệu. Bit, một số siêu tham số nữa mà chúng ta cần quan tâm khi huấn luyện là mô hình B size hay còn gọi là kích thước lô dữ liệu. BSI chính là số lượng mẫu dữ liệu được sử dụng để tính toán và cập nhật tham số mô hình trong mỗi lần lập còn gọi là iteration. Thay vì phải đợi xử lý toàn bộ tập dữ liệu rồi mới cập nhật tham số, mô hình sẽ chia dữ liệu thành nhiều batch nhỏ hơn. Và mỗi lần xử lý xong một batch, mô hình sẽ cập nhật các tham số một lần. Ví dụ như chúng ta đặt",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 24,
      "start_timestamp": "0:14:14",
      "end_timestamp": "0:14:50"
    }
  },
  {
    "page_content": "nhật các tham số một lần. Ví dụ như chúng ta đặt backside bằng 32, nghĩa là sau khi nhìn thấy và xử lý 32 mẫu dữ liệu, mô hình sẽ thực hiện một lần cập nhật tham số. Việc lựa chọn backside phù hợp không chỉ giúp tiết kiệm tài nguyên phần cứng mà còn ảnh hưởng đến tốc độ và hiệu quả của quá trình huấn luyện mô hình. Khi làm việc với các tập dữ liệu lớn, việc sử dụng toàn bộ dữ liệu cho mỗi lần cập nhật tham số là điều không khả thi. Nhiều bộ dữ liệu hiện đại có thể chứa đến hàng triệu mẫu nếu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 25,
      "start_timestamp": "0:14:44",
      "end_timestamp": "0:15:21"
    }
  },
  {
    "page_content": "liệu hiện đại có thể chứa đến hàng triệu mẫu nếu chúng ta cố gắng tải toàn bộ dữ liệu này vô bộ nhớ RAM hoặc VRAM của GPU để tính toán gradient, máy tính sẽ không đủ tài nguyên để xử lý. Chính vì vậy mà back size ra đời để giải quyết vấn đề về giới hạn bộ nhớ. Việc chia nhỏ dữ liệu thành các BCH nhỏ giúp quá trình huấn luyện trở nên khả thi hơn ngay cả trên những máy tính có phần cứng hạn chế. Ngoài việc giải quyết vấn đề bộ nhớ backsite còn mang lại hai lợi ích quan trọng khác. Thứ nhất là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 26,
      "start_timestamp": "0:15:16",
      "end_timestamp": "0:15:54"
    }
  },
  {
    "page_content": "mang lại hai lợi ích quan trọng khác. Thứ nhất là hiệu quả tính toán. Khi cập nhật tham số sau mỗi BCH nhỏ, mô hình được sửa lỗi liên tục, từ đó học nhanh hơn. Chúng ta không cần phải đợi xử lý hết toàn bộ dữ liệu mới cập nhật mà có thể cải thiện mô hình sau từng BCH. Thứ hai là khả năng tổng quát hóa. Việc ước lượng gradient trên một bách nhỏ sẽ tạo ra một chút nhiễu giúp mô hình không bị kẹt ở các điểm cực tiểu cục bộ và có khả năng tìm ra nghiệp tốt hơn. Điều này giúp mô hình hoạt động hiệu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 27,
      "start_timestamp": "0:15:48",
      "end_timestamp": "0:16:28"
    }
  },
  {
    "page_content": "tốt hơn. Điều này giúp mô hình hoạt động hiệu quả hơn trên dữ liệu mới chưa từng thấy trong quá trình huấn luyện. Siz vào. Khi sử dụng backside, chúng ta sẽ thường xuyên gặp cái niệm itteration trong quá trình huấn luyện. Iteration là một lần cập nhật tham số của mô hình tương ứng với việc xử lý một batch dữ liệu. Số iteration trong một ibox sẽ được tính bằng tổng số mẫu dữ liệu chia cho backside. Ví dụ chúng ta có 1000 mẫu và backside là 100 thì mỗi sẽ có 10 iteration. Back size càng nhỏ thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 28,
      "start_timestamp": "0:16:22",
      "end_timestamp": "0:16:56"
    }
  },
  {
    "page_content": "mỗi sẽ có 10 iteration. Back size càng nhỏ thì số iteration trong một ibox càng lớn. đồng nghĩa với việc mô hình sẽ được cập nhật nhiều lần hơn trong mỗi lần lập qua dực liệu dữ liệu. Ngược lại, nếu b size lớn số iteration trong mỗi sẽ xd, tức là mô hình được cập nhật ít lần hơn trên mỗi lường duyệt qua tập dữ liệu. Vậy khi nào nên chọn backsiz nhỏ? Ví dụ như 16, 32 hoặc là 64. Đầu tiên, nếu máy tính có tài nguyên bộ nhớ hạn chế, back size nhỏ sẽ phù hợp hơn. Ngoài ra, với các tập dữ liệu nhỏ,",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 29,
      "start_timestamp": "0:16:52",
      "end_timestamp": "0:17:26"
    }
  },
  {
    "page_content": "phù hợp hơn. Ngoài ra, với các tập dữ liệu nhỏ, back size nhỏ sẽ giúp mô hình có nhiều lần cập nhật hơn. từ đó học hiệu quả hơn. Đặc biệt khi cần khả năng tổng quát hóa tốt, website nhỏ sẽ tạo ra nhiều nhiễu trong quá trình cập nhật giúp mô hình tránh overfitting và hợp lệch tốt hơn trên dữ liệu mới. Ngược lại, back size lớn như 128, 256 hoặc 512 nên được dùng khi nào? Nếu bạn có GPU mạnh, nhiều bộ nhớ thì backsiz lớn sẽ tận dụng tối đa khả năng xử lý song song giúp quá trình huấn luyện nhanh",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 30,
      "start_timestamp": "0:17:21",
      "end_timestamp": "0:17:57"
    }
  },
  {
    "page_content": "xử lý song song giúp quá trình huấn luyện nhanh hơn. Ngoài ra, B size lớn cũng giúp tính toán gradient chính xác hơn, quá trình hỗi tụ sẽ ổn định và mượt mà hơn. Điều này đặc biệt hữu ích khi làm việc với tập dữ liệu lớn và cần tốc độ huấn luyện nhanh. Chọn back size ảnh hưởng sẽ đến khả năng tổng quát khóa của mô hình. Bả nhỏ thì thường giúp mô hình tổng quát hóa tốt hơn nhờ vào tính chất nhiễu trong gradient làm cho mô hình linh hoạt hơn với dữ liệu mới. Tuy nhiên việc lựa chọn website cần",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 31,
      "start_timestamp": "0:17:53",
      "end_timestamp": "0:18:27"
    }
  },
  {
    "page_content": "dữ liệu mới. Tuy nhiên việc lựa chọn website cần vẫn cần dựa vào bài toán cụ thể tài nguyên phần cứng và đặc điểm của dữ liệu. Thông thường thì chúng ta sẽ thử nghiệm với nhiều giá trị bass khác nhau để tìm ra cấu hình tối ưu nhất cho quá trình huấn luyện. Underfitting, overfitting và good fit. Trên slide này thì chúng ta có thể thấy ba biểu đồ minh họa rõ nét ba trạng thái trong huấn luyện học máy. Đó là underfitting, good fit và overfitting. Ở hình bên trái, đây là trường hợp underfitting.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 32,
      "start_timestamp": "0:18:22",
      "end_timestamp": "0:19:06"
    }
  },
  {
    "page_content": "Ở hình bên trái, đây là trường hợp underfitting. Đường màu đỏ là đường dự đoán của mô hình. Bậc một tức là hồi quyến tính đơn giản. Ta thấy mô hình không thể bám sát xu hướng của dữ liệu khiến cả sai số trên tập huấn luyện. Trend MSI và test MSI đều cao. Điều này cho thấy mô hình quá đơn giản, không đủ khả năng học được mối quan hệ thực sự trong dữ liệu. Còn mô hình ở giữa là Goodfit. Lúc này mô hình là đa thức bậc B và vừa đủ linh hoạt để bám sát xu hướng của cả dữ liệu huấn luyện và kiểm tra.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 33,
      "start_timestamp": "0:19:01",
      "end_timestamp": "0:19:14"
    }
  },
  {
    "page_content": "xu hướng của cả dữ liệu huấn luyện và kiểm tra. Đường dự đoán màu đỏ đi gần với các điểm dữ liệu và đặc biệt sai số trên cả tập huấn luyện và kiểm tra đều thấp và khá là cân bằng. Đây là địa trạng thái mà chúng ta mong muốn đạt được. Mô hình học tốt trên từ tập dữ liệu nhưng không bị phức tạp quá mức. Cuối cùng ở hình bên phải là ví dụ về overfitting với mô hình đa thức bậc 15. Đường màu đỏ uống lượng theo từng điểm dữ liệu huấn luyện kể cả các cái điểm nhiễu. Sai số trên tập huấn luyện rất",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 34,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "cái điểm nhiễu. Sai số trên tập huấn luyện rất thấp nhưng trên tập kiểm tra lại tăng đáng kể. Điều này cho thấy mô hình đã ghi nhớ quá kỹ dữ liệu huấn luyện và mất khả năng tổng quát hóa với dữ liệu mới. Qua ba hình này, chúng ta thấy rõ tầm quan trọng của việc chọn mô hình có độ phức tạp phù hợp để tránh cả underfitting lẫn overfitting và hướng đến trạng thái good fit. Chúng ta sẽ có một số câu hỏi thảo luận. Ví dụ à vai trò của tham số B là gì trong mô hình? Ví dụ tại sao không sử dụng hàm",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 35,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "gì trong mô hình? Ví dụ tại sao không sử dụng hàm đơn giản hơn như bậc 1 hoặc mi mà ta lại thường sử dụng min square error. Có những biến thể nào xoay quanh việc đo lường sai số giữa giá trị dự đoán và giá trị thực tế. Trong thực tế dữ liệu có mối quan hệ phi tuyến tính, chúng ta có thể dùng mô hình hồi quyến tính được không?",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=xptu0eq_CXQ",
      "filename": "xptu0eq_CXQ",
      "title": "[CS114 - Chương 3] Mô hình hồi quy đơn biến (Phần 2)",
      "chunk_id": 36,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Chúng ta sẽ cùng đến với câu hỏi tiếp theo đó là mô hình của mình sẽ như thế nào khi mà cái dữ liệu của mình nó bị mất cân bằng. Thế thì trong thực tế chúng ta thấy rằng là cái dữ liệu của mình không phải lúc nào nó cũng đồng đều nhau và nó rất dễ xảy ra cái hiện tượng nó gọi là bị mất cân bằng. Thì nếu như chúng ta vẽ dưới dạng là một cái sơ đồ như thế này, trong đó trục ngang chính là các cái class của mình à các cái class. Còn trục đứng sẽ là cái số mẫu. Thì với những cái nếu mà chúng ta sắp",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XRseUdxehgo",
      "filename": "XRseUdxehgo",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 1)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:54"
    }
  },
  {
    "page_content": "cái số mẫu. Thì với những cái nếu mà chúng ta sắp theo thứ tự giảm dần thì những cái class đầu tiên là những cái cột rất là lớn. đó nhưng mà càng về sau thì chúng ta thấy là cái cột của mình nó càng lúc càng giảm. Đó thì ở đây nó đã có cái sự mất cân bằng giữa những cái mẫu dữ liệu top đầu và những cái mẫu dữ liệu top dưới. Chúng ta thấy có cái sự phân biệt rất là có một cái sự lệch rất là lớn, rất là sai khác về số mẫu. Vậy thì khi chúng ta huấn luyện với những cái dữ liệu mà bị mất cân bằng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XRseUdxehgo",
      "filename": "XRseUdxehgo",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 1)",
      "chunk_id": 1,
      "start_timestamp": "0:00:50",
      "end_timestamp": "0:01:32"
    }
  },
  {
    "page_content": "luyện với những cái dữ liệu mà bị mất cân bằng như thế này thì điều gì sẽ xảy ra? Thì ở đây là một cái hình ảnh để minh họa trực quan cái mô hình của mình khi mà huấn luyện trên cái dữ liệu bị mất cân bằng. Và ở đây chúng ta chỉ xét hai mẫu à hai lớp đối tượng để cho dễ hình dung. Đầu tiên đó là các điểm màu xanh tức là những cái mẫu dữ liệu mà đa số. Còn những cái mẫu dữ liệu màu cam đó là thiểu số. Rồi. Thế thì chúng ta sẽ thấy là ở bình thường chúng ta có cái cuộc thi giống như là cuộc thi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XRseUdxehgo",
      "filename": "XRseUdxehgo",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 1)",
      "chunk_id": 2,
      "start_timestamp": "0:01:27",
      "end_timestamp": "0:02:10"
    }
  },
  {
    "page_content": "chúng ta có cái cuộc thi giống như là cuộc thi kéo co thì nó sẽ kéo về bên nào thì bên nó thắng. Nhưng ở đây chúng ta sẽ ngược lại nó không phải là kéo co mà là đẩy co. Thì bên nào đẩy bên nào mà đẩy về phía đối phương thì bên đó sẽ là chiến thắng. Thế thì ở đây chúng ta thấy cái tập đa số này á là số lượng nhiều hơn nên nó sẽ đẩy mạnh hơn dẫn đến là xu hướng là nó sẽ đẩy cái đường phân lớp về cái phía thiểu số. Đó. Và khi đó thì những cái điểm ở phía ngược lại, những cái điểm thiểu số thì do",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XRseUdxehgo",
      "filename": "XRseUdxehgo",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 1)",
      "chunk_id": 3,
      "start_timestamp": "0:02:04",
      "end_timestamp": "0:02:47"
    }
  },
  {
    "page_content": "ở phía ngược lại, những cái điểm thiểu số thì do cái lực đẩy nó không đủ mạnh đó nên cái đường của mình nó sẽ tiến về sát với lại cái biên của các cái điểm thiểu số. Vậy thì câu hỏi đó là tại sao nó lại có cái vấn đề này? thì chúng ta sẽ giải thích bằng cái công thức của cái hàm loss. Trong công thức của binary cross entropy thì chúng ta thấy nó sẽ chia ra làm hai phần và phần đầu tiên tương ứng là cho cái trường hợp là y là bằng 1. À tức là nếu nhãn dữ liệu của mình á mà bằng 1 thì chúng ta sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XRseUdxehgo",
      "filename": "XRseUdxehgo",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 1)",
      "chunk_id": 4,
      "start_timestamp": "0:02:43",
      "end_timestamp": "0:03:28"
    }
  },
  {
    "page_content": "nhãn dữ liệu của mình á mà bằng 1 thì chúng ta sẽ có cái công thức này. Tại vì sao? Tại vì với y mà bằng 1 á thì cái 1 - y của mình nó bằng 0. Đó, 1 - y nó sẽ là bằng 0 thì 0 nhân với bao nhiêu thì cũng bằng 0. Do đó cái thành phần này nó sẽ không có tác dụng à trong cái hàm loss. Thì giả sử như bây giờ chúng ta bây giờ chúng ta sẽ giả sử như là cái nhãn của cái tập màu xanh này tương ứng là y bằng 1 và nhãn cho những cái điểm màu cam tương ứng sẽ là y = 0. Và chúng ta hoàn toàn có thể lập luận",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XRseUdxehgo",
      "filename": "XRseUdxehgo",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 1)",
      "chunk_id": 5,
      "start_timestamp": "0:03:22",
      "end_timestamp": "0:04:14"
    }
  },
  {
    "page_content": "là y = 0. Và chúng ta hoàn toàn có thể lập luận một cách tương tự như vậy. À thì nếu y = 1 và y bằng 0 thì những cái thành phần mà ở vế bên tay trái khi chúng ta tính tổng các cái sai số lại ha công thức của chúng ta đó là tổng của các cái l ờ i p à li pi với y chạy từ 1 cho đến tổng số mẫu dữ liệu của mình. Ví dụ trong toàn bộ cái tập dữ liệu chúng ta có 100 mẫu đi thì nây sẽ là 100. Thế thì ờ công thức này như chúng ta đã nói đã tách ra làm hai phần đó. Nó sẽ tách ra làm hai phần. Thì những",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XRseUdxehgo",
      "filename": "XRseUdxehgo",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 1)",
      "chunk_id": 6,
      "start_timestamp": "0:04:09",
      "end_timestamp": "0:04:48"
    }
  },
  {
    "page_content": "phần đó. Nó sẽ tách ra làm hai phần. Thì những cái phần nào mà liên quan đến cái nhãn y = 1 á thì nó sẽ cộng dồn lại hết. những cái phần nào mà liên quan đến cái y = 0 á, đây là y = 0, đây là y = 1. Đó thì thành phần nào liên quan đến cái y = 0 thì nó sẽ là lấy vé bên tay phải. Tại vì với y = 0 chúng ta thế vô đây thì bên tay trái là 0 nhân với bao nhiêu cũng bằng 0. Đó. Do đó thì nó sẽ bỏ đi cái thành phần này bên tay trái đi, chỉ chừa thành phần bên tay phải thôi. Đó thì cái ý nghĩa của cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XRseUdxehgo",
      "filename": "XRseUdxehgo",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 1)",
      "chunk_id": 7,
      "start_timestamp": "0:04:43",
      "end_timestamp": "0:05:23"
    }
  },
  {
    "page_content": "bên tay phải thôi. Đó thì cái ý nghĩa của cái công thức binary cross entropy đó là chia ra cho hai tình huống. một tình huống là với nhãn y = 1 tức là bên đây và một tình huống cho cái nhãn là với bên tay trái. Thế thì với cái tổng này thì chúng ta thấy là à giả sử như cái lụp tập hợp các cái điểm màu xanh nó có chứa đa số thì khi đó chúng ta tách nó ra à cái công thức ở trên đó là sẽ là bằng trừ của tổng của y log pi cho những cái tập màu xanh tính tổng trên những cái tập màu xanh. Đó. Rồi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XRseUdxehgo",
      "filename": "XRseUdxehgo",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 1)",
      "chunk_id": 8,
      "start_timestamp": "0:05:15",
      "end_timestamp": "0:06:19"
    }
  },
  {
    "page_content": "tính tổng trên những cái tập màu xanh. Đó. Rồi thành phần thứ hai nó sẽ là tổng của 1 - y nhân với lại log của 1 - p với những cái điểm màu cam. Vậy thì cái lực lượng màu xanh rõ ràng là nó sẽ lấn lướt cái lực lượng màu cam. Do đó cái khi chúng ta huấn luyện một cái mô hình máy học để mà cực tiểu hóa cái giá trị hàm lỗi này thì nó sẽ ưu tiên nó sẽ ưu tiên để mà làm giảm cái giá trị của cái y này. Thì rõ ràng chúng ta thấy là hai cái đường xin lỗi cái hai cái tập điểm màu xanh và màu cam nó sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XRseUdxehgo",
      "filename": "XRseUdxehgo",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 1)",
      "chunk_id": 9,
      "start_timestamp": "0:06:12",
      "end_timestamp": "0:07:04"
    }
  },
  {
    "page_content": "cái hai cái tập điểm màu xanh và màu cam nó sẽ mâu thuẫn với nhau về mặt mục tiêu. À nó sẽ mâu thuẫn với nhau về mặt mục tiêu. Nếu như cái đường này á mà tiến về cái phía màu cam thì chắc chắn cái giá trị lỗi của cái tập điểm màu cam này nó sẽ lớn. Nếu cái đường này nó tiến về màu cam thì giá trị lỗi của ờ giá trị lỗi do cái đường này tạo ra đối với cái à các cái điểm màu cam nó sẽ là lớn. Và ngược lại cái giá trị lỗi do cái đường này tạo ra đối với cái ờ đối với cái phần màu xanh nó sẽ nhỏ đi.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XRseUdxehgo",
      "filename": "XRseUdxehgo",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 1)",
      "chunk_id": 10,
      "start_timestamp": "0:06:57",
      "end_timestamp": "0:07:33"
    }
  },
  {
    "page_content": "với cái ờ đối với cái phần màu xanh nó sẽ nhỏ đi. Đó. Thì tại sao lại như vậy? Tại vì cái lực lượng của màu xanh nó nhiều hơn lực lượng màu cam. Do đó nó sẽ ưu tiên cho cái tại vì chúng ta đang cực tiểu hóa cái giá trị hàm lỗi. Chúng ta muốn làm cho cái size số tổng thể là giảm xuống. Thì bây giờ bên nào mà nó đông hơn thì mình giảm bên đó thì nó sẽ có lợi hơn. Chứ còn nếu mà ví dụ như bên màu cam ít hơn mà mình cứ chăm chăm làm giảm cái độ lỗi cho những cái điểm màu cam thì bù lại là các cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XRseUdxehgo",
      "filename": "XRseUdxehgo",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 1)",
      "chunk_id": 11,
      "start_timestamp": "0:07:27",
      "end_timestamp": "0:08:17"
    }
  },
  {
    "page_content": "cho những cái điểm màu cam thì bù lại là các cái điểm màu xanh nó sẽ lớn thì tổng thể là cái lỗi của mình nó sẽ lớn. Do đó thì xét về mặt trọng số bình quân thì cái mô hình của mình là ưu tiên cho những cái tập nào mà ờ ưu tiên để mà làm giảm cái lỗi cho những cái tập nào mà có lực lượng lớn hơn. Đó. Thế thì ở đây tại sao cái công thức này khi mà cái đường thẳng này nó tiến về sát càng sát cái tầm điểm màu cam thì cái lỗi của mình nó lớn. Đó là vì chúng ta có cái công thức của cái hàm sig của",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XRseUdxehgo",
      "filename": "XRseUdxehgo",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 1)",
      "chunk_id": 12,
      "start_timestamp": "0:08:13",
      "end_timestamp": "0:08:58"
    }
  },
  {
    "page_content": "vì chúng ta có cái công thức của cái hàm sig của wx cộng b. Thì sigmo là một cái hàm mà nó có giải giá trị là từ 0 cho đến 1. Từ 0 cho đến 1. Thì khi mà cái đường thẳng này mà càng tiến về cái đường phân lớp này á mà khi càng tiến về các điểm màu cam thì như chúng ta đã trả lời trong cái câu hỏi thảo luận trước. Ý nghĩa hình học của nó đó chính là cái khoảng cách ý nghĩa hình học của cái này đó chính là cái cái phương trình của là cái tham số của cái phương trình của cái đường phân lớp. Và cái",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XRseUdxehgo",
      "filename": "XRseUdxehgo",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 1)",
      "chunk_id": 13,
      "start_timestamp": "0:08:50",
      "end_timestamp": "0:09:34"
    }
  },
  {
    "page_content": "cái phương trình của cái đường phân lớp. Và cái giá trị khi chúng ta thế vào wx + b á, chúng ta lấy một cái giá trị ở đây, lấy một cái cặp x1 và x2 ở đây á chúng ta thế vào, chúng ta thế vào cái wx + b á thì nó sẽ ra là cái khoảng cách. Đó, thì khi đường thẳng phân lớp này càng tiến về cái điểm màu cam, tức là khoảng cách càng nhỏ hay nói cách khác là càng tiến về 0. Cái này càng tiến về 0 thì cái sig của 0 thì chúng ta thấy là tại vị trí 0 nè, tại vị trí 0 ở đây thì nó sẽ là bằng 0.5 tức là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XRseUdxehgo",
      "filename": "XRseUdxehgo",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 1)",
      "chunk_id": 14,
      "start_timestamp": "0:09:30",
      "end_timestamp": "0:10:14"
    }
  },
  {
    "page_content": "tại vị trí 0 ở đây thì nó sẽ là bằng 0.5 tức là cái size số lớn. Đó là một cái size số lớn. Ngược lại đối với những cái điểm màu xanh thì chúng ta thấy cái khoảng cách của mình là lớn. thì khoảng cách của mình càng lớn, tức là nó sẽ càng tiến về tiến về phía bên đây. Đó thì giả sử như cái khoảng cách của mình là một cái con số nà nó nằm ở đây đi. Thì khi chúng ta thế vào thì sig của mình nó sẽ là bằng 1. À sigmo của mình nó sẽ là bằng 1. Còn ở khí cạnh ngược lại thì cái sigmo của của mình nó sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XRseUdxehgo",
      "filename": "XRseUdxehgo",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 1)",
      "chunk_id": 15,
      "start_timestamp": "0:10:09",
      "end_timestamp": "0:10:29"
    }
  },
  {
    "page_content": "cạnh ngược lại thì cái sigmo của của mình nó sẽ là càng tiến về sig của 0. Tức là giá trị của nó sẽ là 0.5. Còn ở đây sẽ là bằng 1. Rồi thì rõ ràng là giống như là cái cuộc thi kéo co thì bên nào chiếm cái số lượng lớn hơn thì nó sẽ ưu tiên ưu tiên hơn. Thì đó chính là cái ý nghĩa của cái việc và khi chúng ta huấn luyện trên cái dữ liệu bị mất cân bằng.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=XRseUdxehgo",
      "filename": "XRseUdxehgo",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 4 (Phần 1)",
      "chunk_id": 16,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Trong phần tiếp theo thì chúng ta sẽ cùng tìm hiểu về sự nhạy cảm của đường phân lớp hay còn gọi là boundary classifier. Nó ảnh hưởng như thế nào đến cái độ chính xác của mô hình và đặc biệt đó là khi chúng ta làm trên cái tập dữ liệu test. Thế thì ờ như chúng ta đã biết là cái đường phân lớp của mình đó nó sẽ có cái phương trình đường thẳng được tạo ra bởi cái hệ số mà chúng ta đã huấn luyện được đó là wx + b là bằng 0. Thế thì cái w chính là cái hệ số của cái mô hình cái tham số của mô hình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_BDeNgLzVtI",
      "filename": "_BDeNgLzVtI",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 5",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:00:52"
    }
  },
  {
    "page_content": "cái hệ số của cái mô hình cái tham số của mô hình sau khi chúng ta đã huấn luyện ra xong. Vậy thì cái đường phân lớp này nó sẽ có cái sự nhạy cảm như thế nào? Đó thì với cái điểm hình tròn ở đây ha thì đây là một cái tập dữ liệu test. Thì với cái đường phân lớp như thế này thì nó nói cái điểm dữ liệu test này đó là cái điểm màu xanh đó. Nhưng nếu cái mô hình của chúng ta nó học ra và nó ra cái đường phân lớp như thế này thì rõ ràng với cái cách chia như thế này thì cái điểm này nó sẽ nói đó là",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_BDeNgLzVtI",
      "filename": "_BDeNgLzVtI",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 5",
      "chunk_id": 1,
      "start_timestamp": "0:00:46",
      "end_timestamp": "0:01:34"
    }
  },
  {
    "page_content": "chia như thế này thì cái điểm này nó sẽ nói đó là màu cam. Thì đây chính là cái sự nhạy cảm rõ ràng là với cùng một cái vị trí, với cùng một cái đặc trưng là của cái điểm test này thì hai cái đường phân lớp khác nhau nó sẽ cho hai cái kết quả phân lớp khác nhau. Đối với cái đường nét Đức đó thì nó nói rằng điểm này là điểm màu xanh. Đối với cái đường nét liền này thì nó nói cái điểm này là điểm màu cam. Vậy thì rốt cuộc điểm nào ờ phân loại nào là đúng? Thì chúng ta sẽ xét cái tình huống hy hữu",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_BDeNgLzVtI",
      "filename": "_BDeNgLzVtI",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 5",
      "chunk_id": 2,
      "start_timestamp": "0:01:29",
      "end_timestamp": "0:02:11"
    }
  },
  {
    "page_content": "đúng? Thì chúng ta sẽ xét cái tình huống hy hữu tiếp theo đó là cái đường phân lớp của mình nó sẽ bị thiên lệch rồi hẳn về một phía. Lấy ví dụ như chúng ta xét một cái đường phân loại như thế này. Rồi và với cái đường phân loại này thì chúng ta xét cái một cái điểm test. Đây là cái dữ liệu test của mình. Thì rõ ràng là về mặt trực quan và về mặt cảm giác thì chúng ta có thể thấy là cái điểm test này nó nên thuộc về cái lớp màu cam. Nhưng vì cái đường phân lớp này nó đã bị ép về gần với cái phía",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_BDeNgLzVtI",
      "filename": "_BDeNgLzVtI",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 5",
      "chunk_id": 3,
      "start_timestamp": "0:02:03",
      "end_timestamp": "0:02:50"
    }
  },
  {
    "page_content": "phân lớp này nó đã bị ép về gần với cái phía các cái điểm màu cam nên khi đây à chúng ta có một cái mẫu dữ liệu mới ở đây thì nó sẽ nói rằng là đây là cái điểm màu xanh. đó vì nó tiến về quá sát so với cái biên của cái điểm màu cam nên cái mô hình của mình nghĩ rằng cái điểm test này nó sẽ là điểm màu xanh. Đó thì lẽ ra là nếu mà về mặt trực quát trực giác thì cái điểm này nó xứng đáng là điểm màu cam hơn do nó nằm gần với lại các cái tập điểm màu cam hơn. Như vậy thì giữa hai cái đường nét đứt",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_BDeNgLzVtI",
      "filename": "_BDeNgLzVtI",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 5",
      "chunk_id": 4,
      "start_timestamp": "0:02:43",
      "end_timestamp": "0:03:27"
    }
  },
  {
    "page_content": "cam hơn. Như vậy thì giữa hai cái đường nét đứt và nét liền thì chúng ta thấy là cái điểm cái đường nét đứt này nó sẽ tốt hơn. Đó. Nhưng mặt khác thì chúng ta thấy là hai cái đường phân lớp nét Đức và nét liền. Khi chúng ta ờ thử nghiệm trên cái tập dữ liệu trend thì rõ ràng là cái tập dữ liệu trend của mình á cả hai thằng đều co cho cái độ chính xác rất là cao. Tức là tập trend thì đều tốt. Nhưng mà đối với cái đường nét đứt thì khi chúng ta test thì nó sẽ cho cái kết quả phân loại tốt đối với",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_BDeNgLzVtI",
      "filename": "_BDeNgLzVtI",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 5",
      "chunk_id": 5,
      "start_timestamp": "0:03:21",
      "end_timestamp": "0:04:08"
    }
  },
  {
    "page_content": "thì nó sẽ cho cái kết quả phân loại tốt đối với cái điểm mới này. Trong khi đó cái đường à nét liền thì cái tập dữ liệu test của mình nó sẽ không tốt. Và trong lý thuyết về máy học thì trend mà tốt mà test không tốt á thì chúng ta nói cái tập dữ liệu cái mô hình của mình nó bị hiện tượng gọi là overfitting. Như vậy là cái đường phân loại này nó sẽ bị hiện tượng overfitting. Đó. Như vậy thì làm sao chúng ta có thể tạo ra được một cái đường phân loại mà nó cân bằng? à nó cân bằng được giữa các",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_BDeNgLzVtI",
      "filename": "_BDeNgLzVtI",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 5",
      "chunk_id": 6,
      "start_timestamp": "0:04:04",
      "end_timestamp": "0:04:42"
    }
  },
  {
    "page_content": "loại mà nó cân bằng? à nó cân bằng được giữa các cái tập điểm. Thì cái khái niệm cân bằng ở đây sau này thì chúng ta sẽ học về cái mô hình là SVM là support vector machine. Thì cái khái niệm cân bằng ở đây đó là nó sẽ cố gắng để cho cái biên của các cái điểm màu xanh và biên của các cái điểm màu cam thì cái điểm cái đường phân lớp né gạch này nè nó sẽ nằm ở khúc giữa để nó trung hòa nó trung dung giữa hai cái đường phân lớp này nó nằm ở khúc giữa thì đó là cái đường tốt nhất của mình thì đây sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_BDeNgLzVtI",
      "filename": "_BDeNgLzVtI",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 5",
      "chunk_id": 7,
      "start_timestamp": "0:04:37",
      "end_timestamp": "0:05:15"
    }
  },
  {
    "page_content": "thì đó là cái đường tốt nhất của mình thì đây sẽ là cái đường tốt nhất vì khi nó cân bằng được cả hai bên thì đó là cái lý thuyết của SVM. Còn đối với mạng Neuron Network hoặc là đối với logistic regression thì chúng ta không có cái cơ chế để cho cái đường boundary, cái đường phân loại của mình nó nằm ở trung dung ở giữa nhưng mà nhìn chung thì nó cũng ra được một cái kết quả tương đối tốt. Nhưng mà tốt nhất hay không thì nó chưa có đạt được. Thì đó chính là cái sự nhạy cảm của cái đường phân",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_BDeNgLzVtI",
      "filename": "_BDeNgLzVtI",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 5",
      "chunk_id": 8,
      "start_timestamp": "0:05:10",
      "end_timestamp": "0:05:35"
    }
  },
  {
    "page_content": "đó chính là cái sự nhạy cảm của cái đường phân loại đặc trưng của mình.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_BDeNgLzVtI",
      "filename": "_BDeNgLzVtI",
      "title": "[CS114 - Chương 4] Câu hỏi thảo luận 5",
      "chunk_id": 9,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "Tiếp theo thì chúng ta sẽ cùng minh họa cho cái ý tưởng của feature engineering. Thì đầu tiên chúng ta cũng sẽ import thư viện NP. Và bước đầu tiên đó là chúng ta sẽ khởi tạo các cái dữ liệu. Thế thì giả sử như cái mô hình của mình nó được tạo bởi các cái điểm ờ có cái mối quan hệ là một cái dạng đường cong và giả sử như ở đây chúng ta xét là đường cong bậc hai thì x thì chúng ta cũng sẽ lấy là np.range Range từ 0 à cục hiệu từ 1 cho đến 3 đi và bước nhảy là 0.2 2 và y thì sẽ là bằng x cộng cho",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 0,
      "start_timestamp": "0:00:13",
      "end_timestamp": "0:01:05"
    }
  },
  {
    "page_content": "bước nhảy là 0.2 2 và y thì sẽ là bằng x cộng cho x bình phương cộng cho x bình cộng cho x - 3. Ví dụ vậy. Và hệ số ở đây ví dụ như là à -4 cộng cho y à cộng cho 5 nhân x - 3. Đó. Và đương nhiên chúng ta cũng phải thêm vô một cái đại lượng nhiễu tại vì không phải lúc nào nó cũng có thể là à ra một cái gọi là lấy mẫu chúng ta lấy chính xác được mà có thể có cái size số trong cái quá trình chúng ta lấy mẫu thì dẫn đến là mô hình của mình nó có khả năng sẽ dự đoán sai. Do đó thì cái việc thêm mua",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 1,
      "start_timestamp": "0:01:00",
      "end_timestamp": "0:01:50"
    }
  },
  {
    "page_content": "năng sẽ dự đoán sai. Do đó thì cái việc thêm mua một cái đại lượng nhiễu nó cũng giúp cho chúng ta xác định xem mô hình của mình có ổn định trước các cái nhiễu này hay không. Rồi thì ở đây chúng ta sẽ lấy theo nhiễu phân phối chuẩn với tâm mi là bằng 0, standard deviation là bằng 1 và số mẫu của chúng ta thì là sẽ bằng lens của x. Rồi bây giờ chúng ta sẽ à import m plotlift chp plot splt và chúng ta sẽ show chúng ta sẽ đưa lên trên cái sơ đồ để chúng ta xem coi xi của mình nhìn nó như thế nào.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 2,
      "start_timestamp": "0:01:46",
      "end_timestamp": "0:02:53"
    }
  },
  {
    "page_content": "chúng ta xem coi xi của mình nhìn nó như thế nào. Rồi plt.S và chúng ta sẽ chạy thì à đó thì với cái x khoảng này thì nó chưa có đạt được cái à cái hình vẽ gì chúng ta mong muốn. Đó chúng ta phải lấy cái khoảng nó rộng hơn. Thì ví dụ như ở đây chúng ta lấy là từ trừ 3 cho đến 3 và bước nhãy là 0.5. 5 chúng ta nâng cái khoảng giá trị của x lên đó thì chúng ta thấy là nó đã có cái đường cong như thế này và chúng ta có thể tăng lên cái size số cho cái đường cong này bằng cách đó là nhân với 2 và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 3,
      "start_timestamp": "0:02:50",
      "end_timestamp": "0:03:31"
    }
  },
  {
    "page_content": "cái đường cong này bằng cách đó là nhân với 2 và giá trị ở đầu ra bên đây cái bên tay phải có thể tăng lên là bằng 4 đó thì nó sẽ ra cái dạng đường cong bậc hai. Rồi thì bây giờ chúng ta sẽ à nếu đúng như bình thường thì chúng ta chỉ sử dụng một cái đặc trưng X nhưng mà bây giờ chúng ta sẽ đưa thêm cái x bình phương như là một cái đặc trưng mới ha. Thì bây giờ mình sẽ có thêm một cái nữa là xỉ thừa 2 thì sẽ là bằng x mũ 2. Tuy nhiên ở đây chúng ta kiểm chứng lại thì không biết là cái x m 2 này",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 4,
      "start_timestamp": "0:03:25",
      "end_timestamp": "0:04:11"
    }
  },
  {
    "page_content": "ta kiểm chứng lại thì không biết là cái x m 2 này á là nó đang tính có đúng hay không thì chúng ta sẽ in ra x và in ra x bình phương để xem xem là cái giá trị đặc trưng mà chúng ta bình phương ở đây có đúng như những gì chúng ta kỳ vọng hay không. Thì đây chúng ta thấy là à -3 - 2,5 thì tương ứng bình phương nó sẽ ra là 9 6,25 rồi -2 thì là thành 4 đó 3 thì sẽ là thành 9. Như vậy là nó đã tính đúng. Bây giờ chúng ta sẽ khởi tạo à khởi tạo các cái à trọng số của mô hình. Thì à ở đây sẽ có bao ba",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 5,
      "start_timestamp": "0:04:06",
      "end_timestamp": "0:05:07"
    }
  },
  {
    "page_content": "à trọng số của mô hình. Thì à ở đây sẽ có bao ba bao gồm là x1 à w1 tương ứng cho x và w2 là tương ứng cho x bình phương. Thì ví dụ như ở đây chúng ta có là ờ khởi tạo là bằng 10 đi. Rồi W2 thì sẽ là bằng à -10. B là bias thì sẽ là bằng -5. Rồi thì bây giờ chúng ta sẽ huấn chúng ta sẽ cài đặt cho cái thực toán desend Y tr thì bước đầu tiên đó là chúng ta sẽ tính đạo hàm. Bước thứ hai đó là chúng ta sẽ cập nhật các cái tham số của mô hình. Và bước số ba đó là chúng ta sẽ trực quan hóa. Và cuối",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 6,
      "start_timestamp": "0:05:03",
      "end_timestamp": "0:06:02"
    }
  },
  {
    "page_content": "số ba đó là chúng ta sẽ trực quan hóa. Và cuối cùng đó là chúng ta sẽ kiểm tra cái điều kiện dừng. Rồi thì à để tính đạo hàm thì cái công thức cũng tương tự như trong slide trước về cái mô hình hồi quy đa biến. Thì ở đây chúng ta sẽ có cái công thức đó là ờ derivative của J. đúng không? À ở đây derivative theo W1 thì nó sẽ là bằng NP. Min của nó sẽ là giá trị dự đoán trừ cho giá trị thực tế. Thì giá trị dự đoán của chúng ta chính là x W1 nhân với X cộng cho W2 nhân với lại à X bình phương cộng",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 7,
      "start_timestamp": "0:05:53",
      "end_timestamp": "0:07:00"
    }
  },
  {
    "page_content": "X cộng cho W2 nhân với lại à X bình phương cộng cho và tất cả thì chúng ta sẽ trừ cho trừ cho Y. Rồi và ở đây là cái thành phần ờ vì chúng ta đang xét trên cái thành phần là W nên nó sẽ có nhân với lại cái đạo hàm của ờ của cái W1 theo cái vế này thì đạo hàm của W1 theo cái vế này thì nó sẽ là bằng X. Do đó ở đây chúng ta sẽ nhân thêm với X. Tương tự như vậy thì chúng ta sẽ có cái đạo hàm bậc hai à xin lỗi đạo hàm cho w2 thì nó sẽ là bằng w1x + w2x2 + b - y và nhân cho cái đạo hàm của cái vế",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 8,
      "start_timestamp": "0:06:49",
      "end_timestamp": "0:07:37"
    }
  },
  {
    "page_content": "+ w2x2 + b - y và nhân cho cái đạo hàm của cái vế bên trong này theo w2 thì đạo hàm của cái này theo w2 nó sẽ là bằng x2 tức là x bình. À tương tự như vậy cho bias B thì chúng ta sẽ nhân cho cái đạo hàm của cái vế bên trong này theo B. Thì chúng rõ ràng là các cái thành phần này đối với B nó là hằng số do đó đạo hàm bằng 0. Còn đạo hàm của B à theo B thì nó sẽ là bằng 1. Do đó thì chúng ta sẽ không có phải nhân ở đây. Bước tiếp theo cập nhật tham số thì rất là đơn giản đó là W1 sẽ là bằng W1",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 9,
      "start_timestamp": "0:07:31",
      "end_timestamp": "0:08:40"
    }
  },
  {
    "page_content": "số thì rất là đơn giản đó là W1 sẽ là bằng W1 trừ cho alpha nhân cho đạo hàm của W1. Tương tự như vậy cho W2 W2 và tương tự cho B. đạo hàm theo B. Sau khi chúng ta cập nhật xong thì chúng ta sẽ kiểm tra điểm dừng đó là if ABS của derivative của W1 mà bé hơn một cái ngưỡng epsilon và trị tuyệt đối của derivative của W2 bé hơn và trị tuyệt đối của derivative theo B. bé hơn silon thì chúng ta sẽ và ở đây chúng ta sẽ có thêm hai cái siêu tham số đó là alpha là bằng 0.0 à 0.01. Nếu mà nó chậm quá",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 10,
      "start_timestamp": "0:08:34",
      "end_timestamp": "0:09:23"
    }
  },
  {
    "page_content": "là alpha là bằng 0.0 à 0.01. Nếu mà nó chậm quá thì chúng ta có thể tăng cái alpha này lên và ngưỡng dừng thì là bằng 0.001. Thì đây là điều kiện dừng. Bây giờ chúng ta sẽ chạy trước để xem xem là cái W1, W2, W1 và B có tiến về các cái giá trị hệ số ở đây không. Nếu đúng thì W1 là tương ứng à với lại X. À ở đây nó thứ tự nó hơi bị ngược ha. Tức là ở đây chúng ta đang xét là x bình phương xong rồi mới đến xong rồi mới đến bias. Thì ở đây chúng ta sẽ sửa lại một chút xíu W1 thì chúng ta sẽ đi",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 11,
      "start_timestamp": "0:09:14",
      "end_timestamp": "0:10:28"
    }
  },
  {
    "page_content": "ta sẽ sửa lại một chút xíu W1 thì chúng ta sẽ đi theo cái biến x bình phương. Do đó chúng ta sẽ để là x 2. W thì đi theo với biến x và b là bias. Do đó thì tính đạo hàm của cái vế bên trong này theo W1 thì chúng ta sẽ ra là X2 tức là x bình phương rồi cộng B - Y. Đạo hàm của nguyên cái vế này theo W2 thì nó sẽ là bằng X. Do đó ở đây chúng ta sẽ sửa lại là bằng nhân với X. Rồi bây giờ chúng ta sẽ cho chạy và kiểm tra xem là W1 có tiến về -4 và W2 có tiến về 5 và byas có tiến về -3 hay không. Thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 12,
      "start_timestamp": "0:10:18",
      "end_timestamp": "0:11:23"
    }
  },
  {
    "page_content": "có tiến về 5 và byas có tiến về -3 hay không. Thì ở đây chúng ta sẽ in ra ha. Rồi đây sẽ là W2, đây sẽ là bias. Và chúng ta nên có một cái câu thông báo đó là cái tham số của mô hình sau khi tham số của mô hình mà sau khi chúng ta huấn luyện. Rồi thì à sau khi huấn luyện chúng ta thấy là W1 là nó tiến về à 0.2, W2 thì tiến về là 0.7 và B là -33. Thì có vẻ như là ở đây nó vẫn chưa có hội tụ. Thì chúng ta sẽ cùng kiểm tra một lần nữa xem cái công thức của mình cũng như là cái à cách tính đã chính",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 13,
      "start_timestamp": "0:11:14",
      "end_timestamp": "0:12:12"
    }
  },
  {
    "page_content": "của mình cũng như là cái à cách tính đã chính xác hay chưa ha. Thì đạo hàm theo ờ W1 thì nó sẽ là bằng trung bình cộng đúng không? trung bình của các cái size số này à trung bình của cái size số này nhân với lại x2 rồi à nhân với x và wa trong cái công thức của đạo hàm theo b chúng ta cũng đã nhân sai đúng không chúng ta chưa có sửa lại thì w1 phải nhân với lại x bình phương và w2 thì phải nhân với x rồi Và ở đây chúng ta tạm bỏ cái này để chúng ta xem coi thuậc toán nó chạy đúng hay không. Đó.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 14,
      "start_timestamp": "0:12:05",
      "end_timestamp": "0:13:02"
    }
  },
  {
    "page_content": "ta xem coi thuậc toán nó chạy đúng hay không. Đó. Rồi như vậy thì sau khi chạy xong chúng ta thấy là W1 thì đã tiến đến -4 đúng như ở đây. W thì tiến đến 5 đúng như ở đây và B thì tiến đến là trừ 20 2,8. Thì rõ ràng chúng ta thấy là công thức này à cái thuật toán gradient nó rất là hiệu quả và nó vẫn có thể tính đúng khi chúng ta cung cấp thêm một cái feature phù hợp. Bây giờ tiếp theo chúng ta sẽ thử trực quan hóa thì chúng ta sẽ dùng là PLT.plot của X và Y kèm theo đó là à R0 rồi plt.p à ví",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 15,
      "start_timestamp": "0:12:57",
      "end_timestamp": "0:13:46"
    }
  },
  {
    "page_content": "của X và Y kèm theo đó là à R0 rồi plt.p à ví dụ như là 0.2 đi. Rồi và đồng thời chúng ta cũng sẽ vẽ cái đường cong của mình. Thế thì cái đường cong của mình mình sẽ dựa trên x và mình sẽ tính toán theo cái công thức này. Nhưng mà chúng ta sẽ không tính theo cái à bộ trọng số round tức là cái bộ trọng số -4 5 - 3 mà chúng ta sẽ tính theo đúng cái công thức nguyên bản của mình. Đó là B. Ở đây sẽ là cộng B. Đây sẽ là W1. Rồi đây sẽ là W2 và X thì chúng ta không có phải tạo lại nữa. Chúng ta đã có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 16,
      "start_timestamp": "0:13:41",
      "end_timestamp": "0:14:37"
    }
  },
  {
    "page_content": "ta không có phải tạo lại nữa. Chúng ta đã có rồi. X và X bình phương chúng ta đã có rồi. Rồi Y chính là cái giá trị mà chúng ta cần phải vẽ. Thế thì ở đây chúng ta sẽ vẽ bằng dạng đường. Do đó chúng ta sẽ không có cái R0 ở đây. Và X thì thay vì chúng ta vẽ là X và Y thì chúng ta sẽ vẽ X và cái giá trị chúng ta dự đoán. Thì nếu mà chúng ta muốn ký hiệu thì cũng được. Đó là y predict ha. Viết tắc của chữ predict sẽ là bằng w1 nhân cho x bình cộng cho w2 nh x cộng cho b. Và ở đây thì chúng ta sẽ",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 17,
      "start_timestamp": "0:14:32",
      "end_timestamp": "0:15:34"
    }
  },
  {
    "page_content": "cho w2 nh x cộng cho b. Và ở đây thì chúng ta sẽ để là y predict. Rồi bây giờ chúng ta sẽ chạy thử cái à chương trình này ha. Đó, chúng ta thấy ban đầu cái đường cong của mình là nó cong lên như thế này đó. Sau đó thì đường cong của mình nó dành fit vào các cái tập điểm lấy mẫu đã lấy mẫu của chúng ta đó. đã dần lấy vào fit vào. Rồi thì đây là chúng ta đang minh họa cho cái tình huống đó là good fitting. Điều gì xảy ra nếu như chúng ta underfitting thì chúng ta sẽ thay cái mô hình này bằng một",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 18,
      "start_timestamp": "0:15:26",
      "end_timestamp": "0:16:29"
    }
  },
  {
    "page_content": "thì chúng ta sẽ thay cái mô hình này bằng một cái mô hình chỉ có một tham số thôi. Đó là W1 thôi. Thôi thì ở đây chúng ta vẫn cứ để là W2 ha. Nhưng mà cái mô hình của mình sẽ giảm số tham số xuống. tính đạo hàm thì đây là cái tình huống gọi là good fitting. Còn ở đây sẽ là underfitting. Thì lúc này chỉ có W1 và công thức của mình là chỉ có X thôi. w1 nh x + b - y rồi w1 nh x cộng b tr- y và ở đây sẽ là nhân với x tại vì đạo hàm của w1 à đạo hàm của cái hàm này theo w1 thì nó sẽ là bằng x rồi à",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 19,
      "start_timestamp": "0:16:25",
      "end_timestamp": "0:17:12"
    }
  },
  {
    "page_content": "của cái hàm này theo w1 thì nó sẽ là bằng x rồi à ở đây thì chúng ta sẽ không có cập nhật w2 chúng ta chỉ cập nhật w1 và b thôi. Và khi chúng ta trực quan thì chúng ta cũng sẽ à chỉ có W1 và X rồi cộng cho B pa. Rồi đạo hàm điều kiện dừng thì ở đây chúng ta sẽ không có W2 mà chỉ có W1 và B. Rồi ờ bây giờ chúng ta sẽ chạy thử. Đó thì rõ ràng là cái đường thẳng của chúng ta sẽ không thể nào mà có thể fit được vào khi mà cái đường cái phía bên trên này nó đi rớt xuống đúng không? Nó đi xuống thì",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 20,
      "start_timestamp": "0:17:05",
      "end_timestamp": "0:18:25"
    }
  },
  {
    "page_content": "này nó đi rớt xuống đúng không? Nó đi xuống thì đồng thời cái cạnh còn lại nó sẽ đi lên nó sẽ thoát ra khỏi các cái điểm bên dưới. Thì đây là cái tình huống underfitting. Nó tiến rất là chậm. Tức là nó đã gần như hội tụ rồi. Và rõ ràng là à không thể nào mà có thể dùng một cái đường thẳng để xắp xỉ được cái à các cái điểm mà dạng parabol như thế này. Rồi tiếp theo thì chúng ta sẽ cùng thử nghiệm cho cái tình huống gọi là overfitting. Thế thì ở đây chúng ta sẽ tạo ra một cái mô hình mà gồm có",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 21,
      "start_timestamp": "0:18:18",
      "end_timestamp": "0:19:26"
    }
  },
  {
    "page_content": "đây chúng ta sẽ tạo ra một cái mô hình mà gồm có bậc ví dụ như là bậc 4 thì chúng ta sẽ tạo thêm là W3 W4. Đó. Rồi mô hình của chúng ta thì ở đây là good fitting đúng không? Thì à chúng ta bỏ đi good fitting và underfitting. À rồi chúng ta chỉ để là overfitting. Và với cái công thức của overfitting thì chúng ta sẽ mượn lại cái code của good fitting chúng ta bổ sung thêm. Rồi thì bây giờ giả sử W1 sẽ là X mũ ở đây chúng ta có bao nhiêu bậc? Có bốn bậc. Do đó chúng ta sẽ bổ sung thêm à bậc 3 và",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 22,
      "start_timestamp": "0:19:20",
      "end_timestamp": "0:20:09"
    }
  },
  {
    "page_content": "bậc. Do đó chúng ta sẽ bổ sung thêm à bậc 3 và bậc 4. 3 và bậc 4. Rồi à W1, W2, W3, W4 và ở đây sẽ là W 1 nhân với lại X m 4, W2 nhân với lại X mũ 3 cộng cho W3 nhân cho X và mũ 2 cộng cho W1 nhân với à W4. Ok. Để cho nó có cái sự tương đồng thì lẽ ra chúng ta nên để W1 với X W4 với X4 đi ha. W4, W3 và W2, W1 nhân với lại X nhân với lại X1 cộng cho B. Rồi các cái công thức ở dưới đây chúng ta không nên tái sử dụng lại tại vì à nó rất là phức tạp nên chúng ta nên copy từ công thức ở phía trên",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 23,
      "start_timestamp": "0:20:02",
      "end_timestamp": "0:20:56"
    }
  },
  {
    "page_content": "nên chúng ta nên copy từ công thức ở phía trên xuống. Thế thì W1 ờ rồi thì chúng ta sẽ có cái sai số giữa giá trị dự đoán và giá trị thực tế ở đây và nhân với lại cái đạo hàm của cái vế bên trong theo W1 thì W1 nó sẽ là đạo hàm của nó sẽ là X. Do đó ở đây chúng ta sẽ sửa lại là X. Rồi tương tự như vậy chúng ta sẽ có cái công thức cho x2 W2 W3 và W4 và B. Thì đối với W2 thì khi chúng ta lấy cái phần đạo hàm của cái vế bên trong này theo W2 thì nó sẽ ra là x bình phương. Rồi cứ như vậy tăng lên x",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 24,
      "start_timestamp": "0:20:52",
      "end_timestamp": "0:21:42"
    }
  },
  {
    "page_content": "sẽ ra là x bình phương. Rồi cứ như vậy tăng lên x lũy thừ 3 x lũy thừ 4 và cuối cùng là B thì nó sẽ không có đó. Tại vì đạo hàm của cái vế bên trong này theo B thì đạo hàm của B theo B là 1. Còn các cái thành phần còn lại thì nó xem như là hàng số. Và bây giờ chúng ta sẽ cập nhật. Thì cái công thức cập nhật ở đây cũng không có thay đổi nhiều. W đạo hàm bậc hai đạo hàm theo W3 và đạo hàm theo W4. đạo hàm theo W4B. Rồi và khi chúng ta plot thì chúng ta sẽ dùng cái công thức này để chúng ta plot.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 25,
      "start_timestamp": "0:21:38",
      "end_timestamp": "0:22:14"
    }
  },
  {
    "page_content": "ta sẽ dùng cái công thức này để chúng ta plot. Thì đây chính là cái y dự đoán à chúng ta thiếu cho cộng B nữa ha. Đó. Rồi sau đó chúng ta sẽ xem cái mô hình. Ok. Ở đây chúng ta chủ yếu là trực quan hóa. Còn tham số thì chắc chắn là sai rồi. Do đó chúng ta cũng không cần thiết phải hiển thị kiểu này ha. Bây giờ chúng ta sẽ chạy thử cái mô hình khi mà nó bị underfitting. Thì chúng ta thấy là à nó cứ nhảy lên nhảy xuống. Lý do đó là vì cái ờ đường đi đúng không? Một cái cái cái bước nhảy của mình",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 26,
      "start_timestamp": "0:22:09",
      "end_timestamp": "0:22:19"
    }
  },
  {
    "page_content": "đi đúng không? Một cái cái cái bước nhảy của mình nó quá lớn dẫn đến là mô hình của mình nó không hồi tụ được. Đó. Thế thì bây giờ để không có cái hiện tượng này thì có thể chúng ta sẽ phải giảm cái learning rate xuống. Ví dụ như là 0.00 01 rồi. Thế thì chúng ta nhìn thấy là một lúc sau thì chúng ta cũng sẽ thấy là cái đường của mình nó sẽ cố gắng nó bám theo các cái điểm. Thì đó là cái hiện tượng overfitting. Đó là cái hiện tượng overfitting. Nhưng mà thực tế thì nó có cái dữ liệu của mình nó",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 27,
      "start_timestamp": null,
      "end_timestamp": null
    }
  },
  {
    "page_content": "mà thực tế thì nó có cái dữ liệu của mình nó đâu có đi theo cái đường cong à nó đâu có đi theo các cái đường cong à bậc hai à xin lộ đâu có đi theo các đường cong bậc 4 rất là lớn như thế này đâu đúng không? Thì rõ ràng là có thể là nó đúng ở một vài điểm ở bên trong nhưng mà khi đi ra ngoài biên thì cái mô hình của mình nó chắc chắn là sẽ bị lệch lạc. Thì đây là cái hiện tượng là underfitting à overfitting.",
    "metadata": {
      "video_url": "https://www.youtube.com/watch?v=_nPettH0cXg",
      "filename": "_nPettH0cXg",
      "title": "[CS114 - Chương 3] Minh họa Feature Engineering (Phần 1)",
      "chunk_id": 28,
      "start_timestamp": null,
      "end_timestamp": null
    }
  }
]