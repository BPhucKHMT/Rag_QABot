"""
File: preprocess.py
Ch·ª©c nƒÉng:
- Duy·ªát qua t·∫•t c·∫£ transcript trong data/<playlist>/transcripts/
- Ph√°t hi·ªán file transcript b·ªã l·ªói (r·ªóng, k√Ω t·ª± l·∫°, hallucination, l·∫∑p, language switching, timestamp)
- S·ª≠a ch√≠nh t·∫£ b·∫±ng Gemini API (free)
- Refetch l·∫°i transcript b·ªã l·ªói b·∫±ng Whisper (th·ª≠ nhi·ªÅu model)
- L∆∞u T·∫§T C·∫¢ file ƒë√£ x·ª≠ l√Ω v√†o data/<playlist>/processed_transcripts/
- GI·ªÆ NGUY√äN file g·ªëc trong transcripts/

C√°ch d√πng:
    python -m data_loader.preprocess                    # Ki·ªÉm tra t·∫•t c·∫£
    python -m data_loader.preprocess --force-refetch    # Refetch file l·ªói
    python -m data_loader.preprocess --playlist <n>  # Ch·ªâ x·ª≠ l√Ω 1 playlist
"""

from __future__ import annotations
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
import json
import argparse
import re
import os
from dotenv import load_dotenv

try:
    import google.generativeai as genai
except ImportError:
    print("‚ö†Ô∏è C·∫ßn c√†i ƒë·∫∑t: pip install google-generativeai")
    genai = None

from .youtube_fetchers import (
    TranscriptWhisperFetcher,
    normalize_whisper_segments,
    segments_to_txt_with_timestamp,
    save_json,
)

# =====================================================================
# ƒê∆∞·ªùng d·∫´n
# =====================================================================
ROOT_DIR = Path(__file__).resolve().parents[1]
DATA_ROOT = ROOT_DIR / "data"

# Load env
load_dotenv()


# =====================================================================
# TranscriptValidator - Ph√°t hi·ªán l·ªói (IMPROVED)
# =====================================================================
class TranscriptValidator:
    """Ph√°t hi·ªán transcript b·ªã l·ªói - Version n√¢ng cao"""

    @staticmethod
    def is_corrupted(text: str) -> Tuple[bool, str]:
        """
        Ki·ªÉm tra transcript c√≥ b·ªã l·ªói kh√¥ng
        Returns: (is_corrupted: bool, reason: str)
        """
        if not text or len(text.strip()) < 50:
            return True, "empty_or_too_short"

        # T·ª∑ l·ªá k√Ω t·ª± l·∫°
        weird_chars = sum(
            1 for c in text if not c.isalnum() and c not in " .,!?-\n'\"()[]:"
        )
        if len(text) > 0 and weird_chars / len(text) > 0.3:
            return True, "too_many_weird_chars"

        # 1. Ph√°t hi·ªán hallucination c·ªßa Whisper
        hallucination_patterns = [
            "thank you for watching",
            "subscribe to my channel",
            "like and subscribe",
            "c·∫£m ∆°n c√°c b·∫°n ƒë√£ xem",
            "ƒëƒÉng k√Ω k√™nh",
            "nh·∫•n like",
            "nh·∫•n subscribe",
            "don't forget to subscribe",
            "thanks for watching",
        ]
        text_lower = text.lower()
        repeated_count = sum(text_lower.count(p) for p in hallucination_patterns)
        if repeated_count > 5:
            return True, "whisper_hallucination"

        # 2. Ph√°t hi·ªán l·∫∑p l·∫°i c√¢u/ƒëo·∫°n (Repetition)
        lines = [line.strip() for line in text.split("\n") if line.strip()]
        if len(lines) > 5:
            # Ki·ªÉm tra c√¢u l·∫∑p li√™n ti·∫øp
            consecutive_repeats = 0
            for i in range(len(lines) - 1):
                # L·∫•y ph·∫ßn text (b·ªè timestamp)
                text1 = re.sub(r"^\d+:\d+:\d+ - \d+:\d+:\d+,\s*", "", lines[i])
                text2 = re.sub(r"^\d+:\d+:\d+ - \d+:\d+:\d+,\s*", "", lines[i + 1])

                if text1 == text2:
                    consecutive_repeats += 1
                    if consecutive_repeats >= 3:  # 3 c√¢u gi·ªëng nhau li√™n ti·∫øp
                        return True, "excessive_line_repetition"
                else:
                    consecutive_repeats = 0

        # 3. Ph√°t hi·ªán l·∫∑p t·ª´ qu√° nhi·ªÅu
        words = text.split()
        if len(words) > 10:
            unique_ratio = len(set(words)) / len(words)
            if unique_ratio < 0.1:  # Qu√° 90% t·ª´ l·∫∑p l·∫°i
                return True, "excessive_word_repetition"

        # 4. Ph√°t hi·ªán language switching (chuy·ªÉn ng√¥n ng·ªØ ƒë·ªôt ng·ªôt)
        # ƒê·∫øm t·ª∑ l·ªá ch·ªØ Latin vs ch·ªØ Vi·ªát
        latin_chars = sum(1 for c in text if c.isalpha() and ord(c) < 128)
        vietnamese_chars = sum(1 for c in text if c.isalpha() and ord(c) >= 128)
        total_chars = latin_chars + vietnamese_chars

        if total_chars > 100:
            # N·∫øu >80% l√† Latin ‚Üí c√≥ th·ªÉ b·ªã d·ªãch sang ti·∫øng Anh
            if latin_chars / total_chars > 0.8:
                # Ki·ªÉm tra c√≥ ph·∫£i ti·∫øng Anh th·∫≠t kh√¥ng (c√≥ nhi·ªÅu t·ª´ ti·∫øng Anh ph·ªï bi·∫øn)
                english_words = [
                    "the",
                    "and",
                    "is",
                    "are",
                    "was",
                    "were",
                    "have",
                    "has",
                    "will",
                    "can",
                ]
                english_count = sum(1 for word in english_words if word in text_lower)
                if english_count >= 5:
                    return True, "language_switched_to_english"

        # 5. Ki·ªÉm tra timestamp kh√¥ng li√™n t·ª•c
        timestamp_pattern = r"(\d+):(\d+):(\d+) - (\d+):(\d+):(\d+)"
        timestamps = re.findall(timestamp_pattern, text)

        if len(timestamps) > 2:
            gaps = []
            for i in range(len(timestamps) - 1):
                # End time c·ªßa d√≤ng hi·ªán t·∫°i
                h1, m1, s1 = (
                    int(timestamps[i][3]),
                    int(timestamps[i][4]),
                    int(timestamps[i][5]),
                )
                end_time1 = h1 * 3600 + m1 * 60 + s1

                # Start time c·ªßa d√≤ng ti·∫øp theo
                h2, m2, s2 = (
                    int(timestamps[i + 1][0]),
                    int(timestamps[i + 1][1]),
                    int(timestamps[i + 1][2]),
                )
                start_time2 = h2 * 3600 + m2 * 60 + s2

                gap = start_time2 - end_time1
                gaps.append(gap)

            # N·∫øu c√≥ kho·∫£ng c√°ch >60s gi·ªØa c√°c d√≤ng ‚Üí c√≥ v·∫•n ƒë·ªÅ
            if any(gap > 60 for gap in gaps):
                return True, "timestamp_discontinuous"

        return False, "ok"


# =====================================================================
# GeminiSpellChecker - S·ª≠a ch√≠nh t·∫£
# =====================================================================
class GeminiSpellChecker:
    """S·ª≠a ch√≠nh t·∫£ b·∫±ng Gemini API"""

    def __init__(self, api_key: str):
        if not genai:
            raise ImportError(
                "C·∫ßn c√†i ƒë·∫∑t google-generativeai: pip install google-generativeai"
            )

        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel("models/gemini-2.5-flash")

    def correct_text(self, text: str, language: str = "vi") -> Optional[str]:
        """S·ª≠a ch√≠nh t·∫£ cho transcript"""
        prompt = f"""B·∫°n l√† chuy√™n gia s·ª≠a l·ªói transcript t·ª´ video b√†i gi·∫£ng v·ªÅ Machine Learning/Deep Learning.

NHI·ªÜM V·ª§: S·ª≠a T·∫§T C·∫¢ l·ªói ch√≠nh t·∫£, l·ªói phi√™n √¢m, l·ªói ng·ªØ ph√°p trong transcript.

L·ªñI TH∆Ø·ªúNG G·∫∂P trong transcript ti·∫øng Vi·ªát t·ª´ Whisper:
- "ng·ª±a c·∫£nh" ‚Üí "ng·ªØ c·∫£nh"
- "t·ªëng t·∫Øc" ‚Üí "t√≥m t·∫Øt"  
- "n·∫ßu v√†o" / "ƒë·ªì v√†o" ‚Üí "ƒë·∫ßu v√†o"
- "theoretically" ‚Üí "terribly" (n·∫øu ng·ªØ c·∫£nh ph√π h·ª£p)
- "Ionate" ‚Üí "RNN"
- "Anand" ‚Üí "RNN" ho·∫∑c t√™n h√†m
- "feedforward", "backward", "bidirectional" - ki·ªÉm tra ch√≠nh t·∫£
- "ph·ªü b·ªùt" ‚Üí "PhoBERT"
- "tr·∫£i thay ·∫©n" ‚Üí "tr·∫°ng th√°i ·∫©n"

Y√äU C·∫¶U QUAN TR·ªåNG:
1. S·ª¨A T·∫§T C·∫¢ l·ªói ch√≠nh t·∫£ ti·∫øng Vi·ªát
2. S·ª¨A c√°c thu·∫≠t ng·ªØ Machine Learning b·ªã sai (RNN, CNN, BERT, transformer, encoder, decoder, LSTM, GRU, etc.)
3. GI·ªÆ NGUY√äN TUY·ªÜT ƒê·ªêI timestamp (H:MM:SS - H:MM:SS) - KH√îNG ƒê∆Ø·ª¢C S·ª¨A
4. GI·ªÆ NGUY√äN c·∫•u tr√∫c c√¢u, ch·ªâ s·ª≠a t·ª´ sai
5. V·ªõi c√°c t·ª´ kh√¥ng ch·∫Øc ch·∫Øn: ∆∞u ti√™n ng·ªØ c·∫£nh Machine Learning/Deep Learning
6. Ng√¥n ng·ªØ: {language}

TRANSCRIPT C·∫¶N S·ª¨A:
{text}

TR·∫¢ V·ªÄ: Ch·ªâ tr·∫£ v·ªÅ transcript ƒë√£ s·ª≠a theo ƒê√öNG format g·ªëc, KH√îNG gi·∫£i th√≠ch, KH√îNG th√™m b·∫•t k·ª≥ text n√†o kh√°c.
"""
        try:
            response = self.model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói Gemini API: {e}")
            return None


# =====================================================================
# TranscriptPreprocessor - Main
# =====================================================================
class TranscriptPreprocessor:
    """Module preprocess ch√≠nh"""

    def __init__(self, gemini_api_key: Optional[str] = None):
        self.validator = TranscriptValidator()

        # Gemini spell checker (optional)
        if gemini_api_key:
            try:
                self.spell_checker = GeminiSpellChecker(gemini_api_key)
                print("‚úÖ Gemini spell checker ƒë√£ s·∫µn s√†ng")
            except Exception as e:
                print(f"‚ö†Ô∏è Kh√¥ng kh·ªüi t·∫°o ƒë∆∞·ª£c Gemini: {e}")
                self.spell_checker = None
        else:
            self.spell_checker = None

        self.whisper_fetcher = None

    def process_all_playlists(self, force_refetch: bool = False):
        """Duy·ªát t·∫•t c·∫£ playlist trong data/"""
        if not DATA_ROOT.exists():
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c data: {DATA_ROOT}")
            return

        for playlist_folder in DATA_ROOT.iterdir():
            if not playlist_folder.is_dir():
                continue
            if playlist_folder.name in ["logs"]:
                continue

            print(f"\n{'=' * 70}")
            print(f"üìÇ Playlist: {playlist_folder.name}")
            print(f"{'=' * 70}")
            self.process_playlist(playlist_folder, force_refetch)

    def process_playlist(self, playlist_folder: Path, force_refetch: bool = False):
        """X·ª≠ l√Ω m·ªôt playlist"""
        transcripts_dir = playlist_folder / "transcripts"
        if not transcripts_dir.exists():
            print(f"‚è≠Ô∏è Kh√¥ng c√≥ th∆∞ m·ª•c transcripts, b·ªè qua")
            return

        # T·∫°o th∆∞ m·ª•c processed_transcripts
        processed_dir = playlist_folder / "processed_transcripts"
        processed_dir.mkdir(exist_ok=True)
        print(f"üìÅ Output folder: {processed_dir.resolve()}\n")

        # Load metadata ƒë·ªÉ l·∫•y video titles v√† index
        metadata_file = playlist_folder / "metadata.json"
        video_info = {}  # {video_id: {"title": ..., "index": ...}}
        if metadata_file.exists():
            try:
                metadata = json.loads(metadata_file.read_text(encoding="utf-8"))
                for idx, video in enumerate(metadata.get("videos", []), start=1):
                    video_info[video["video_id"]] = {
                        "title": video.get("title", "Unknown"),
                        "index": idx,
                    }
            except Exception as e:
                print(f"‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c metadata: {e}")

        audio_dir = playlist_folder / "audio"
        corrupted_log = playlist_folder / "corrupted_transcripts.json"
        corrupted_data = []

        txt_files = list(transcripts_dir.glob("*.txt"))
        if not txt_files:
            print("‚è≠Ô∏è Kh√¥ng c√≥ file transcript n√†o")
            return

        print(f"üìä T√¨m th·∫•y {len(txt_files)} transcript\n")

        processed_count = 0
        skipped_count = 0

        for txt_file in txt_files:
            video_id = txt_file.stem
            info = video_info.get(video_id, {"title": "Unknown", "index": "?"})

            print(f"\n{'‚îÄ' * 70}")
            print(f"üîç Video #{info['index']}: {info['title']}")
            print(f"üìÑ Source: {txt_file.resolve()}")

            # ƒê·ªçc transcript g·ªëc
            try:
                text = txt_file.read_text(encoding="utf-8")
            except Exception as e:
                print(f"‚ùå Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file: {e}")
                skipped_count += 1
                continue

            # skip n·∫øu ƒë√£ qua r·ªìi
            output_file = processed_dir / f"{video_id}.txt"

            if output_file.exists() and not force_refetch:
                print(f"‚è≠Ô∏è ƒê√£ x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥, b·ªè qua")
                skipped_count += 1
                continue

            # 1. Ki·ªÉm tra l·ªói
            is_corrupted, reason = self.validator.is_corrupted(text)

            if is_corrupted:
                print(f"‚ùå File b·ªã l·ªói: {reason}")
                corrupted_data.append(
                    {
                        "video_id": video_id,
                        "video_index": info["index"],
                        "title": info["title"],
                        "reason": reason,
                        "source_path": str(txt_file.relative_to(DATA_ROOT)),
                    }
                )

                if force_refetch:
                    print(f"üîÑ Refetching transcript...")
                    refetched_text = self._refetch_transcript(video_id, audio_dir)

                    if refetched_text:
                        # L∆∞u v√†o processed
                        output_file = processed_dir / f"{video_id}.txt"
                        output_file.write_text(refetched_text, encoding="utf-8")
                        print(f"üíæ Saved: {output_file.resolve()}")
                        processed_count += 1
                    else:
                        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ refetch, b·ªè qua file n√†y")
                        skipped_count += 1
                else:
                    print(f"‚ö†Ô∏è D√πng --force-refetch ƒë·ªÉ t·ª± ƒë·ªông refetch")
                    skipped_count += 1
                continue

            # 2. File OK - S·ª≠a ch√≠nh t·∫£ (n·∫øu c√≥ Gemini)
            print(f"‚úÖ File OK")
            final_text = text

            if self.spell_checker:
                print(f"‚úèÔ∏è ƒêang s·ª≠a ch√≠nh t·∫£ v·ªõi Gemini...")
                corrected = self.spell_checker.correct_text(text)

                if corrected:
                    # Validate l·∫°i sau khi s·ª≠a
                    is_corrupted_after, reason_after = self.validator.is_corrupted(
                        corrected
                    )

                    if is_corrupted_after:
                        print(f"‚ö†Ô∏è Gemini s·ª≠a b·ªã l·ªói ({reason_after}), d√πng b·∫£n g·ªëc")
                        final_text = text
                    else:
                        if corrected != text:
                            print(f"‚úÖ ƒê√£ s·ª≠a ch√≠nh t·∫£")
                        else:
                            print(f"‚è≠Ô∏è Kh√¥ng c·∫ßn s·ª≠a")
                        final_text = corrected
                else:
                    print(f"‚ö†Ô∏è Gemini l·ªói, d√πng b·∫£n g·ªëc")
                    final_text = text
            else:
                print(f"‚è≠Ô∏è B·ªè qua s·ª≠a ch√≠nh t·∫£ (kh√¥ng c√≥ Gemini API key)")

            # 3. L∆∞u v√†o processed_transcripts
            output_file = processed_dir / f"{video_id}.txt"
            output_file.write_text(final_text, encoding="utf-8")
            print(f"üíæ Saved: {output_file.resolve()}")
            processed_count += 1

        # L∆∞u log file l·ªói
        if corrupted_data:
            save_json({"corrupted_files": corrupted_data}, corrupted_log)
            print(f"\nüìã ƒê√£ l∆∞u danh s√°ch file l·ªói: {corrupted_log.resolve()}")
            print(f"   T·ªïng file l·ªói: {len(corrupted_data)}")

        # Summary
        print(f"\n{'=' * 70}")
        print(f"üìä SUMMARY:")
        print(f"   ‚úÖ Processed: {processed_count}")
        print(f"   ‚è≠Ô∏è Skipped: {skipped_count}")
        print(f"   ‚ùå Corrupted: {len(corrupted_data)}")
        print(f"{'=' * 70}")

    def _refetch_transcript(self, video_id: str, audio_dir: Path) -> Optional[str]:
        """
        Refetch transcript b·∫±ng Whisper
        Th·ª≠ nhi·ªÅu model sizes n·∫øu model hi·ªán t·∫°i th·∫•t b·∫°i
        Returns: transcript text ho·∫∑c None
        """
        # Model sizes ƒë·ªÉ th·ª≠ (t·ª´ nh·ªè ƒë·∫øn l·ªõn)
        model_sizes = ["base", "small", "medium", "large"]

        try:
            # Th·ª≠ t·ª´ng model size
            for model_size in model_sizes:
                print(f"   üîÑ Th·ª≠ Whisper model '{model_size}'...")

                try:
                    # T·∫°o fetcher m·ªõi v·ªõi model size kh√°c
                    fetcher = TranscriptWhisperFetcher(
                        audio_dir=str(audio_dir), model_size=model_size
                    )

                    # Fetch
                    whisper_data = fetcher.fetch_transcript_from(
                        video_id,
                        cleanup=False,  # Kh√¥ng x√≥a audio ƒë·ªÉ th·ª≠ model kh√°c
                        show_segments=False,
                    )

                    if not whisper_data or not whisper_data.get("segments"):
                        print(f"   ‚ö†Ô∏è Model '{model_size}' kh√¥ng tr·∫£ v·ªÅ segments")
                        continue

                    # Normalize
                    segments = normalize_whisper_segments(whisper_data["segments"])
                    txt = segments_to_txt_with_timestamp(segments)

                    # Ki·ªÉm tra k·∫øt qu·∫£ c√≥ b·ªã l·ªói kh√¥ng
                    is_corrupted, reason = self.validator.is_corrupted(txt)

                    if is_corrupted:
                        print(f"   ‚ö†Ô∏è Model '{model_size}' v·∫´n b·ªã l·ªói: {reason}")
                        continue

                    # OK - cleanup audio v√† return
                    print(f"   ‚úÖ Th√†nh c√¥ng v·ªõi model '{model_size}'!")
                    self._cleanup_audio(video_id, audio_dir)
                    return txt

                except Exception as e:
                    print(f"   ‚ùå L·ªói v·ªõi model '{model_size}': {e}")
                    continue

            # N·∫øu t·∫•t c·∫£ model ƒë·ªÅu th·∫•t b·∫°i
            print(f"   ‚ùå T·∫•t c·∫£ Whisper models ƒë·ªÅu th·∫•t b·∫°i")
            self._cleanup_audio(video_id, audio_dir)
            return None

        except Exception as e:
            print(f"   ‚ùå L·ªói refetch: {e}")
            self._cleanup_audio(video_id, audio_dir)
            return None

    def _cleanup_audio(self, video_id: str, audio_dir: Path):
        """X√≥a file audio t·∫°m"""
        try:
            audio_file = audio_dir / f"{video_id}.wav"
            if audio_file.exists():
                audio_file.unlink()
                print(f"   üßπ ƒê√£ x√≥a audio t·∫°m")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Kh√¥ng x√≥a ƒë∆∞·ª£c audio: {e}")


# =====================================================================
# CLI
# =====================================================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Transcript Preprocessor - Version 2.0",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
V√≠ d·ª•:
  # X·ª≠ l√Ω t·∫•t c·∫£ playlist (ch·ªâ validate + s·ª≠a ch√≠nh t·∫£)
  python -m data_loader.preprocess
  
  # X·ª≠ l√Ω t·∫•t c·∫£ + refetch file l·ªói
  python -m data_loader.preprocess --force-refetch
  
  # X·ª≠ l√Ω 1 playlist c·ª• th·ªÉ
  python -m data_loader.preprocess --playlist "cs431-cac-ki-thuat-hoc-sau-va-ung-dung"
        """,
    )

    parser.add_argument(
        "--force-refetch",
        action="store_true",
        help="T·ª± ƒë·ªông refetch transcript b·ªã l·ªói b·∫±ng Whisper",
    )

    parser.add_argument(
        "--playlist", type=str, help="Ch·ªâ x·ª≠ l√Ω m·ªôt playlist c·ª• th·ªÉ (folder name)"
    )

    parser.add_argument(
        "--api-key",
        type=str,
        help="Gemini API key (ho·∫∑c set GEMINI_API_KEY trong .env)",
    )

    args = parser.parse_args()

    # Load API key
    gemini_key = args.api_key or os.getenv("GEMINI_API_KEY")
    if not gemini_key:
        print("‚ö†Ô∏è Kh√¥ng c√≥ Gemini API key, s·∫Ω b·ªè qua s·ª≠a ch√≠nh t·∫£")
        print("   Set GEMINI_API_KEY trong .env ho·∫∑c d√πng --api-key\n")

    preprocessor = TranscriptPreprocessor(gemini_api_key=gemini_key)

    if args.playlist:
        # X·ª≠ l√Ω 1 playlist
        playlist_folder = DATA_ROOT / args.playlist
        if not playlist_folder.exists():
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y playlist: {args.playlist}")
        else:
            preprocessor.process_playlist(playlist_folder, args.force_refetch)
    else:
        # X·ª≠ l√Ω t·∫•t c·∫£
        preprocessor.process_all_playlists(args.force_refetch)
