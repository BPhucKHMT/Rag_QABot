"""
File: pipeline.py
Ch·ª©c nƒÉng: Ch·∫°y to√†n b·ªô pipeline t·ª± ƒë·ªông
    1. Crawl transcript t·ª´ YouTube (coordinator.py)
    2. Preprocess transcript (preprocess.py) 
    3. Index v√†o Vector Database (index_data.py)

C√°ch d√πng:
    python pipeline.py                    # Ch·∫°y full pipeline
    python pipeline.py --skip-crawl       # B·ªè qua crawl (n·∫øu ƒë√£ c√≥ transcript)
    python pipeline.py --skip-preprocess  # B·ªè qua preprocess
    python pipeline.py --only-index       # Ch·ªâ index (b·ªè qua 2 b∆∞·ªõc ƒë·∫ßu)
"""

from __future__ import annotations
import argparse
import sys
from pathlib import Path
from dotenv import load_dotenv
import os

# Load environment variables
load_dotenv()

# Import c√°c module c·∫ßn thi·∫øt
try:
    from data_loader.coordinator import ConfigBasedCoordinator
    from data_loader.preprocess import TranscriptPreprocessor
    from vector_store.vectorstore import VectorDB
    from data_loader.file_loader import Loader
except ImportError as e:
    print(f"‚ùå L·ªói import module: {e}")
    print("   ƒê·∫£m b·∫£o b·∫°n ƒëang ch·∫°y t·ª´ root project")
    sys.exit(1)


class DataPipeline:
    """Pipeline t·ª± ƒë·ªông: Crawl ‚Üí Preprocess ‚Üí Index"""
    
    def __init__(self):
        self.gpt_key = os.getenv("myAPIKey")
        self.gemini_key = os.getenv("GEMINI_API_KEY")
        
        if not self.gpt_key:
            print("‚ö†Ô∏è Thi·∫øu myAPIKey trong .env - c·∫ßn cho embedding")
        if not self.gemini_key:
            print("‚ö†Ô∏è Thi·∫øu GEMINI_API_KEY trong .env - s·∫Ω b·ªè qua s·ª≠a ch√≠nh t·∫£")
        
        # Paths
        self.root_data_dir = "data/"
        self.transcript_dir = "processed_transcripts/"
        self.metadata_dir = "metadata.json"
        self.output_dir = "chunks/"
    
    def step1_crawl_transcripts(self):
        """B∆∞·ªõc 1: Crawl transcript t·ª´ YouTube"""
        print("\n" + "=" * 70)
        print("üì• B∆Ø·ªöC 1: CRAWL TRANSCRIPTS T·ª™ YOUTUBE")
        print("=" * 70 + "\n")
        
        try:
            coordinator = ConfigBasedCoordinator()
            coordinator.process_all_enabled_playlists()
            print("\n‚úÖ Ho√†n t·∫•t crawl transcripts")
            return True
        except Exception as e:
            print(f"\n‚ùå L·ªói crawl: {e}")
            return False
    
    def step2_preprocess_transcripts(self, force_refetch: bool = False, playlist: str = None):
        """B∆∞·ªõc 2: Preprocess transcript (validate + s·ª≠a l·ªói)"""
        print("\n" + "=" * 70)
        print("üîß B∆Ø·ªöC 2: PREPROCESS TRANSCRIPTS")
        print("=" * 70 + "\n")
        
        try:
            preprocessor = TranscriptPreprocessor(gemini_api_key=self.gemini_key)
            
            if playlist:
                from pathlib import Path
                playlist_folder = Path("data") / playlist
                if not playlist_folder.exists():
                    print(f"‚ùå Kh√¥ng t√¨m th·∫•y playlist: {playlist}")
                    return False
                preprocessor.process_playlist(playlist_folder, force_refetch)
            else:
                preprocessor.process_all_playlists(force_refetch)
            
            print("\n‚úÖ Ho√†n t·∫•t preprocess")
            return True
        except Exception as e:
            print(f"\n‚ùå L·ªói preprocess: {e}")
            return False
    
    def step3_index_to_vectordb(self):
        """B∆∞·ªõc 3: Index v√†o Vector Database"""
        print("\n" + "=" * 70)
        print("üóÑÔ∏è B∆Ø·ªöC 3: INDEX V√ÄO VECTOR DATABASE")
        print("=" * 70 + "\n")
        
        try:
            if not self.gpt_key:
                print("‚ùå Thi·∫øu myAPIKey - kh√¥ng th·ªÉ t·∫°o embedding")
                return False
            
            vector_db = VectorDB().db
            loader = Loader(
                open_api_key=self.gpt_key,
                vector_db=vector_db
            )
            
            print("üìÇ ƒêang load v√† chunk documents...")
            chunks = loader.load_dir(
                root_data_dir=self.root_data_dir,
                transcript_dir=self.transcript_dir,
                metadata_dir=self.metadata_dir,
                output_dir=self.output_dir,
                workers=2
            )
            
            print(f"üìä ƒê√£ t·∫°o {len(chunks)} chunks")
            
            if chunks:
                print("üíæ ƒêang index v√†o vector database...")
                vector_db.add_documents(chunks)
                print(f"‚úÖ ƒê√£ index {len(chunks)} documents")
                return True
            else:
                print("‚ö†Ô∏è Kh√¥ng c√≥ chunks n√†o ƒë·ªÉ index")
                return False
                
        except Exception as e:
            print(f"\n‚ùå L·ªói indexing: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run_full_pipeline(
        self, 
        skip_crawl: bool = False,
        skip_preprocess: bool = False,
        skip_index: bool = False,
        force_refetch: bool = False,
        playlist: str = None
    ):
        """Ch·∫°y to√†n b·ªô pipeline"""
        print("\n" + "üöÄ" * 35)
        print("üöÄ B·∫ÆT ƒê·∫¶U PIPELINE: CRAWL ‚Üí PREPROCESS ‚Üí INDEX")
        print("üöÄ" * 35 + "\n")
        
        # Step 1: Crawl
        if not skip_crawl:
            success = self.step1_crawl_transcripts()
            if not success:
                print("\n‚ö†Ô∏è Crawl th·∫•t b·∫°i, ti·∫øp t·ª•c v·ªõi data hi·ªán c√≥...")
        else:
            print("\n‚è≠Ô∏è B·ªè qua b∆∞·ªõc crawl")
        
        # Step 2: Preprocess
        if not skip_preprocess:
            success = self.step2_preprocess_transcripts(force_refetch, playlist)
            if not success:
                print("\n‚ùå Preprocess th·∫•t b·∫°i, d·ª´ng pipeline")
                return False
        else:
            print("\n‚è≠Ô∏è B·ªè qua b∆∞·ªõc preprocess")
        
        # Step 3: Index
        if not skip_index:
            success = self.step3_index_to_vectordb()
            if not success:
                print("\n‚ùå Indexing th·∫•t b·∫°i")
                return False
        else:
            print("\n‚è≠Ô∏è B·ªè qua b∆∞·ªõc indexing")
        
        print("\n" + "üéâ" * 35)
        print("üéâ HO√ÄN TH√ÄNH TO√ÄN B·ªò PIPELINE!")
        print("üéâ" * 35 + "\n")
        return True


# =====================================================================
# CLI
# =====================================================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Data Pipeline: Crawl ‚Üí Preprocess ‚Üí Index",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
V√≠ d·ª•:
  # Ch·∫°y full pipeline
  python pipeline.py
  
  # B·ªè qua crawl (n·∫øu ƒë√£ c√≥ transcript)
  python pipeline.py --skip-crawl
  
  # B·ªè qua preprocess
  python pipeline.py --skip-preprocess
  
  # Ch·ªâ ch·∫°y indexing
  python pipeline.py --only-index
  
  # Ch·∫°y preprocess + index cho 1 playlist
  python pipeline.py --skip-crawl --playlist "cs431-cac-ki-thuat-hoc-sau-va-ung-dung"
  
  # Force refetch transcript l·ªói
  python pipeline.py --skip-crawl --force-refetch
        """
    )
    
    parser.add_argument(
        "--skip-crawl",
        action="store_true",
        help="B·ªè qua b∆∞·ªõc crawl transcript"
    )
    
    parser.add_argument(
        "--skip-preprocess",
        action="store_true",
        help="B·ªè qua b∆∞·ªõc preprocess"
    )
    
    parser.add_argument(
        "--skip-index",
        action="store_true",
        help="B·ªè qua b∆∞·ªõc indexing"
    )
    
    parser.add_argument(
        "--only-index",
        action="store_true",
        help="Ch·ªâ ch·∫°y indexing (b·ªè qua crawl + preprocess)"
    )
    
    parser.add_argument(
        "--force-refetch",
        action="store_true",
        help="Force refetch transcript b·ªã l·ªói b·∫±ng Whisper"
    )
    
    parser.add_argument(
        "--playlist",
        type=str,
        help="Ch·ªâ x·ª≠ l√Ω 1 playlist c·ª• th·ªÉ (folder name)"
    )
    
    args = parser.parse_args()
    
    # X·ª≠ l√Ω --only-index
    if args.only_index:
        args.skip_crawl = True
        args.skip_preprocess = True
    
    # Ch·∫°y pipeline
    pipeline = DataPipeline()
    success = pipeline.run_full_pipeline(
        skip_crawl=args.skip_crawl,
        skip_preprocess=args.skip_preprocess,
        skip_index=args.skip_index,
        force_refetch=args.force_refetch,
        playlist=args.playlist
    )
    
    sys.exit(0 if success else 1)